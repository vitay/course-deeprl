<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Deep Reinforcement Learning - 31&nbsp; Monte-Carlo control</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../exercises/9-TD-solution.html" rel="next">
<link href="../exercises/7-Gym-solution.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-title">Monte-Carlo control</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../" class="sidebar-logo-link">
      <img src="../notes/img/tuc-new.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Deep Reinforcement Learning</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Introduction</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.1-Introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.2-Math.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Math basics (optional)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Tabular RL</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.1-Bandits.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.2-MDP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Markov Decision Processes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.3-DP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Dynamic Programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.4-MC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Monte-Carlo (MC) methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.5-TD.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Temporal Difference learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.6-FA.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Function approximation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.7-NN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Model-free RL</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.1-DQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Q-Learning (DQN)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.2-BeyondDQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Beyond DQN</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.3-PG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Policy gradient (PG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.4-A3C.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Advantage actor-critic (A2C, A3C)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.5-DDPG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Deterministic Policy Gradient (DDPG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.6-PPO.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Natural gradients (TRPO, PPO)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.7-SAC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Maximum Entropy RL (SAC)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Model-based RL</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.1-MB.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Model-based RL</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.2-LearnedModels.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Learned world models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.3-AlphaGo.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">AlphaGo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.4-SR.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Successor representations</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">Outlook</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.1-Outlook.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Outlook</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">Exercises</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Content.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">List of exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Installation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Python installation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/1-Python-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction To Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/2-Numpy-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Numpy and Matplotlib</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/3-Sampling-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Sampling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/4-Bandits-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/5-Bandits2-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits - part 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/6-DP-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Dynamic programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/7-Gym-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Gym environments</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/8-MonteCarlo-solution.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Monte-Carlo control</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/9-TD-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Q-learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/10-Eligibilitytraces-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Eligibility traces</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/11-Keras-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Keras tutorial</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/12-DQN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">DQN</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-taxi-environment" id="toc-the-taxi-environment" class="nav-link active" data-scroll-target="#the-taxi-environment">The taxi environment</a></li>
  <li><a href="#random-agent" id="toc-random-agent" class="nav-link" data-scroll-target="#random-agent">Random agent</a></li>
  <li><a href="#on-policy-monte-carlo-control" id="toc-on-policy-monte-carlo-control" class="nav-link" data-scroll-target="#on-policy-monte-carlo-control">On-policy Monte-Carlo control</a></li>
  <li><a href="#experiments" id="toc-experiments" class="nav-link" data-scroll-target="#experiments">Experiments</a>
  <ul class="collapse">
  <li><a href="#early-stopping" id="toc-early-stopping" class="nav-link" data-scroll-target="#early-stopping">Early stopping</a></li>
  <li><a href="#discount-rate" id="toc-discount-rate" class="nav-link" data-scroll-target="#discount-rate">Discount rate</a></li>
  <li><a href="#learning-rate" id="toc-learning-rate" class="nav-link" data-scroll-target="#learning-rate">Learning rate</a></li>
  <li><a href="#exploration-parameter" id="toc-exploration-parameter" class="nav-link" data-scroll-target="#exploration-parameter">Exploration parameter</a></li>
  <li><a href="#exploration-scheduling" id="toc-exploration-scheduling" class="nav-link" data-scroll-target="#exploration-scheduling">Exploration scheduling</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-title">Monte-Carlo control</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>We start by importing gym. The environment we will use is text-based, so there is no need for all the boilerplate of last exercise: we simply pip install gym if we are on Colab.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng()</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> clear_output</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> google.colab</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    IN_COLAB <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span>:</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    IN_COLAB <span class="op">=</span> <span class="va">False</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> IN_COLAB:</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="op">!</span>pip install gym <span class="op">&gt;</span> <span class="op">/</span>dev<span class="op">/</span>null <span class="dv">2</span><span class="op">&gt;&amp;</span><span class="dv">1</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gym</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="the-taxi-environment" class="level2">
<h2 class="anchored" data-anchor-id="the-taxi-environment">The taxi environment</h2>
<p>In this exercise, we are going to apply <strong>on-policy Monte-Carlo control</strong> on the Taxi environment available in gym:</p>
<p><a href="https://gym.openai.com/envs/Taxi-v3/" class="uri">https://gym.openai.com/envs/Taxi-v3/</a></p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gym.envs.toy_text.taxi</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="bu">help</span>(gym.envs.toy_text.taxi)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Help on module gym.envs.toy_text.taxi in gym.envs.toy_text:

NAME
    gym.envs.toy_text.taxi

CLASSES
    gym.envs.toy_text.discrete.DiscreteEnv(gym.core.Env)
        TaxiEnv
    
    class TaxiEnv(gym.envs.toy_text.discrete.DiscreteEnv)
     |  The Taxi Problem
     |  from "Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition"
     |  by Tom Dietterich
     |  
     |  Description:
     |  There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.
     |  
     |  Observations: 
     |  There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations. 
     |  
     |  Passenger locations:
     |  - 0: R(ed)
     |  - 1: G(reen)
     |  - 2: Y(ellow)
     |  - 3: B(lue)
     |  - 4: in taxi
     |  
     |  Destinations:
     |  - 0: R(ed)
     |  - 1: G(reen)
     |  - 2: Y(ellow)
     |  - 3: B(lue)
     |      
     |  Actions:
     |  There are 6 discrete deterministic actions:
     |  - 0: move south
     |  - 1: move north
     |  - 2: move east 
     |  - 3: move west 
     |  - 4: pickup passenger
     |  - 5: dropoff passenger
     |  
     |  Rewards: 
     |  There is a default per-step reward of -1,
     |  except for delivering the passenger, which is +20,
     |  or executing "pickup" and "drop-off" actions illegally, which is -10.
     |  
     |  
     |  Rendering:
     |  - blue: passenger
     |  - magenta: destination
     |  - yellow: empty taxi
     |  - green: full taxi
     |  - other letters (R, G, Y and B): locations for passengers and destinations
     |  
     |  
     |  state space is represented by:
     |      (taxi_row, taxi_col, passenger_location, destination)
     |  
     |  Method resolution order:
     |      TaxiEnv
     |      gym.envs.toy_text.discrete.DiscreteEnv
     |      gym.core.Env
     |      builtins.object
     |  
     |  Methods defined here:
     |  
     |  __init__(self)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |  
     |  decode(self, i)
     |  
     |  encode(self, taxi_row, taxi_col, pass_loc, dest_idx)
     |  
     |  render(self, mode='human')
     |      Renders the environment.
     |      
     |      The set of supported modes varies per environment. (And some
     |      environments do not support rendering at all.) By convention,
     |      if mode is:
     |      
     |      - human: render to the current display or terminal and
     |        return nothing. Usually for human consumption.
     |      - rgb_array: Return an numpy.ndarray with shape (x, y, 3),
     |        representing RGB values for an x-by-y pixel image, suitable
     |        for turning into a video.
     |      - ansi: Return a string (str) or StringIO.StringIO containing a
     |        terminal-style text representation. The text can include newlines
     |        and ANSI escape sequences (e.g. for colors).
     |      
     |      Note:
     |          Make sure that your class's metadata 'render.modes' key includes
     |            the list of supported modes. It's recommended to call super()
     |            in implementations to use the functionality of this method.
     |      
     |      Args:
     |          mode (str): the mode to render with
     |      
     |      Example:
     |      
     |      class MyEnv(Env):
     |          metadata = {'render.modes': ['human', 'rgb_array']}
     |      
     |          def render(self, mode='human'):
     |              if mode == 'rgb_array':
     |                  return np.array(...) # return RGB frame suitable for video
     |              elif mode == 'human':
     |                  ... # pop up a window and render
     |              else:
     |                  super(MyEnv, self).render(mode=mode) # just raise an exception
     |  
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |  
     |  metadata = {'render.modes': ['human', 'ansi']}
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from gym.envs.toy_text.discrete.DiscreteEnv:
     |  
     |  reset(self)
     |      Resets the environment to an initial state and returns an initial
     |      observation.
     |      
     |      Note that this function should not reset the environment's random
     |      number generator(s); random variables in the environment's state should
     |      be sampled independently between multiple calls to `reset()`. In other
     |      words, each call of `reset()` should yield an environment suitable for
     |      a new episode, independent of previous episodes.
     |      
     |      Returns:
     |          observation (object): the initial observation.
     |  
     |  seed(self, seed=None)
     |      Sets the seed for this env's random number generator(s).
     |      
     |      Note:
     |          Some environments use multiple pseudorandom number generators.
     |          We want to capture all such seeds used in order to ensure that
     |          there aren't accidental correlations between multiple generators.
     |      
     |      Returns:
     |          list&lt;bigint&gt;: Returns the list of seeds used in this env's random
     |            number generators. The first value in the list should be the
     |            "main" seed, or the value which a reproducer should pass to
     |            'seed'. Often, the main seed equals the provided 'seed', but
     |            this won't be true if seed=None, for example.
     |  
     |  step(self, a)
     |      Run one timestep of the environment's dynamics. When end of
     |      episode is reached, you are responsible for calling `reset()`
     |      to reset this environment's state.
     |      
     |      Accepts an action and returns a tuple (observation, reward, done, info).
     |      
     |      Args:
     |          action (object): an action provided by the agent
     |      
     |      Returns:
     |          observation (object): agent's observation of the current environment
     |          reward (float) : amount of reward returned after previous action
     |          done (bool): whether the episode has ended, in which case further step() calls will return undefined results
     |          info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from gym.core.Env:
     |  
     |  __enter__(self)
     |      Support with-statement for the environment.
     |  
     |  __exit__(self, *args)
     |      Support with-statement for the environment.
     |  
     |  __str__(self)
     |      Return str(self).
     |  
     |  close(self)
     |      Override close in your subclass to perform any necessary cleanup.
     |      
     |      Environments will automatically close() themselves when
     |      garbage collected or when the program exits.
     |  
     |  ----------------------------------------------------------------------
     |  Readonly properties inherited from gym.core.Env:
     |  
     |  unwrapped
     |      Completely unwrap this env.
     |      
     |      Returns:
     |          gym.Env: The base non-wrapped gym.Env instance
     |  
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from gym.core.Env:
     |  
     |  __dict__
     |      dictionary for instance variables (if defined)
     |  
     |  __weakref__
     |      list of weak references to the object (if defined)
     |  
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from gym.core.Env:
     |  
     |  action_space = None
     |  
     |  observation_space = None
     |  
     |  reward_range = (-inf, inf)
     |  
     |  spec = None

DATA
    MAP = ['+---------+', '|R: | : :G|', '| : | : : |', '| : : : : |', '| ...

FILE
    /usr/lib/python3.8/site-packages/gym/envs/toy_text/taxi.py

</code></pre>
</div>
</div>
<p>Let’s create the environment, initialize it and render the first state:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"Taxi-v3"</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>state <span class="op">=</span> env.reset()</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>env.render()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+---------+
|R: | : :G|
| : | : : |
| : : : : |
| | : | : |
|Y| : |B: |
+---------+
</code></pre>
</div>
</div>
<p>The agent is the yellow square. It can move up, down, left or right if there is no wall (the pipes and dashes). Its goal is to pick clients at the blue location and drop them off at the pink location. These locations are fixed (R, G, B, Y), but which one is the pick-up location and which one is the drop-off destination changes between each episode.</p>
<p><strong>Q:</strong> Re-run the previous cell multiple times to observe the diversity of initial states.</p>
<p>The following cell prints the action space of the environment:</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Action Space"</span>, env.action_space)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of actions"</span>, env.action_space.n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Action Space Discrete(6)
Number of actions 6</code></pre>
</div>
</div>
<p>There are 6 discrete actions: south, north, east, west, pickup, dropoff.</p>
<p>Let’s now look at the observation space (state space):</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"State Space"</span>, env.observation_space)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of states"</span>, env.observation_space.n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>State Space Discrete(500)
Number of states 500</code></pre>
</div>
</div>
<p>There are 500 discrete states. What are they?</p>
<ul>
<li>The taxi can be anywhere in the 5x5 grid, giving 25 different locations.</li>
<li>The passenger can be at any of the four locations R, G, B, Y or in the taxi: 5 values.</li>
<li>The destination can be any of the four locations: 4 values.</li>
</ul>
<p>This gives indeed 25x5x4 = 500 different combinations.</p>
<p>The internal representation of a state is a number between 0 and 499. You can use the <code>encode</code> and <code>decode</code> methods of the environment to relate it to the state variables.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>state <span class="op">=</span> env.encode(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>) <span class="co"># (taxi row, taxi column, passenger index, destination index)</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"State:"</span>, state)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>state <span class="op">=</span> env.decode(<span class="dv">328</span>) </span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"State:"</span>, <span class="bu">list</span>(state))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>State: 224
State: [3, 1, 2, 0]</code></pre>
</div>
</div>
<p>The reward function is simple:</p>
<ul>
<li>r = 20 when delivering the client at the correct location.</li>
<li>r = -10 when picking or dropping a client illegally (picking where there is no client, dropping a client somewhere else, etc)</li>
<li>r = -1 for all other transitions in order to incent the agent to be as fast as possible.</li>
</ul>
<p>The actions pickup and dropoff are very dangerous: take them at the wrong time and your return will be very low. The navigation actions are less critical.</p>
<p>Depending on the initial state, the taxi will need at least 10 steps to deliver the client, so the maximal return you can expect is around 10 (+20 for the success, -1 for all the steps).</p>
<p>The task is episodic: if you have not delivered the client within 200 steps, the episode stops (no particular reward).</p>
</section>
<section id="random-agent" class="level2">
<h2 class="anchored" data-anchor-id="random-agent">Random agent</h2>
<p>Let’s now define a random agent that just samples the action space.</p>
<p><strong>Q:</strong> Modify the random agent of last time, so that the agent performs a fixed number of <strong>episodes</strong>, not steps. Make sure to use the <code>done</code> flag to break the for loop and start in a new state. Optionally render the state of the agent at every step. At the end of each episode, compute its return using <span class="math inline">\gamma = 1.0</span> (i.e.&nbsp;simply sum the obtained rewards) and print the list in the end.</p>
<p><em>Tip:</em> If you render the state at every step, they will be printed one after the other. To have an animation, you can clear the output of the cell using:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> clear_output <span class="co"># already imported</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> episode: </span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    clear_output(wait<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    env.render()</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><em>Tip:</em> The animation may be too fast to visualize anything. To force the framerate to be low enough, you can make Python “sleep” (do nothing) for a few milliseconds after each rendering:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time <span class="co"># already imported</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> episode: </span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    clear_output(wait<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    env.render()</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    time.sleep(<span class="fl">0.1</span>) <span class="co"># sleep for 100 milliseconds</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RandomAgent:</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Random agent exploring uniformly the environment.</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, env):</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co">        :param env: gym-like environment</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.env <span class="op">=</span> env</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> act(<span class="va">self</span>, state):</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        <span class="co">"Returns a random action by sampling the action space."</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.env.action_space.sample()</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update(<span class="va">self</span>, state, action, reward, next_state):</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">"Updates the agent using the transition (s, a, r, s')."</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(<span class="va">self</span>, nb_episodes, render<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">"Runs the agent on the environment for nb_episodes. Returns the list of obtained returns."</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># List of returns</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>        returns <span class="op">=</span> []</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sample nb_episodes episodes</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(nb_episodes):</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Reset</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>            state <span class="op">=</span> <span class="va">self</span>.env.reset()</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>            done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>            rewards <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Sample the episode</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>            <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Render the current state</span></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> render:</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>                    clear_output(wait<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.env.render()</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>                    time.sleep(<span class="fl">0.1</span>)</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Select an action </span></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>                action <span class="op">=</span> agent.act(state)</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Perform the action</span></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>                next_state, reward, done, info <span class="op">=</span> <span class="va">self</span>.env.step(action)</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Store the reward</span></span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>                rewards <span class="op">+=</span> reward</span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Go in the next state</span></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a>                state <span class="op">=</span> next_state</span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Store the return of the episode</span></span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>            returns.append(rewards)</span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> returns</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the environment</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"Taxi-v3"</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the agent</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> RandomAgent(env)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Train for 10 episodes</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>returns <span class="op">=</span> agent.train(<span class="dv">10</span>, render<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>env.close()</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the returns</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> episode, ret <span class="kw">in</span> <span class="bu">enumerate</span>(returns):</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Episode'</span>, episode<span class="op">+</span><span class="dv">1</span>, <span class="st">': return'</span>, ret)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+---------+
|R: | : :G|
| : | : : |
| : : : : |
| | : | : |
|Y| : |B: |
+---------+
  (North)
Episode 1 : return -704.0
Episode 2 : return -583.0
Episode 3 : return -956.0
Episode 4 : return -767.0
Episode 5 : return -776.0
Episode 6 : return -866.0
Episode 7 : return -785.0
Episode 8 : return -767.0
Episode 9 : return -812.0
Episode 10 : return -839.0</code></pre>
</div>
</div>
<p><strong>Q:</strong> What do you think of the returns obtained by the random agent? Conclude on the difficulty of the task.</p>
<p><strong>A:</strong> the optimal returns are around 10, but the obtained returns with a random policy are very negative (-700, mostly due to many illegal pickups or dropoffs). One can expect a very huge variance of the returns, so learning will be slow.</p>
</section>
<section id="on-policy-monte-carlo-control" class="level2">
<h2 class="anchored" data-anchor-id="on-policy-monte-carlo-control">On-policy Monte-Carlo control</h2>
<p>Now let’s apply on-policy MC control on the Taxi environment. As a reminder, here the meta-algorithm:</p>
<ul>
<li><p><strong>while</strong> True:</p>
<ol type="1">
<li><p>Generate an episode <span class="math inline">\tau = (s_0, a_0, r_1, \ldots, s_T)</span> using the current <strong>stochastic</strong> policy <span class="math inline">\pi</span>.</p></li>
<li><p>For each state-action pair <span class="math inline">(s_t, a_t)</span> in the episode, update the estimated Q-value:</p></li>
</ol>
<p><span class="math display">
      Q(s_t, a_t) = Q(s_t, a_t) + \alpha \, (R_t - Q(s_t, a_t))
  </span></p>
<ol start="3" type="1">
<li>For each state <span class="math inline">s_t</span> in the episode, improve the policy (e.g.&nbsp;<span class="math inline">\epsilon</span>-greedy):</li>
</ol>
<p><span class="math display">
      \pi(s_t, a) = \begin{cases}
                      1 - \epsilon \; \text{if} \; a = a^* \\
                      \frac{\epsilon}{|\mathcal{A(s_t)}-1|} \; \text{otherwise.} \\
                    \end{cases}
  </span></p></li>
</ul>
<p>In practice, we will need:</p>
<ul>
<li><p>a <strong>Q-table</strong> storing the estimated Q-value of each state-action pair: its size will be (500, 6).</p></li>
<li><p>an <span class="math inline">\epsilon</span>-greedy action selection to select actions in the current state.</p></li>
<li><p>an learning mechanism allowing to update the Q-value of all state-action pairs encountered in the episode.</p></li>
</ul>
<p><strong>Q:</strong> Create a <code>MonteCarloAgent</code> agent implementing on-policy MC for the Taxi environment. Use <span class="math inline">\gamma = 0.9</span>, <span class="math inline">\epsilon = 0.1</span> and <span class="math inline">\alpha=0.01</span> (pass these parameters to the constructor of the agent and store them). Train the agent for 20000 episodes (yes, 20000… Start with one episode to debug everything and then launch the simulation. It should take around one minute). Save the return of each episode in a list and plot them in the end.</p>
<p>Implementing the action selection should not be a problem, it is the same as for bandits. Little trick (not obligatory): you can implement <span class="math inline">\epsilon</span>-greedy as:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>action <span class="op">=</span> <span class="va">self</span>.Q[state, :].argmax()</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> rng.random() <span class="op">&lt;</span> epsilon:</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    action <span class="op">=</span> <span class="va">self</span>.env.action_space.sample()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is not exactly <span class="math inline">\epsilon</span>-greedy, as <code>env.action_space.sample()</code> may select the greedy action again. In practice it does not matter, it only changes the meaning of <span class="math inline">\epsilon</span>, but the action selection stays similar. It is better to rely on <code>env.action_space.sample()</code> for the exploration, as some Gym problem work better with a normal distribution for the exploration than with uniform (e.g.&nbsp;continuous problems).</p>
<p>Do not select the greedy action with <code>self.Q[state, :].argmax()</code> but <code>rng.random.choice(np.where(self.Q[state, :] == self.Q[state, :].max())[0])</code>: at the beginning of learning, where the Q-values are all 0, you would otherwise always take the first action (south).</p>
<p>The <code>update()</code> method should take a complete episode as argument, using a list of (state, action, reward) transitions. It should be called at the end of an episode only, not after every step.</p>
<p>A bit tricky is the calculation of the returns for each visited state. The naive approach would look like:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="bu">len</span>(episode)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    state, action, reward <span class="op">=</span> episode[t]</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    return_state <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(t, T): <span class="co"># rewards coming after t</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        next_state, next_action, next_reward <span class="op">=</span> episode[k]</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>        return_state <span class="op">+=</span> gamma<span class="op">**</span>k <span class="op">*</span> reward</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.Q[state, action] <span class="op">+=</span> alpha <span class="op">*</span> (return_state <span class="op">-</span> <span class="va">self</span>.Q[state, action])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The double for loop can be computationally expensive for long episodes (complexity T log T). It is much more efficient to iterate <strong>backwards</strong> on the episode, starting from the last transition and iterating until the first one, and using the fact that:</p>
<p><span class="math display">R_{t} = r_{t+1} + \gamma \, R_{t+1}</span></p>
<p>The terminal state <span class="math inline">s_T</span> has a return of 0 by definition. The last transition <span class="math inline">s_{T-1} \rightarrow s_{T}</span> has therefore a return of <span class="math inline">R_{T-1} = r_T</span>. The transition before that has a return of <span class="math inline">R_{T-2} = r_{T-1} + \gamma \, R_{T-1}</span>, and so on. You can then compute the returns of each action taken in the episode (and update its Q-value) in <strong>linear time</strong>.</p>
<p>To iterate backwards over the list of transitions, use the <code>reversed()</code> operator:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">10</span>]</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a <span class="kw">in</span> <span class="bu">reversed</span>(l):</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MonteCarloAgent:</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Online Monte-Carlo agent.</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, env, gamma, epsilon, alpha):</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co">        :param env: gym-like environment</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co">        :param gamma: discount factor</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co">        :param epsilon: exploration parameter</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co">        :param alpha: learning rate</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.env <span class="op">=</span> env</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gamma <span class="op">=</span> gamma</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilon <span class="op">=</span> epsilon</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha <span class="op">=</span> alpha</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Q_table</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q <span class="op">=</span> np.zeros([<span class="va">self</span>.env.observation_space.n, <span class="va">self</span>.env.action_space.n])</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> act(<span class="va">self</span>, state):</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">"Returns an action using epsilon-greedy action selection."</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>        action <span class="op">=</span> rng.choice(np.where(<span class="va">self</span>.Q[state, :] <span class="op">==</span> <span class="va">self</span>.Q[state, :].<span class="bu">max</span>())[<span class="dv">0</span>])</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> rng.random() <span class="op">&lt;</span> <span class="va">self</span>.epsilon:</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> <span class="va">self</span>.env.action_space.sample() </span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> action</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update(<span class="va">self</span>, episode):</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>        <span class="co">"Updates the agent using a complete episode."</span></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Terminal states have a return of 0</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>        return_episode <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Iterate backwards over the episode</span></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> state, action, reward <span class="kw">in</span> <span class="bu">reversed</span>(episode):</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute the return</span></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>            return_episode <span class="op">=</span> reward <span class="op">+</span> <span class="va">self</span>.gamma <span class="op">*</span> return_episode</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update the Q-value</span></span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.Q[state, action] <span class="op">+=</span> <span class="va">self</span>.alpha <span class="op">*</span> (return_episode <span class="op">-</span> <span class="va">self</span>.Q[state, action])</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> return_episode</span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(<span class="va">self</span>, nb_episodes, render<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>        <span class="co">"Runs the agent on the environment for nb_episodes. Returns the list of obtained returns."</span></span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Returns</span></span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>        returns <span class="op">=</span> []</span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fixed number of episodes</span></span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(nb_episodes):</span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Reset</span></span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>            state <span class="op">=</span> <span class="va">self</span>.env.reset()</span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>            done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a>            nb_steps <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Store transitions</span></span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a>            transitions <span class="op">=</span> []</span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Sample the episode</span></span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a>            <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Render the current state</span></span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> render:</span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a>                    clear_output(wait<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.env.render()</span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a>                    time.sleep(<span class="fl">0.1</span>)</span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-73"><a href="#cb20-73" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Select an action </span></span>
<span id="cb20-74"><a href="#cb20-74" aria-hidden="true" tabindex="-1"></a>                action <span class="op">=</span> <span class="va">self</span>.act(state)</span>
<span id="cb20-75"><a href="#cb20-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-76"><a href="#cb20-76" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Perform the action</span></span>
<span id="cb20-77"><a href="#cb20-77" aria-hidden="true" tabindex="-1"></a>                next_state, reward, done, info <span class="op">=</span> <span class="va">self</span>.env.step(action)</span>
<span id="cb20-78"><a href="#cb20-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-79"><a href="#cb20-79" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Store the transition</span></span>
<span id="cb20-80"><a href="#cb20-80" aria-hidden="true" tabindex="-1"></a>                transitions.append([state, action, reward])</span>
<span id="cb20-81"><a href="#cb20-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-82"><a href="#cb20-82" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Go in the next state</span></span>
<span id="cb20-83"><a href="#cb20-83" aria-hidden="true" tabindex="-1"></a>                state <span class="op">=</span> next_state</span>
<span id="cb20-84"><a href="#cb20-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-85"><a href="#cb20-85" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Increment time</span></span>
<span id="cb20-86"><a href="#cb20-86" aria-hidden="true" tabindex="-1"></a>                nb_steps <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb20-87"><a href="#cb20-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-88"><a href="#cb20-88" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update the Monte Carlo agent after the episode is completed</span></span>
<span id="cb20-89"><a href="#cb20-89" aria-hidden="true" tabindex="-1"></a>            return_episode <span class="op">=</span> <span class="va">self</span>.update(transitions)    </span>
<span id="cb20-90"><a href="#cb20-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-91"><a href="#cb20-91" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Store info</span></span>
<span id="cb20-92"><a href="#cb20-92" aria-hidden="true" tabindex="-1"></a>            returns.append(return_episode)</span>
<span id="cb20-93"><a href="#cb20-93" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb20-94"><a href="#cb20-94" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Print info</span></span>
<span id="cb20-95"><a href="#cb20-95" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> episode <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb20-96"><a href="#cb20-96" aria-hidden="true" tabindex="-1"></a>                clear_output(wait<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-97"><a href="#cb20-97" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">'Episode'</span>, episode, <span class="st">':'</span>)</span>
<span id="cb20-98"><a href="#cb20-98" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">' return:'</span>, return_episode)</span>
<span id="cb20-99"><a href="#cb20-99" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">' steps:'</span>, nb_steps)</span>
<span id="cb20-100"><a href="#cb20-100" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb20-101"><a href="#cb20-101" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb20-102"><a href="#cb20-102" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> returns</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>nb_episodes <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the environment</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"Taxi-v3"</span>)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the agent</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> MonteCarloAgent(env, gamma, epsilon, alpha)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the agent </span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>returns <span class="op">=</span> agent.train(nb_episodes)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>env.close()</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>plt.plot(returns)</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Episodes"</span>)</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Returns"</span>)</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Episode 19900 :
 return: -10.071396867876462
 steps: 22</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-MonteCarlo-solution_files/figure-html/cell-11-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As you may observe, the returns have a huge variance due to the exploration, what makes the plot quite ugly and unreadable. The following function allows to smooth the returns using a sliding average over the last <span class="math inline">N</span> epochs:</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> running_average(x, N):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    cumsum <span class="op">=</span> np.cumsum(np.insert(np.array(x), <span class="dv">0</span>, <span class="dv">0</span>)) </span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (cumsum[N:] <span class="op">-</span> cumsum[:<span class="op">-</span>N]) <span class="op">/</span> N</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Q:</strong> Plot the returns as well as their sliding average. Comment on the influence of exploration.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the returns</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>plt.plot(returns)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>plt.plot(running_average(returns, <span class="dv">1000</span>))</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Episodes"</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Returns"</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-MonteCarlo-solution_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>Q:</strong> Extend the agent with a <code>test(self, render)</code> method that performs a single episode on the environment without exploration, optionally renders each state but does <strong>not</strong> learn. You will have to re-train the agent, because the definition of its class has changed. Backup the previous value of <code>epsilon</code> in a temporary variable and reset it at the end of the episode. Have the method return the <strong>undiscounted</strong> sum of rewards in the episode, as well as the number of steps until termination.</p>
<p>After training (you can reduce the number of episodes), first observe a couple of test episodes with rendering on. Is the policy any good?</p>
<p>Perform 1000 test episodes without rendering and report the mean return over these 1000 episodes as the final performance of your agent.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MonteCarloAgentTest (MonteCarloAgent):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Online Monte-Carlo agent with a test method.</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test(<span class="va">self</span>, render<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">"Performs a test episode without exploration."</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>        previous_epsilon <span class="op">=</span> <span class="va">self</span>.epsilon</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilon <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reset</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> <span class="va">self</span>.env.reset()</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>        done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>        nb_steps <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>        rewards<span class="op">=</span> <span class="dv">0</span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sample the episode</span></span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> render:</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>                clear_output(wait<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.env.render()</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>                time.sleep(<span class="fl">0.1</span>)</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> <span class="va">self</span>.act(state)</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>            next_state, reward, done, info <span class="op">=</span> <span class="va">self</span>.env.step(action)</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>            rewards <span class="op">+=</span> reward</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>            state <span class="op">=</span> next_state</span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>            nb_steps <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilon <span class="op">=</span> previous_epsilon</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> rewards, nb_steps</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>nb_episodes <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the environment</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"Taxi-v3"</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the agent</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> MonteCarloAgentTest(env, gamma, epsilon, alpha)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the agent</span></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>returns <span class="op">=</span> agent.train(nb_episodes)</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot training returns</span></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>plt.plot(returns)</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>plt.plot(running_average(returns, <span class="dv">1000</span>))</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Episodes"</span>)</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Returns"</span>)</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the agent for 1000 episodes</span></span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>test_returns <span class="op">=</span> []</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>test_steps <span class="op">=</span> []</span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>    return_episode, nb_steps <span class="op">=</span> agent.test(render<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>    test_returns.append(return_episode)</span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>    test_steps.append(nb_steps)</span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test performance"</span>, np.mean(test_returns))</span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">121</span>)</span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a>plt.hist(test_returns)</span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Returns"</span>)</span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">122</span>)</span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a>plt.hist(test_steps)</span>
<span id="cb26-39"><a href="#cb26-39" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Number of steps"</span>)</span>
<span id="cb26-40"><a href="#cb26-40" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Episode 19900 :
 return: -5.88217735810702
 steps: 16</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-MonteCarlo-solution_files/figure-html/cell-15-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Test performance 6.325</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-MonteCarlo-solution_files/figure-html/cell-15-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize some episodes</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>returns <span class="op">=</span> []</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>length_episode <span class="op">=</span> []</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perform one episode</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>    return_episode, nb_steps <span class="op">=</span> agent.test(render<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Store info</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    returns.append(return_episode)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>    length_episode.append(nb_steps)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Returns:"</span>, returns)</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of steps"</span>, length_episode)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+---------+
|R: | : :G|
| : | : : |
| : : : : |
| | : | : |
|Y| : |B: |
+---------+
  (West)
Returns: [1, 7, 3, 12, 3]
Number of steps [20, 14, 18, 9, 18]</code></pre>
</div>
</div>
<p><strong>A:</strong> the agent successfully delivers the client at the correct location. The path is not always the shortest one (especially in the vast area before G), but that is fine.</p>
</section>
<section id="experiments" class="level2">
<h2 class="anchored" data-anchor-id="experiments">Experiments</h2>
<section id="early-stopping" class="level3">
<h3 class="anchored" data-anchor-id="early-stopping">Early stopping</h3>
<p><strong>Q:</strong> Train the agent for the smallest number of episodes where the returns seem to have stabilized (e.g.&nbsp;2000 episodes). Test the agent. Does it work? Why?</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>nb_episodes <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the environment</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"Taxi-v3"</span>)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the agent</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> MonteCarloAgentTest(env, gamma, epsilon, alpha)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the agent</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>returns <span class="op">=</span> agent.train(nb_episodes)</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot training returns</span></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>plt.plot(returns)</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>plt.plot(running_average(returns, <span class="dv">1000</span>))</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Episodes"</span>)</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Returns"</span>)</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the agent for 1000 episodes</span></span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>test_returns <span class="op">=</span> []</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>test_steps <span class="op">=</span> []</span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>    return_episode, nb_steps <span class="op">=</span> agent.test(render<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a>    test_returns.append(return_episode)</span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a>    test_steps.append(nb_steps)</span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test performance"</span>, np.mean(test_returns))</span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">121</span>)</span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a>plt.hist(test_returns)</span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Returns"</span>)</span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">122</span>)</span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a>plt.hist(test_steps)</span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Number of steps"</span>)</span>
<span id="cb31-40"><a href="#cb31-40" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Episode 1900 :
 return: -11.052893778075973
 steps: 200</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-MonteCarlo-solution_files/figure-html/cell-17-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Test performance -147.357</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-MonteCarlo-solution_files/figure-html/cell-17-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>A:</strong> Although the returns are already between -20 and +10, as after 50000 episodes, the greedy policy is not optimal as some states still have a very bad policy (the agent sometimes goes back and forth between two states) or just a bad one (not optimal path). The negative returns at the end of learning are due to the exploration: the agent performs illegal pickups or dropoffs. Early in learning, this is due to bad estimates of the Q-values. The return of an episode is a bad estimate of the performance: it report both the exploration and the exploitation. Unfortunately, it is the only one we have…</p>
</section>
<section id="discount-rate" class="level3">
<h3 class="anchored" data-anchor-id="discount-rate">Discount rate</h3>
<p><strong>Q:</strong> Change the value of the discount factor <span class="math inline">\gamma</span>. As the task is episodic (maximum 200 steps), try a discount rate of 1. What happens? Conclude.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>nb_episodes <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the environment</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"Taxi-v3"</span>)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the agent</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> MonteCarloAgentTest(env, gamma, epsilon, alpha)</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the agent</span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>returns <span class="op">=</span> agent.train(nb_episodes)</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot training returns</span></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>plt.plot(returns)</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>plt.plot(running_average(returns, <span class="dv">1000</span>))</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Episodes"</span>)</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Returns"</span>)</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the agent for 1000 episodes</span></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>test_returns <span class="op">=</span> []</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>test_steps <span class="op">=</span> []</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>    return_episode, nb_steps <span class="op">=</span> agent.test(render<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>    test_returns.append(return_episode)</span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>    test_steps.append(nb_steps)</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test performance"</span>, np.mean(test_returns))</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">121</span>)</span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>plt.hist(test_returns)</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Returns"</span>)</span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">122</span>)</span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>plt.hist(test_steps)</span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Number of steps"</span>)</span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Episode 19900 :
 return: -13.0
 steps: 25</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-MonteCarlo-solution_files/figure-html/cell-18-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Test performance -64.025</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-MonteCarlo-solution_files/figure-html/cell-18-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>A:</strong> Rather tricky question… With a discount factor of 1, the agent does not converge as fast as with gamma = 0.9. This is due to the <strong>variance</strong> of the returns: imagine your episode is optimal all along, but at the last moment, the agent performs an illegal dropoff action. The undiscounted return of the episode will be negative and <strong>all</strong> actions taken during the episode will be punished, although only the last one is responsible for the bad return.</p>
<p>Using a discount factor &lt; 1 allows the first actions to stop caring about the final rewards, as they are discounted by <span class="math inline">\gamma^T</span>, which is very small. Take-home message: even if your task is episodic, use <span class="math inline">\gamma &lt; 1</span>.</p>
</section>
<section id="learning-rate" class="level3">
<h3 class="anchored" data-anchor-id="learning-rate">Learning rate</h3>
<p><strong>Q:</strong> Vary the learning rate <code>alpha</code>. What happens?</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>nb_episodes <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the environment</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"Taxi-v3"</span>)</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the agent</span></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> MonteCarloAgentTest(env, gamma, epsilon, alpha)</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the agent</span></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>returns <span class="op">=</span> agent.train(nb_episodes)</span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot training returns</span></span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a>plt.plot(returns)</span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a>plt.plot(running_average(returns, <span class="dv">1000</span>))</span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Episodes"</span>)</span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Returns"</span>)</span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the agent for 1000 episodes</span></span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a>test_returns <span class="op">=</span> []</span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a>test_steps <span class="op">=</span> []</span>
<span id="cb37-27"><a href="#cb37-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb37-28"><a href="#cb37-28" aria-hidden="true" tabindex="-1"></a>    return_episode, nb_steps <span class="op">=</span> agent.test(render<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb37-29"><a href="#cb37-29" aria-hidden="true" tabindex="-1"></a>    test_returns.append(return_episode)</span>
<span id="cb37-30"><a href="#cb37-30" aria-hidden="true" tabindex="-1"></a>    test_steps.append(nb_steps)</span>
<span id="cb37-31"><a href="#cb37-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test performance"</span>, np.mean(test_returns))</span>
<span id="cb37-32"><a href="#cb37-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-33"><a href="#cb37-33" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb37-34"><a href="#cb37-34" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">121</span>)</span>
<span id="cb37-35"><a href="#cb37-35" aria-hidden="true" tabindex="-1"></a>plt.hist(test_returns)</span>
<span id="cb37-36"><a href="#cb37-36" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Returns"</span>)</span>
<span id="cb37-37"><a href="#cb37-37" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">122</span>)</span>
<span id="cb37-38"><a href="#cb37-38" aria-hidden="true" tabindex="-1"></a>plt.hist(test_steps)</span>
<span id="cb37-39"><a href="#cb37-39" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Number of steps"</span>)</span>
<span id="cb37-40"><a href="#cb37-40" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Episode 19900 :
 return: -3.8232660371605287
 steps: 16</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-MonteCarlo-solution_files/figure-html/cell-19-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Test performance -148.272</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-MonteCarlo-solution_files/figure-html/cell-19-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>A:</strong> if the learning rate is too high (0.5), the network does not converge and becomes unstable, as updates “erases” very quickly the previous values, what is bad given the high variance of the returns. If the learning rate is too low, learning takes forever. Classical machine learning problem… 0.01 works actually quite well.</p>
</section>
<section id="exploration-parameter" class="level3">
<h3 class="anchored" data-anchor-id="exploration-parameter">Exploration parameter</h3>
<p><strong>Q:</strong> Vary the exploration parameter <code>epsilon</code> and observe its impact on learning.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">0.01</span> <span class="co"># or 0.3, etc.</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>nb_episodes <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the environment</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"Taxi-v3"</span>)</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the agent</span></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> MonteCarloAgentTest(env, gamma, epsilon, alpha)</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the agent</span></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>returns <span class="op">=</span> agent.train(nb_episodes)</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot training returns</span></span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>plt.plot(returns)</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>plt.plot(running_average(returns, <span class="dv">1000</span>))</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Episodes"</span>)</span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Returns"</span>)</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the agent for 1000 episodes</span></span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>test_returns <span class="op">=</span> []</span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a>test_steps <span class="op">=</span> []</span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a>    return_episode, nb_steps <span class="op">=</span> agent.test(render<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a>    test_returns.append(return_episode)</span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a>    test_steps.append(nb_steps)</span>
<span id="cb40-31"><a href="#cb40-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test performance"</span>, np.mean(test_returns))</span>
<span id="cb40-32"><a href="#cb40-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-33"><a href="#cb40-33" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb40-34"><a href="#cb40-34" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">121</span>)</span>
<span id="cb40-35"><a href="#cb40-35" aria-hidden="true" tabindex="-1"></a>plt.hist(test_returns)</span>
<span id="cb40-36"><a href="#cb40-36" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Returns"</span>)</span>
<span id="cb40-37"><a href="#cb40-37" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">122</span>)</span>
<span id="cb40-38"><a href="#cb40-38" aria-hidden="true" tabindex="-1"></a>plt.hist(test_steps)</span>
<span id="cb40-39"><a href="#cb40-39" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Number of steps"</span>)</span>
<span id="cb40-40"><a href="#cb40-40" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Episode 19900 :
 return: -0.5856821172999982
 steps: 12</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-MonteCarlo-solution_files/figure-html/cell-20-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Test performance -107.576</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-MonteCarlo-solution_files/figure-html/cell-20-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>A:</strong> The exploration should be at least 0.1 in order to find the optimal policy. 0.2 or 0.3 find a good policy faster, but explore too much at the end of learning.</p>
</section>
<section id="exploration-scheduling" class="level3">
<h3 class="anchored" data-anchor-id="exploration-scheduling">Exploration scheduling</h3>
<p>Even with a good learning rate (0.01) and a discount factor of 0.9, the exploration parameter as a huge impact on the performance: too low and the agent does not find the optimal policy, too high and the agent is inefficient at the end of learning.</p>
<p><strong>Q:</strong> Implement scheduling for epsilon. You can use exponential scheduling as in the bandits exercise:</p>
<p><span class="math display">\epsilon = \epsilon \times (1 - \epsilon_\text{decay})</span></p>
<p>at the end of each episode, with <span class="math inline">\epsilon_\text{decay}</span> being a small decay parameter (<code>1e-5</code> or so).</p>
<p>Find a correct value for <span class="math inline">\epsilon_\text{decay}</span>. Do not hesitate to fine-tune alpha at the same time.</p>
<p><em>Tip:</em> Prepare and visualize the scheduling in a different cell, and use the initial value of <span class="math inline">\epsilon</span> and <span class="math inline">\epsilon_\text{decay}</span> that seem to make sense.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DecayMonteCarloAgent (MonteCarloAgentTest):</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Monte-Carlo agent with decay of the exploration parameter.</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, env, gamma, epsilon, decay_epsilon, alpha):</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a><span class="co">        :param env: gym-like environment</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="co">        :param gamma: discount factor</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a><span class="co">        :param epsilon: exploration parameter</span></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a><span class="co">        :param decay_epsilon: exploration decay parameter</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a><span class="co">        :param alpha: learning rate</span></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decay_epsilon <span class="op">=</span> decay_epsilon</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(env, gamma, epsilon, alpha)</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(<span class="va">self</span>, nb_episodes, render<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>        <span class="co">"Runs the agent on the environment for nb_episodes. Returns the list of obtained returns."</span></span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Returns</span></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>        returns <span class="op">=</span> []</span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fixed number of episodes</span></span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(nb_episodes):</span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Reset</span></span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a>            state <span class="op">=</span> <span class="va">self</span>.env.reset()</span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a>            done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a>            nb_steps <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-33"><a href="#cb43-33" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Store transitions</span></span>
<span id="cb43-34"><a href="#cb43-34" aria-hidden="true" tabindex="-1"></a>            transitions <span class="op">=</span> []</span>
<span id="cb43-35"><a href="#cb43-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-36"><a href="#cb43-36" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Sample the episode</span></span>
<span id="cb43-37"><a href="#cb43-37" aria-hidden="true" tabindex="-1"></a>            <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb43-38"><a href="#cb43-38" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb43-39"><a href="#cb43-39" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Render the current state</span></span>
<span id="cb43-40"><a href="#cb43-40" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> render:</span>
<span id="cb43-41"><a href="#cb43-41" aria-hidden="true" tabindex="-1"></a>                    clear_output(wait<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb43-42"><a href="#cb43-42" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.env.render()</span>
<span id="cb43-43"><a href="#cb43-43" aria-hidden="true" tabindex="-1"></a>                    time.sleep(<span class="fl">0.1</span>)</span>
<span id="cb43-44"><a href="#cb43-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-45"><a href="#cb43-45" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Select an action </span></span>
<span id="cb43-46"><a href="#cb43-46" aria-hidden="true" tabindex="-1"></a>                action <span class="op">=</span> <span class="va">self</span>.act(state)</span>
<span id="cb43-47"><a href="#cb43-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-48"><a href="#cb43-48" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Perform the action</span></span>
<span id="cb43-49"><a href="#cb43-49" aria-hidden="true" tabindex="-1"></a>                next_state, reward, done, info <span class="op">=</span> <span class="va">self</span>.env.step(action)</span>
<span id="cb43-50"><a href="#cb43-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-51"><a href="#cb43-51" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Store the transition</span></span>
<span id="cb43-52"><a href="#cb43-52" aria-hidden="true" tabindex="-1"></a>                transitions.append([state, action, reward])</span>
<span id="cb43-53"><a href="#cb43-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-54"><a href="#cb43-54" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Go in the next state</span></span>
<span id="cb43-55"><a href="#cb43-55" aria-hidden="true" tabindex="-1"></a>                state <span class="op">=</span> next_state</span>
<span id="cb43-56"><a href="#cb43-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-57"><a href="#cb43-57" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Increment time</span></span>
<span id="cb43-58"><a href="#cb43-58" aria-hidden="true" tabindex="-1"></a>                nb_steps <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb43-59"><a href="#cb43-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-60"><a href="#cb43-60" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update the Monte Carlo agent after the episode is completed</span></span>
<span id="cb43-61"><a href="#cb43-61" aria-hidden="true" tabindex="-1"></a>            return_episode <span class="op">=</span> <span class="va">self</span>.update(transitions)   </span>
<span id="cb43-62"><a href="#cb43-62" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb43-63"><a href="#cb43-63" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Decay epsilon</span></span>
<span id="cb43-64"><a href="#cb43-64" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.epsilon <span class="op">=</span> <span class="va">self</span>.epsilon <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.decay_epsilon)</span>
<span id="cb43-65"><a href="#cb43-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-66"><a href="#cb43-66" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Store info</span></span>
<span id="cb43-67"><a href="#cb43-67" aria-hidden="true" tabindex="-1"></a>            returns.append(return_episode)</span>
<span id="cb43-68"><a href="#cb43-68" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb43-69"><a href="#cb43-69" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Print info</span></span>
<span id="cb43-70"><a href="#cb43-70" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> episode <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb43-71"><a href="#cb43-71" aria-hidden="true" tabindex="-1"></a>                clear_output(wait<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb43-72"><a href="#cb43-72" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">'Episode'</span>, episode, <span class="st">':'</span>)</span>
<span id="cb43-73"><a href="#cb43-73" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">' return:'</span>, return_episode)</span>
<span id="cb43-74"><a href="#cb43-74" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">' steps:'</span>, nb_steps)</span>
<span id="cb43-75"><a href="#cb43-75" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">' epsilon:'</span>, <span class="va">self</span>.epsilon)</span>
<span id="cb43-76"><a href="#cb43-76" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb43-77"><a href="#cb43-77" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb43-78"><a href="#cb43-78" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> returns</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">0.4</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>decay_epsilon <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>nb_episodes <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the environment</span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"Taxi-v3"</span>)</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the agent</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> DecayMonteCarloAgent(env, gamma, epsilon, decay_epsilon, alpha)</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the agent</span></span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>returns <span class="op">=</span> agent.train(nb_episodes)</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot training returns</span></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>plt.plot(returns)</span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a>plt.plot(running_average(returns, <span class="dv">1000</span>))</span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Episodes"</span>)</span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Returns"</span>)</span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the agent for 1000 episodes</span></span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a>test_returns <span class="op">=</span> []</span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a>test_steps <span class="op">=</span> []</span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a>    return_episode, nb_steps <span class="op">=</span> agent.test(render<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a>    test_returns.append(return_episode)</span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a>    test_steps.append(nb_steps)</span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test performance"</span>, np.mean(test_returns))</span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">121</span>)</span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a>plt.hist(test_returns)</span>
<span id="cb44-37"><a href="#cb44-37" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Returns"</span>)</span>
<span id="cb44-38"><a href="#cb44-38" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">122</span>)</span>
<span id="cb44-39"><a href="#cb44-39" aria-hidden="true" tabindex="-1"></a>plt.hist(test_steps)</span>
<span id="cb44-40"><a href="#cb44-40" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Number of steps"</span>)</span>
<span id="cb44-41"><a href="#cb44-41" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Episode 19900 :
 return: -10.148872536022827
 steps: 25
 epsilon: 0.054667262335293364</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-MonteCarlo-solution_files/figure-html/cell-22-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Test performance 6.8</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="8-MonteCarlo-solution_files/figure-html/cell-22-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize some episodes</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>returns <span class="op">=</span> []</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>length_episode <span class="op">=</span> []</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perform one episode</span></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>    return_episode, nb_steps <span class="op">=</span> agent.test(render<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Store info</span></span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>    returns.append(return_episode)</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>    length_episode.append(nb_steps)</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Returns:"</span>, returns)</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of steps"</span>, length_episode)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+---------+
|R: | : :G|
| : | : : |
| : : : : |
| | : | : |
|Y| : |B: |
+---------+
  (East)
Returns: [5, 4, 6, 1, 2]
Number of steps [16, 17, 15, 20, 19]</code></pre>
</div>
</div>
<p><strong>A:</strong> As seen with bandits, decaying the exploration parameter with the right schedule improves very significantly the speed of learning and the optimality.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../exercises/7-Gym-solution.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Gym environments</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../exercises/9-TD-solution.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">Q-learning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">Copyright 2022, Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de" class="email">julien.vitay@informatik.tu-chemnitz.de</a></div>
  </div>
</footer>



<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>