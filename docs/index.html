<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.550">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Julien Vitay">

<title>Deep Reinforcement Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page"> 
<span class="menu-text">Materials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/vitay/notebooks-deeprl"> 
<span class="menu-text">Notebooks</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/vitay/course-deeprl"> <i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content column-page" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Deep Reinforcement Learning</h1>
</div>


<div class="quarto-title-meta-author column-page">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://julien-vitay.net">Julien Vitay</a> <a href="mailto:julien.vitay@informatik.tu-chemnitz.de" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0001-5229-2349" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://tu-chemnitz.de">
            Chemnitz University of Technology
            </a>
          </p>
      </div>
  </div>

<div class="quarto-title-meta column-page">

      
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    This website contains the materials for the module <strong>Deep Reinforcement Learning</strong> (573140) taught by Dr.&nbsp;Julien Vitay at the Technische Universit√§t Chemnitz.
  </div>
</div>


</header>


<section id="information" class="level2">
<h2 class="anchored" data-anchor-id="information">Information</h2>
<p>Registration on <a href="https://bildungsportal.sachsen.de/opal/auth/RepositoryEntry/21637267457">OPAL</a> to receive updates per email.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Summer semester 2024
</div>
</div>
<div class="callout-body-container callout-body">
<p>No lectures in the summer semester. There will be a (retake) exam in July/August.</p>
</div>
</div>
</section>
<section id="lectures" class="level2">
<h2 class="anchored" data-anchor-id="lectures">Lectures</h2>
<p>You will find below the links to the slides for each lecture (html and pdf). Outdated videos can be found in this <a href="https://www.youtube.com/playlist?list=PLIEjdhhAF7UJwegwyWUcDrUNJTQfxMcUw">playlist</a>.</p>
<section id="introduction" class="level4">
<h4 class="anchored" data-anchor-id="introduction">1 - Introduction</h4>
<div id="455bcede" class="cell" data-execution_count="2">
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="2">
<table class="table table-sm table-striped small">
<colgroup>
<col style="width: 90%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Slides</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>1.1 - Introduction</strong><br>
Introduction to the main concepts of reinforcement learning and showcasing of the current applications.</td>
<td><a href="./slides/1.1-Introduction.html" target="_blank">html</a>, <a href="slides/pdf/1.1-Introduction.pdf" target="_blank">pdf</a></td>
</tr>
<tr class="even">
<td><strong>1.2 - Basics in mathematics</strong> (optional)<br>
This lecture quickly reminds the main mathematical concepts needed to understand this course: linear algebra, calculus, probabilities, statistics and information theory. It is only intended for students who are not sure of the prerequisites.</td>
<td><a href="./slides/1.2-Basics.html" target="_blank">html</a>, <a href="slides/pdf/1.2-Basics.pdf" target="_blank">pdf</a></td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="tabular-rl" class="level4">
<h4 class="anchored" data-anchor-id="tabular-rl">2 - Tabular RL</h4>
<div id="c8acbd8c" class="cell" data-execution_count="3">
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="3">
<table class="table table-sm table-striped small">
<colgroup>
<col style="width: 90%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Slides</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>2.1 - Bandits</strong><br>
n-armed bandits, the simplest RL setting that can be solved by sampling.</td>
<td><a href="./slides/2.1-Bandits.html" target="_blank">html</a>, <a href="slides/pdf/2.1-Bandits.pdf" target="_blank">pdf</a></td>
</tr>
<tr class="even">
<td><strong>2.2 - Markov Decision Processes</strong><br>
MDPs are the basic RL framework. The value functions and the Bellman equations fully characterize a MDP.</td>
<td><a href="./slides/2.2-MDP.html" target="_blank">html</a>, <a href="slides/pdf/2.2-MDP.pdf" target="_blank">pdf</a></td>
</tr>
<tr class="odd">
<td><strong>2.3 - Dynamic Programming</strong><br>
Dynamic programming is a model-based method allowing to iteratively solve the Bellman equations.</td>
<td><a href="./slides/2.3-DP.html" target="_blank">html</a>, <a href="slides/pdf/2.3-DP.pdf" target="_blank">pdf</a></td>
</tr>
<tr class="even">
<td><strong>2.4 - Monte Carlo control</strong><br>
Monte Carlo control estimates value functions through sampling of complete episodes and infers the optimal policy using action selection, either on- or off-policy.</td>
<td><a href="./slides/2.4-MC.html" target="_blank">html</a>, <a href="slides/pdf/2.4-MC.pdf" target="_blank">pdf</a></td>
</tr>
<tr class="odd">
<td><strong>2.5 - Temporal Difference</strong><br>
TD algorithms allow the learning of value functions using single transitions. Q-learning is the famous off-policy variant.</td>
<td><a href="./slides/2.5-TD.html" target="_blank">html</a>, <a href="slides/pdf/2.5-TD.pdf" target="_blank">pdf</a></td>
</tr>
<tr class="even">
<td><strong>2.6 - Function Approximation</strong><br>
The value functions can actually be approximated by any function approximator, allowing to apply RL to continuous state of action spaces.</td>
<td><a href="./slides/2.6-FunctionApproximation.html" target="_blank">html</a>, <a href="slides/pdf/2.6-FunctionApproximation.pdf" target="_blank">pdf</a></td>
</tr>
<tr class="odd">
<td><strong>2.7 - Deep Neural Networks</strong><br>
Quick overview of the main neural network architectures needed for the rest of the course.</td>
<td><a href="./slides/2.7-DeepNetworks.html" target="_blank">html</a>, <a href="slides/pdf/2.7-DeepNetworks.pdf" target="_blank">pdf</a></td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="model-free-rl" class="level4">
<h4 class="anchored" data-anchor-id="model-free-rl">3 - Model-free RL</h4>
<div id="3189062a" class="cell" data-execution_count="4">
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="4">
<table class="table table-sm table-striped small">
<colgroup>
<col style="width: 90%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Slides</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>3.1 - DQN: Deep Q-Network</strong><br>
DQN (Mnih et al.&nbsp;2013) was the first successful application of deep networks to the RL problem. It has been applied to Atari video games and started the interest for deep RL methods.</td>
<td><a href="./slides/3.1-DQN.html" target="_blank">html</a>, <a href="slides/pdf/3.1-DQN.pdf" target="_blank">pdf</a></td>
</tr>
<tr class="even">
<td><strong>3.2 - Beyond DQN</strong><br>
Various extensions to the DQN algorithms have been proposed in the following years: distributional learning, parameter noise, distributed learning or recurrent architectures.</td>
<td><a href="./slides/3.2-BeyondDQN.html" target="_blank">html</a>, <a href="slides/pdf/3.2-BeyondDQN.pdf" target="_blank">pdf</a></td>
</tr>
<tr class="odd">
<td><strong>3.3 - PG: Policy Gradient</strong><br>
Policy gradient methods allow to directly learn the policy without requiring action selection over value functions.</td>
<td><a href="./slides/3.3-PG.html" target="_blank">html</a>, <a href="slides/pdf/3.3-PG.pdf" target="_blank">pdf</a></td>
</tr>
<tr class="even">
<td><strong>3.4 - A3C: Asynchronous Advantage Actor-Critic</strong><br>
A3C (Mnih et al., 2016) is an actor-critic architecture estimating the policy gradient from multiple parallel workers.</td>
<td><a href="./slides/3.4-A3C.html" target="_blank">html</a>, <a href="slides/pdf/3.4-A3C.pdf" target="_blank">pdf</a></td>
</tr>
<tr class="odd">
<td><strong>3.5 - DDPG: Deep Deterministic Policy Gradient</strong><br>
DDPG (Lillicrap et al., is an off-policy actor-critic architecture particularly suited for continuous control problems such as robotics.</td>
<td><a href="./slides/3.5-DDPG.html" target="_blank">html</a>, <a href="slides/pdf/3.5-DDPG.pdf" target="_blank">pdf</a></td>
</tr>
<tr class="even">
<td><strong>3.6 - PPO: Proximal Policy Optimization</strong><br>
PPO (Schulman et al., 2017) allows stable learning by estimating trust regions for the policy updates.</td>
<td><a href="./slides/3.6-PPO.html" target="_blank">html</a>, <a href="slides/pdf/3.6-PPO.pdf" target="_blank">pdf</a></td>
</tr>
<tr class="odd">
<td><strong>3.7 - SAC: Soft Actor-Critic</strong><br>
Maximum Entropy RL modifies the RL objective by learning optimal policies that also explore the environment as much as possible.. SAC (Haarnoja et al., 2018) is an off-policy actor-critic architecture for soft RL.</td>
<td><a href="./slides/3.7-SAC.html" target="_blank">html</a>, <a href="slides/pdf/3.7-SAC.pdf" target="_blank">pdf</a></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>In WS2023-24, the lectures 3.4 to 3.6 were compressed into these slides: <a href="./slides/3.8-A3C-DDPG-PPO.html" target="_blank">html</a>. The lecture 3.7 was skipped.</p>
</div>
</div>
</div>
</section>
<section id="model-based-rl" class="level4">
<h4 class="anchored" data-anchor-id="model-based-rl">4 - Model-based RL</h4>
<div id="fdcd4ab5" class="cell" data-execution_count="5">
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="5">
<table class="table table-sm table-striped small">
<colgroup>
<col style="width: 90%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Slides</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>4.1 - Model-based RL</strong><br>
Two main paradigms in model-based RL: model-based augmentation of model-free learning (Dyna architectures) and planning (model predictive control, MPC)</td>
<td><a href="./slides/4.1-ModelBased.html" target="_blank">html</a>, <a href="slides/pdf/4.1-ModelBased.pdf" target="_blank">pdf</a></td>
</tr>
<tr class="even">
<td><strong>4.2 - Learned World models</strong><br>
Learning a world model from data is much easier than learning the optimal policy, as it is just supervised learning. Modern model-based algorithms (TDM, World models, PlaNet, Dreamer) make use of this property to reduce the sample complexity.</td>
<td><a href="./slides/4.2-LearnedModels.html" target="_blank">html</a>, <a href="slides/pdf/4.2-LearnedModels.pdf" target="_blank">pdf</a></td>
</tr>
<tr class="odd">
<td><strong>4.3 - AlphaGo</strong><br>
AlphaGo surprised the world in 2016 by beating Lee Seedol, the world champion of Go. It combines model-free learning through policy gradient and self-play with model-based planning using MCTS (Monte Carlo Tree Search).</td>
<td><a href="./slides/4.3-AlphaGo.html" target="_blank">html</a>, <a href="slides/pdf/4.3-AlphaGo.pdf" target="_blank">pdf</a></td>
</tr>
<tr class="even">
<td><strong>4.4 - Successor representations</strong><br>
Successor representations provide a trade-off between model-free and model-based learning.</td>
<td><a href="./slides/4.4-SR.html" target="_blank">html</a>, <a href="slides/pdf/4.4-SR.pdf" target="_blank">pdf</a></td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="outlook" class="level4">
<h4 class="anchored" data-anchor-id="outlook">5 - Outlook</h4>
<div id="7b7bbb73" class="cell" data-execution_count="6">
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="6">
<table class="table table-sm table-striped small">
<colgroup>
<col style="width: 90%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Slides</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>5.1 - Outlook</strong><br>
Current RL research investigates many different directions: inverse RL, intrinsic motivation, hierarchical RL, meta RL, offline RL, multi-agent RL, etc.</td>
<td><a href="./slides/5.1-Outlook.html" target="_blank">html</a>, <a href="slides/pdf/5.1-Outlook.pdf" target="_blank">pdf</a></td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
</section>
<section id="exercises" class="level2">
<h2 class="anchored" data-anchor-id="exercises">Exercises</h2>
<p>You will find below links to download the notebooks for the exercises (which you have to fill) and their solution (which you can look at after you have finished the exercise). It is recommended not to look at the solution while doing the exercise unless you are lost. Alternatively, you can run the notebooks directly on Colab (<a href="https://colab.research.google.com/" class="uri">https://colab.research.google.com/</a>) if you have a Google account.</p>
<p>For instructions on how to install a Python distribution on your computer, check this <a href="./webpage/Installation.html">page</a>.</p>
<div id="624d7944" class="cell" data-execution_count="8">
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="8">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th></th>
<th>Notebook</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>1 - Introduction to Python</strong><br>
Introduction to the Python programming language. Optional for students already knowing Python.</td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/1-Python.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/1-Python.ipynb" target="_blank">colab</a></td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/1-Python-solution.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/1-Python-solution.ipynb" target="_blank">colab</a></td>
</tr>
<tr class="even">
<td><strong>2 - Numpy and Matplotlib</strong><br>
Presentation of the numpy library for numerical computations and matplotlib for visualization. Also optional for students already familiar.</td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/2-Numpy.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/2-Numpy.ipynb" target="_blank">colab</a></td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/2-Numpy-solution.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/2-Numpy-solution.ipynb" target="_blank">colab</a></td>
</tr>
<tr class="odd">
<td><strong>3 - Sampling</strong><br>
Simple exercise to investigate random sampling and its properties.</td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/3-Sampling.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/3-Sampling.ipynb" target="_blank">colab</a></td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/3-Sampling-solution.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/3-Sampling-solution.ipynb" target="_blank">colab</a></td>
</tr>
<tr class="even">
<td><strong>4 - Bandits</strong><br>
Implementation of various action selection methods to the n-armed bandit.</td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/4-Bandits.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/4-Bandits.ipynb" target="_blank">colab</a></td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/4-Bandits-solution.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/4-Bandits-solution.ipynb" target="_blank">colab</a></td>
</tr>
<tr class="odd">
<td><strong>5 - Bandits (part 2)</strong><br>
Advanced bandit methods.</td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/5-Bandits2.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/5-Bandits2.ipynb" target="_blank">colab</a></td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/5-Bandits2-solution.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/5-Bandits2-solution.ipynb" target="_blank">colab</a></td>
</tr>
<tr class="even">
<td><strong>6 - Dynamic programming</strong><br>
Calculation of the Bellman equations for the recycling robot and application of policy iteration and value iteration.</td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/6-DP.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/6-DP.ipynb" target="_blank">colab</a></td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/6-DP-solution.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/6-DP-solution.ipynb" target="_blank">colab</a></td>
</tr>
<tr class="odd">
<td><strong>7 - Gym environments</strong><br>
Introdcution to the gym(nasium) RL environments.</td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/7-Gym.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/7-Gym.ipynb" target="_blank">colab</a></td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/7-Gym-solution.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/7-Gym-solution.ipynb" target="_blank">colab</a></td>
</tr>
<tr class="even">
<td><strong>8 - Monte Carlo control</strong><br>
Study of on-policy Monte Carlo control on the Taxi environment.</td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/8-MonteCarlo.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/8-MonteCarlo.ipynb" target="_blank">colab</a></td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/8-MonteCarlo-solution.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/8-MonteCarlo-solution.ipynb" target="_blank">colab</a></td>
</tr>
<tr class="odd">
<td><strong>9 - Temporal Difference, Q-learning</strong><br>
Q-learning on the Taxi environment.</td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/9-TD.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/9-TD.ipynb" target="_blank">colab</a></td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/9-TD-solution.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/9-TD-solution.ipynb" target="_blank">colab</a></td>
</tr>
<tr class="even">
<td><strong>10 - Eligibility traces</strong><br>
Investigation of eligibility traces for Q-learning in a gridworld environment.</td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/10-Eligibilitytraces.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/10-Eligibilitytraces.ipynb" target="_blank">colab</a></td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/10-Eligibilitytraces-solution.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/10-Eligibilitytraces-solution.ipynb" target="_blank">colab</a></td>
</tr>
<tr class="odd">
<td><strong>11 - Keras</strong><br>
Quick tutorial for Keras.</td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/11-Keras.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/11-Keras.ipynb" target="_blank">colab</a></td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/11-Keras-solution.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/11-Keras-solution.ipynb" target="_blank">colab</a></td>
</tr>
<tr class="even">
<td><strong>12 - DQN</strong><br>
Implementation of the DQN algorithm for Cartpole from scratch.</td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/12-DQN.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/12-DQN.ipynb" target="_blank">colab</a></td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/12-DQN-solution.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/12-DQN-solution.ipynb" target="_blank">colab</a></td>
</tr>
<tr class="odd">
<td><strong>13 - PPO</strong><br>
DQN and PPO on cartpole using the tianshou library.</td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/13-PPO.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/13-PPO.ipynb" target="_blank">colab</a></td>
<td><a href="https://raw.githubusercontent.com/vitay/notebooks-deeprl/main/13-PPO-solution.ipynb" target="_blank">ipynb</a>, <a href="https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main/13-PPO-solution.ipynb" target="_blank">colab</a></td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="recommended-readings" class="level2">
<h2 class="anchored" data-anchor-id="recommended-readings">Recommended readings</h2>
<ul>
<li>Richard Sutton and Andrew Barto (2017). Reinforcement Learning: An Introduction. MIT press.</li>
</ul>
<p><a href="http://incompleteideas.net/book/the-book-2nd.html" class="uri">http://incompleteideas.net/book/the-book-2nd.html</a></p>
<ul>
<li>CS294 course of Sergey Levine at Berkeley.</li>
</ul>
<p><a href="http://rll.berkeley.edu/deeprlcourse/" class="uri">http://rll.berkeley.edu/deeprlcourse/</a></p>
<ul>
<li>Reinforcement Learning course by David Silver at UCL.</li>
</ul>
<p><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" class="uri">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html</a></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de" class="email">julien.vitay@informatik.tu-chemnitz.de</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>