<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Deep Reinforcement Learning - Introduction</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/1.2-Math.html" rel="next">
<link href="../index.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notes/1.1-Introduction.html"><strong>Introduction</strong></a></li><li class="breadcrumb-item"><a href="../notes/1.1-Introduction.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../notes/img/tuc-new.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Deep Reinforcement Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text"><strong>Introduction</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.1-Introduction.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.2-Math.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Math basics (optional)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text"><strong>Tabular RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.1-Bandits.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.2-MDP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Markov Decision Processes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.3-DP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Dynamic Programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.4-MC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Monte-Carlo (MC) methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.5-TD.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Temporal Difference learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.6-FA.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Function approximation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.7-NN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text"><strong>Model-free RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.1-DQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Q-Learning (DQN)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.2-BeyondDQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Beyond DQN</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.3-PG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Policy gradient (PG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.4-A3C.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Advantage actor-critic (A2C, A3C)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.5-DDPG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Deterministic Policy Gradient (DDPG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.6-PPO.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Natural gradients (TRPO, PPO)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.7-SAC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Maximum Entropy RL (SAC)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text"><strong>Model-based RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.1-MB.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Model-based RL</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.2-LearnedModels.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Learned world models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.3-AlphaGo.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">AlphaGo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.4-SR.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Successor representations</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text"><strong>Outlook</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.1-Outlook.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Outlook</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text"><strong>Exercises</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Content.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">List of exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Installation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Python installation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/1-Python-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction To Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/2-Numpy-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Numpy and Matplotlib</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/3-Sampling-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Sampling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/4-Bandits-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/5-Bandits2-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits - part 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/6-DP-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Dynamic programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/7-Gym-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Gym environments</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/8-MonteCarlo-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Monte-Carlo control</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/9-TD-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Q-learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/10-Eligibilitytraces-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Eligibility traces</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/11-Keras-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Keras tutorial</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/12-DQN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">DQN</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/13-PPO-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">PPO</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#history-of-rl" id="toc-history-of-rl" class="nav-link active" data-scroll-target="#history-of-rl">History of RL</a></li>
  <li><a href="#the-agent-environment-interface" id="toc-the-agent-environment-interface" class="nav-link" data-scroll-target="#the-agent-environment-interface">The agent-environment interface</a></li>
  <li><a href="#difference-between-supervised-and-reinforcement-learning" id="toc-difference-between-supervised-and-reinforcement-learning" class="nav-link" data-scroll-target="#difference-between-supervised-and-reinforcement-learning">Difference between supervised and reinforcement learning</a></li>
  <li><a href="#applications-of-tabular-rl" id="toc-applications-of-tabular-rl" class="nav-link" data-scroll-target="#applications-of-tabular-rl">Applications of tabular RL</a>
  <ul class="collapse">
  <li><a href="#pendulum" id="toc-pendulum" class="nav-link" data-scroll-target="#pendulum">Pendulum</a></li>
  <li><a href="#cartpole" id="toc-cartpole" class="nav-link" data-scroll-target="#cartpole">Cartpole</a></li>
  <li><a href="#backgammon" id="toc-backgammon" class="nav-link" data-scroll-target="#backgammon">Backgammon</a></li>
  </ul></li>
  <li><a href="#deep-reinforcement-learning-drl" id="toc-deep-reinforcement-learning-drl" class="nav-link" data-scroll-target="#deep-reinforcement-learning-drl">Deep Reinforcement Learning (DRL)</a></li>
  <li><a href="#suggested-readings" id="toc-suggested-readings" class="nav-link" data-scroll-target="#suggested-readings">Suggested readings</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Introduction</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Slides: <a href="../slides/1.1-Introduction.html" target="_blank">html</a> <a href="../slides/pdf/1.1-Introduction.pdf" target="_blank">pdf</a></p>
<p>Deep reinforcement learning (deep RL or DRL) is the integration of deep learning methods, classically used in supervised or unsupervised learning contexts, with reinforcement learning (RL), a well-studied adaptive control method used in problems with delayed and partial feedback.</p>
<section id="history-of-rl" class="level2">
<h2 class="anchored" data-anchor-id="history-of-rl">History of RL</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/m2Y_k8A4iHU" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<ul>
<li><p><strong>Early 20th century</strong>: animal behavior, psychology, operant conditioning</p>
<ul>
<li>Ivan Pavlov, Edward Thorndike, B.F. Skinner</li>
</ul></li>
<li><p><strong>1950s</strong>: optimal control, Markov Decision Process, dynamic programming</p>
<ul>
<li>Richard Bellman, Ronald Howard</li>
</ul></li>
<li><p><strong>1970s</strong>: trial-and-error learning</p>
<ul>
<li>Marvin Minsky, Harry Klopf, Robert Rescorla, Allan Wagner</li>
</ul></li>
<li><p><strong>1980s</strong>: temporal difference learning, Q-learning</p>
<ul>
<li>Richard Sutton, Andrew Barto, Christopher Watkins, Peter Dayan</li>
</ul></li>
<li><p><strong>2013-now</strong>: deep reinforcement learning</p>
<ul>
<li>Deepmind (Mnih, Silver, Graves…)</li>
<li>OpenAI (Sutskever, Schulman…)</li>
<li>Sergey Levine (Berkeley)</li>
</ul></li>
</ul>
<p>Reinforcement learning comes from animal behavior studies, especially <strong>operant conditioning / instrumental learning</strong>. <strong>Thorndike’s Law of Effect</strong> (1874–1949) suggested that behaviors followed by satisfying consequences tend to be repeated and those that produce unpleasant consequences are less likely to be repeated. Positive reinforcements (<strong>rewards</strong>) or negative reinforcements (<strong>punishments</strong>) can be used to modify behavior (<strong>Skinner’s box, 1936</strong>). This form of learning applies to all animals, including humans:</p>
<ul>
<li><p>Training (animals, children…)</p></li>
<li><p>Addiction, economics, gambling, psychological manipulation…</p></li>
</ul>
<p><strong>Behaviorism:</strong> only behavior matters, not mental states.</p>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/y-g2OmRXb0g" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>The key concept of RL is <strong>trial and error</strong> learning. The agent (rat, robot, algorithm) tries out an <strong>action</strong> and observes the <strong>outcome</strong>.</p>
<ul>
<li>If the outcome is positive (reward), the action is reinforced (more likely to occur again).</li>
<li>If the outcome is negative (punishment), the action will be avoided.</li>
</ul>
<p>After enough interactions, the agent has <strong>learned</strong> which action to perform in a given situation.</p>
<p>RL is merely a formalization of the trial-and-error learning paradigm. The agent has to <strong>explore</strong> its environment via trial-and-error in order to gain knowledge. The biggest issue with this approach is that exploring large action spaces might necessitate a <strong>lot</strong> of trials (<strong>sample complexity</strong>). The modern techniques we will see in this course try to reduce the sample complexity.</p>
</section>
<section id="the-agent-environment-interface" class="level2">
<h2 class="anchored" data-anchor-id="the-agent-environment-interface">The agent-environment interface</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/QjhKJmFV8T4" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/rl-agent.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Agent-environment interface. Source: <span class="citation" data-cites="Sutton2017">(<a href="../references.html#ref-Sutton2017" role="doc-biblioref">Sutton and Barto, 2017</a>)</span>.</figcaption>
</figure>
</div>
<p>The agent and the environment interact at discrete time steps: <span class="math inline">t</span>=0, 1, … The agent observes its state at time t: <span class="math inline">s_t \in \mathcal{S}</span>, produces an action at time t, depending on the available actions in the current state: <span class="math inline">a_t \in \mathcal{A}(s_t)</span> and receives a reward according to this action at time t+1: <span class="math inline">r_{t+1} \in \Re</span>. It then updates its state: <span class="math inline">s_{t+1} \in \mathcal{S}</span>. The behavior of the agent is therefore is a sequence of <strong>state-action-reward-state</strong> <span class="math inline">(s, a, r, s')</span> transitions.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/rl-sequence.jpg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">State-action-reward-state sequences. Source: <span class="citation" data-cites="Sutton2017">(<a href="../references.html#ref-Sutton2017" role="doc-biblioref">Sutton and Barto, 2017</a>)</span>.</figcaption>
</figure>
</div>
<p>Sequences <span class="math inline">\tau = (s_0, a_0, r_1, s_1, a_1, \ldots, s_T)</span> are called <strong>episodes</strong>, <strong>trajectories</strong>, <strong>histories</strong> or <strong>rollouts</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/rl-loop.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Agent-environment interface for video games. Source: David Silver <a href="https://www.davidsilver.uk/teaching/" class="uri">https://www.davidsilver.uk/teaching/</a>.</figcaption>
</figure>
</div>
<p>The state <span class="math inline">s_t</span> can relate to:</p>
<ul>
<li><p>the <strong>environment state</strong>, i.e.&nbsp;all information external to the agent (position of objects, other agents, etc).</p></li>
<li><p>the <strong>internal state</strong>, information about the agent itself (needs, joint positions, etc).</p></li>
</ul>
<p>Generally, the state represents all the information necessary to solve the task. The agent generally has no access to the states directly, but to <strong>observations</strong> <span class="math inline">o_t</span>:</p>
<p><span class="math display">
    o_t = f(s_t)
</span></p>
<p>Example: camera inputs do not contain all the necessary information such as the agent’s position. Imperfect information define <strong>partially observable problems</strong>.</p>
<p>What we search in RL is the optimal <strong>policy</strong>: which action <span class="math inline">a</span> should the agent perform in a state <span class="math inline">s</span>? The policy <span class="math inline">\pi</span> maps states into actions. It is defined as a <strong>probability distribution</strong> over states and actions:</p>
<p><span class="math display">
\begin{align}
    \pi &amp;: \mathcal{S} \times \mathcal{A} \rightarrow P(\mathcal{S})\\
    \\
    &amp; (s, a) \rightarrow \pi(s, a)  = P(a_t = a | s_t = s) \\
\end{align}
</span></p>
<p><span class="math inline">\pi(s, a)</span> is the probability of selecting the action <span class="math inline">a</span> in <span class="math inline">s</span>. We have of course:</p>
<p><span class="math display">\sum_{a \in \mathcal{A}(s)} \pi(s, a) = 1</span></p>
<p>Policies can be <strong>probabilistic</strong> / <strong>stochastic</strong>. <strong>Deterministic policies</strong> select a single action <span class="math inline">a^*</span>in <span class="math inline">s</span>:</p>
<p><span class="math display">\pi(s, a) = \begin{cases} 1 \; \text{if} \; a = a^* \\ 0 \; \text{if} \; a \neq a^* \\ \end{cases}</span></p>
<p>The only teaching signal in RL is the <strong>reward function</strong>. The reward is a scalar value <span class="math inline">r_{t+1}</span> provided to the system after each transition <span class="math inline">(s_t,a_t, s_{t+1})</span>. Rewards can also be probabilistic (casino). The mathematical expectation of these rewards defines the <strong>expected reward</strong> of a transition:</p>
<p><span class="math display">
    r(s, a, s') = \mathbb{E}_t [r_{t+1} | s_t = s, a_t = a, s_{t+1} = s']
</span></p>
<p>Rewards can be:</p>
<ul>
<li><p><strong>dense</strong>: a non-zero value is provided after each time step (easy).</p></li>
<li><p><strong>sparse</strong>: non-zero rewards are given very seldom (difficult).</p></li>
</ul>
<p>The goal of the agent is to find a policy that <strong>maximizes</strong> the sum of future rewards at each timestep. The discounted sum of future rewards is called the <strong>return</strong>:</p>
<p><span class="math display">
    R_t = \sum_{k=0}^\infty \gamma ^k \, r_{t+k+1}
</span></p>
<p>Rewards can be delayed w.r.t to an action: we care about all future rewards to select an action, not only the immediate ones. Example: in chess, the first moves are as important as the last ones in order to win, but they do not receive reward.</p>
<p>The <strong>expected return</strong> in a state <span class="math inline">s</span> is called its <strong>value</strong>:</p>
<p><span class="math display">
    V^\pi(s) = \mathbb{E}_\pi(R_t | s_t = s)
</span></p>
<p>The value of a state defines how good it is to be in that state. If a state has a high value, it means we will be able to collect a lot of rewards <strong>on the long term</strong> and <strong>on average</strong>. Value functions are central to RL: if we know the value of all states, we can infer the policy. The optimal action is the one that leads to the state with the highest value. Most RL methods deal with estimating the value function from experience (trial and error).</p>
<p><strong>Simple maze</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/maze.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Simple maze. Source: David Silver <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" class="uri">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html</a>.</figcaption>
</figure>
</div>
<p>The goal is to find a path from start to goal as fast as possible.</p>
<ul>
<li><p><strong>States</strong>: position in the maze (1, 2, 3…).</p></li>
<li><p><strong>Actions</strong>: up, down, left, right.</p></li>
<li><p><strong>Rewards</strong>: -1 for each step until the exit.</p></li>
</ul>
<p>The value of each state indicates how good it is to be in that state. It can be learned by trial-and-error given a policy.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/maze-value.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Value of each state. Source: David Silver <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" class="uri">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html</a>.</figcaption>
</figure>
</div>
<p>When the value of all states is known, we can infer the optimal policy by choosing actions leading to the states with the highest value.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/maze-policy.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Optimal policy. Source: David Silver <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" class="uri">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html</a>.</figcaption>
</figure>
</div>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>As we will see, the story is actually much more complicated…</p>
</div>
</div>
</div>
</section>
<section id="difference-between-supervised-and-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="difference-between-supervised-and-reinforcement-learning">Difference between supervised and reinforcement learning</h2>
<p><strong>Supervised learning</strong></p>
<ul>
<li><p>Correct input/output samples are provided by a <strong>superviser</strong> (training set).</p></li>
<li><p>Learning is driven by <strong>prediction errors</strong>, the difference between the prediction and the target.</p></li>
<li><p>Feedback is <strong>instantaneous</strong>: the target is immediately known.</p></li>
<li><p><strong>Time</strong> does not matter: training samples are randomly sampled from the training set.</p></li>
</ul>
<p><strong>Reinforcement learning</strong></p>
<ul>
<li><p>Behavior is acquired through <strong>trial and error</strong>, no supervision.</p></li>
<li><p><strong>Reinforcements</strong> (rewards or punishments) change the probability of selecting particular actions.</p></li>
<li><p>Feedback is <strong>delayed</strong>: which action caused the reward? Credit assignment.</p></li>
<li><p><strong>Time</strong> matters: as behavior gets better, the observed data changes.</p></li>
</ul>
</section>
<section id="applications-of-tabular-rl" class="level2">
<h2 class="anchored" data-anchor-id="applications-of-tabular-rl">Applications of tabular RL</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/EGI89ypJiv4" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<section id="pendulum" class="level3">
<h3 class="anchored" data-anchor-id="pendulum">Pendulum</h3>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/v6IEpH4vYq0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
</section>
<section id="cartpole" class="level3">
<h3 class="anchored" data-anchor-id="cartpole">Cartpole</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/cartpole-before.gif" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption class="figure-caption">Cartpole before training. Source: <a href="https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288" class="uri">https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288</a>.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/cartpole-after.gif" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption class="figure-caption">Cartpole after training. Source: <a href="https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288" class="uri">https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288</a>.</figcaption>
</figure>
</div>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/XiigTGKZfks" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
</section>
<section id="backgammon" class="level3">
<h3 class="anchored" data-anchor-id="backgammon">Backgammon</h3>
<p>TD-Gammon <span class="citation" data-cites="Tesauro1995">(<a href="../references.html#ref-Tesauro1995" role="doc-biblioref">Tesauro, 1995</a>)</span> was one of the first AI to beat human experts at a complex game, Backgammon.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/tdgammon.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="figure-caption">Backgammon. Source: <span class="citation" data-cites="Tesauro1995">(<a href="../references.html#ref-Tesauro1995" role="doc-biblioref">Tesauro, 1995</a>)</span>.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/tdgammon2.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption class="figure-caption">TD-Gammon. Source: <span class="citation" data-cites="Tesauro1995">(<a href="../references.html#ref-Tesauro1995" role="doc-biblioref">Tesauro, 1995</a>)</span>.</figcaption>
</figure>
</div>
</section>
</section>
<section id="deep-reinforcement-learning-drl" class="level2">
<h2 class="anchored" data-anchor-id="deep-reinforcement-learning-drl">Deep Reinforcement Learning (DRL)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/deeprl.jpg" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">In deep RL, the policy is approximated by a deep neural network.</figcaption>
</figure>
</div>
<p>Classical tabular RL was limited to toy problems, with few states and actions. It is only when coupled with <strong>deep neural networks</strong> that interesting applications of RL became possible. Deepmind (now Google) started the deep RL hype in 2013 by learning to solve 50+ Atari games with a CNN, the <strong>deep Q-network</strong> (DQN) <span class="citation" data-cites="Mnih2013">(<a href="../references.html#ref-Mnih2013" role="doc-biblioref">Mnih et al., 2013</a>)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/discrete.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Architecture of the deep Q-network. Source: <span class="citation" data-cites="Mnih2013">(<a href="../references.html#ref-Mnih2013" role="doc-biblioref">Mnih et al., 2013</a>)</span>.</figcaption>
</figure>
</div>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/rQIShnTz1kU" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>Deep RL methods we since then improved and applied to a variety of control tasks, including simulated cars:</p>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/0xo1Ldx3L5Q" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>or Parkour:</p>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/faDKMMwOS2Q" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>One very famous success of deep RL is when <strong>AlphaGo</strong> managed to beat Lee Sedol at the ancient game of Go:</p>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/8tq1C8spV_g" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>DeepRL has since been applied to real-world robotics:</p>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/l8zKZLqkfII" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/jwSbzNHGflM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>or even autonomous driving (<a href="https://wayve.ai/blog/learning-to-drive-in-a-day-with-reinforcement-learning" class="uri">https://wayve.ai/blog/learning-to-drive-in-a-day-with-reinforcement-learning</a>):</p>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/eRwTbRtnT1I" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>It is also used for more complex video games, such as <strong>DotA II</strong>:</p>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/eHipy_j29Xw" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>or Starcraft II (AlphaStar, <a href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii" class="uri">https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii</a>)</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/alphastar.gif" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Deep RL is gaining a lot of importance in AI research, with lots of applications in control: video games, robotics, industrial applications… It may be AI’s best shot at producing intelligent behavior, as it does not rely on annotated data. A lot of problems have to be solved before becoming as mainstream as deep learning.</p>
<ul>
<li>Sample complexity is often prohibitive.</li>
<li>Energy consumption and computing power simply crazy (AlphaGo: 1 MW, Dota2: 800 petaflop/s-days)</li>
<li>The correct reward function is hard to design, ethical aspects. (<em>inverse RL</em>)</li>
<li>Hard to incorporate expert knowledge. (<em>model-based RL</em>)</li>
<li>Learns single tasks, does not generalize (<em>hierarchical RL</em>, <em>meta-learning</em>)</li>
</ul>
</section>
<section id="suggested-readings" class="level2">
<h2 class="anchored" data-anchor-id="suggested-readings">Suggested readings</h2>
<ul>
<li>Sutton and Barto (1998, 2017). Reinforcement Learning: An Introduction. MIT Press.</li>
</ul>
<p><a href="http://incompleteideas.net/sutton/book/the-book.html" class="uri">http://incompleteideas.net/sutton/book/the-book.html</a></p>
<ul>
<li>Szepesvari (2010). Algorithms for Reinforcement Learning. Morgan and Claypool.</li>
</ul>
<p><a href="http://www.ualberta.ca/∼szepesva/papers/RLAlgsInMDPs.pdf" class="uri">http://www.ualberta.ca/∼szepesva/papers/RLAlgsInMDPs.pdf</a></p>
<ul>
<li>CS294 course of Sergey Levine at Berkeley.</li>
</ul>
<p><a href="http://rll.berkeley.edu/deeprlcourse/" class="uri">http://rll.berkeley.edu/deeprlcourse/</a></p>
<ul>
<li>Reinforcement Learning course by David Silver at UCL.</li>
</ul>
<p><a href="https://www.davidsilver.uk/teaching/" class="uri">https://www.davidsilver.uk/teaching/</a></p>


<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-Mnih2013" class="csl-entry" role="listitem">
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., et al. (2013). Playing <span>Atari</span> with <span>Deep Reinforcement Learning</span>. <a href="http://arxiv.org/abs/1312.5602">http://arxiv.org/abs/1312.5602</a>.
</div>
<div id="ref-Sutton2017" class="csl-entry" role="listitem">
Sutton, R. S., and Barto, A. G. (2017). <em>Reinforcement <span>Learning</span>: <span>An Introduction</span></em>. 2nd ed. <span>Cambridge, MA</span>: <span>MIT Press</span> <a href="http://incompleteideas.net/book/the-book-2nd.html">http://incompleteideas.net/book/the-book-2nd.html</a>.
</div>
<div id="ref-Tesauro1995" class="csl-entry" role="listitem">
Tesauro, G. (1995). <span>“<span>TD-Gammon</span>: <span>A Self-Teaching Backgammon Program</span>,”</span> in <em>Applications of <span>Neural Networks</span></em>, ed. A. F. Murray (<span>Boston, MA</span>: <span>Springer US</span>), 267–285. doi:<a href="https://doi.org/10.1007/978-1-4757-2379-3_11">10.1007/978-1-4757-2379-3_11</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../index.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Overview</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/1.2-Math.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">Math basics (optional)</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">Copyright Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de" class="email">julien.vitay@informatik.tu-chemnitz.de</a></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>