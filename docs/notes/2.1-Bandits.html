<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Deep Reinforcement Learning - Bandits</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/2.2-MDP.html" rel="next">
<link href="../notes/1.2-Math.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notes/2.1-Bandits.html"><strong>Tabular RL</strong></a></li><li class="breadcrumb-item"><a href="../notes/2.1-Bandits.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bandits</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../notes/img/tuc-new.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Deep Reinforcement Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text"><strong>Introduction</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.1-Introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.2-Math.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Math basics (optional)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text"><strong>Tabular RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.1-Bandits.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.2-MDP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Markov Decision Processes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.3-DP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Dynamic Programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.4-MC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Monte-Carlo (MC) methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.5-TD.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Temporal Difference learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.6-FA.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Function approximation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.7-NN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text"><strong>Model-free RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.1-DQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Q-Learning (DQN)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.2-BeyondDQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Beyond DQN</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.3-PG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Policy gradient (PG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.4-A3C.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Advantage actor-critic (A2C, A3C)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.5-DDPG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Deterministic Policy Gradient (DDPG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.6-PPO.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Natural gradients (TRPO, PPO)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.7-SAC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Maximum Entropy RL (SAC)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text"><strong>Model-based RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.1-MB.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Model-based RL</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.2-LearnedModels.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Learned world models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.3-AlphaGo.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">AlphaGo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.4-SR.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Successor representations</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text"><strong>Outlook</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.1-Outlook.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Outlook</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text"><strong>Exercises</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Content.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">List of exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Installation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Python installation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/1-Python-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction To Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/2-Numpy-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Numpy and Matplotlib</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/3-Sampling-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Sampling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/4-Bandits-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/5-Bandits2-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits - part 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/6-DP-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Dynamic programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/7-Gym-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Gym environments</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/8-MonteCarlo-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Monte-Carlo control</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/9-TD-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Q-learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/10-Eligibilitytraces-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Eligibility traces</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/11-Keras-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Keras tutorial</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/12-DQN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">DQN</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/13-PPO-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">PPO</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#n-armed-bandits" id="toc-n-armed-bandits" class="nav-link active" data-scroll-target="#n-armed-bandits">n-armed bandits</a></li>
  <li><a href="#sampling-based-evaluation" id="toc-sampling-based-evaluation" class="nav-link" data-scroll-target="#sampling-based-evaluation">Sampling-based evaluation</a></li>
  <li><a href="#action-selection" id="toc-action-selection" class="nav-link" data-scroll-target="#action-selection">Action selection</a>
  <ul class="collapse">
  <li><a href="#greedy-action-selection" id="toc-greedy-action-selection" class="nav-link" data-scroll-target="#greedy-action-selection">Greedy action selection</a></li>
  <li><a href="#exploration-exploitation-dilemma" id="toc-exploration-exploitation-dilemma" class="nav-link" data-scroll-target="#exploration-exploitation-dilemma">Exploration-exploitation dilemma</a></li>
  <li><a href="#epsilon-greedy-action-selection" id="toc-epsilon-greedy-action-selection" class="nav-link" data-scroll-target="#epsilon-greedy-action-selection"><span class="math inline">\epsilon</span>-greedy action selection</a></li>
  <li><a href="#softmax-action-selection" id="toc-softmax-action-selection" class="nav-link" data-scroll-target="#softmax-action-selection">Softmax action selection</a></li>
  <li><a href="#example-of-action-selection-for-the-10-armed-bandit" id="toc-example-of-action-selection-for-the-10-armed-bandit" class="nav-link" data-scroll-target="#example-of-action-selection-for-the-10-armed-bandit">Example of action selection for the 10-armed bandit</a></li>
  <li><a href="#exploration-schedule" id="toc-exploration-schedule" class="nav-link" data-scroll-target="#exploration-schedule">Exploration schedule</a></li>
  <li><a href="#optimistic-initial-values" id="toc-optimistic-initial-values" class="nav-link" data-scroll-target="#optimistic-initial-values">Optimistic initial values</a></li>
  <li><a href="#reinforcement-comparison" id="toc-reinforcement-comparison" class="nav-link" data-scroll-target="#reinforcement-comparison">Reinforcement comparison</a></li>
  <li><a href="#gradient-bandit-algorithm" id="toc-gradient-bandit-algorithm" class="nav-link" data-scroll-target="#gradient-bandit-algorithm">Gradient bandit algorithm</a></li>
  <li><a href="#upper-confidence-bound-action-selection" id="toc-upper-confidence-bound-action-selection" class="nav-link" data-scroll-target="#upper-confidence-bound-action-selection">Upper-Confidence-Bound action selection</a></li>
  <li><a href="#summary-of-evaluative-feedback-methods" id="toc-summary-of-evaluative-feedback-methods" class="nav-link" data-scroll-target="#summary-of-evaluative-feedback-methods">Summary of evaluative feedback methods</a></li>
  </ul></li>
  <li><a href="#contextual-bandits" id="toc-contextual-bandits" class="nav-link" data-scroll-target="#contextual-bandits">Contextual bandits</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Bandits</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Slides: <a href="../slides/2.1-Bandits.html" target="_blank">html</a> <a href="../slides/pdf/2.1-Bandits.pdf" target="_blank">pdf</a></p>
<section id="n-armed-bandits" class="level2">
<h2 class="anchored" data-anchor-id="n-armed-bandits">n-armed bandits</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/PVKWm3rrIOc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>RL evaluates actions through <strong>trial-and-error</strong> rather than comparing its predictions to the correct actions. This is called <strong>evaluative feedback</strong>, as the feedback depends completely on the action taken. Oppositely, supervised learning is a form of <strong>instructive feedback</strong>, as the targets (labels / ground truth) do not depend at all on the prediction.</p>
<p>Evaluative feedback indicates how good the action is, but not whether it is the best or worst action possible. Two forms of RL learning can be distinguished:</p>
<ul>
<li><p><strong>Associative learning</strong>: inputs are mapped to the best possible outputs (general RL).</p></li>
<li><p><strong>Non-associative learning</strong>: finds one best output, regardless of the current state or past history (bandits).</p></li>
</ul>
<p>The <strong>n-armed bandit</strong> (or multi-armed bandit) is a non-associative evaluative feedback procedure. Learning and action selection take place in the same single state, while the <span class="math inline">n</span> actions have different reward distributions. The goal is to find out through trial and error which action provides the most reward on average.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-example.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">10-armed bandit with the true value of each action.</figcaption>
</figure>
</div>
<p>We have the choice between <span class="math inline">N</span> different actions <span class="math inline">(a_1, ..., a_N)</span>. Each action <span class="math inline">a</span> taken at time <span class="math inline">t</span> provides a <strong>reward</strong> <span class="math inline">r_t</span> drawn from the action-specific probability distribution <span class="math inline">r(a)</span>. The mathematical expectation of that distribution is the <strong>expected reward</strong>, called the <strong>true value</strong> of the action <span class="math inline">Q^*(a)</span>.</p>
<p><span class="math display">
    Q^*(a) = \mathbb{E} [r(a)]
</span></p>
<p>The reward distribution also has a <strong>variance</strong>: we usually ignore it in RL, as all we care about is the <strong>optimal action</strong> (but see distributional RL later).</p>
<p><span class="math display">a^* = \text{argmax}_a Q^*(a)</span></p>
<p>If we take the optimal action an infinity of times, we maximize the reward intake <strong>on average</strong>. The question is how to find out the optimal action through <strong>trial and error</strong>, i.e.&nbsp;without knowing the exact reward distribution <span class="math inline">r(a)</span>.</p>
<p>We only have access to <strong>samples</strong> of <span class="math inline">r(a)</span> by taking the action <span class="math inline">a</span> at time <span class="math inline">t</span> (a <strong>trial</strong>, <strong>play</strong> or <strong>step</strong>).</p>
<p><span class="math display">r_t \sim r(a)</span></p>
<p>The received rewards <span class="math inline">r_t</span> vary around the true value over time. We need to build <strong>estimates</strong> <span class="math inline">Q_t(a)</span> of the value of each action based on the samples. These estimates will be very wrong at the beginning, but should get better over time.</p>
</section>
<section id="sampling-based-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="sampling-based-evaluation">Sampling-based evaluation</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/mHyySub8wVc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>The expectation of the reward distribution can be approximated by the <strong>mean</strong> of its samples (sampling average):</p>
<p><span class="math display">
    \mathbb{E} [r(a)] \approx  \frac{1}{N} \sum_{t=1}^N r_t |_{a_t = a}
</span></p>
<p>Suppose that the action <span class="math inline">a</span> had been selected <span class="math inline">t</span> times, producing rewards</p>
<p><span class="math display">
    (r_1, r_2, ..., r_t)
</span></p>
<p>The estimated value of action <span class="math inline">a</span> at play <span class="math inline">t</span> is then:</p>
<p><span class="math display">
    Q_t (a) = \frac{r_1 + r_2 + ... + r_t }{t}
</span></p>
<p>Over time, the estimated action-value converges to the true action-value:</p>
<p><span class="math display">
   \lim_{t \to \infty} Q_t (a) = Q^* (a)
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-samples2.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">The sampling average can correctly approximate the true value of an action given enough samples.</figcaption>
</figure>
</div>
<p>The drawback of maintaining the mean of the received rewards is that it consumes a lot of memory:</p>
<p><span class="math display">
    Q_t (a) = \frac{r_1 + r_2 + ... + r_t }{t} = \frac{1}{t} \, \sum_{i=1}^{t} r_i
</span></p>
<p>It is possible to update an estimate of the mean in an <strong>online</strong> or incremental manner:</p>
<p><span class="math display">
\begin{aligned}
    Q_{t+1}(a) &amp;= \frac{1}{t+1} \, \sum_{i=1}^{t+1} r_i \\
            &amp;= \frac{1}{t+1} \, (r_{t+1} + \sum_{i=1}^{t} r_i )\\
            &amp;= \frac{1}{t+1} \, (r_{t+1} + t \,  Q_{t}(a) ) \\
            &amp;= \frac{1}{t+1} \, (r_{t+1} + (t + 1) \,  Q_{t}(a) - Q_t(a))
\end{aligned}
</span></p>
<p>The estimate at time <span class="math inline">t+1</span> depends on the previous estimate at time <span class="math inline">t</span> and the last reward <span class="math inline">r_{t+1}</span>:</p>
<p><span class="math display">
    Q_{t+1}(a) = Q_t(a) + \frac{1}{t+1} \, (r_{t+1} - Q_t(a))
</span></p>
<p>The problem with the exact mean is that it is only exact when the reward distribution is <strong>stationary</strong>, i.e.&nbsp;when the probability distribution does not change over time. If the reward distribution is <strong>non-stationary</strong>, the <span class="math inline">\frac{1}{t+1}</span> term will become very small and prevent rapid updates of the mean.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-nonstationary1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">The sampling average have problems when the reward distribution is non-stationary.</figcaption>
</figure>
</div>
<p>The solution is to replace <span class="math inline">\frac{1}{t+1}</span> with a fixed parameter called the <strong>learning rate</strong> (or <strong>step size</strong>) <span class="math inline">\alpha</span>:</p>
<p><span class="math display">
\begin{aligned}
    Q_{t+1}(a) &amp; = Q_t(a) + \alpha \, (r_{t+1} - Q_t(a)) \\
                &amp; \\
                &amp; = (1 - \alpha) \, Q_t(a) + \alpha \, r_{t+1}
\end{aligned}
</span></p>
<p>The computed value is called a <strong>moving average</strong> (or sliding average), as if one used only a small window of the past history.</p>
<p><span class="math display">
    Q_{t+1}(a) = Q_t(a) + \alpha \, (r_{t+1} - Q_t(a))
</span></p>
<p>or:</p>
<p><span class="math display">
    \Delta Q(a) = \alpha \, (r_{t+1} - Q_t(a))
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-nonstationary2.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">The moving average is much more flexible for non-stationary distributions.</figcaption>
</figure>
</div>
<p>The moving average adapts very fast to changes in the reward distribution and should be used in <strong>non-stationary problems</strong>. It is however not exact and sensible to noise. Choosing the right value for <span class="math inline">\alpha</span> can be difficult.</p>
<p>The form of this <strong>update rule</strong> is very important to remember:</p>
<p><span class="math display">
    \text{new estimate} = \text{current estimate} + \alpha \, (\text{target} - \text{current estimate})
</span></p>
<p>Estimates following this update rule track the mean of their sampled target values. <span class="math inline">\text{target} - \text{current estimate}</span> is the <strong>prediction error</strong> between the target and the estimate.</p>
</section>
<section id="action-selection" class="level2">
<h2 class="anchored" data-anchor-id="action-selection">Action selection</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/yg3yI7YmEa8" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<section id="greedy-action-selection" class="level3">
<h3 class="anchored" data-anchor-id="greedy-action-selection">Greedy action selection</h3>
<p>Let’s suppose we have formed reasonable estimates of the Q-values <span class="math inline">Q_t(a)</span> at time <span class="math inline">t</span>. Which action should we do next? If we select the next action <span class="math inline">a_{t+1}</span> randomly (<strong>random agent</strong>), we do not maximize the rewards we receive, but we can continue learning the Q-values. Choosing the action to perform next is called <strong>action selection</strong> and several schemes are possible.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-estimates-greedy.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Greedy action selection.</figcaption>
</figure>
</div>
<p>The <strong>greedy action</strong> is the action whose estimated value is <strong>maximal</strong> at time <span class="math inline">t</span> based on our current estimates:</p>
<p><span class="math display">
    a^*_t = \text{argmax}_{a} Q_t(a)
</span></p>
<p>If our estimates <span class="math inline">Q_t</span> are correct (i.e.&nbsp;close from <span class="math inline">Q^*</span>), the greedy action is the <strong>optimal action</strong> and we maximize the rewards on average. If our estimates are wrong, the agent will perform <strong>sub-optimally</strong>.</p>
<p>This defines the <strong>greedy policy</strong>, where the probability of taking the greedy action is 1 and the probability of selecting another action is 0:</p>
<p><span class="math display">
    \pi(a) = \begin{cases} 1 \; \text{if} \; a = a_t^* \\ 0 \; \text{otherwise.} \end{cases}
</span></p>
<p>The greedy policy is <strong>deterministic</strong>: the action taken is always the same for a fixed <span class="math inline">Q_t</span>.</p>
</section>
<section id="exploration-exploitation-dilemma" class="level3">
<h3 class="anchored" data-anchor-id="exploration-exploitation-dilemma">Exploration-exploitation dilemma</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-estimates-greedy-problem1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Greedy action selection only works when the estimates are good enough.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-estimates-greedy-problem2.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Estimates are initially bad (e.g.&nbsp;0 here), so an action is sampled randomly and a reward is received.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-estimates-greedy-problem3.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">The Q-value of that action becomes positive, so it becomes the greedy action.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-estimates-greedy-problem4.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Greedy action selection will always select that action, although the second one would have been better.</figcaption>
</figure>
</div>
<p>This <strong>exploration-exploitation</strong> dilemma is the hardest problem in RL:</p>
<ul>
<li><strong>Exploitation</strong> is using the current estimates to select an action: they might be wrong!</li>
<li><strong>Exploration</strong> is selecting non-greedy actions in order to improve their estimates: they might not be optimal!</li>
</ul>
<p>One has to balance exploration and exploitation over the course of learning:</p>
<ul>
<li>More exploration at the beginning of learning, as the estimates are initially wrong.</li>
<li>More exploitation at the end of learning, as the estimates get better.</li>
</ul>
</section>
<section id="epsilon-greedy-action-selection" class="level3">
<h3 class="anchored" data-anchor-id="epsilon-greedy-action-selection"><span class="math inline">\epsilon</span>-greedy action selection</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-estimates-epsilongreedy.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption"><span class="math inline">\epsilon</span>-greedy action selection</figcaption>
</figure>
</div>
<p><strong><span class="math inline">\epsilon</span>-greedy action selection</strong> ensures a trade-off between exploitation and exploration. The greedy action is selected with probability <span class="math inline">1 - \epsilon</span> (with <span class="math inline">0 &lt; \epsilon &lt;1</span>), the others with probability <span class="math inline">\epsilon</span>:</p>
<p><span class="math display">
    \pi(a) = \begin{cases} 1 - \epsilon \; \text{if} \; a = a_t^* \\ \frac{\epsilon}{|\mathcal{A}| - 1} \; \text{otherwise.} \end{cases}
</span></p>
<p>The parameter <span class="math inline">\epsilon</span> controls the level of exploration: the higher <span class="math inline">\epsilon</span>, the more exploration. One can set <span class="math inline">\epsilon</span> high at the beginning of learning and progressively reduce it to exploit more. However, it chooses equally among all actions: the worst action is as likely to be selected as the next-to-best action.</p>
</section>
<section id="softmax-action-selection" class="level3">
<h3 class="anchored" data-anchor-id="softmax-action-selection">Softmax action selection</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-estimates-softmax.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Softmax action selection</figcaption>
</figure>
</div>
<p><strong>Softmax action selection</strong> defines the probability of choosing an action using all estimated value. It represents the policy using a Gibbs (or Boltzmann) distribution:</p>
<p><span class="math display">
    \pi(a) = \dfrac{\exp \dfrac{Q_t(a)}{\tau}}{ \displaystyle\sum_{a'} \exp \dfrac{Q_t(a')}{\tau}}
</span></p>
<p>where <span class="math inline">\tau</span> is a positive parameter called the <strong>temperature</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-estimates-softmax2.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Just as <span class="math inline">\epsilon</span>, the temperature <span class="math inline">\tau</span> controls the level of exploration:</p>
<ul>
<li><p>High temperature causes the actions to be nearly equiprobable (<strong>random agent</strong>).</p></li>
<li><p>Low temperature causes the greediest actions only to be selected (<strong>greedy agent</strong>).</p></li>
</ul>
</section>
<section id="example-of-action-selection-for-the-10-armed-bandit" class="level3">
<h3 class="anchored" data-anchor-id="example-of-action-selection-for-the-10-armed-bandit">Example of action selection for the 10-armed bandit</h3>
<p><strong>Procedure:</strong></p>
<ul>
<li><p>N = 10 possible actions with Q-values <span class="math inline">Q^*(a_1), ... , Q^*(a_{10})</span> randomly chosen in <span class="math inline">\mathcal{N}(0, 1)</span>.</p></li>
<li><p>Each reward <span class="math inline">r_t</span> is drawn from a normal distribution <span class="math inline">\mathcal{N}(Q^*(a), 1)</span> depending on the selected action.</p></li>
<li><p>Estimates <span class="math inline">Q_t(a)</span> are initialized to 0.</p></li>
<li><p>The algorithms run for 1000 plays, and the results are averaged 200 times.</p></li>
</ul>
<p><strong>Greedy action selection</strong></p>
<p>Greedy action selection allows to get rid quite early of the actions with negative rewards. However, it may stick with the first positive action it founds, probably not the optimal one. The more actions you have, the more likely you will get stuck in a <strong>suboptimal policy</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-greedy.gif" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Greedy action selection on a 10-armed bandit for 200 plays.</figcaption>
</figure>
</div>
<p><strong><span class="math inline">\epsilon</span>-greedy action selection</strong></p>
<p><span class="math inline">\epsilon</span>-greedy action selection continues to explore after finding a good (but often suboptimal) action. It is not always able to recognize the optimal action (it depends on the variance of the rewards).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-egreedy.gif" class="img-fluid figure-img"></p>
<figcaption class="figure-caption"><span class="math inline">\epsilon</span>-greedy action selection on a 10-armed bandit for 200 plays.</figcaption>
</figure>
</div>
<p><strong>Softmax action selection</strong></p>
<p>Softmax action selection explores more consistently the available actions. The estimated Q-values are much closer to the true values than with (<span class="math inline">\epsilon</span>-)greedy methods.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-softmax.gif" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Softmax action selection on a 10-armed bandit for 200 plays.</figcaption>
</figure>
</div>
<p><strong>Comparison</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-comparison1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Average reward per step.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-comparison2.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Frequency of selection of the optimal action.</figcaption>
</figure>
</div>
<p>The <strong>greedy</strong> method learns faster at the beginning, but get stuck in the long-term by choosing <strong>suboptimal</strong> actions (50% of trials). <span class="math inline">\epsilon</span>-greedy methods perform better on the long term, because they continue to explore. High values for <span class="math inline">\epsilon</span> provide more exploration, hence find the optimal action earlier, but also tend to deselect it more often: with a limited number of plays, it may collect less reward than smaller values of <span class="math inline">\epsilon</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-comparison3.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Average reward per step.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-comparison4.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Frequency of selection of the optimal action.</figcaption>
</figure>
</div>
<p>The softmax does not necessarily find a better solution than <span class="math inline">\epsilon</span>-greedy, but it tends to find it <strong>faster</strong> (depending on <span class="math inline">\epsilon</span> or <span class="math inline">\tau</span>), as it does not lose time exploring obviously bad solutions. <span class="math inline">\epsilon</span>-greedy or softmax methods work best when the variance of rewards is high. If the variance is zero (always the same reward value), the greedy method would find the optimal action more rapidly: the agent only needs to try each action once.</p>
</section>
<section id="exploration-schedule" class="level3">
<h3 class="anchored" data-anchor-id="exploration-schedule">Exploration schedule</h3>
<p>A useful technique to cope with the <strong>exploration-exploitation dilemma</strong> is to slowly decrease the value of <span class="math inline">\epsilon</span> or <span class="math inline">\tau</span> with the number of plays. This allows for more exploration at the beginning of learning and more exploitation towards the end. It is however hard to find the right <strong>decay rate</strong> for the exploration parameters.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-scheduling.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Exploration parameters such as <span class="math inline">\epsilon</span> or <span class="math inline">\tau</span> can be scheduled to allow more exploration at the beginning of learning.</figcaption>
</figure>
</div>
<p>The performance is worse at the beginning, as the agent explores with a high temperature. But as the agent becomes greedier and greedier, the performance become more <strong>optimal</strong> than with a fixed temperature.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-comparison5.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Average reward per step.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-comparison6.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Frequency of selection of the optimal action.</figcaption>
</figure>
</div>
</section>
<section id="optimistic-initial-values" class="level3">
<h3 class="anchored" data-anchor-id="optimistic-initial-values">Optimistic initial values</h3>
<p>The problem with online evaluation is that it depends a lot on the initial estimates <span class="math inline">Q_0</span>.</p>
<ul>
<li><p>If the initial estimates are already quite good (expert knowledge), the Q-values will converge very fast.</p></li>
<li><p>If the initial estimates are very wrong, we will need a lot of updates to correctly estimate the true values.</p></li>
</ul>
<p><span class="math display">
\begin{aligned}
    &amp;Q_{t+1}(a) = (1 - \alpha) \, Q_t(a) + \alpha \, r_{t+1}  \\
    &amp;\\
    &amp; \rightarrow Q_1(a) = (1 - \alpha) \, Q_0(a) + \alpha \, r_1 \\
    &amp; \rightarrow Q_2(a) = (1 - \alpha) \, Q_1(a) + \alpha \, r_2 = (1- \alpha)^2 \, Q_0(a) + (1-\alpha)\alpha \, r_1 + \alpha r_2 \\
\end{aligned}
</span></p>
<p>The influence of <span class="math inline">Q_0</span> on <span class="math inline">Q_t</span> <strong>fades</strong> quickly with <span class="math inline">(1-\alpha)^t</span>, but that can be lost time or lead to a suboptimal policy. However, we can use this at our advantage with <strong>optimistic initialization</strong>.</p>
<p>By choosing very high initial values for the estimates (they can only decrease), one can ensure that all possible actions will be selected during learning by the greedy method, solving the <strong>exploration problem</strong>. This leads however to an <strong>overestimation</strong> of the value of other actions.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-optimistic.gif" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Optimistic initialization.</figcaption>
</figure>
</div>
</section>
<section id="reinforcement-comparison" class="level3">
<h3 class="anchored" data-anchor-id="reinforcement-comparison">Reinforcement comparison</h3>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/C5xXGfgeDOk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>Actions followed by large rewards should be made more likely to recur, whereas actions followed by small rewards should be made less likely to recur. But what is a large/small reward? Is a reward of 5 large or small? <strong>Reinforcement comparison</strong> methods only maintain a <strong>preference</strong> <span class="math inline">p_t(a)</span> for each action, which is not exactly its Q-value. The preference for an action is updated after each play, according to the update rule:</p>
<p><span class="math display">
    p_{t+1}(a_t) =    p_{t}(a_t) + \beta \, (r_t - \tilde{r}_t)
</span></p>
<p>where <span class="math inline">\tilde{r}_t</span> is the moving average of the recently received rewards (regardless the action):</p>
<p><span class="math display">
    \tilde{r}_{t+1} =  \tilde{r}_t + \alpha \, (r_t - \tilde{r}_t)
</span></p>
<ul>
<li><p>If an action brings more reward than usual (<strong>good surprise</strong>), we increase the preference for that action.</p></li>
<li><p>If an action brings less reward than usual (<strong>bad surprise</strong>), we decrease the preference for that action.</p></li>
</ul>
<p><span class="math inline">\beta &gt; 0</span> and <span class="math inline">0 &lt; \alpha &lt; 1</span> are two constant parameters.</p>
<p>The preferences can be used to select the action using the softmax method just like the Q-values (without temperature):</p>
<p><span class="math display">
    \pi_t (a) = \dfrac{\exp p_t(a)}{ \displaystyle\sum_{a'} \exp p_t(a')}
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-reinforcementcomparison.gif" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Reinforcement comparison.</figcaption>
</figure>
</div>
<p>Reinforcement comparison can be very effective, as it does not rely only on the rewards received, but also on their comparison with a <strong>baseline</strong>, the average reward. This idea is at the core of <strong>actor-critic</strong> architectures which we will see later. The initial average reward <span class="math inline">\tilde{r}_{0}</span> can be set optimistically to encourage exploration.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-comparison7.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Average reward per step.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-comparison8.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Frequency of selection of the optimal action.</figcaption>
</figure>
</div>
</section>
<section id="gradient-bandit-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="gradient-bandit-algorithm">Gradient bandit algorithm</h3>
<p>One can even go further than reinforcement comparison: Instead of only increasing the preference for the executed action if it brings more reward than usual, we could also decrease the preference for the other actions. The preferences are used to select an action <span class="math inline">a_t</span> <em>via</em> softmax:</p>
<p><span class="math display">
    \pi_t (a) = \dfrac{\exp p_t(a)}{ \displaystyle\sum_{a'} \exp p_t(a')}
</span></p>
<p>The update rule for the <strong>action taken</strong> <span class="math inline">a_t</span> is:</p>
<p><span class="math display">
    p_{t+1}(a_t) =    p_{t}(a_t) + \beta \, (r_t - \tilde{r}_t) \, (1 - \pi_t(a_t))
</span></p>
<p>The update rule for the <strong>other actions</strong> <span class="math inline">a a \neq a_t</span> is:</p>
<p><span class="math display">
    p_{t+1}(a) =    p_{t}(a) - \beta \, (r_t - \tilde{r}_t) \, \pi_t(a)
</span></p>
<p>The reward <strong>baseline</strong> is updated with:</p>
<p><span class="math display">
    \tilde{r}_{t+1} =  \tilde{r}_t + \alpha \, (r_t - \tilde{r}_t)
</span></p>
<p>The preference can increase become quite high, making the policy greedy towards the end. No need for a temperature parameter!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-gradientbandit.gif" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Reinforcement comparison.</figcaption>
</figure>
</div>
<p>Gradient bandit is not always better than reinforcement comparison, but learns initially faster (depending on the parameters <span class="math inline">\alpha</span> and <span class="math inline">\beta</span>).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-comparison9.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Average reward per step.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-comparison10.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Frequency of selection of the optimal action.</figcaption>
</figure>
</div>
</section>
<section id="upper-confidence-bound-action-selection" class="level3">
<h3 class="anchored" data-anchor-id="upper-confidence-bound-action-selection">Upper-Confidence-Bound action selection</h3>
<p>In the previous methods, <strong>exploration</strong> is controlled by an external parameter (<span class="math inline">\epsilon</span> or <span class="math inline">\tau</span>) which is <strong>global</strong> to each action an must be scheduled. A much better approach would be to decide whether to explore an action based on the <strong>uncertainty</strong> about its Q-value:</p>
<ul>
<li>If we are certain about the value of an action, there is no need to explore it further, we only have to exploit it if it is good.</li>
</ul>
<p>The <strong>central limit theorem</strong> tells us that the variance of a sampling estimator decreases with the number of samples:</p>
<ul>
<li>The distribution of sample averages is normally distributed with mean <span class="math inline">\mu</span> and variance <span class="math inline">\frac{\sigma^2}{N}</span>.</li>
</ul>
<p><span class="math display">S_N \sim \mathcal{N}(\mu, \frac{\sigma}{\sqrt{N}})</span></p>
<p>The more you explore an action <span class="math inline">a</span>, the smaller the variance of <span class="math inline">Q_t(a)</span>, the more certain you are about the estimation, the less you need to explore it.</p>
<p><strong>Upper-Confidence-Bound</strong> (UCB) action selection is a <strong>greedy</strong> action selection method that uses an <strong>exploration</strong> bonus:</p>
<p><span class="math display">
    a^*_t = \text{argmax}_{a} \left( Q_t(a) + c \, \sqrt{\frac{\ln t}{N_t(a)}} \right)
</span></p>
<p><span class="math inline">Q_t(a)</span> is the current estimated value of <span class="math inline">a</span> and <span class="math inline">N_t(a)</span> is the number of times the action <span class="math inline">a</span> has already been selected. It realizes a balance between trusting the estimates <span class="math inline">Q_t(a)</span> and exploring uncertain actions which have not been explored much yet.</p>
<p>The term <span class="math inline">\sqrt{\frac{\ln t}{N_t(a)}}</span> is an estimate of the variance of <span class="math inline">Q_t(a)</span>. The sum of both terms is an <strong>upper-bound</strong> of the true value <span class="math inline">\mu + \sigma</span>. When an action has not been explored much yet, the uncertainty term will dominate and the action be explored, although its estimated value might be low. When an action has been sufficiently explored, the uncertainty term goes to 0 and we greedily follow <span class="math inline">Q_t(a)</span>. The <strong>exploration-exploitation</strong> trade-off is automatically adjusted by counting visits to an action.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-ucb.gif" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Upper-Confidence-Bound action selection.</figcaption>
</figure>
</div>
<p>The “smart” exploration of UCB allows to find the optimal action faster.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-comparison11.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Average reward per step.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-comparison12.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Frequency of selection of the optimal action.</figcaption>
</figure>
</div>
</section>
<section id="summary-of-evaluative-feedback-methods" class="level3">
<h3 class="anchored" data-anchor-id="summary-of-evaluative-feedback-methods">Summary of evaluative feedback methods</h3>
<p>Greedy, <span class="math inline">\epsilon</span>-greedy, softmax, reinforcement comparison, gradient bandit and UCB all have their own advantages and disadvantages depending on the type of problem: stationary or not, high or low reward variance, etc…</p>
<p>These simple techniques are the most useful ones for bandit-like problems: more sophisticated ones exist, but they either make too restrictive assumptions, or are computationally intractable.</p>
<p><strong>Take home messages:</strong></p>
<ol type="1">
<li>RL tries to <strong>estimate values</strong> based on sampled rewards.</li>
<li>One has to balance <strong>exploitation and exploration</strong> throughout learning with the right <strong>action selection scheme</strong>.</li>
<li>Methods exploring more find <strong>better policies</strong>, but are initially slower.</li>
</ol>
</section>
</section>
<section id="contextual-bandits" class="level2">
<h2 class="anchored" data-anchor-id="contextual-bandits">Contextual bandits</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/hI9CzfE8xtA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>In contextual bandits, the obtained rewards do not only depend on the action <span class="math inline">a</span>, but also on the <strong>state</strong> or <strong>context</strong> <span class="math inline">s</span>:</p>
<p><span class="math display">
    r_{t+1} \sim r(s, a)
</span></p>
<p>For example, the n-armed bandit could deliver rewards with different probabilities depending on who plays, the time of the year or the availability of funds in the casino. The problem is simply to estimate <span class="math inline">Q(s, a)</span> instead of <span class="math inline">Q(a)</span>…</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/bandit-hierarchy.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Contextual bandits are an intermediary problem between bandits and the full RL setup, as rewards depend on the state, but the state is not influenced by the action. Source: <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c" class="uri">https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c</a></figcaption>
</figure>
</div>
<p>Contextual bandits are for example useful for <strong>Recommender systems</strong>: actions are the display of an advertisement, the context (or state) represents the user features / identity (possibly learned by an autoencoder), and the reward represents whether the user clicks on the ad or not.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/contextualbandit.gif" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption class="figure-caption">Contextual bandits are used in recommender systems. Source: <a href="https://aws.amazon.com/blogs/machine-learning/power-contextual-bandits-using-continual-learning-with-amazon-sagemaker-rl/" class="uri">https://aws.amazon.com/blogs/machine-learning/power-contextual-bandits-using-continual-learning-with-amazon-sagemaker-rl/</a></figcaption>
</figure>
</div>
<p>Some efficient algorithms have been developed recently, for example <span class="citation" data-cites="Agarwal2014">(<a href="../references.html#ref-Agarwal2014" role="doc-biblioref">Agarwal et al., 2014</a>)</span>, but we will not go into details in this course.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-Agarwal2014" class="csl-entry" role="listitem">
Agarwal, A., Hsu, D., Kale, S., Langford, J., Li, L., and Schapire, R. E. (2014). Taming the <span>Monster</span>: <span>A Fast</span> and <span>Simple Algorithm</span> for <span>Contextual Bandits</span>. in <em>Proceedings of the 31 st <span>International Conference</span> on <span>Machine Learning</span></em> (<span>Beijing, China</span>), 9. <a href="https://arxiv.org/abs/1402.0555">https://arxiv.org/abs/1402.0555</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../notes/1.2-Math.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Math basics (optional)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/2.2-MDP.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">Markov Decision Processes</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">Copyright Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de" class="email">julien.vitay@informatik.tu-chemnitz.de</a></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>