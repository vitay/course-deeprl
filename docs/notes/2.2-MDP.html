<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Deep Reinforcement Learning - 4&nbsp; Markov Decision Processes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/2.3-DP.html" rel="next">
<link href="../notes/2.1-Bandits.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-title">Markov Decision Processes</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../" class="sidebar-logo-link">
      <img src="../notes/img/tuc-new.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Deep Reinforcement Learning</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true"><strong>Introduction</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.1-Introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.2-Math.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Math basics (optional)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true"><strong>Tabular RL</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.1-Bandits.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.2-MDP.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Markov Decision Processes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.3-DP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Dynamic Programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.4-MC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Monte-Carlo (MC) methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.5-TD.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Temporal Difference learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.6-FA.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Function approximation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.7-NN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true"><strong>Model-free RL</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.1-DQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Q-Learning (DQN)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.2-BeyondDQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Beyond DQN</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.3-PG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Policy gradient (PG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.4-A3C.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Advantage actor-critic (A2C, A3C)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.5-DDPG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Deterministic Policy Gradient (DDPG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.6-PPO.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Natural gradients (TRPO, PPO)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.7-SAC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Maximum Entropy RL (SAC)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true"><strong>Model-based RL</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.1-MB.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Model-based RL</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.2-LearnedModels.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Learned world models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.3-AlphaGo.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">AlphaGo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.4-SR.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Successor representations</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true"><strong>Outlook</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.1-Outlook.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Outlook</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true"><strong>Exercises</strong></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Content.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">List of exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Installation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Python installation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/1-Python-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction To Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/2-Numpy-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Numpy and Matplotlib</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/3-Sampling-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Sampling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/4-Bandits-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/5-Bandits2-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits - part 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/6-DP-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Dynamic programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/7-Gym-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Gym environments</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/8-MonteCarlo-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Monte-Carlo control</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/9-TD-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Q-learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/10-Eligibilitytraces-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Eligibility traces</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/11-Keras-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Keras tutorial</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/12-DQN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">DQN</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#from-finite-state-machines-to-markov-decision-process" id="toc-from-finite-state-machines-to-markov-decision-process" class="nav-link active" data-scroll-target="#from-finite-state-machines-to-markov-decision-process">From Finite State Machines to Markov Decision Process</a>
  <ul class="collapse">
  <li><a href="#finite-state-machine-fsm" id="toc-finite-state-machine-fsm" class="nav-link" data-scroll-target="#finite-state-machine-fsm">Finite State Machine (FSM)</a></li>
  <li><a href="#markov-chain-mc" id="toc-markov-chain-mc" class="nav-link" data-scroll-target="#markov-chain-mc">Markov Chain (MC)</a></li>
  <li><a href="#markov-reward-process-mrp" id="toc-markov-reward-process-mrp" class="nav-link" data-scroll-target="#markov-reward-process-mrp">Markov Reward Process (MRP)</a></li>
  <li><a href="#markov-decision-process-mdp" id="toc-markov-decision-process-mdp" class="nav-link" data-scroll-target="#markov-decision-process-mdp">Markov Decision Process (MDP)</a></li>
  </ul></li>
  <li><a href="#bellman-equations" id="toc-bellman-equations" class="nav-link" data-scroll-target="#bellman-equations">Bellman equations</a>
  <ul class="collapse">
  <li><a href="#value-functions" id="toc-value-functions" class="nav-link" data-scroll-target="#value-functions">Value Functions</a></li>
  <li><a href="#bellman-equations-1" id="toc-bellman-equations-1" class="nav-link" data-scroll-target="#bellman-equations-1">Bellman equations</a></li>
  </ul></li>
  <li><a href="#bellman-optimality-equations" id="toc-bellman-optimality-equations" class="nav-link" data-scroll-target="#bellman-optimality-equations">Bellman optimality equations</a>
  <ul class="collapse">
  <li><a href="#optimal-policy" id="toc-optimal-policy" class="nav-link" data-scroll-target="#optimal-policy">Optimal policy</a></li>
  <li><a href="#bellman-optimality-equations-1" id="toc-bellman-optimality-equations-1" class="nav-link" data-scroll-target="#bellman-optimality-equations-1">Bellman optimality equations</a></li>
  </ul></li>
  <li><a href="#generalized-policy-iteration" id="toc-generalized-policy-iteration" class="nav-link" data-scroll-target="#generalized-policy-iteration">Generalized Policy Iteration</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-title">Markov Decision Processes</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>Slides: <a href="../slides/2.2-MDP.html" target="_blank">html</a> <a href="../slides/pdf/2.2-MDP.pdf" target="_blank">pdf</a></p>
<section id="from-finite-state-machines-to-markov-decision-process" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="from-finite-state-machines-to-markov-decision-process">From Finite State Machines to Markov Decision Process</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/u8_DgEuqo3g" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>The kind of task that can be solved by RL is called a <strong>Markov Decision Process</strong> (MDP). For a MDP, the environment is <strong>fully observable</strong>, i.e.&nbsp;the current state <span class="math inline">s_t</span> completely characterizes the process at time <span class="math inline">t</span>. <strong>Actions</strong> <span class="math inline">a_t</span> provoke transitions between two states <span class="math inline">s_t</span> and <span class="math inline">s_{t+1}</span>, according to <strong>transition probabilities</strong>. A <strong>reward</strong> <span class="math inline">r_{t+1}</span> is (probabilistically) associated to each transition.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="../slides/img/rl-loop.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption margin-caption">Agent-environment interface for video games. Source: David Silver <a href="https://www.davidsilver.uk/teaching/" class="uri">https://www.davidsilver.uk/teaching/</a></figcaption><p></p>
</figure>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>n-armed bandits are MDPs with only one state.</p>
</div>
</div>
<section id="finite-state-machine-fsm" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="finite-state-machine-fsm">Finite State Machine (FSM)</h3>
<p>A <strong>finite state machine</strong> (or finite state automaton) is a mathematical model of computation. A FSM can only be in a single <strong>state</strong> <span class="math inline">s</span> at any given time. <strong>Transitions</strong> between states are governed by external inputs, when some condition is met.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="../slides/img/fsm.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption margin-caption">Finite state machine. Source: <a href="https://web.stanford.edu/class/archive/cs/cs103/cs103.1142/button-fsm/" class="uri">https://web.stanford.edu/class/archive/cs/cs103/cs103.1142/button-fsm/</a></figcaption><p></p>
</figure>
</div>
<p>A FSM is fully defined by:</p>
<ul>
<li>The <strong>state set</strong> <span class="math inline">\mathcal{S} = \{ s_i\}_{i=1}^N</span>.</li>
<li>Its initial state <span class="math inline">S_0</span>.</li>
<li>A list of <strong>conditions</strong> for each transition.</li>
</ul>
<p>A FSM is usually implemented by a series of if/then/else statements:</p>
<ul>
<li>if state == “hover” and press == true:
<ul>
<li>state = “pressed”</li>
</ul></li>
<li>elif …</li>
</ul>
</section>
<section id="markov-chain-mc" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="markov-chain-mc">Markov Chain (MC)</h3>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="../slides/img/student-markovchain.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption margin-caption">Markov chain. Credit: David Silver <a href="https://www.davidsilver.uk/teaching/" class="uri">https://www.davidsilver.uk/teaching/</a></figcaption><p></p>
</figure>
</div>
<p>A first-order <strong>Markov Chain</strong> (or Markov process) is a stochastic process generated by a FSM, where transitions between states are governed by <strong>state transition probabilities</strong>.</p>
<p>A Markov Chain is defined by:</p>
<ul>
<li>The <strong>state set</strong> <span class="math inline">\mathcal{S} = \{ s_i\}_{i=1}^N</span>.</li>
<li>The <strong>state transition probability function</strong>:</li>
</ul>
<p><span class="math display">
\begin{aligned}
    \mathcal{P}: \mathcal{S} \rightarrow &amp; P(\mathcal{S}) \\
    p(s' | s) &amp; =  P (s_{t+1} = s' | s_t = s) \\
\end{aligned}
</span></p>
<section id="markov-property" class="level4">
<h4 class="anchored" data-anchor-id="markov-property">Markov property</h4>
<p>When the states have the <strong>Markov property</strong>, the state transition probabilities fully describe the MC. The Markov property states that:</p>
<blockquote class="blockquote">
<p>The future is independent of the past given the present.</p>
</blockquote>
<p>Formally, the state <span class="math inline">s_t</span> (state at time <span class="math inline">t</span>) is <strong>Markov</strong> (or Markovian) if and only if:</p>
<p><span class="math display">
    P(s_{t+1} | s_t) = P(s_{t+1} | s_t, s_{t-1}, \ldots, s_0)
</span></p>
<p>The knowledge of the current state <span class="math inline">s_t</span> is <strong>enough</strong> to predict in which state <span class="math inline">s_{t+1}</span> the system will be at the next time step. We do not need the whole <strong>history</strong> <span class="math inline">\{s_0, s_1, \ldots, s_t\}</span> of the system to predict what will happen.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If we need only <span class="math inline">s_{t-1}</span> and <span class="math inline">s_t</span> to predict <span class="math inline">s_{t+1}</span>, we have a second-order Markov chain.</p>
</div>
</div>
<p>For example, the probability 0.8 of transitioning from “Class 2” to “Class 3” is the same regardless we were in “Class 1” or “Pub” before. If this is not the case, the states are not Markov, and this is not a Markov chain. We would need to create two distinct states:</p>
<ul>
<li>“Class 2 coming from Class 1”</li>
<li>“Class 2 coming from the pub”</li>
</ul>
<p>Single <strong>video frames</strong> are not Markov states: you cannot generally predict what will happen based on a single image. A simple solution is to <strong>stack</strong> or <strong>concatenate</strong> multiple frames: By measuring the displacement of the ball between two consecutive frames, we can predict where it is going. One can also <strong>learn</strong> state representations containing the history using recurrent neural networks (see later).</p>
</section>
<section id="state-transition-matrix" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="state-transition-matrix">State transition matrix</h4>
<p>Supposing that the states have the Markov property, the transitions in the system can be summarized by the <strong>state transition matrix</strong> <span class="math inline">\mathcal{P}</span>:</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="../slides/img/student-transitionmatrix.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption margin-caption">State transition matrix. Credit: David Silver <a href="https://www.davidsilver.uk/teaching/" class="uri">https://www.davidsilver.uk/teaching/</a></figcaption><p></p>
</figure>
</div>
<p>Each element of the state transition matrix corresponds to <span class="math inline">p(s' | s)</span>. Each row of the state transition matrix sums to 1:</p>
<p><span class="math display">\sum_{s'} p(s' | s)  = 1</span></p>
<p>The tuple <span class="math inline">&lt;\mathcal{S}, \mathcal{P}&gt;</span> fully describes the Markov chain.</p>
</section>
</section>
<section id="markov-reward-process-mrp" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="markov-reward-process-mrp">Markov Reward Process (MRP)</h3>
<p>A <strong>Markov Reward Process</strong> is a Markov Chain where each transition is associated with a scalar <strong>reward</strong> <span class="math inline">r</span>, coming from some probability distribution.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="../slides/img/student-mrp.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption margin-caption">Markov Reward Process. Credit: David Silver <a href="https://www.davidsilver.uk/teaching/" class="uri">https://www.davidsilver.uk/teaching/</a></figcaption><p></p>
</figure>
</div>
<p>A Markov Reward Process is defined by the tuple <span class="math inline">&lt;\mathcal{S}, \mathcal{P}, \mathcal{R}, \gamma&gt;</span>.</p>
<ul>
<li>The finite <strong>state set</strong> <span class="math inline">\mathcal{S}</span>.</li>
<li>The <strong>state transition probability function</strong>: <span class="math display">
\begin{aligned}
  \mathcal{P}: \mathcal{S} \rightarrow &amp; P(\mathcal{S}) \\
  p(s' | s) &amp; =  P (s_{t+1} = s' | s_t = s) \\
\end{aligned}
</span></li>
<li>The <strong>expected reward function</strong>: <span class="math display">
\begin{aligned}
  \mathcal{R}: \mathcal{S} \times \mathcal{S} \rightarrow &amp; \Re \\
  r(s, s') &amp;=  \mathbb{E} (r_{t+1} | s_t = s, s_{t+1} = s') \\
\end{aligned}
</span></li>
<li>The <strong>discount factor</strong> <span class="math inline">\gamma \in [0, 1]</span>.</li>
</ul>
<p>As with n-armed bandits, we only care about the <strong>expected reward</strong> received during a transition <span class="math inline">s \rightarrow s'</span> (<em>on average</em>), but the actual reward received <span class="math inline">r_{t+1}</span> may vary around the expected value.</p>
<p><span class="math display">r(s, s') =  \mathbb{E} (r_{t+1} | s_t = s, s_{t+1} = s')</span></p>
<p>The main difference with n-armed bandits is that the MRP will be in a sequence of states (possibly infinite):</p>
<p><span class="math display">s_0 \rightarrow s_1 \rightarrow s_2  \rightarrow \ldots \rightarrow s_T</span></p>
<p>and collect a sequence of reward samples:</p>
<p><span class="math display">r_1 \rightarrow r_2 \rightarrow r_3  \rightarrow \ldots \rightarrow r_{T}</span></p>
<p>In a MRP, we are interested in estimating the <strong>return</strong> <span class="math inline">R_t</span>, i.e.&nbsp;the discounted sum of <strong>future</strong> rewards after the step <span class="math inline">t</span>:</p>
<p><span class="math display">
    R_t = r_{t+1} + \gamma \, r_{t+2} + \gamma^2 \, r_{t+3} + \ldots = \sum_{k=0}^\infty \gamma^k \, r_{t+k+1}
</span></p>
<p>Of course, you never know the return at time <span class="math inline">t</span>: transitions and rewards are probabilistic, so the received rewards in the future are not exactly predictable at <span class="math inline">t</span>. <span class="math inline">R_t</span> is therefore purely theoretical: RL is all about <strong>estimating</strong> the return.</p>
<p>The <strong>discount factor</strong> (or discount rate, or discount) <span class="math inline">\gamma \in [0, 1]</span> is a very important parameter in RL: It defines the <strong>present value of future rewards</strong>. Receiving 10 euros now has a higher <strong>value</strong> than receiving 10 euros in ten years, although the reward is the same: you do not have to wait.</p>
<p>The value of receiving a reward <span class="math inline">r</span> after <span class="math inline">k+1</span> time steps is <span class="math inline">\gamma^k \, r</span>. <strong>Immediate rewards</strong> are better than <strong>delayed rewards</strong>. When <span class="math inline">\gamma &lt; 1</span>, <span class="math inline">\gamma^k</span> tends to 0 when <span class="math inline">k</span> goes to infinity: this makes sure that the return is always <strong>finite</strong>. This is particularly important when the MRP is cyclic / periodic. If all sequences terminate at some time step <span class="math inline">T</span>, we can set <span class="math inline">\gamma= 1</span>.</p>
</section>
<section id="markov-decision-process-mdp" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="markov-decision-process-mdp">Markov Decision Process (MDP)</h3>
<p>A <strong>Markov Decision Process</strong> is a MRP where transitions are influenced by <strong>actions</strong> <span class="math inline">a \in \mathcal{A}</span>.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="../slides/img/student-mdp.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption margin-caption">Markov decision process. Credit: David Silver <a href="https://www.davidsilver.uk/teaching/" class="uri">https://www.davidsilver.uk/teaching/</a></figcaption><p></p>
</figure>
</div>
<p>A finite MDP is defined by the tuple <span class="math inline">&lt;\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma&gt;</span>:</p>
<ul>
<li>The finite <strong>state set</strong> <span class="math inline">\mathcal{S}</span>.</li>
<li>The finite <strong>action set</strong> <span class="math inline">\mathcal{A}</span>.</li>
<li>The <strong>state transition probability function</strong>:</li>
</ul>
<p><span class="math display">
\begin{aligned}
    \mathcal{P}: \mathcal{S} \times \mathcal{A} \rightarrow &amp; P(\mathcal{S}) \\
    p(s' | s, a) &amp; =  P (s_{t+1} = s' | s_t = s, a_t = a) \\
\end{aligned}
</span></p>
<ul>
<li>The <strong>expected reward function</strong>:</li>
</ul>
<p><span class="math display">
\begin{aligned}
    \mathcal{R}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow &amp; \Re \\
    r(s, a, s') &amp;=  \mathbb{E} (r_{t+1} | s_t = s, a_t = a, s_{t+1} = s') \\
\end{aligned}
</span></p>
<ul>
<li>The <strong>discount factor</strong> <span class="math inline">\gamma \in [0, 1]</span>.</li>
</ul>
<p>Why do we need transition probabilities in RL?</p>
<p><span class="math display"> p(s' | s, a) =  P (s_{t+1} = s' | s_t = s, a_t = a)</span></p>
<p>Some RL tasks are <strong>deterministic</strong>: an action <span class="math inline">a</span> in a state <span class="math inline">s</span> always leads to the state <span class="math inline">s'</span> (board games, video games…). Others are <strong>stochastic</strong>: the same action <span class="math inline">a</span> can lead to different states <span class="math inline">s'</span>: Casino games (throwing a dice, etc), two-opponent games (the next state depends on what the other player chooses), uncertainty (shoot at basketball, slippery wheels, robotic grasping)…</p>
<p>For a transition <span class="math inline">(s, a, s')</span>, the received reward can be also stochastic: casino games (armed bandit), incomplete information, etc. Most of the problems we will see in this course have deterministic rewards, but we only care about expectations anyway.</p>
<section id="markov-property-1" class="level4">
<h4 class="anchored" data-anchor-id="markov-property-1">Markov property</h4>
<p>The state of the agent at step <span class="math inline">t</span> refers to whatever information is available about its environment or its own “body”. The state can include immediate “sensations”, highly processed sensations, and structures built up over time from sequences of sensations. A state should summarize all past sensations so as to retain all essential information, i.e.&nbsp;it should have the <strong>Markov Property</strong>:</p>
<p><span class="math display">\begin{aligned}
     P( s_{t+1} = s, r_{t+1} = r &amp; | s_t, a_t, r_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = P( s_{t+1} = s, r_{t+1} = r | s_t, a_t ) \\
     &amp;\text{for all s, r, and past histories} \quad (s_{t}, a_{t}, ..., s_0, a_0)
\end{aligned}
</span></p>
<p>This means that the current state representation <span class="math inline">s</span> contains enough information to predict the probability of arriving in the next state <span class="math inline">s'</span> given the chosen action <span class="math inline">a</span>. When the Markovian property is not met, we have a <strong>Partially-Observable Markov Decision Process</strong> (POMDP).</p>
<p>In a POMDP, the agent does not have access to the true state <span class="math inline">s_t</span> of the environment, but only <strong>observations</strong> <span class="math inline">o_t</span>. Observations are partial views of the state, without the Markov property. The dynamics of the environment (transition probabilities, reward expectations) only depend on the state, not the observations. The agent can only make decisions (actions) based on the sequence of observations, as it does not have access to the state directly (Plato’s cavern).</p>
<p>In a POMDP, the state <span class="math inline">s_t</span> of the agent is implicitly the concatenation of the past observations and actions:</p>
<p><span class="math display">
s_t = (o_0, a_0, o_1, a_1, \ldots, a_{t-1}, o_t)
</span></p>
<p>Under conditions, this inferred state can have the Markov property and the POMDP is solvable.</p>
</section>
<section id="returns" class="level4">
<h4 class="anchored" data-anchor-id="returns">Returns</h4>
<p>Suppose the sequence of rewards obtained after step <span class="math inline">t</span> (after being in state <span class="math inline">s_t</span> and choosing action <span class="math inline">a_t</span>) is:</p>
<p><span class="math display"> r_{t+1}, r_{t+2}, r_{t+3}, ... </span></p>
<p>What we want to maximize is the <strong>return</strong> (reward-to-go) at each time step <span class="math inline">t</span>, i.e.&nbsp;the sum of all future rewards:</p>
<p><span class="math display">
    R_t = r_{t+1} + \gamma \, r_{t+2} +  \gamma^2 \, r_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k \, r_{t+k+1}
</span></p>
<p>More generally, for a trajectory (episode) <span class="math inline">\tau = (s_0, a_0, r_1, s_1, a_1, \ldots, s_T)</span>, one can define its return as:</p>
<p><span class="math display"> R(\tau) = \sum_{t=0}^{T} \gamma^t \, r_{t+1} </span></p>
<p>For <strong>episodic tasks</strong> (which break naturally into finite episodes of length <span class="math inline">T</span>, e.g.&nbsp;plays of a game, trips through a maze), the return is always finite and easy to compute at the end of the episode. The discount factor can be set to 1:</p>
<p><span class="math display">
    R_t = \sum_{k=0}^{T} r_{t+k+1}
</span></p>
<p>For <strong>continuing tasks</strong> (which can not be split into episodes), the return could become infinite if <span class="math inline">\gamma = 1</span>. The discount factor has to be smaller than 1.</p>
<p><span class="math display">
    R_t = \sum_{k=0}^{\infty} \gamma^k \, r_{t+k+1}
</span></p>
<p>The discount rate <span class="math inline">\gamma</span> determines the relative importance of future rewards for the behavior:</p>
<ul>
<li>if <span class="math inline">\gamma</span> is close to 0, only the immediately available rewards will count: the agent is greedy or <strong>myopic</strong>.</li>
<li>if <span class="math inline">\gamma</span> is close to 1, even far-distance rewards will be taken into account: the agent is <strong>farsighted</strong>.</li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Why the reward on the long term?
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/return-example.svg" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Selecting the action <span class="math inline">a_1</span> in <span class="math inline">s_1</span> does not bring reward immediately (<span class="math inline">r_1 = 0</span>) but allows to reach <span class="math inline">s_5</span> in the future and get a reward of 10. Selecting <span class="math inline">a_2</span> in <span class="math inline">s_1</span> brings immediately a reward of 1, but that will be all. <span class="math inline">a_1</span> is <strong>better</strong> than <span class="math inline">a_2</span>, because it will bring more reward <strong>on the long term</strong>.</p>
<p>When selecting <span class="math inline">a_1</span> in <span class="math inline">s_1</span>, the discounted return is:</p>
<p><span class="math display">
    R = 0 + \gamma \, 0 + \gamma^2 \, 0 + \gamma^3 \, 10 + \ldots = 10 \, \gamma^3
</span></p>
<p>while it is <span class="math inline">R= 1</span> for the action <span class="math inline">a_2</span>.</p>
<p>For small values of <span class="math inline">\gamma</span> (e.g.&nbsp;0.1), <span class="math inline">10\, \gamma^3</span> becomes smaller than one, so the action <span class="math inline">a_2</span> leads to a higher discounted return. The discount rate <span class="math inline">\gamma</span> changes the behavior of the agent. It is usually taken somewhere between 0.9 and 0.999.</p>
</div>
</div>
</section>
<section id="example-the-cartpole-balancing-task" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="example-the-cartpole-balancing-task">Example: the cartpole balancing task</h4>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="../slides/img/cartpole-after.gif" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption margin-caption">Cartpole balancing task</figcaption><p></p>
</figure>
</div>
<ul>
<li><strong>State:</strong> Position and velocity of the cart, angle and speed of the pole.</li>
<li><strong>Actions:</strong> Commands to the motors for going left or right.</li>
<li><strong>Reward function:</strong> Depends on whether we consider the task as episodic or continuing.</li>
</ul>
<p>The problem can be viewed both as an episodic or continuing task:</p>
<ul>
<li><strong>Episodic</strong> task where episode ends upon failure:
<ul>
<li><strong>reward</strong> = +1 for every step before failure, 0 at failure.</li>
<li><strong>return</strong> = number of steps before failure.</li>
</ul></li>
<li><strong>Continuing</strong> task with discounted return:
<ul>
<li><strong>reward</strong> = -1 at failure, 0 otherwise.</li>
<li><strong>return</strong> = <span class="math inline">- \gamma^k</span> for <span class="math inline">k</span> steps before failure.</li>
</ul></li>
</ul>
<p>In both cases, the goal is to maximize the return by maintaining the pole vertical as long as possible.</p>
</section>
<section id="example-the-recycling-robot" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="example-the-recycling-robot">Example: the recycling robot</h4>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="../slides/img/recyclingrobot.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption margin-caption">Recycling robot. Source: <span class="citation" data-cites="Sutton1998">(<a href="../references.html#ref-Sutton1998" role="doc-biblioref">Sutton and Barto, 1998</a>)</span></figcaption><p></p>
</figure>
</div>
<p>At each step, the recycling robot has to decide whether it should:</p>
<ol type="1">
<li>actively search for a can,</li>
<li>wait for someone to bring it a can, or</li>
<li>go to home base and recharge.</li>
</ol>
<p>Searching is better (more reward) but runs down the battery (probability 1-<span class="math inline">\alpha</span> to empty the battery): if the robot runs out of power while searching, he has to be rescued (which leads to punishment and should be avoided). Decisions must be made on basis of the current energy level: high, low. This will be the state of the robot. The return is the number of cans collected on the long term.</p>
<ul>
<li><span class="math inline">\mathcal{S} = \{ \text{high}, \text{low} \}</span></li>
<li><span class="math inline">\mathcal{A}(\text{high} ) = \{ \text{search}, \text{wait} \}</span></li>
<li><span class="math inline">\mathcal{A}(\text{low} ) = \{ \text{search}, \text{wait}, \text{recharge} \}</span></li>
<li><span class="math inline">R^{\text{search}}</span> = expected number of cans while searching.</li>
<li><span class="math inline">R^{\text{wait}}</span> = expected number of cans while waiting.</li>
<li><span class="math inline">R^{\text{search}} &gt; R^{\text{wait}}</span></li>
</ul>
<p>The MDP is fully described by the following table:</p>
<table class="table">
<colgroup>
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 14%">
<col style="width: 20%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">s</span></th>
<th style="text-align: center;"><span class="math inline">s'</span></th>
<th style="text-align: center;"><span class="math inline">a</span></th>
<th style="text-align: center;"><span class="math inline">p(s' / s, a)</span></th>
<th style="text-align: center;"><span class="math inline">r(s, a, s')</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">high</td>
<td style="text-align: center;">high</td>
<td style="text-align: center;">search</td>
<td style="text-align: center;"><span class="math inline">\alpha</span></td>
<td style="text-align: center;"><span class="math inline">\mathcal{R}^\text{search}</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">high</td>
<td style="text-align: center;">low</td>
<td style="text-align: center;">search</td>
<td style="text-align: center;"><span class="math inline">1 - \alpha</span></td>
<td style="text-align: center;"><span class="math inline">\mathcal{R}^\text{search}</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">low</td>
<td style="text-align: center;">high</td>
<td style="text-align: center;">search</td>
<td style="text-align: center;"><span class="math inline">1 - \beta</span></td>
<td style="text-align: center;"><span class="math inline">-3</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">low</td>
<td style="text-align: center;">low</td>
<td style="text-align: center;">search</td>
<td style="text-align: center;"><span class="math inline">\beta</span></td>
<td style="text-align: center;"><span class="math inline">\mathcal{R}^\text{search}</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">high</td>
<td style="text-align: center;">high</td>
<td style="text-align: center;">wait</td>
<td style="text-align: center;"><span class="math inline">1</span></td>
<td style="text-align: center;"><span class="math inline">\mathcal{R}^\text{wait}</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">high</td>
<td style="text-align: center;">low</td>
<td style="text-align: center;">wait</td>
<td style="text-align: center;"><span class="math inline">0</span></td>
<td style="text-align: center;"><span class="math inline">\mathcal{R}^\text{wait}</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">low</td>
<td style="text-align: center;">high</td>
<td style="text-align: center;">wait</td>
<td style="text-align: center;"><span class="math inline">0</span></td>
<td style="text-align: center;"><span class="math inline">\mathcal{R}^\text{wait}</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">low</td>
<td style="text-align: center;">low</td>
<td style="text-align: center;">wait</td>
<td style="text-align: center;"><span class="math inline">1</span></td>
<td style="text-align: center;"><span class="math inline">\mathcal{R}^\text{wait}</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">low</td>
<td style="text-align: center;">high</td>
<td style="text-align: center;">recharge</td>
<td style="text-align: center;"><span class="math inline">1</span></td>
<td style="text-align: center;"><span class="math inline">0</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">low</td>
<td style="text-align: center;">low</td>
<td style="text-align: center;">recharge</td>
<td style="text-align: center;"><span class="math inline">0</span></td>
<td style="text-align: center;"><span class="math inline">0</span></td>
</tr>
</tbody>
</table>
</section>
<section id="the-policy" class="level4">
<h4 class="anchored" data-anchor-id="the-policy">The policy</h4>
<p>The probability that an agent selects a particular action <span class="math inline">a</span> in a given state <span class="math inline">s</span> is called the <strong>policy</strong> <span class="math inline">\pi</span>.</p>
<p><span class="math display">
\begin{align}
    \pi &amp;: \mathcal{S} \times \mathcal{A} \rightarrow P(\mathcal{S})\\
    (s, a) &amp;\rightarrow \pi(s, a)  = P(a_t = a | s_t = s) \\
\end{align}
</span></p>
<p>The policy can be <strong>deterministic</strong> (one action has a probability of 1, the others 0) or <strong>stochastic</strong>. The goal of an agent is to find a policy that maximizes the sum of received rewards <strong>on the long term</strong>, i.e.&nbsp;the <strong>return</strong> <span class="math inline">R_t</span> at each each time step. This policy is called the <strong>optimal policy</strong> <span class="math inline">\pi^*</span>.</p>
<p><span class="math display">
    \mathcal{J}(\pi) = \mathbb{E}_{\rho_\pi} [R_t] \qquad
    \pi^* = \text{argmax} \, \mathcal{J}(\pi)
</span></p>
</section>
<section id="goal-of-reinforcement-learning" class="level4">
<h4 class="anchored" data-anchor-id="goal-of-reinforcement-learning">Goal of Reinforcement Learning</h4>
<p>RL is an <strong>adaptive optimal control</strong> method for Markov Decision Processes using (sparse) rewards as a partial feedback. At each time step <span class="math inline">t</span>, the agent observes its Markov state <span class="math inline">s_t \in \mathcal{S}</span>, produces an action <span class="math inline">a_t \in \mathcal{A}(s_t)</span>, receives a reward according to this action <span class="math inline">r_{t+1} \in \Re</span> and updates its state: <span class="math inline">s_{t+1} \in \mathcal{S}</span>.</p>
<p>The agent generates trajectories <span class="math inline">\tau = (s_0, a_0, r_1, s_1, a_1, \ldots, s_T)</span> depending on its policy <span class="math inline">\pi(s ,a)</span>.</p>
<p>The return of a trajectory is the (discounted) sum of rewards accumulated during the sequence:</p>
<p><span class="math display"> R(\tau) = \sum_{t=0}^{T} \gamma^t \, r_{t+1} </span></p>
<p>The goal is to find the <strong>optimal policy</strong> <span class="math inline">\pi^* (s, a)</span> that maximizes in expectation the return of each possible trajectory under that policy:</p>
<p><span class="math display">
    \mathcal{J}(\pi) = \mathbb{E}_{\tau \sim \rho_\pi} [R(\tau)] \qquad
    \pi^* = \text{argmax} \, \mathcal{J}(\pi)
</span></p>
</section>
</section>
</section>
<section id="bellman-equations" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="bellman-equations">Bellman equations</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/1Z5sMSCEMRo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<section id="value-functions" class="level3">
<h3 class="anchored" data-anchor-id="value-functions">Value Functions</h3>
<p>A central notion in RL is to estimate the <strong>value</strong> (or <strong>utility</strong>) of every state and action of the MDP. The value of a state <span class="math inline">V^{\pi} (s)</span> is the expected return when starting from that state and thereafter following the agent’s current policy <span class="math inline">\pi</span>.</p>
<p>The <strong>state-value function</strong> <span class="math inline">V^{\pi} (s)</span> of a state <span class="math inline">s</span> given the policy <span class="math inline">\pi</span> is defined as the mathematical expectation of the return after that state:</p>
<p><span class="math display">  V^{\pi} (s) = \mathbb{E}_{\rho_\pi} ( R_t | s_t = s) = \mathbb{E}_{\rho_\pi} ( \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} |s_t=s ) </span></p>
<p>The mathematical expectation operator <span class="math inline">\mathbb{E}(\cdot)</span> is indexed by <span class="math inline">\rho_\pi</span>, the probability distribution of states achievable with <span class="math inline">\pi</span>.</p>
<p>Several trajectories are possible after the state <span class="math inline">s</span>:</p>
<ul>
<li>The <strong>state transition probability function</strong> <span class="math inline">p(s' | s, a)</span> leads to different states <span class="math inline">s'</span>, even if the same actions are taken.</li>
<li>The <strong>expected reward function</strong> <span class="math inline">r(s, a, s')</span> provides stochastic rewards, even if the transition <span class="math inline">(s, a, s')</span> is the same.</li>
<li>The <strong>policy</strong> <span class="math inline">\pi</span> itself is stochastic.</li>
</ul>
<p>Only rewards that are obtained using the policy <span class="math inline">\pi</span> should be taken into account, not the complete distribution of states and rewards.</p>
<p>The value of a state is not intrinsic to the state itself, it depends on the policy: One could be in a state which is very close to the goal (only one action left to win game), but if the policy is very bad, the “good” action will not be chosen and the state will have a small value.</p>
<p>The value of taking an action <span class="math inline">a</span> in a state <span class="math inline">s</span> under policy <span class="math inline">\pi</span> is the expected return starting from that state, taking that action, and thereafter following the following <span class="math inline">\pi</span>. The <strong>action-value function</strong> for a state-action pair <span class="math inline">(s, a)</span> under the policy <span class="math inline">\pi</span> (or <strong>Q-value</strong>) is defined as:</p>
<p><span class="math display">
\begin{align}
    Q^{\pi} (s, a)  &amp; = \mathbb{E}_{\rho_\pi} ( R_t | s_t = s, a_t =a) \\
                    &amp; = \mathbb{E}_{\rho_\pi} ( \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} |s_t=s, a_t=a) \\
\end{align}
</span></p>
<p>State- and action-value functions are linked to each other. The value of a state <span class="math inline">V^{\pi}(s)</span> depends on the value <span class="math inline">Q^{\pi} (s, a)</span> of the action that will be chosen by the policy <span class="math inline">\pi</span> in <span class="math inline">s</span>:</p>
<p><span class="math display">
    V^{\pi}(s) = \mathbb{E}_{a \sim \pi(s,a)} [Q^{\pi} (s, a)] = \sum_{a \in \mathcal{A}(s)} \pi(s, a) \, Q^{\pi} (s, a)
</span></p>
<p>If the policy <span class="math inline">\pi</span> is deterministic (the same action is chosen every time), the value of the state is the same as the value of that action (same expected return). If the policy <span class="math inline">\pi</span> is stochastic (actions are chosen with different probabilities), the value of the state is the expectation (weighted average) of the value of the actions. If the Q-values are known, the V-values can be found easily.</p>
<p>We can note that the return at time <span class="math inline">t</span> depends on the <strong>immediate reward</strong> <span class="math inline">r_{t+1}</span> and the return at the next time step <span class="math inline">t+1</span>:</p>
<p><span class="math display">
\begin{aligned}
    R_t &amp;= r_{t+1} + \gamma \, r_{t+2} +  \gamma^2  \, r_{t+3} + \dots + \gamma^k \, r_{t+k+1} + \dots \\
        &amp;= r_{t+1} + \gamma \, ( r_{t+2} +  \gamma \, r_{t+3} + \dots + \gamma^{k-1} \, r_{t+k+1} + \dots) \\
        &amp;= r_{t+1} + \gamma \,  R_{t+1} \\
\end{aligned}
</span></p>
<p>When taking the mathematical expectation of that identity, we obtain:</p>
<p><span class="math display">
    \mathbb{E}_{\rho_\pi}[R_t] = r(s_t, a_t, s_{t+1}) + \gamma \, \mathbb{E}_{\rho_\pi}[R_{t+1}]
</span></p>
<p>It becomes clear that the value of an action depends on the immediate reward received just after the action, as well as the value of the next state:</p>
<p><span class="math display">
        Q^{\pi}(s_t, a_t) = r(s_t, a_t, s_{t+1}) + \gamma \,  V^{\pi} (s_{t+1})
</span></p>
<p>But that is only for a fixed <span class="math inline">(s_t, a_t, s_{t+1})</span> transition. Taking transition probabilities into account, one can obtain the Q-values when the V-values are known:</p>
<p><span class="math display">
        Q^{\pi}(s, a) = \mathbb{E}_{s' \sim p(s'|s, a)} [ r(s, a, s') + \gamma \, V^{\pi} (s') ] = \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, V^{\pi} (s') ]
</span></p>
<p>The value of an action depends on:</p>
<ul>
<li>the states <span class="math inline">s'</span> one can arrive after the action (with a probability <span class="math inline">p(s' | s, a)</span>).</li>
<li>the value of that state <span class="math inline">V^{\pi} (s')</span>, weighted by <span class="math inline">\gamma</span> as it is one step in the future.</li>
<li>the reward received immediately after taking that action <span class="math inline">r(s, a, s')</span> (as it is not included in the value of <span class="math inline">s'</span>).</li>
</ul>
</section>
<section id="bellman-equations-1" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="bellman-equations-1">Bellman equations</h3>
<p>A fundamental property of value functions used throughout reinforcement learning is that they satisfy a particular recursive relationship:</p>
<p><span class="math display">
\begin{aligned}
        V^{\pi}(s)  &amp;= \sum_{a \in \mathcal{A}(s)} \pi(s, a) \, Q^{\pi} (s, a)\\
                    &amp;= \sum_{a \in \mathcal{A}(s)} \pi(s, a) \, \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, V^{\pi} (s') ]
\end{aligned}
</span></p>
<p>This equation is called the <strong>Bellman equation</strong> for <span class="math inline">V^{\pi}</span>. It expresses the relationship between the value of a state and the value of its successors, depending on the dynamics of the MDP (<span class="math inline">p(s' | s, a)</span> and <span class="math inline">r(s, a, s')</span>) and the current policy <span class="math inline">\pi</span>. The interesting property of the Bellman equation for RL is that it admits one and only one solution <span class="math inline">V^{\pi}(s)</span>.</p>
<p>The same recursive relationship stands for <span class="math inline">Q^{\pi}(s, a)</span>:</p>
<p><span class="math display">
\begin{aligned}
        Q^{\pi}(s, a)  &amp;= \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, V^{\pi} (s') ] \\
                    &amp;=  \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [ r(s, a, s') + \gamma \, \sum_{a' \in \mathcal{A}(s')} \pi(s', a') \, Q^{\pi} (s', a')]
\end{aligned}
</span></p>
<p>which is called the <strong>Bellman equation</strong> for <span class="math inline">Q^{\pi}</span>.</p>
<p>The following <strong>backup diagrams</strong> denote these recursive relationships.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="../slides/img/backup-V.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption margin-caption">Backup diagrams of the Bellman equations. Left: the value of a state <span class="math inline">s</span> depends on the policy <span class="math inline">\pi</span> and the value of the succeeding states <span class="math inline">s'</span>. Right: the value of an action depends on the value of the action that can be taken in the next states. Source: <span class="citation" data-cites="Sutton1998">(<a href="../references.html#ref-Sutton1998" role="doc-biblioref">Sutton and Barto, 1998</a>)</span></figcaption><p></p>
</figure>
</div>
</section>
</section>
<section id="bellman-optimality-equations" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="bellman-optimality-equations">Bellman optimality equations</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/mFucN5K351A" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<section id="optimal-policy" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="optimal-policy">Optimal policy</h3>
<p>The optimal policy is the policy that gathers the maximum of reward on the long term. Value functions define a partial ordering over policies:</p>
<ul>
<li>a policy <span class="math inline">\pi</span> is better than another policy <span class="math inline">\pi'</span> if its expected return is greater or equal than that of <span class="math inline">\pi'</span> for all states <span class="math inline">s</span>.</li>
</ul>
<p><span class="math display">
        \pi \geq \pi' \Leftrightarrow V^{\pi}(s) \geq V^{\pi'}(s) \quad \forall s \in \mathcal{S}
</span></p>
<p>There exists at least one policy that is better than all the others: this is the <strong>optimal policy</strong> <span class="math inline">\pi^*</span>. We note <span class="math inline">V^*(s)</span> and <span class="math inline">Q^*(s, a)</span> the optimal value of the different states and actions under <span class="math inline">\pi^*</span>.</p>
<p><span class="math display">
   V^* (s) = \max_{\pi} V^{\pi}(s) \quad \forall s \in \mathcal{S}
</span></p>
<p><span class="math display">
    Q^* (s, a) = \max_{\pi} Q^{\pi}(s, a) \quad \forall s \in \mathcal{S}, \quad \forall a \in \mathcal{A}
</span></p>
<p>When the policy is optimal <span class="math inline">\pi^*</span>, the link between the V and Q values is even easier. The V and Q values are maximal for the optimal policy: there is no better alternative.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="../slides/img/fullvi.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption margin-caption">Backup diagrams of the Bellman optimality equations. Source: <span class="citation" data-cites="Sutton1998">(<a href="../references.html#ref-Sutton1998" role="doc-biblioref">Sutton and Barto, 1998</a>)</span></figcaption><p></p>
</figure>
</div>
<p><strong>The optimal action <span class="math inline">a^*</span> to perform in the state <span class="math inline">s</span> is the one with the highest optimal Q-value <span class="math inline">Q^*(s, a)</span></strong>.</p>
<p><span class="math display">
    a^* = \text{argmax}_a \, Q^*(s, a)
</span></p>
<p>By definition, this action will bring the maximal return when starting in <span class="math inline">s</span>.</p>
<p><span class="math display">
    Q^*(s, a) = \mathbb{E}_{\rho_{\pi^*}} [R_t]
</span></p>
<p>The optimal policy is <strong>greedy</strong> with respect to <span class="math inline">Q^*(s, a)</span>, i.e.&nbsp;<strong>deterministic</strong>.</p>
<p><span class="math display">
    \pi^*(s, a) = \begin{cases}
                1 \; \text{if} \; a = a^* \\
                0 \; \text{otherwise.}
                \end{cases}
</span></p>
</section>
<section id="bellman-optimality-equations-1" class="level3">
<h3 class="anchored" data-anchor-id="bellman-optimality-equations-1">Bellman optimality equations</h3>
<p>As the optimal policy is deterministic, the optimal value of a state is equal to the value of the optimal action:</p>
<p><span class="math display">
    V^* (s)  = \max_{a \in \mathcal{A}(s)} Q^{\pi^*} (s, a)
</span></p>
<p>The expected return after being in <span class="math inline">s</span> is the same as the expected return after being in <span class="math inline">s</span> and choosing the optimal action <span class="math inline">a^*</span>, as this is the only action that can be taken. This allows to find the <strong>Bellman optimality equation</strong> for <span class="math inline">V^*</span>:</p>
<p><span class="math display">
    V^* (s)  = \max_{a \in \mathcal{A}(s)} \sum_{s' \in \mathcal{S}}  p(s' | s, a) \, [ r(s, a, s') + \gamma \, V^{*} (s') ]
</span></p>
<p>The same Bellman optimality equation stands for <span class="math inline">Q^*</span>:</p>
<p><span class="math display">
    Q^* (s, a)  = \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [r(s, a, s')  + \gamma \max_{a' \in \mathcal{A}(s')} Q^* (s', a') ]
</span></p>
<p>The optimal value of <span class="math inline">(s, a)</span> depends on the optimal action in the next state <span class="math inline">s'</span>.</p>
<p>The Bellman optimality equations for <span class="math inline">V^*</span> form a system of equations:</p>
<ul>
<li>If there are <span class="math inline">N</span> states <span class="math inline">s</span>, there are <span class="math inline">N</span> Bellman equations with <span class="math inline">N</span> unknowns <span class="math inline">V^*(s)</span>.</li>
</ul>
<p><span class="math display">
    V^* (s)  = \max_{a \in \mathcal{A}(s)} \sum_{s' \in \mathcal{S}}  p(s' | s, a) \, [ r(s, a, s') + \gamma \, V^{*} (s') ]
</span></p>
<p>If the dynamics of the environment are known (<span class="math inline">p(s' | s, a)</span> and <span class="math inline">r(s, a, s')</span>), then in principle one can solve this system of equations using linear algebra. For finite MDPs, the Bellman optimality equation for <span class="math inline">V^*</span> has a unique solution (one and only one): This is the principle of <strong>dynamic programming</strong>.</p>
<p>The same is true for the Bellman optimality equation for <span class="math inline">Q^*</span>: If there are <span class="math inline">N</span> states and <span class="math inline">M</span> actions available, there are <span class="math inline">N\times M</span> equations with <span class="math inline">N\times M</span> unknowns <span class="math inline">Q^*(s, a)</span>.</p>
<p><span class="math display">
    Q^* (s, a)  = \sum_{s' \in \mathcal{S}} p(s' | s, a) \, [r(s, a, s')  + \gamma \max_{a' \in \mathcal{A}(s')} Q^* (s', a') ]
</span></p>
<p><span class="math inline">V^*</span> and <span class="math inline">Q^*</span> are interdependent: one needs only to compute one of them.</p>
<p><span class="math display">V^* (s)  = \max_{a \in \mathcal{A}(s)} \, Q^{*} (s, a)</span></p>
<p><span class="math display">Q^* (s, a)  = \sum_{s' \in \mathcal{S}} \, p(s' | s, a) \, [r(s, a, s') + \gamma V^*(s') ] </span></p>
<p>If you only have <span class="math inline">V^*(s)</span>, you need to perform a <strong>one-step-ahead</strong> search using the dynamics of the MDP:</p>
<p><span class="math display">
    Q^* (s, a)  = \sum_{s' \in \mathcal{S}} \, p(s' | s, a) \, [r(s, a, s') + \gamma V^*(s') ]
</span></p>
<p>and then select the optimal action with the highest <span class="math inline">Q^*</span>-value. Using the <span class="math inline">V^*(s)</span> values is called <strong>model-based</strong>: you need to know the model of the environment to act, at least locally.</p>
<p>If you have all <span class="math inline">Q^*(s, a)</span>, the optimal policy is straightforward:</p>
<p><span class="math display">
    \pi^*(s, a) = \begin{cases}
                1 \; \text{if} \; a = \text{argmax}_a \, Q^*(s, a) \\
                0 \; \text{otherwise.}
                \end{cases}
</span></p>
<p>Finding <span class="math inline">Q^*</span> makes the selection of optimal actions easy:</p>
<ul>
<li>no need to iterate over all actions and to know the dynamics <span class="math inline">p(s' | s, a)</span> and <span class="math inline">r(s, a, s')</span>.</li>
<li>for any state <span class="math inline">s</span>, it can simply find the action that maximizes <span class="math inline">Q^*(s,a)</span>.</li>
</ul>
<p>The action-value function effectively <strong>caches</strong> the results of all one-step-ahead searches into a single value: <strong>model-free</strong>. At the cost of representing a function of all state-action pairs, the optimal action-value function allows optimal actions to be selected without having to know anything about the environment’s dynamics. But there are <span class="math inline">N \times M</span> equations to solve instead of just <span class="math inline">N</span>…</p>
<p>Finding an optimal policy by solving the <strong>Bellman optimality equations</strong> requires the following:</p>
<ul>
<li><p>accurate knowledge of environment dynamics <span class="math inline">p(s' | s, a)</span> and <span class="math inline">r(s, a, s')</span> for all transitions;</p></li>
<li><p>enough memory and time to do the computations;</p></li>
<li><p>the Markov property.</p></li>
</ul>
<p>How much space and time do we need? A solution requires an exhaustive search, looking ahead at all possibilities, computing their probabilities of occurrence and their desirability in terms of expected rewards. The number of states is often huge or astronomical (e.g., Go has about <span class="math inline">10^{170}</span> states). <strong>Dynamic programming</strong> solves exactly the Bellman equations; <strong>Monte-Carlo</strong> and <strong>temporal-difference</strong> methods approximate them.</p>
</section>
</section>
<section id="generalized-policy-iteration" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="generalized-policy-iteration">Generalized Policy Iteration</h2>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="../slides/img/gpi-scheme.png" class="img-fluid figure-img" style="width:30.0%"></p>
<p></p><figcaption class="figure-caption margin-caption">Generalized Policy Iteration. Source: <span class="citation" data-cites="Sutton1998">(<a href="../references.html#ref-Sutton1998" role="doc-biblioref">Sutton and Barto, 1998</a>)</span></figcaption><p></p>
</figure>
</div>
<p>RL algorithms iterate over two steps:</p>
<ol type="1">
<li><strong>Policy evaluation</strong>
<ul>
<li>For a given policy <span class="math inline">\pi</span>, the value of all states <span class="math inline">V^\pi(s)</span> or all state-action pairs <span class="math inline">Q^\pi(s, a)</span> is calculated, either based on:
<ul>
<li>the Bellman equations (Dynamic Programming)</li>
<li>sampled experience (Monte-Carlo and Temporal Difference)</li>
</ul></li>
</ul></li>
<li><strong>Policy improvement</strong>
<ul>
<li>From the current estimated values <span class="math inline">V^\pi(s)</span> or <span class="math inline">Q^\pi(s, a)</span>, a new <strong>better</strong> policy <span class="math inline">\pi</span> is derived.</li>
</ul></li>
</ol>
<p>After enough iterations, the policy converges to the <strong>optimal policy</strong> (if the states are Markov).</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Different notations in RL
</div>
</div>
<div class="callout-body-container callout-body">
<p>Notations can vary depending on the source. The ones used in this course use what you can read in most modern deep RL papers (Deepmind, OpenAI), but beware that you can encounter <span class="math inline">G_t</span> for the return…</p>
<table class="table">
<colgroup>
<col style="width: 23%">
<col style="width: 23%">
<col style="width: 22%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;"><strong>This course</strong></th>
<th style="text-align: center;"><strong>Sutton and Barto 1998</strong></th>
<th style="text-align: center;"><strong>Sutton and Barto 2017</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Current state</td>
<td style="text-align: center;"><span class="math inline">s_t</span></td>
<td style="text-align: center;"><span class="math inline">s_t</span></td>
<td style="text-align: center;"><span class="math inline">S_t</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Selected action</td>
<td style="text-align: center;"><span class="math inline">a_t</span></td>
<td style="text-align: center;"><span class="math inline">a_t</span></td>
<td style="text-align: center;"><span class="math inline">A_t</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Sampled reward</td>
<td style="text-align: center;"><span class="math inline">r_{t+1}</span></td>
<td style="text-align: center;"><span class="math inline">r_{t+1}</span></td>
<td style="text-align: center;"><span class="math inline">R_{t+1}</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Transition probability</td>
<td style="text-align: center;"><span class="math inline">p(s' / s,a)</span></td>
<td style="text-align: center;"><span class="math inline">\mathcal{P}_{ss'}^a</span></td>
<td style="text-align: center;"><span class="math inline">p(s'/s, a)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Expected reward</td>
<td style="text-align: center;"><span class="math inline">r(s,a, s')</span></td>
<td style="text-align: center;"><span class="math inline">\mathcal{R}_{ss'}^a</span></td>
<td style="text-align: center;"><span class="math inline">r(s, a, s')</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Return</td>
<td style="text-align: center;"><span class="math inline">R_t</span></td>
<td style="text-align: center;"><span class="math inline">R_t</span></td>
<td style="text-align: center;"><span class="math inline">G_t</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">State value function</td>
<td style="text-align: center;"><span class="math inline">V^\pi(s)</span></td>
<td style="text-align: center;"><span class="math inline">V^\pi(s)</span></td>
<td style="text-align: center;"><span class="math inline">v_\pi(s)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Action value function</td>
<td style="text-align: center;"><span class="math inline">Q^\pi(s, a)</span></td>
<td style="text-align: center;"><span class="math inline">Q^\pi(s, a)</span></td>
<td style="text-align: center;"><span class="math inline">q_\pi(s, a)</span></td>
</tr>
</tbody>
</table>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-Sutton1998" class="csl-entry" role="doc-biblioentry">
Sutton, R. S., and Barto, A. G. (1998). <em>Reinforcement <span>Learning</span>: <span>An</span> introduction</em>. <span>Cambridge, MA</span>: <span>MIT press</span>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../notes/2.1-Bandits.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Bandits</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/2.3-DP.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">Dynamic Programming</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">Copyright 2022, Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de" class="email">julien.vitay@informatik.tu-chemnitz.de</a></div>
  </div>
</footer>



<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>