<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Deep Reinforcement Learning - Monte-Carlo (MC) methods</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/2.5-TD.html" rel="next">
<link href="../notes/2.3-DP.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notes/2.1-Bandits.html"><strong>Tabular RL</strong></a></li><li class="breadcrumb-item"><a href="../notes/2.4-MC.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Monte-Carlo (MC) methods</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../notes/img/tuc-new.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Deep Reinforcement Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text"><strong>Introduction</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.1-Introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.2-Math.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Math basics (optional)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text"><strong>Tabular RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.1-Bandits.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.2-MDP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Markov Decision Processes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.3-DP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Dynamic Programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.4-MC.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Monte-Carlo (MC) methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.5-TD.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Temporal Difference learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.6-FA.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Function approximation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.7-NN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text"><strong>Model-free RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.1-DQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Q-Learning (DQN)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.2-BeyondDQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Beyond DQN</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.3-PG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Policy gradient (PG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.4-A3C.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Advantage actor-critic (A2C, A3C)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.5-DDPG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Deterministic Policy Gradient (DDPG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.6-PPO.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Natural gradients (TRPO, PPO)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.7-SAC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Maximum Entropy RL (SAC)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text"><strong>Model-based RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.1-MB.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Model-based RL</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.2-LearnedModels.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Learned world models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.3-AlphaGo.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">AlphaGo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.4-SR.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Successor representations</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text"><strong>Outlook</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.1-Outlook.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Outlook</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text"><strong>Exercises</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Content.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">List of exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Installation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Python installation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/1-Python-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction To Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/2-Numpy-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Numpy and Matplotlib</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/3-Sampling-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Sampling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/4-Bandits-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/5-Bandits2-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits - part 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/6-DP-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Dynamic programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/7-Gym-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Gym environments</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/8-MonteCarlo-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Monte-Carlo control</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/9-TD-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Q-learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/10-Eligibilitytraces-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Eligibility traces</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/11-Keras-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Keras tutorial</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/12-DQN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">DQN</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#monte-carlo-policy-evaluation" id="toc-monte-carlo-policy-evaluation" class="nav-link active" data-scroll-target="#monte-carlo-policy-evaluation">Monte-Carlo policy evaluation</a></li>
  <li><a href="#monte-carlo-policy-improvement" id="toc-monte-carlo-policy-improvement" class="nav-link" data-scroll-target="#monte-carlo-policy-improvement">Monte-Carlo policy improvement</a></li>
  <li><a href="#on-policy-monte-carlo-control" id="toc-on-policy-monte-carlo-control" class="nav-link" data-scroll-target="#on-policy-monte-carlo-control">On-policy Monte-Carlo control</a></li>
  <li><a href="#importance-sampling" id="toc-importance-sampling" class="nav-link" data-scroll-target="#importance-sampling">Importance sampling</a></li>
  <li><a href="#off-policy-monte-carlo-control" id="toc-off-policy-monte-carlo-control" class="nav-link" data-scroll-target="#off-policy-monte-carlo-control">Off-policy Monte-Carlo control</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Monte-Carlo (MC) methods</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Slides: <a href="../slides/2.4-MC.html" target="_blank">html</a> <a href="../slides/pdf/2.4-MC.pdf" target="_blank">pdf</a></p>
<section id="monte-carlo-policy-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="monte-carlo-policy-evaluation">Monte-Carlo policy evaluation</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/XD1h9iMxCTQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>The value of a state <span class="math inline">s</span> is defined as the mathematical expectation of the return obtained after that state and thereafter following the policy <span class="math inline">\pi</span>:</p>
<p><span class="math display">V^{\pi} (s) = \mathbb{E}_{\rho_\pi} ( R_t | s_t = s) = \mathbb{E}_{\rho_\pi} ( \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} | s_t = s )</span></p>
<p><strong>Monte-Carlo methods</strong> (MC) approximate this mathematical expectation by <strong>sampling</strong> <span class="math inline">M</span> trajectories <span class="math inline">\tau_i</span> starting from <span class="math inline">s</span> and computing the sampling average of the obtained returns:</p>
<p><span class="math display">
    V^{\pi}(s) = \mathbb{E}_{\rho_\pi} (R_t | s_t = s) \approx \frac{1}{M} \sum_{i=1}^M R(\tau_i)
</span></p>
<p>If you have enough trajectories, the sampling average is an unbiased estimator of the value function. The advantage of Monte-Carlo methods is that they require only <strong>experience</strong>, not the complete dynamics <span class="math inline">p(s' | s,a)</span> and <span class="math inline">r(s, a, s')</span>.</p>
<p>The idea of MC policy evaluation is to repeatedly sample <strong>episodes</strong> starting from each possible state <span class="math inline">s_0</span> and maintain a <strong>running average</strong> of the obtained returns for each state:</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Monte-Carlo policy evaluation of state values
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>while</strong> True:</p>
<ul>
<li><p>Start from an initial state <span class="math inline">s_0</span>.</p></li>
<li><p>Generate a sequence of transitions according to the current policy <span class="math inline">\pi</span> until a terminal state <span class="math inline">s_T</span> is reached.</p></li>
</ul>
<p><span class="math display">
      \tau = (s_o, a_o, r_ 1, s_1, a_1, \ldots, s_T)
  </span></p>
<ul>
<li><p>Compute the return <span class="math inline">R_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1}</span> for all encountered states <span class="math inline">s_0, s_1, \ldots, s_T</span>.</p></li>
<li><p>Update the estimated state value <span class="math inline">V(s_t)</span> of all encountered states using the obtained return:</p></li>
</ul>
<p><span class="math display">
      V(s_t) \leftarrow V(s_t) + \alpha \, (R_t - V(s_t))
  </span></p></li>
</ul>
</div>
</div>
<p>The same method can be used to estimate Q-values.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Online Monte-Carlo policy evaluation of action values
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>while</strong> True:</p>
<ul>
<li><p>Start from an initial state <span class="math inline">s_0</span>.</p></li>
<li><p>Generate a sequence of transitions according to the current policy <span class="math inline">\pi</span> until a terminal state <span class="math inline">s_T</span> is reached.</p></li>
</ul>
<p><span class="math display">
      \tau = (s_o, a_o, r_ 1, s_1, a_1, \ldots, s_T)
  </span></p>
<ul>
<li><p>Compute the return <span class="math inline">R_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1}</span> for all encountered state-action pairs <span class="math inline">(s_0, a_0), (s_1, a_1), \ldots, (s_{T-1}, a_{T-1})</span>.</p></li>
<li><p>Update the estimated action value <span class="math inline">Q(s_t, a_t)</span> of all encountered state-action pairs using the obtained return:</p></li>
</ul>
<p><span class="math display">
      Q(s_t, a_t) = Q(s_t, a_t) + \alpha \, (R_t - Q(s_t, a_t))
  </span></p></li>
</ul>
</div>
</div>
<p>There are much more values to estimate (one per state-action pair), but the policy will be easier to derive.</p>
</section>
<section id="monte-carlo-policy-improvement" class="level2">
<h2 class="anchored" data-anchor-id="monte-carlo-policy-improvement">Monte-Carlo policy improvement</h2>
<p>After each episode, the state or action values of the visited <span class="math inline">(s, a)</span> pairs have changed, so the current policy might not be optimal anymore. As in DP, the policy can then be improved in a greedy manner:</p>
<p><span class="math display">\begin{aligned}
        \pi'(s) &amp; = \text{argmax}_a Q(s, a)\\
        &amp;\\
        &amp; = \text{argmax}_a \sum_{s' \in \mathcal{S}} p(s'|s, a) \, [r(s, a, s') + \gamma \, V(s')] \\
\end{aligned}
</span></p>
<p>Estimating the Q-values allows to act greedily, while estimating the V-values still requires the dynamics <span class="math inline">p(s' | s,a)</span> and <span class="math inline">r(s, a, s')</span>.</p>
</section>
<section id="on-policy-monte-carlo-control" class="level2">
<h2 class="anchored" data-anchor-id="on-policy-monte-carlo-control">On-policy Monte-Carlo control</h2>
<p><strong>Monte-Carlo control</strong> alternates between <strong>MC policy evaluation</strong> and <strong>policy improvement</strong> until the optimal policy is found: generalized policy iteration (GPI).</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Principle of Monte-Carlo control
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>while</strong> True:</p>
<ul>
<li><p>Select an initial state <span class="math inline">s_0</span>.</p></li>
<li><p>Generate a sequence of transitions according to the current policy <span class="math inline">\pi</span> until a terminal state <span class="math inline">s_T</span> is reached.</p></li>
</ul>
<p><span class="math display">
      \tau = (s_o, a_o, r_ 1, s_1, a_1, \ldots, s_T)
  </span></p>
<ul>
<li><p>Compute the return <span class="math inline">R_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1}</span> of all encountered state-action pairs.</p></li>
<li><p>Update the estimated action value <span class="math inline">Q_k(s_t, a_t)</span> of all encountered state-action pairs:</p></li>
</ul>
<p><span class="math display">
      Q(s_t, a_t) = Q(s_t, a_t) + \alpha \, (R_t - Q(s_t, a_t))
  </span></p>
<ul>
<li>For each state <span class="math inline">s_t</span> in the episode, <strong>improve</strong> the policy greedily:</li>
</ul>
<p><span class="math display">
      \pi(s_t, a) = \begin{cases}
                      1\; \text{if} \; a = \text{argmax} \, Q(s_t, a) \\
                      0 \; \text{otherwise.} \\
                    \end{cases}
  </span></p></li>
</ul>
</div>
</div>
<p>The problem with MC control is that we need a policy to generate the sample episodes, but it is that policy that we want to learn. We have the same <strong>exploration/exploitation</strong> problem as in bandits:</p>
<ul>
<li>If I trust my estimates too much (<strong>exploitation</strong>), I may miss more interesting solutions by keeping generating the same episodes.</li>
<li>If I act randomly (<strong>exploration</strong>), I will find more interesting solutions, but I won’t keep doing them.</li>
</ul>
<p><strong>Exploitation</strong> is using the current estimated values to select the greedy action: The estimated values represent how good we think an action is, so we have to use this value to update the policy.</p>
<p><strong>Exploration</strong> is executing non-greedy actions to try to reduce our uncertainty about the true values: The values are only estimates: they may be wrong so we can not trust them completely.</p>
<p>If you only <strong>exploit</strong> your estimates, you may miss interesting solutions. If you only <strong>explore</strong>, you do not use what you know: you act randomly and do not obtain as much reward as you could. <strong>You can’t exploit all the time; you can’t explore all the time.</strong> You can never stop exploring; but you can reduce it if your performance is good enough.</p>
<p>An easy solution to ensure exploration is to assume <strong>exploring starts</strong>, where every state-action pair has a non-zero probability to be selected as the start of an episode.</p>
<p>Exploration can be ensured by forcing the learned policy to be <strong>stochastic</strong>, aka <strong><span class="math inline">\epsilon</span>-soft</strong>.</p>
<ul>
<li><strong><span class="math inline">\epsilon</span>-Greedy action selection</strong> randomly selects non-greedy actions with a small probability <span class="math inline">\epsilon</span>:</li>
</ul>
<p><span class="math display">
    \pi(s, a) = \begin{cases} 1 - \epsilon \; \text{if} \; a = \text{argmax}\, Q(s, a) \\ \frac{\epsilon}{|\mathcal{A}| - 1} \; \text{otherwise.} \end{cases}
</span></p>
<ul>
<li><strong>Softmax action selection</strong> uses a Gibbs (or Boltzmann) distribution to represent the probability of choosing the action <span class="math inline">a</span> in state <span class="math inline">s</span>:</li>
</ul>
<p><span class="math display">
    \pi(s, a) = \frac{\exp Q(s, a) / \tau}{ \sum_b \exp Q(s, b) / \tau}
</span></p>
<p><span class="math inline">\epsilon</span>-greedy choses non-greedy actions randomly, while softmax favors the best alternatives.</p>
<p>In <strong>on-policy</strong> control methods, the learned policy has to be <span class="math inline">\epsilon</span>-soft, which means all actions have a probability of at least <span class="math inline">\frac{\epsilon}{|\mathcal{A}|}</span> to be visited. <span class="math inline">\epsilon</span>-greedy and softmax policies meet this criteria. Each sample episode is generated using this policy, which ensures exploration, while the control method still converges towards the optimal <span class="math inline">\epsilon</span>-policy.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
On-policy Monte-Carlo control
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>while</strong> True:</p>
<ul>
<li><p>Generate an episode <span class="math inline">\tau = (s_0, a_0, r_1, \ldots, s_T)</span> using the current <strong>stochastic</strong> policy <span class="math inline">\pi</span>.</p></li>
<li><p>For each state-action pair <span class="math inline">(s_t, a_t)</span> in the episode, update the estimated Q-value:</p></li>
</ul>
<p><span class="math display">
      Q(s_t, a_t) = Q(s_t, a_t) + \alpha \, (R_t - Q(s_t, a_t))
  </span></p>
<ul>
<li>For each state <span class="math inline">s_t</span> in the episode, improve the policy (e.g.&nbsp;<span class="math inline">\epsilon</span>-greedy):</li>
</ul>
<p><span class="math display">
      \pi(s_t, a) = \begin{cases}
                      1 - \epsilon \; \text{if} \; a = \text{argmax}\, Q(s, a) \\
                      \frac{\epsilon}{|\mathcal{A(s_t)}-1|} \; \text{otherwise.} \\
                    \end{cases}
  </span></p></li>
</ul>
</div>
</div>
<p>Another option to ensure exploration is to generate the sample episodes using a policy <span class="math inline">b(s, a)</span> different from the current policy <span class="math inline">\pi(s, a)</span> of the agent. The <strong>behavior policy</strong> <span class="math inline">b(s, a)</span> used to generate the episodes is only required to select at least occasionally the same actions as the <strong>learned policy</strong> <span class="math inline">\pi(s, a)</span> (coverage assumption):</p>
<p><span class="math display"> \pi(s,a) &gt; 0 \Rightarrow b(s,a) &gt; 0</span></p>
<p>There are mostly two choices regarding the behavior policy:</p>
<ol type="1">
<li><p>An <span class="math inline">\epsilon</span>-soft behavior policy over the <strong>Q-values</strong> as in on-policy MC is often enough, while a deterministic (greedy) policy can be learned implicitly.</p></li>
<li><p>The behavior policy could also come from <strong>expert knowledge</strong>, i.e.&nbsp;known episodes from the MDP generated by somebody else (human demonstrator, classical algorithm).</p></li>
</ol>
<p>But are we mathematically allowed to do this?</p>
</section>
<section id="importance-sampling" class="level2">
<h2 class="anchored" data-anchor-id="importance-sampling">Importance sampling</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/g0qd41d62Gg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>We search for the optimal policy that maximizes in expectation the return of each <strong>trajectory</strong> (episode) possible under the learned policy <span class="math inline">\pi</span>:</p>
<p><span class="math display">\mathcal{J}(\pi) = \mathbb{E}_{\tau \sim \rho_\pi} [R(\tau)]</span></p>
<p><span class="math inline">\rho_\pi</span> denotes the probability distribution of trajectories achievable using the policy <span class="math inline">\pi</span>. If we generate the trajectories from the behavior policy <span class="math inline">b(s, a)</span>, we end up maximizing something else:</p>
<p><span class="math display">\mathcal{J}'(\pi) = \mathbb{E}_{\tau \sim \rho_b} [R(\tau)]</span></p>
<p>The policy that maximizes <span class="math inline">\mathcal{J}'(\pi)</span> is <strong>not</strong> the optimal policy of the MDP.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/importancesampling.svg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">You cannot use samples from one distribution to estimate something about another.</figcaption>
</figure>
</div>
<p>If you try to estimate a parameter of a random distribution <span class="math inline">\pi</span> using samples of another distribution <span class="math inline">b</span>, the sample average will have a strong <strong>bias</strong>. We need to <strong>correct</strong> the samples from <span class="math inline">b</span> in order to be able to estimate the parameters of <span class="math inline">\pi</span> correctly: this is called <strong>importance sampling</strong> (IS).</p>
<p>We want to estimate the expected return of the trajectories generated by the policy <span class="math inline">\pi</span>:</p>
<p><span class="math display">\mathcal{J}(\pi) = \mathbb{E}_{\tau \sim \rho_\pi} [R(\tau)]</span></p>
<p>We start by using the definition of the mathematical expectation:</p>
<p><span class="math display">\mathcal{J}(\pi) = \int_\tau \rho_\pi(\tau) \, R(\tau) \, d\tau</span></p>
<p>The expectation is the integral over all possible trajectories of their return <span class="math inline">R(\tau</span>), weighted by the likelihood <span class="math inline">\rho_\pi(\tau)</span> that a trajectory <span class="math inline">\tau</span> is generated by the policy <span class="math inline">\pi</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/importancesampling2.svg" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">RL maximizes the expected return of the trajectories generated by the policy.</figcaption>
</figure>
</div>
<p>The trick is to introduce the behavior policy <span class="math inline">b</span> in what we want to estimate:</p>
<p><span class="math display">\mathcal{J}(\pi) = \int_\tau \frac{\rho_b(\tau)}{\rho_b(\tau)} \, \rho_\pi(\tau) \, R(\tau) \, d\tau</span></p>
<p><span class="math inline">\rho_b(\tau)</span> is the likelihood that a trajectory <span class="math inline">\tau</span> is generated by the behavior policy <span class="math inline">b</span>. We shuffle a bit the terms:</p>
<p><span class="math display">\mathcal{J}(\pi) = \int_\tau \rho_b(\tau) \, \frac{\rho_\pi(\tau)}{\rho_b(\tau)} \,  R(\tau) \, d\tau</span></p>
<p>and notice that it has the form of an expectation over trajectories generated by <span class="math inline">b</span>:</p>
<p><span class="math display">\mathcal{J}(\pi) = \mathbb{E}_{\tau \sim \rho_b} [\frac{\rho_\pi(\tau)}{\rho_b(\tau)} \, R(\tau)]</span></p>
<p>This means that we can sample trajectories from <span class="math inline">b</span>, but we need to <strong>correct</strong> the observed return by the <strong>importance sampling weight</strong> <span class="math inline">\frac{\rho_\pi(\tau)}{\rho_b(\tau)}</span>.</p>
<p>The importance sampling weight corrects the mismatch between <span class="math inline">\pi</span> and <span class="math inline">b</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/importancesampling3.svg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Importance sampling gives more weight to samples that are likely under the distribution to estimate.</figcaption>
</figure>
</div>
<p>If the two distributions are the same (on-policy), the IS weight is 1, there is no need to correct the returns. If a sample is likely under <span class="math inline">b</span> but not under <span class="math inline">\pi</span>, we should not care about its return: <span class="math inline">\frac{\rho_\pi(\tau)}{\rho_b(\tau)} &lt;&lt; 1</span>. If a sample is likely under <span class="math inline">\pi</span> but not much under <span class="math inline">b</span>, we increase its importance in estimating the return: <span class="math inline">\frac{\rho_\pi(\tau)}{\rho_b(\tau)} &gt;&gt; 1</span>. The sampling average of the corrected samples will be closer from the true estimate (unbiased).</p>
<p>How do we compute these probability distributions <span class="math inline">\rho_\pi(\tau)</span> and <span class="math inline">\rho_b(\tau)</span> for a trajectory <span class="math inline">\tau</span>? A trajectory <span class="math inline">\tau</span> is a sequence of state-action transitions <span class="math inline">(s_0, a_0, s_1, a_1, \ldots, s_T)</span> whose probability depends on:</p>
<ul>
<li>the probability of choosing an action <span class="math inline">a_t</span> in state <span class="math inline">s_t</span>: the <strong>policy</strong> <span class="math inline">\pi(s, a)</span>.</li>
<li>the probability of arriving in the state <span class="math inline">s_{t+1}</span> from the state <span class="math inline">s_t</span> with the action <span class="math inline">a_t</span>: the <strong>transition probability</strong> <span class="math inline">p(s_{t+1} | s_t, a_t)</span>.</li>
</ul>
<p>The <strong>likelihood</strong> of a trajectory <span class="math inline">\tau = (s_0, a_0, s_1, a_1, \ldots, s_T)</span> under a policy <span class="math inline">\pi</span> depends on the policy and the transition probabilities (Markov property):</p>
<p><span class="math display">
    \rho_\pi(\tau) = p_\pi(s_0, a_0, s_1, a_1, \ldots, s_T) = p(s_0) \, \prod_{t=0}^{T-1} \pi_\theta(s_t, a_t) \, p(s_{t+1} | s_t, a_t)
</span></p>
<p><span class="math inline">p(s_0)</span> is the probability of starting an episode in <span class="math inline">s_0</span>, we do not have control over it.</p>
<p>What is interesting is that the transition probabilities disappear when calculating the <strong>importance sampling weight</strong>:</p>
<p><span class="math display">
    \rho_{0:T-1} = \frac{\rho_\pi(\tau)}{\rho_b(\tau)} = \frac{p_0 (s_0) \, \prod_{t=0}^{T-1} \pi(s_t, a_t) p(s_{t+1} | s_t, a_t)}{p_0 (s_0) \, \prod_{t=0}^T b(s_t, a_t) p(s_{t+1} | s_t, a_t)} = \frac{\prod_{t=0}^{T-1} \pi(s_t, a_t)}{\prod_{t=0}^T b(s_t, a_t)} = \prod_{t=0}^{T-1} \frac{\pi(s_t, a_t)}{b(s_t, a_t)}
</span></p>
<p>The importance sampling weight is simply the product over the length of the episode of the ratio between <span class="math inline">\pi(s_t, a_t)</span> and <span class="math inline">b(s_t, a_t)</span>.</p>
</section>
<section id="off-policy-monte-carlo-control" class="level2">
<h2 class="anchored" data-anchor-id="off-policy-monte-carlo-control">Off-policy Monte-Carlo control</h2>
<p>In <strong>off-policy MC control</strong>, we generate episodes using the behavior policy <span class="math inline">b</span> and update <strong>greedily</strong> the learned policy <span class="math inline">\pi</span>. For the state <span class="math inline">s_t</span>, the obtained returns just need to be weighted by the relative probability of occurrence of the <strong>rest of the episode</strong> following the policies <span class="math inline">\pi</span> and <span class="math inline">b</span>:</p>
<p><span class="math display">\rho_{t:T-1} = \prod_{k=t}^{T-1} \frac{\pi(s_k, a_k)}{b(s_k, a_k)}</span></p>
<p><span class="math display">V^\pi(s_t) = \mathbb{E}_{\tau \sim \rho_b} [\rho_{t:T-1} \, R_t]</span></p>
<p>This gives us the updates:</p>
<p><span class="math display">
    V(s_t) = V(s_t) + \alpha  \, \rho_{t:T-1} \, (R_t - V(s_t))
</span></p>
<p>and:</p>
<p><span class="math display">
    Q(s_t, a_t) = Q(s_t, a_t) + \alpha  \, \rho_{t:T-1} \, (R_t - Q(s_t, a_t))
</span></p>
<p>Unlikely episodes under <span class="math inline">\pi</span> are barely used for learning, likely ones are used a lot.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Off-policy Monte-Carlo control
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>while</strong> True:</p>
<ul>
<li><p>Generate an episode <span class="math inline">\tau = (s_0, a_0, r_1, \ldots, s_T)</span> using the <strong>behavior</strong> policy <span class="math inline">b</span>.</p></li>
<li><p>For each state-action pair <span class="math inline">(s_t, a_t)</span> in the episode, update the estimated Q-value:</p></li>
</ul>
<p><span class="math display">\rho_{t:T-1} = \prod_{k=t}^{T-1} \frac{\pi(s_k, a_k)}{b(s_k, a_k)}</span></p>
<p><span class="math display">
      Q(s_t, a_t) = Q(s_t, a_t) + \alpha  \, \rho_{t:T-1} \, (R_t - Q(s_t, a_t))
  </span></p>
<ul>
<li>For each state <span class="math inline">s_t</span> in the episode, update the <strong>learned</strong> deterministic policy (greedy):</li>
</ul>
<p><span class="math display">
      \pi(s_t, a) = \begin{cases}
                      1\; \text{if} \; a = \text{argmax} \, Q(s_t, a) \\
                      0 \; \text{otherwise.} \\
                    \end{cases}
  </span></p></li>
</ul>
</div>
</div>
<ul>
<li><strong>Problem 1:</strong> if the learned policy is greedy, the IS weight becomes quickly 0 for a non-greedy action <span class="math inline">a_t</span>:</li>
</ul>
<p><span class="math display">\pi(s_t, a_t) = 0 \rightarrow \rho_{0:T-1} = \prod_{k=0}^{T-1} \frac{\pi(s_k, a_k)}{b(s_k, a_k)} = 0</span></p>
<p>Off-policy MC control only learns from the last greedy actions, what is slow at the beginning.</p>
<p><strong>Solution:</strong> <span class="math inline">\pi</span> and <span class="math inline">b</span> should not be very different. Usually <span class="math inline">\pi</span> is greedy and <span class="math inline">b</span> is a softmax (or <span class="math inline">\epsilon</span>-greedy) over it.</p>
<ul>
<li><strong>Problem 2:</strong> if the learned policy is stochastic, the IS weights can quickly <strong>vanish</strong> to 0 or <strong>explode</strong> to infinity:</li>
</ul>
<p><span class="math display">\rho_{t:T-1} = \prod_{k=t}^{T-1} \frac{\pi(s_k, a_k)}{b(s_k, a_k)}</span></p>
<p>If <span class="math inline">\dfrac{\pi(s_k, a_k)}{b(s_k, a_k)}</span> is smaller than 1, the products go to 0. If it is bigger than 1, it grows to infinity.</p>
<p><strong>Solution:</strong> one can normalize the IS weight between different episodes (see Sutton and Barto) or <strong>clip</strong> it (e.g.&nbsp;restrict it to [0.9, 1.1], see PPO later in this course).</p>
<p>The main advantage of <strong>off-policy</strong> strategies is that you can learn from other’s actions, you don’t have to rely on your initially wrong policies to discover the solution by chance. Example: learning to play chess by studying thousands/millions of plays by chess masters. In a given state, only a subset of the possible actions are actually executed by experts: the others may be too obviously wrong. The exploration is then guided by this expert knowledge, not randomly among all possible actions.</p>
<p>Off-policy methods greatly reduce the number of transitions needed to learn a policy: very stupid actions are not even considered, but the estimation policy learns an optimal strategy from the “classical” moves. Drawback: if a good move is not explored by the behavior policy, the learned policy will never try it.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Properties of Monte-Carlo methods
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Monte-Carlo evaluation estimates value functions via <strong>sampling</strong> of entire episodes.</p></li>
<li><p>Monte-Carlo control (evaluation-improvement) is a generalized policy iteration method.</p></li>
<li><p>MC for action values is <strong>model-free</strong>: you do not need to know <span class="math inline">p(s' | s, a)</span> to learn the optimal policy, you just sample transitions (trial and error).</p></li>
<li><p>MC only applies to <strong>episodic tasks</strong>: as you learn at the end of an episode, it is not possible to learn continuing tasks.</p></li>
<li><p>MC suffers from the <strong>exploration-exploitation</strong> problem:</p>
<ul>
<li><p><strong>on-policy</strong> MC learns a stochastic policy (<span class="math inline">\epsilon</span>-greedy, softmax) to ensure exploration.</p></li>
<li><p><strong>off-policy</strong> MC learns a greedy policy, but explores via a behavior policy (importance sampling).</p></li>
</ul></li>
<li><p>Monte-Carlo methods have:</p>
<ul>
<li><p>a <strong>small bias</strong>: with enough sampled episodes, the estimated values converge to the true values.</p></li>
<li><p>a <strong>huge variance</strong>: the slightest change of the policy can completely change the episode and its return. You will need a lot of samples to form correct estimates: <strong>sample complexity</strong>.</p></li>
</ul></li>
</ul>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../notes/2.3-DP.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Dynamic Programming</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/2.5-TD.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">Temporal Difference learning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">Copyright Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de" class="email">julien.vitay@informatik.tu-chemnitz.de</a></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>