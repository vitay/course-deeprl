<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Deep Reinforcement Learning - Temporal Difference learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/2.6-FA.html" rel="next">
<link href="../notes/2.4-MC.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notes/2.1-Bandits.html"><strong>Tabular RL</strong></a></li><li class="breadcrumb-item"><a href="../notes/2.5-TD.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Temporal Difference learning</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../notes/img/tuc-new.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Deep Reinforcement Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text"><strong>Introduction</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.1-Introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.2-Math.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Math basics (optional)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text"><strong>Tabular RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.1-Bandits.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.2-MDP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Markov Decision Processes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.3-DP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Dynamic Programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.4-MC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Monte-Carlo (MC) methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.5-TD.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Temporal Difference learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.6-FA.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Function approximation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.7-NN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text"><strong>Model-free RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.1-DQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Q-Learning (DQN)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.2-BeyondDQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Beyond DQN</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.3-PG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Policy gradient (PG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.4-A3C.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Advantage actor-critic (A2C, A3C)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.5-DDPG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Deterministic Policy Gradient (DDPG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.6-PPO.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Natural gradients (TRPO, PPO)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.7-SAC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Maximum Entropy RL (SAC)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text"><strong>Model-based RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.1-MB.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Model-based RL</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.2-LearnedModels.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Learned world models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.3-AlphaGo.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">AlphaGo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.4-SR.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Successor representations</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text"><strong>Outlook</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.1-Outlook.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Outlook</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text"><strong>Exercises</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Content.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">List of exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Installation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Python installation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/1-Python-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction To Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/2-Numpy-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Numpy and Matplotlib</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/3-Sampling-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Sampling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/4-Bandits-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/5-Bandits2-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits - part 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/6-DP-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Dynamic programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/7-Gym-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Gym environments</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/8-MonteCarlo-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Monte-Carlo control</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/9-TD-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Q-learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/10-Eligibilitytraces-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Eligibility traces</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/11-Keras-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Keras tutorial</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/12-DQN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">DQN</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#temporal-difference-algorithms" id="toc-temporal-difference-algorithms" class="nav-link active" data-scroll-target="#temporal-difference-algorithms">Temporal Difference algorithms</a>
  <ul class="collapse">
  <li><a href="#sarsa-on-policy-td-control" id="toc-sarsa-on-policy-td-control" class="nav-link" data-scroll-target="#sarsa-on-policy-td-control">SARSA: On-policy TD control</a></li>
  <li><a href="#q-learning-off-policy-td-control" id="toc-q-learning-off-policy-td-control" class="nav-link" data-scroll-target="#q-learning-off-policy-td-control">Q-learning: Off-policy TD control</a></li>
  </ul></li>
  <li><a href="#actor-critic-methods" id="toc-actor-critic-methods" class="nav-link" data-scroll-target="#actor-critic-methods">Actor-critic methods</a></li>
  <li><a href="#eligibility-traces-and-advantage-estimation" id="toc-eligibility-traces-and-advantage-estimation" class="nav-link" data-scroll-target="#eligibility-traces-and-advantage-estimation">Eligibility traces and advantage estimation</a>
  <ul class="collapse">
  <li><a href="#n-step-returns" id="toc-n-step-returns" class="nav-link" data-scroll-target="#n-step-returns">n-step returns</a></li>
  <li><a href="#eligibility-traces" id="toc-eligibility-traces" class="nav-link" data-scroll-target="#eligibility-traces">Eligibility traces</a></li>
  <li><a href="#generalized-advantage-estimation-gae" id="toc-generalized-advantage-estimation-gae" class="nav-link" data-scroll-target="#generalized-advantage-estimation-gae">Generalized advantage estimation (GAE)</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Temporal Difference learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Slides: <a href="../slides/2.5-TD.html" target="_blank">html</a> <a href="../slides/pdf/2.5-TD.pdf" target="_blank">pdf</a></p>
<section id="temporal-difference-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="temporal-difference-algorithms">Temporal Difference algorithms</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/XLB98ZFsy8w" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>MC methods wait until the end of the episode to compute the obtained return, and update the estimates of all encountered states:</p>
<p><span class="math display">
    V(s_t) = V(s_t) + \alpha (R_t - V(s_t))
</span></p>
<p>If the episode is very long, learning might be very slow. If the task is continuing, it is impossible. Considering that the return at time <span class="math inline">t</span> is the immediate reward plus the return in the next step:</p>
<p><span class="math display">
    R_t = r_{t+1} + \gamma \,  R_{t+1}
</span></p>
<p>we could replace <span class="math inline">R_{t+1}</span> by an estimate, which is the value of the next state <span class="math inline">V^\pi(s_{t+1}) = \mathbb{E}_\pi [R_{t+1} | s_{t+1}=s]</span>:</p>
<p><span class="math display">R_t \approx r_{t+1} + \gamma \,  V^\pi(s_{t+1})</span></p>
<p><strong>Temporal-Difference (TD)</strong> methods simply replace the actual return by an estimation in the update rule:</p>
<p><span class="math display">
    V(s_t) = V(s_t) + \alpha \, (r_{t+1} + \gamma \, V(s_{t+1}) - V(s_t))
</span></p>
<p>where <span class="math inline">r_{t+1} + \gamma\, V(s_{t+1})</span> is a sampled estimate of the return.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/MCTD.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="figure-caption">TD replaces <span class="math inline">R_{t+1}</span> with an estimate <span class="math inline">V(s_{t+1})</span>. Adapted from <span class="citation" data-cites="Sutton1998">(<a href="../references.html#ref-Sutton1998" role="doc-biblioref">Sutton and Barto, 1998</a>)</span>.</figcaption>
</figure>
</div>
<p>The quantity</p>
<p><span class="math display">\delta_t = r_{t+1} + \gamma \, V(s_{t+1}) - V(s_t)</span></p>
<p>is called equivalently the <strong>reward prediction error</strong> (RPE), the <strong>TD error</strong> or the <strong>advantage</strong> of the action <span class="math inline">a_t</span>. It is the difference between the estimated return in state <span class="math inline">s_t</span> <span class="math inline">V(s_t)</span> and the actual return <span class="math inline">r_{t+1} + \gamma \, V(s_{t+1})</span>, computed with an estimation.</p>
<p>If <span class="math inline">\delta_t &gt; 0</span>, it means that we received more reward <span class="math inline">r_{t+1}</span> than expected, or that we arrived in a state <span class="math inline">s_{t+1}</span> that is better than expected: we should increase the value of <span class="math inline">s_t</span> as we <strong>underestimate</strong> it. If <span class="math inline">\delta_t &lt; 0</span>, we should decrease the value of <span class="math inline">s_t</span> as we <strong>overestimate</strong> it.</p>
<p>The learning procedure in TD is then possible after each transition: the backup diagram is limited to only one state and its follower.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TD(0) policy evaluation
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>while</strong> True:</p>
<ul>
<li><p>Start from an initial state <span class="math inline">s_0</span>.</p></li>
<li><p><strong>foreach</strong> step <span class="math inline">t</span> of the episode:</p>
<ul>
<li><p>Select <span class="math inline">a_t</span> using the current policy <span class="math inline">\pi</span> in state <span class="math inline">s_t</span>.</p></li>
<li><p>Apply <span class="math inline">a_t</span>, observe <span class="math inline">r_{t+1}</span> and <span class="math inline">s_{t+1}</span>.</p></li>
<li><p>Compute the TD error:</p></li>
</ul>
<p><span class="math display">\delta_t = r_{t+1} + \gamma \, V(s_{t+1}) - V(s_t)</span></p>
<ul>
<li>Update the state-value function of <span class="math inline">s_t</span>:</li>
</ul>
<p><span class="math display">
      V(s_t) = V(s_t) + \alpha \, \delta_t
  </span></p>
<ul>
<li><strong>if</strong> <span class="math inline">s_{t+1}</span> is terminal: <strong>break</strong></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<p>TD learns from experience in a fully incremental manner. It does not need to wait until the end of an episode. It is therefore possible to learn continuing tasks. TD converges to <span class="math inline">V^{\pi}</span> if the step-size parameter <span class="math inline">\alpha</span> is small enough.</p>
<p>The <strong>TD error</strong> is used to evaluate the policy:</p>
<p><span class="math display">
    V(s_t) = V(s_t) + \alpha \, (r_{t+1} + \gamma \, V(s_{t+1}) - V(s_t)) = V(s_t) + \alpha \, \delta_t
</span></p>
<p>The estimates converge to:</p>
<p><span class="math display">V^\pi(s) = \mathbb{E}_\pi [r(s, a, s') + \gamma \, V^\pi(s')]</span></p>
<p>By using an <strong>estimate of the return</strong> <span class="math inline">R_t</span> instead of directly the return as in MC, we <strong>increase the bias</strong> (estimates are always wrong, especially at the beginning of learning) but we <strong>reduce the variance</strong>: only <span class="math inline">r(s, a, s')</span> is stochastic, not the value function <span class="math inline">V^\pi</span>. We can therefore expect <strong>less optimal solutions</strong>, but we will also need <strong>less samples</strong>: better <strong>sample efficiency</strong> than MC but worse <strong>convergence</strong> (suboptimal).</p>
<p>Q-values can be estimated in the same way:</p>
<p><span class="math display">
    Q(s_t, a_t) = Q(s_t, a_t) + \alpha \, (r_{t+1} + \gamma \, Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))
</span></p>
<p>Like for MC, the exploration/exploitation trade-off has to be managed: what is the next action <span class="math inline">a_{t+1}</span>? There are therefore two classes of TD control algorithms: <strong>on-policy</strong> (SARSA) and <strong>off-policy</strong> (Q-learning).</p>
<section id="sarsa-on-policy-td-control" class="level3">
<h3 class="anchored" data-anchor-id="sarsa-on-policy-td-control">SARSA: On-policy TD control</h3>
<p><strong>SARSA</strong> (state-action-reward-state-action) updates the value of a state-action pair by using the predicted value of the next state-action pair according to the current policy.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/sarsa-sequence.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">State-action-reward-state-action transitions. Source: <span class="citation" data-cites="Sutton1998">(<a href="../references.html#ref-Sutton1998" role="doc-biblioref">Sutton and Barto, 1998</a>)</span>.</figcaption>
</figure>
</div>
<p>When arriving in <span class="math inline">s_{t+1}</span> from <span class="math inline">(s_t, a_t)</span>, we already sample the next action:</p>
<p><span class="math display">a_{t+1} \sim \pi(s_{t+1}, a)</span></p>
<p>We can now update the value of <span class="math inline">(s_t, a_t)</span>:</p>
<p><span class="math display">
    Q(s_t, a_t) = Q(s_t, a_t) + \alpha \, (r_{t+1} + \gamma \, Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))
</span></p>
<p>The next action <span class="math inline">a_{t+1}</span> will <strong>have to</strong> be executed next: SARSA is <strong>on-policy</strong>. You cannot change your mind and execute another <span class="math inline">a_{t+1}</span>. The learned policy must be <span class="math inline">\epsilon</span>-soft (stochastic) to ensure exploration. SARSA converges to the optimal policy if <span class="math inline">\alpha</span> is small enough and if <span class="math inline">\epsilon</span> (or <span class="math inline">\tau</span>) slowly decreases to 0.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
SARSA: On-policy TD control
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>while</strong> True:</p>
<ul>
<li><p>Start from an initial state <span class="math inline">s_0</span> and select <span class="math inline">a_0</span> using the current policy <span class="math inline">\pi</span>.</p></li>
<li><p><strong>foreach</strong> step <span class="math inline">t</span> of the episode:</p>
<ul>
<li><p>Apply <span class="math inline">a_{t}</span>, observe <span class="math inline">r_{t+1}</span> and <span class="math inline">s_{t+1}</span>.</p></li>
<li><p>Select <span class="math inline">a_{t+1}</span> using the current <strong>stochastic</strong> policy <span class="math inline">\pi</span>.</p></li>
<li><p>Update the action-value function of <span class="math inline">(s_t, a_t)</span>:</p></li>
</ul>
<p><span class="math display"> Q(s_t, a_t) = Q(s_t, a_t) + \alpha \, (r_{t+1} + \gamma \, Q(s_{t+1}, a_{t+1})  - Q(s_t, a_t)) </span></p>
<ul>
<li>Improve the stochastic policy, e.g:</li>
</ul>
<p><span class="math display">
      \pi(s_t, a) = \begin{cases}
                      1 - \epsilon \; \text{if} \; a = \text{argmax} \, Q(s_t, a) \\
                      \frac{\epsilon}{|\mathcal{A}(s_t) -1|} \; \text{otherwise.} \\
                    \end{cases}
  </span></p>
<ul>
<li><strong>if</strong> <span class="math inline">s_{t+1}</span> is terminal: <strong>break</strong></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</section>
<section id="q-learning-off-policy-td-control" class="level3">
<h3 class="anchored" data-anchor-id="q-learning-off-policy-td-control">Q-learning: Off-policy TD control</h3>
<p><strong>Q-learning</strong> directly approximates the optimal action-value function <span class="math inline">Q^*</span> independently of the current policy, using the greedy action in the next state.</p>
<p><span class="math display">Q(s_t, a_t) = Q(s_t, a_t) + \alpha \, (r_{t+1} + \gamma \, \max_a Q(s_{t+1}, a) - Q(s_t, a_t))</span></p>
<p>The next action <span class="math inline">a_{t+1}</span> can be generated by a behavior policy: Q-learning is <strong>off-policy</strong>, but the learned policy can be deterministic. The behavior policy can be an <span class="math inline">\epsilon</span>-soft policy derived from <span class="math inline">Q</span> or expert knowledge. The behavior policy only needs to visit all state-action pairs during learning to ensure optimality.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Q-learning: Off-policy TD control
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>while</strong> True:</p>
<ul>
<li><p>Start from an initial state <span class="math inline">s_0</span>.</p></li>
<li><p><strong>foreach</strong> step <span class="math inline">t</span> of the episode:</p>
<ul>
<li><p>Select <span class="math inline">a_{t}</span> using the behavior policy <span class="math inline">b</span> (e.g.&nbsp;derived from <span class="math inline">\pi</span>).</p></li>
<li><p>Apply <span class="math inline">a_t</span>, observe <span class="math inline">r_{t+1}</span> and <span class="math inline">s_{t+1}</span>.</p></li>
<li><p>Update the action-value function of <span class="math inline">(s_t, a_t)</span>:</p></li>
</ul>
<p><span class="math display">Q(s_t, a_t) = Q(s_t, a_t) + \alpha \, (r_{t+1} + \gamma \, \max_a Q(s_{t+1}, a) - Q(s_t, a_t))</span></p>
<ul>
<li>Improve greedily the learned policy:</li>
</ul>
<p><span class="math display">\pi(s_t, a) = \begin{cases}
                  1\; \text{if} \; a = \text{argmax} \, Q(s_t, a) \\
                  0 \; \text{otherwise.} \\
                \end{cases}
  </span></p>
<ul>
<li><strong>if</strong> <span class="math inline">s_{t+1}</span> is terminal: <strong>break</strong></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<p>In off-policy Monte-Carlo, Q-values are estimated using the return of the rest of the episode on average:</p>
<p><span class="math display">Q^\pi(s, a) = \mathbb{E}_{\tau \sim \rho_b}[\rho_{0:T-1} \, R(\tau) | s_0 = s, a_0=a]</span></p>
<p>As the rest of the episode is generated by <span class="math inline">b</span>, we need to correct the returns using the importance sampling weight. In Q-learning, Q-values are estimated using other estimates:</p>
<p><span class="math display">Q^\pi(s, a) = \mathbb{E}_{s_t \sim \rho_b, a_t \sim b}[ r_{t+1} + \gamma \, \max_a Q^\pi(s_{t+1}, a) | s_t = s, a_t=a]</span></p>
<p>As we only sample <strong>transitions</strong> using <span class="math inline">b</span> and not episodes, there is no need to correct the returns: the returns use estimates <span class="math inline">Q^\pi</span>, which depend on <span class="math inline">\pi</span> and not <span class="math inline">b</span>. The immediate reward <span class="math inline">r_{t+1}</span> is stochastic, but is the same whether you sample <span class="math inline">a_t</span> from <span class="math inline">\pi</span> or from <span class="math inline">b</span>.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Temporal Difference learning
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>Temporal Difference</strong> allow to learn Q-values from single transitions instead of complete episodes.</p></li>
<li><p>MC methods can only be applied to episodic problems, while TD works for continuing tasks.</p></li>
<li><p>MC and TD methods are <strong>model-free</strong>: you do not need to know anything about the environment (<span class="math inline">p(s' |s, a)</span> and <span class="math inline">r(s, a, s')</span>) to learn.</p></li>
<li><p>The <strong>exploration-exploitation</strong> dilemma must be dealt with:</p>
<ul>
<li><strong>On-policy</strong> TD (SARSA) follows the learned stochastic policy.</li>
</ul>
<p><span class="math display">
      Q(s, a) = Q(s, a) + \alpha \, (r(s, a, s') + \gamma \, Q(s', a') - Q(s, a))
  </span></p>
<ul>
<li><strong>Off-policy</strong> TD (Q-learning) follows a behavior policy and learns a deterministic policy.</li>
</ul>
<p><span class="math display">
      Q(s, a) = Q(s, a) + \alpha \, (r(s, a, s') + \gamma \, \max_a Q(s', a) - Q(s, a))
  </span></p></li>
<li><p>TD uses <strong>bootstrapping</strong> like DP: it uses other estimates to update one estimate.</p></li>
<li><p>Q-learning is the go-to method in tabular RL.</p></li>
</ul>
</div>
</div>
</section>
</section>
<section id="actor-critic-methods" class="level2">
<h2 class="anchored" data-anchor-id="actor-critic-methods">Actor-critic methods</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/6IyZ3BGmuSI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>Actor-critic methods are TD methods that have a separate memory structure to explicitly represent the policy independent of the value function. The policy <span class="math inline">\pi</span> is implemented by the <strong>actor</strong>, because it is used to select actions. The estimated values <span class="math inline">V(s)</span> are implemented by the <strong>critic</strong>, because it criticizes the actions made by the actor. Learning is always <strong>on-policy</strong>: the critic must learn about and critique whatever policy is currently being followed by the actor.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/actorcritic.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="figure-caption">Actor-critic architecture. Source: <span class="citation" data-cites="Sutton1998">(<a href="../references.html#ref-Sutton1998" role="doc-biblioref">Sutton and Barto, 1998</a>)</span>.</figcaption>
</figure>
</div>
<p>The critic computes the <strong>TD error</strong> or <strong>1-step advantage</strong> after each transition <span class="math inline">(s_t, a_t, r_{t+1}, s_{t+1})</span>:</p>
<p><span class="math display">\delta_t = r_{t+1} + \gamma \, V(s_{t+1}) - V(s_t)</span></p>
<p>This scalar signal is the output of the critic and drives learning in both the actor and the critic. It tells us how good the action <span class="math inline">a_t</span> was compared to our expectation <span class="math inline">V(s_t)</span>.</p>
<p>When the advantage <span class="math inline">\delta_t &gt; 0</span>, this means that the action lead to a better reward or a better state than what was expected by <span class="math inline">V(s_t)</span>, which is a <strong>good surprise</strong>, so the action should be reinforced (selected again) and the value of that state increased.</p>
<p>When <span class="math inline">\delta_t &lt; 0</span>, this means that the previous estimation of <span class="math inline">(s_t, a_t)</span> was too high (<strong>bad surprise</strong>), so the action should be avoided in the future and the value of the state reduced.</p>
<p>The critic is updated using this scalar signal:</p>
<p><span class="math display"> V(s_t) \leftarrow V(s_t) + \alpha \, \delta_t</span></p>
<p>The actor is updated according to this TD error signal. For example a softmax actor over preferences:</p>
<p><span class="math display">p(s_t, a_t) \leftarrow p(s_t, a_t) + \beta \, \delta_t</span></p>
<p><span class="math display">\pi(s, a) = \frac{\exp{p(s, a)}}{\sum_b \exp{p(s, b)}}</span></p>
<p>When <span class="math inline">\delta_t &gt;0</span>, the preference is increased, so the probability of selecting it again increases. When <span class="math inline">\delta_t &lt;0</span>, the preference is decreased, so the probability of selecting it again decreases. This is the equivalent of <strong>reinforcement comparison</strong> for bandits.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Actor-critic algorithm with preferences
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Start in <span class="math inline">s_0</span>. Initialize the preferences <span class="math inline">p(s,a)</span> for each state action pair and the critic <span class="math inline">V(s)</span> for each state.</p></li>
<li><p><strong>foreach</strong> step <span class="math inline">t</span>:</p>
<ul>
<li>Select <span class="math inline">a_t</span> using the <strong>actor</strong> <span class="math inline">\pi</span> in state <span class="math inline">s_t</span>:</li>
</ul>
<p><span class="math display">\pi(s_t, a) = \frac{\exp{p(s, a)}}{\sum_b \exp{p(s, b)}}</span></p>
<ul>
<li><p>Apply <span class="math inline">a_t</span>, observe <span class="math inline">r_{t+1}</span> and <span class="math inline">s_{t+1}</span>.</p></li>
<li><p>Compute the TD error in <span class="math inline">s_t</span> using the <strong>critic</strong>:</p></li>
</ul>
<p><span class="math display">
      \delta_t = r_{t+1} + \gamma \, V(s_{t+1}) - V(s_t)
  </span></p>
<ul>
<li>Update the <strong>actor</strong>:</li>
</ul>
<p><span class="math display">
      p(s_t, a_t) \leftarrow p(s_t, a_t) + \beta \, \delta_t
  </span></p>
<ul>
<li>Update the <strong>critic</strong>:</li>
</ul>
<p><span class="math display">
      V(s_t) \leftarrow V(s_t) + \alpha \, \delta_t
  </span></p></li>
</ul>
</div>
</div>
<p>The advantage of the separation between the actor and the critic is that now the actor can take any form (preferences, linear approximation, deep networks). It requires minimal computation in order to select the actions, in particular when the action space is huge or even continuous. It can learn stochastic policies, which is particularly useful in non-Markov problems.</p>
<p><strong>It is obligatory to learn on-policy:</strong> the critic must evaluate the actions taken by the current actor and the actor must learn from the current critic, not “old” V-values.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Value-based vs.&nbsp;policy-based algorithms
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>Value-based</strong> methods use value estimates <span class="math inline">Q_t(s, a)</span> to infer a policy:</p>
<ul>
<li><p><strong>On-policy</strong> methods learn and use a stochastic policy to explore.</p></li>
<li><p><strong>Off-policy</strong> methods learn a deterministic policy but use a (stochastic) behavior policy to explore.</p></li>
</ul></li>
<li><p><strong>Policy-based</strong> methods directly learn the policy <span class="math inline">\pi_t(s, a)</span> (<strong>actor</strong>) using preferences or function approximators.</p>
<ul>
<li><p>A <strong>critic</strong> learning values is used to improve the policy w.r.t a performance baseline.</p></li>
<li><p>Actor-critic architectures are strictly <strong>on-policy</strong>.</p></li>
</ul></li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">Bandits</th>
<th style="text-align: center;">MDP</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>Value-based</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\qquad</span>On-policy</td>
<td style="text-align: center;"><span class="math inline">\epsilon</span>-greedy, softmax</td>
<td style="text-align: center;">SARSA</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\qquad</span>Off-policy</td>
<td style="text-align: center;">greedy</td>
<td style="text-align: center;">Q-learning</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Policy-based</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\qquad</span>On-policy</td>
<td style="text-align: center;">Reinforcement comparison</td>
<td style="text-align: center;">Actor-critic</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="eligibility-traces-and-advantage-estimation" class="level2">
<h2 class="anchored" data-anchor-id="eligibility-traces-and-advantage-estimation">Eligibility traces and advantage estimation</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/Coe9U4bv-nI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>MC has <strong>high variance, zero bias</strong>: it has good convergence properties and we are more likely to find the optimal policy. It is not very sensitive to initial estimates, and very simple to understand and use.</p>
<p>TD has <strong>low variance, some bias</strong>, so it is usually more <strong>sample efficient</strong> than MC. TD(0) converges to <span class="math inline">V^\pi(s)</span> (but not always with function approximation). The policy might be suboptimal. It is more sensitive to initial values (bootstrapping).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/gridworld-lambda.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Gridworld environment. Source: <span class="citation" data-cites="Sutton1998">(<a href="../references.html#ref-Sutton1998" role="doc-biblioref">Sutton and Barto, 1998</a>)</span>.</figcaption>
</figure>
</div>
<p>When the reward function is sparse (e.g.&nbsp;only at the end of a game), only the last action, leading to that reward, will be updated the first time in TD.</p>
<p><span class="math display">
    Q(s, a) = Q(s, a) + \alpha \, (r(s, a, s') + \gamma \, \max_a Q(s', a) - Q(s, a))
</span></p>
<p>The previous actions, which were equally important in obtaining the reward, will only be updated the next time they are visited. This makes learning very slow: if the path to the reward has <span class="math inline">n</span> steps, you will need to repeat the same episode at least <span class="math inline">n</span> times to learn the Q-value of the first action.</p>
<section id="n-step-returns" class="level3">
<h3 class="anchored" data-anchor-id="n-step-returns">n-step returns</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/nstep.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">n-step returns. Source: <span class="citation" data-cites="Sutton1998">(<a href="../references.html#ref-Sutton1998" role="doc-biblioref">Sutton and Barto, 1998</a>)</span>.</figcaption>
</figure>
</div>
<p>Optimally, we would like a trade-off between:</p>
<ul>
<li>TD (only one state/action is updated each time, small variance but significant bias)</li>
<li>Monte-Carlo (all states/actions in an episode are updated, no bias but huge variance).</li>
</ul>
<p>In <strong>n-step TD prediction</strong>, the next <span class="math inline">n</span> rewards are used to estimate the return, the rest is approximated. The <strong>n-step return</strong> is the discounted sum of the <span class="math inline">n</span> next rewards is computed as in MC plus the predicted value at step <span class="math inline">t+n</span> which replaces the rest as in TD.</p>
<p><span class="math display">
    R^n_t = \sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \,  V(s_{t+n})
</span></p>
<p>We can update the value of the state with this n-step return:</p>
<p><span class="math display">
    V(s_t) = V(s_t) + \alpha \, (R^n_t - V (s_t))
</span></p>
<p>The <strong>n-step advantage</strong> at time <span class="math inline">t</span> is:</p>
<p><span class="math display">
A^n_t = \sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \,  V(s_{t+n}) - V (s_t)
</span></p>
<p>It is easy to check that the <strong>TD error</strong> is the 1-step advantage:</p>
<p><span class="math display">
    \delta_t = A^1_t = r_{t+1} + \gamma \, V(s_{t+1}) - V(s_t)
</span></p>
<p>As you use more “real” rewards, you <strong>reduce the bias</strong> of Q-learning. As you use estimates for the rest of the episode, you <strong>reduce the variance</strong> of MC methods. But how to choose <span class="math inline">n</span>?</p>
</section>
<section id="eligibility-traces" class="level3">
<h3 class="anchored" data-anchor-id="eligibility-traces">Eligibility traces</h3>
<p>One solution is to <strong>average</strong> the n-step returns, using a discount factor <span class="math inline">\lambda</span> :</p>
<p><span class="math display">R^\lambda_t = (1  - \lambda) \, \sum_{n=1}^\infty \lambda^{n-1} \, R^n_t</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/eligibility-forward.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption"><span class="math inline">\lambda</span>-returns are averages of all n-step returns. Source: <span class="citation" data-cites="Sutton1998">(<a href="../references.html#ref-Sutton1998" role="doc-biblioref">Sutton and Barto, 1998</a>)</span>.</figcaption>
</figure>
</div>
<p>The term <span class="math inline">1- \lambda</span> is there to ensure that the coefficients <span class="math inline">\lambda^{n-1}</span> sum to one.</p>
<p><span class="math display">\sum_{n=1}^\infty \lambda^{n-1} = \dfrac{1}{1 - \lambda}</span></p>
<p>Each reward <span class="math inline">r_{t+k+1}</span> will count multiple times in the <span class="math inline">\lambda</span>-return. Distant rewards are discounted by <span class="math inline">\lambda^k</span> in addition to <span class="math inline">\gamma^k</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/eligibility-forward-decay.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption"><span class="math inline">\lambda</span> controls the importance of large n-step returns. Source: <span class="citation" data-cites="Sutton1998">(<a href="../references.html#ref-Sutton1998" role="doc-biblioref">Sutton and Barto, 1998</a>)</span>.</figcaption>
</figure>
</div>
<p>Large n-step returns (MC) should not have as much importance as small ones (TD), as they have a high variance.</p>
<p>To understand the role of <span class="math inline">\lambda</span>, let’s split the infinite sum w.r.t the end of the episode at time <span class="math inline">T</span>. n-step returns with <span class="math inline">n \geq T</span> all have a MC return of <span class="math inline">R_t</span>:</p>
<p><span class="math display">R^\lambda_t = (1  - \lambda) \, \sum_{n=1}^{T-t-1} \lambda^{n-1} \, R^n_t + \lambda^{T-t-1} \, R_t</span></p>
<p><span class="math inline">\lambda</span> controls the bias-variance trade-off:</p>
<ul>
<li>If <span class="math inline">\lambda=0</span>, the <span class="math inline">\lambda</span>-return is equal to <span class="math inline">R^1_t = r_{t+1} + \gamma \, V(s_{t+1})</span>, i.e.&nbsp;TD: high bias, low variance.</li>
<li>If <span class="math inline">\lambda=1</span>, the <span class="math inline">\lambda</span>-return is equal to <span class="math inline">R_t = \sum_{k=0}^{\infty} \gamma^{k} \, r_{t+k+1}</span>, i.e.&nbsp;MC: low bias, high variance.</li>
</ul>
<p>This <strong>forward view</strong> of eligibility traces implies to look at all future rewards until the end of the episode to perform a value update. This prevents online learning using single transitions.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/eligibility-forward-view.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">Forward view of eligibility traces. Source: <span class="citation" data-cites="Sutton1998">(<a href="../references.html#ref-Sutton1998" role="doc-biblioref">Sutton and Barto, 1998</a>)</span>.</figcaption>
</figure>
</div>
<p>Another view on eligibility traces is that the <strong>TD reward prediction error</strong> at time <span class="math inline">t</span> is sent backwards in time:</p>
<p><span class="math display">
\delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/eligibility-backwards.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">Backward view of eligibility traces. Source: <span class="citation" data-cites="Sutton1998">(<a href="../references.html#ref-Sutton1998" role="doc-biblioref">Sutton and Barto, 1998</a>)</span>.</figcaption>
</figure>
</div>
<p>Every state <span class="math inline">s</span> previously visited during the episode will be updated proportionally to the current TD error and an <strong>eligibility trace</strong> <span class="math inline">e_t(s)</span>:</p>
<p><span class="math display">
    V(s) \leftarrow V(s) + \alpha \, \delta_t \, e_t(s)
</span></p>
<p>The eligibility trace defines since how long the state was visited:</p>
<p><span class="math display">
    e_t(s) = \begin{cases}
                \gamma \, \lambda \, e_{t-1}(s) \qquad\qquad \text{if} \quad s \neq s_t \\
                e_{t-1}(s) + 1 \qquad \text{if} \quad s = s_t \\
            \end{cases}
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/traces.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">Updating of eligibility traces. Source: <span class="citation" data-cites="Sutton1998">(<a href="../references.html#ref-Sutton1998" role="doc-biblioref">Sutton and Barto, 1998</a>)</span>.</figcaption>
</figure>
</div>
<p><span class="math inline">\lambda</span> defines how important is a future TD error for the current state.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TD(<span class="math inline">\lambda</span>) algorithm: policy evaluation
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>foreach</strong> step <span class="math inline">t</span> of the episode:</p>
<ul>
<li><p>Select <span class="math inline">a_t</span> using the current policy <span class="math inline">\pi</span> in state <span class="math inline">s_t</span>, observe <span class="math inline">r_{t+1}</span> and <span class="math inline">s_{t+1}</span>.</p></li>
<li><p>Compute the TD error in <span class="math inline">s_t</span>:</p></li>
</ul>
<p><span class="math display">
      \delta_t = r_{t+1} + \gamma \, V_k(s_{t+1}) - V_k(s_t)
  </span></p>
<ul>
<li>Increment the trace of <span class="math inline">s_t</span>:</li>
</ul>
<p><span class="math display">
      e_{t+1}(s_t) = e_t(s_t) + 1
  </span></p>
<ul>
<li><p><strong>foreach</strong> state <span class="math inline">s \in [s_o, \ldots, s_t]</span> in the episode:</p>
<ul>
<li>Update the state value function:</li>
</ul>
<p><span class="math display">
      V_{k+1}(s) = V_k(s) + \alpha \, \delta_t \, e_t(s)
  </span></p>
<ul>
<li>Decay the eligibility trace:</li>
</ul>
<p><span class="math display">
      e_{t+1}(s) = \lambda \, \gamma \, e_t(s)
  </span></p></li>
<li><p><strong>if</strong> <span class="math inline">s_{t+1}</span> is terminal: <strong>break</strong></p></li>
</ul></li>
</ul>
</div>
</div>
<p>The backward view of eligibility traces can be applied on single transitions, given we know the history of visited states and maintain a trace for each of them. Eligibility traces are a very useful way to speed learning up in TD methods and control the bias/variance trade-off. This modification can be applied to all TD methods: TD(<span class="math inline">\lambda</span>) for states, SARSA(<span class="math inline">\lambda</span>) and Q(<span class="math inline">\lambda</span>) for actions.</p>
<p>The main drawback is that we need to keep a trace for ALL possible state-action pairs: memory consumption. Clever programming can limit this issue. The value of <span class="math inline">\lambda</span> has to be carefully chosen for the problem: perhaps initial actions are random and should not be reinforced. If your problem is not strictly Markov (POMDP), eligibility traces can help as they update the history!</p>
</section>
<section id="generalized-advantage-estimation-gae" class="level3">
<h3 class="anchored" data-anchor-id="generalized-advantage-estimation-gae">Generalized advantage estimation (GAE)</h3>
<p>The <strong>n-step advantage</strong> at time <span class="math inline">t</span>:</p>
<p><span class="math display">
A^n_t = \sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \,  V(s_{t+n}) - V (s_t)
</span></p>
<p>can be written as function of the TD error of the next <span class="math inline">n</span> transitions:</p>
<p><span class="math display">
    A^{n}_t = \sum_{l=0}^{n-1} \gamma^l \, \delta_{t+l}
</span></p>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof with <span class="math inline">n=2</span>:
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\begin{aligned}
A^2_t &amp;= r_{t+1} + \gamma \, r_{t+2} + \gamma^2 \, V(s_{t+2}) - V(s_{t}) \\
&amp;\\
&amp;= (r_{t+1} - V(s_t)) + \gamma \, (r_{t+2} + \gamma \, V(s_{t+2}) ) \\
&amp;\\
&amp;= (r_{t+1} + \gamma \, V(s_{t+1}) - V(s_t)) + \gamma \, (r_{t+2} + \gamma \, V(s_{t+2}) - V(s_{t+1})) \\
&amp;\\
&amp;= \delta_t + \gamma \, \delta_{t+1}
\end{aligned}
</span></p>
</div>
</div>
<p>The <strong>n-step advantage</strong> realizes a bias/variance trade-off, but which value of <span class="math inline">n</span> should we choose?</p>
<p><span class="math display">
A^n_t = \sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \,  V(s_{t+n}) - V (s_t)
</span></p>
<p>Schulman et al.&nbsp;(2015) <span class="citation" data-cites="Schulman2015a">(<a href="../references.html#ref-Schulman2015a" role="doc-biblioref">Schulman et al., 2015</a>)</span> proposed a <strong>generalized advantage estimate</strong> (GAE) <span class="math inline">A_t^{\text{GAE}(\gamma, \lambda)}</span> summing all possible n-step advantages with a discount parameter <span class="math inline">\lambda</span>:</p>
<p><span class="math display">A_t^{\text{GAE}(\gamma, \lambda)} = (1 - \lambda) \sum_{n=1}^\infty \lambda^n \, A^n_t</span></p>
<p>This is just a forward eligibility trace over distant n-step advantages: the 1-step advantage is more important the the 1000-step advantage (too much variance). We can show that the GAE can be expressed as a function of the future 1-step TD errors:</p>
<p><span class="math display">A_t^{\text{GAE}(\gamma, \lambda)} = \sum_{k=0}^\infty (\gamma \, \lambda)^k \, \delta_{t+k}</span></p>
<p>The parameter <span class="math inline">\lambda</span> controls the <strong>bias-variance</strong> trade-off.</p>
<ul>
<li>When <span class="math inline">\lambda=0</span>, the generalized advantage is the TD error:</li>
</ul>
<p><span class="math display">A_t^{\text{GAE}(\gamma, 0)} = r_{t+1} + \gamma \, V(s_{t+1}) - V(s_t)  = \delta_{t}</span></p>
<ul>
<li>When <span class="math inline">\lambda=1</span>, the generalized advantage is the MC advantage:</li>
</ul>
<p><span class="math display">A_t^{\text{GAE}(\gamma, 1)} = \sum_{k=0}^\infty \gamma^k \, r_{t+k+1} - V(s_t) = R_t - V(s_t)</span></p>
<p>Any value in between controls the bias-variance trade-off: from the high bias / low variance of TD to the small bias / high variance of MC. In practice, it leads to a better estimation than n-step advantages, but is more computationally expensive.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-Schulman2015a" class="csl-entry" role="listitem">
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015). Trust <span>Region Policy Optimization</span>. in <em>Proceedings of the 31 st <span>International Conference</span> on <span>Machine Learning</span></em>, 1889–1897. <a href="http://proceedings.mlr.press/v37/schulman15.html">http://proceedings.mlr.press/v37/schulman15.html</a>.
</div>
<div id="ref-Sutton1998" class="csl-entry" role="listitem">
Sutton, R. S., and Barto, A. G. (1998). <em>Reinforcement <span>Learning</span>: <span>An</span> introduction</em>. <span>Cambridge, MA</span>: <span>MIT press</span>.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../notes/2.4-MC.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Monte-Carlo (MC) methods</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/2.6-FA.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">Function approximation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">Copyright Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de" class="email">julien.vitay@informatik.tu-chemnitz.de</a></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>