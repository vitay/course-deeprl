<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Deep Reinforcement Learning - 12&nbsp; Policy gradient (PG)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/3.4-A3C.html" rel="next">
<link href="../notes/3.2-BeyondDQN.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-title">Policy gradient (PG)</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../" class="sidebar-logo-link">
      <img src="../notes/img/tuc-new.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Deep Reinforcement Learning</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Introduction</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.1-Introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.2-Math.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Math basics (optional)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Tabular RL</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.1-Bandits.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.2-MDP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Markov Decision Processes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.3-DP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Dynamic Programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.4-MC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Monte-Carlo (MC) methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.5-TD.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Temporal Difference learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.6-FA.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Function approximation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.7-NN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Model-free RL</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.1-DQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Q-Learning (DQN)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.2-BeyondDQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Beyond DQN</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.3-PG.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Policy gradient (PG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.4-A3C.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Advantage actor-critic (A2C, A3C)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.5-DDPG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Deterministic Policy Gradient (DDPG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.6-PPO.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Natural gradients (TRPO, PPO)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.7-SAC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Maximum Entropy RL (SAC)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Model-based RL</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.1-MB.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Model-based RL</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.2-LearnedModels.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Learned world models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.3-AlphaGo.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">AlphaGo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.4-SR.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Successor representations</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">Outlook</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.1-Outlook.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Outlook</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">Exercises</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Content.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">List of exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Installation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Python installation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/1-Python-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction To Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/2-Numpy-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Numpy and Matplotlib</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/3-Sampling-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Sampling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/4-Bandits-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/5-Bandits2-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits - part 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/6-DP-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Dynamic programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/7-Gym-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Gym environments</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/8-MonteCarlo-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Monte-Carlo control</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/9-TD-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Q-learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/10-Eligibilitytraces-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Eligibility traces</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/11-Keras-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Keras tutorial</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/12-DQN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">DQN</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#policy-search" id="toc-policy-search" class="nav-link active" data-scroll-target="#policy-search">Policy Search</a></li>
  <li><a href="#reinforce" id="toc-reinforce" class="nav-link" data-scroll-target="#reinforce">REINFORCE</a>
  <ul class="collapse">
  <li><a href="#reinforce-algorithm" id="toc-reinforce-algorithm" class="nav-link" data-scroll-target="#reinforce-algorithm">REINFORCE algorithm</a></li>
  <li><a href="#reinforce-with-baseline" id="toc-reinforce-with-baseline" class="nav-link" data-scroll-target="#reinforce-with-baseline">REINFORCE with baseline</a></li>
  </ul></li>
  <li><a href="#policy-gradient" id="toc-policy-gradient" class="nav-link" data-scroll-target="#policy-gradient">Policy Gradient</a>
  <ul class="collapse">
  <li><a href="#policy-gradient-theorem" id="toc-policy-gradient-theorem" class="nav-link" data-scroll-target="#policy-gradient-theorem">Policy Gradient theorem</a></li>
  <li><a href="#policy-gradient-theorem-with-function-approximation" id="toc-policy-gradient-theorem-with-function-approximation" class="nav-link" data-scroll-target="#policy-gradient-theorem-with-function-approximation">Policy Gradient Theorem with function approximation</a></li>
  <li><a href="#actor-critic-architectures" id="toc-actor-critic-architectures" class="nav-link" data-scroll-target="#actor-critic-architectures">Actor-critic architectures</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-title">Policy gradient (PG)</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>Slides: <a href="../slides/3.3-PG.html" target="_blank">html</a> <a href="../slides/pdf/3.3-PG.pdf" target="_blank">pdf</a></p>
<section id="policy-search" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="policy-search">Policy Search</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/oMnUUa2qV1A" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>Learning directly the Q-values in value-based methods (DQN) suffers from many problems:</p>
<ol type="1">
<li>The Q-values are <strong>unbounded</strong>: they can take any value (positive or negative), so the output layer must be linear.</li>
<li>The Q-values have a <strong>high variability</strong>: some <span class="math inline">(s,a)</span> pairs have very negative values, others have very positive values. Difficult to learn for a NN.</li>
<li>Works only for small <strong>discrete action spaces</strong>: need to iterate over all actions to find the greedy action.</li>
</ol>
<p>Instead of learning the Q-values, one could approximate directly the policy <span class="math inline">\pi_\theta(s, a)</span> with a neural network. <span class="math inline">\pi_\theta(s, a)</span> is called a <strong>parameterized policy</strong>: it depends directly on the parameters <span class="math inline">\theta</span> of the NN. For discrete action spaces, the output of the NN can be a <strong>softmax</strong> layer, directly giving the probability of selecting an action. For continuous action spaces, the output layer can directly control the effector (joint angles). Parameterized policies can represent continuous policies and avoid the curse of dimensionality.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="../slides/img/conv_agent.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption margin-caption">Policy search methods learn directly the policy.</figcaption><p></p>
</figure>
</div>
<p><strong>Policy search</strong> methods aim at maximizing directly the expected return over all possible trajectories (episodes) <span class="math inline">\tau = (s_0, a_0, \dots, s_T, a_T)</span></p>
<p><span class="math display">
    \mathcal{J}(\theta) = \mathbb{E}_{\tau \sim \rho_\theta}[R(\tau)] = \int_{\tau} \rho_\theta(\tau) \; R(\tau) \; d\tau
</span></p>
<p>All trajectories <span class="math inline">\tau</span> selected by the policy <span class="math inline">\pi_\theta</span> should be associated with a high expected return <span class="math inline">R(\tau)</span> in order to maximize this objective function. <span class="math inline">\rho_\theta</span> is the space of trajectories possible under <span class="math inline">\pi_\theta</span>. This means that the optimal policy should only select actions that maximizes the expected return: exactly what we want.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="../slides/img/policysearch.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption margin-caption">Policy search maximizes the return of the trajectories generated by the policy.</figcaption><p></p>
</figure>
</div>
<p>The objective function is however not <strong>model-free</strong>, as the probability of a trajectory does depend on the environments dynamics:</p>
<p><span class="math display">
    \rho_\theta(\tau) = p_\theta(s_0, a_0, \ldots, s_T, a_T) = p_0 (s_0) \, \prod_{t=0}^T \pi_\theta(s_t, a_t) \, p(s_{t+1} | s_t, a_t)
</span></p>
<p>The objective function is furthermore <strong>not computable</strong>:</p>
<ul>
<li>An <strong>infinity</strong> of possible trajectories to integrate if the action space is continuous.</li>
<li>Even if we sample trajectories, we would need a huge number of them to correctly estimate the objective function (<strong>sample complexity</strong>) because of the huge <strong>variance</strong> of the returns.</li>
</ul>
<p><span class="math display">
    \mathcal{J}(\theta) = \mathbb{E}_{\tau \sim \rho_\theta}[R(\tau)] \approx \frac{1}{M} \, \sum_{i=1}^M R(\tau_i)
</span></p>
<p>All we need to find is a computable gradient <span class="math inline">\nabla_\theta \mathcal{J}(\theta)</span> to apply gradient ascent and backpropagation.</p>
<p><span class="math display">
    \Delta \theta = \eta \, \nabla_\theta \mathcal{J}(\theta)
</span></p>
<p><strong>Policy Gradient</strong> (PG) methods only try to estimate this gradient, but do not care about the objective function itself…</p>
<p><span class="math display">
    g = \nabla_\theta \mathcal{J}(\theta)
</span></p>
<p>In particular, any function <span class="math inline">\mathcal{J}'(\theta)</span> whose gradient is locally the same (or has the same direction) will do:</p>
<p><span class="math display">\mathcal{J}'(\theta) = \alpha \, \mathcal{J}(\theta) + \beta \; \Rightarrow \; \nabla_\theta \mathcal{J}'(\theta) \propto \nabla_\theta \mathcal{J}(\theta)  \; \Rightarrow \; \Delta \theta = \eta \, \nabla_\theta \mathcal{J}'(\theta)</span></p>
<p>This is called <strong>surrogate optimization</strong>: we actually want to maximize <span class="math inline">\mathcal{J}(\theta)</span> but we cannot compute it. We instead create a surrogate objective <span class="math inline">\mathcal{J}'(\theta)</span> which is locally the same as <span class="math inline">\mathcal{J}(\theta)</span> and tractable.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="../slides/img/pg-idea.png" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption margin-caption">Policy gradient methods only care about the gradient of the objective function. Source: <a href="https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/" class="uri">https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/</a>.</figcaption><p></p>
</figure>
</div>
</section>
<section id="reinforce" class="level2">
<h2 class="anchored" data-anchor-id="reinforce">REINFORCE</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/qjZk7VcNndI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<section id="reinforce-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="reinforce-algorithm">REINFORCE algorithm</h3>
<p>The <strong>REINFORCE</strong> algorithm <span class="citation" data-cites="Williams1992">(<a href="../references.html#ref-Williams1992" role="doc-biblioref">Williams, 1992</a>)</span> proposes an unbiased estimate of the policy gradient:</p>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) = \nabla_\theta \int_\tau \rho_\theta (\tau) \, R(\tau) \, d\tau =  \int_\tau (\nabla_\theta \rho_\theta (\tau)) \, R(\tau) \, d\tau
</span></p>
<p>by noting that the return of a trajectory does not depend on the weights <span class="math inline">\theta</span> (the agent only controls its actions, not the environment).</p>
<p>We now use the <strong>log-trick</strong>, a simple identity based on the fact that:</p>
<p><span class="math display">
    \frac{d \log f(x)}{dx} = \frac{f'(x)}{f(x)}
</span></p>
<p>to rewrite the policy gradient of a single trajectory:</p>
<p><span class="math display">
    \nabla_\theta \rho_\theta (\tau) = \rho_\theta (\tau) \, \nabla_\theta \log \rho_\theta (\tau)
</span></p>
<p>The policy gradient becomes:</p>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \int_\tau \rho_\theta (\tau) \, \nabla_\theta \log \rho_\theta (\tau) \, R(\tau) \, d\tau
</span></p>
<p>which now has the form of a mathematical expectation:</p>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[ \nabla_\theta \log \rho_\theta (\tau) \, R(\tau) ]
</span></p>
<p>The advantage of REINFORCE is that it is <strong>model-free</strong>:</p>
<p><span class="math display">
    \rho_\theta(\tau) = p_\theta(s_0, a_0, \ldots, s_T, a_T) = p_0 (s_0) \, \prod_{t=0}^T \pi_\theta(s_t, a_t) p(s_{t+1} | s_t, a_t)
</span></p>
<p><span class="math display">
    \log \rho_\theta(\tau) = \log p_0 (s_0) + \sum_{t=0}^T \log \pi_\theta(s_t, a_t) + \sum_{t=0}^T \log p(s_{t+1} | s_t, a_t)
</span></p>
<p><span class="math display">
    \nabla_\theta \log \rho_\theta(\tau) = \sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t)
</span></p>
<p>The transition dynamics <span class="math inline">p(s_{t+1} | s_t, a_t)</span> disappear from the gradient. The <strong>Policy Gradient</strong> does not depend on the dynamics of the environment:</p>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, R(\tau) ]
</span></p>
<p>The REINFORCE algorithm is a policy-based variant of Monte-Carlo control:</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
REINFORCE algorithm <span class="citation" data-cites="Williams1992">(<a href="../references.html#ref-Williams1992" role="doc-biblioref">Williams, 1992</a>)</span>.
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>while</strong> not converged:</p>
<ul>
<li><p>Sample <span class="math inline">M</span> trajectories <span class="math inline">\{\tau_i\}</span> using the current policy <span class="math inline">\pi_\theta</span> and observe the returns <span class="math inline">\{R(\tau_i)\}</span>.</p></li>
<li><p>Estimate the policy gradient as an average over the trajectories:</p></li>
</ul>
<p><span class="math display">
     \nabla_\theta \mathcal{J}(\theta) \approx \frac{1}{M} \sum_{i=1}^M \sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, R(\tau_i)
  </span></p>
<ul>
<li>Update the policy using gradient ascent:</li>
</ul>
<p><span class="math display">
      \theta \leftarrow \theta + \eta \, \nabla_\theta \mathcal{J}(\theta)
  </span></p></li>
</ul>
</div>
</div>
<p><strong>Advantages</strong></p>
<ul>
<li>The policy gradient is <strong>model-free</strong>.</li>
<li>Works with <strong>partially observable</strong> problems (POMDP): as the return is computed over complete trajectories, it does not matter whether the states are Markov or not.</li>
</ul>
<p><strong>Inconvenients</strong></p>
<ul>
<li>Only for <strong>episodic tasks</strong>.</li>
<li>The gradient has a <strong>high variance</strong>: returns may change a lot during learning.</li>
<li>It has therefore a high <strong>sample complexity</strong>: we need to sample many episodes to correctly estimate the policy gradient.</li>
<li>Strictly <strong>on-policy</strong>: trajectories must be frequently sampled and immediately used to update the policy.</li>
</ul>
</section>
<section id="reinforce-with-baseline" class="level3">
<h3 class="anchored" data-anchor-id="reinforce-with-baseline">REINFORCE with baseline</h3>
<p>To reduce the variance of the estimated gradient, a baseline is often subtracted from the return:</p>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, (R(\tau) - b) ]
</span></p>
<p>As long as the baseline <span class="math inline">b</span> is independent from <span class="math inline">\theta</span>, it does not introduce a bias:</p>
<p><span class="math display">
\begin{aligned}
    \mathbb{E}_{\tau \sim \rho_\theta}[\nabla_\theta \log \rho_\theta (\tau) \, b ] &amp; = \int_\tau \rho_\theta (\tau) \nabla_\theta \log \rho_\theta (\tau) \, b \, d\tau \\
    &amp; = \int_\tau \nabla_\theta  \rho_\theta (\tau) \, b \, d\tau \\
    &amp;= b \, \nabla_\theta \int_\tau \rho_\theta (\tau) \, d\tau \\
    &amp;=  b \, \nabla_\theta 1 \\
    &amp;= 0
\end{aligned}
</span></p>
<p>A simple baseline that reduces the variance of the returns is a <strong>moving average</strong> of the returns obtained during all episodes:</p>
<p><span class="math display">b = \alpha \, R(\tau) + (1 - \alpha) \, b</span></p>
<p>This is similar to <strong>reinforcement comparison</strong> for bandits, except we compute the mean return instead of the mean reward. A trajectory <span class="math inline">\tau</span> should be <strong>reinforced</strong> if it brings more return than average.</p>
<p><span class="citation" data-cites="Williams1992">(<a href="../references.html#ref-Williams1992" role="doc-biblioref">Williams, 1992</a>)</span> showed that the best baseline (the one that reduces the variance the most) is actually:</p>
<p><span class="math display">
    b = \frac{\mathbb{E}_{\tau \sim \rho_\theta}[(\nabla_\theta \log \rho_\theta (\tau))^2 \, R(\tau)]}{\mathbb{E}_{\tau \sim \rho_\theta}[(\nabla_\theta \log \rho_\theta (\tau))^2]}
</span></p>
<p>but it is complex to compute. In practice, a baseline that works well is the value of the encountered states:</p>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, (R(\tau) - V^\pi(s_t)) ]
</span></p>
<p><span class="math inline">R(\tau) - V^\pi(s_t)</span> becomes the <strong>advantage</strong> of the action <span class="math inline">a_t</span> in <span class="math inline">s_t</span>: how much return does it provide compared to what can be expected in <span class="math inline">s_t</span> generally. As in <strong>dueling networks</strong>, it reduces the variance of the returns. Problem: the value of each state has to be learned separately (see actor-critic architectures).</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Application of REINFORCE to resource management
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/reinforce-cluster.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>REINFORCE with baseline can be used to allocate resources (CPU cores, memory, etc) when scheduling jobs on a cloud of compute servers. In DeepRM <span class="citation" data-cites="Mao2016">(<a href="../references.html#ref-Mao2016" role="doc-biblioref">Mao et al., 2016</a>)</span>, the policy is approximated by a shallow NN (one hidden layer with 20 neurons). The state space is the current occupancy of the cluster as well as the job waiting list. The action space is sending a job to a particular resource. The reward is the negative <strong>job slowdown</strong>: how much longer the job needs to complete compared to the optimal case. DeepRM outperforms all alternative job schedulers.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/reinforce-cluster2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="policy-gradient" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="policy-gradient">Policy Gradient</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/1b7E_byZNGU" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<section id="policy-gradient-theorem" class="level3">
<h3 class="anchored" data-anchor-id="policy-gradient-theorem">Policy Gradient theorem</h3>
<p>The REINFORCE gradient estimate is the following:</p>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, R(\tau) ] =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T (\nabla_\theta \log \pi_\theta(s_t, a_t)) \, (\sum_{t'=0}^T \gamma^{t'} \, r_{t'+1}) ]
</span></p>
<p>For each state-action pair <span class="math inline">(s_t, a_t)</span> encountered during the episode, the gradient of the log-policy is multiplied by the complete return of the episode:</p>
<p><span class="math display">R(\tau) = \sum_{t'=0}^T \gamma^{t'} \, r_{t'+1}</span></p>
<p>The <strong>causality principle</strong> states that rewards obtained before time <span class="math inline">t</span> are not caused by that action. The policy gradient can be rewritten as:</p>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, (\sum_{t'=t}^T \gamma^{t' - t} \, r_{t'+1}) ] =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, R_t ]
</span></p>
<p>The return at time <span class="math inline">t</span> (<strong>reward-to-go</strong>) multiplies the gradient of the log-likelihood of the policy(the <strong>score</strong>) for each transition in the episode:</p>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, R_t ]
</span></p>
<p>As we have:</p>
<p><span class="math display">Q^\pi(s, a) = \mathbb{E}_\pi [R_t | s_t =s; a_t =a]</span></p>
<p>we can replace <span class="math inline">R_t</span> with <span class="math inline">Q^{\pi_\theta}(s_t, a_t)</span> without introducing any bias:</p>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, Q^{\pi_\theta}(s_t, a_t) ]
</span></p>
<p>This is true on average (no bias if the Q-value estimates are correct) and has a much lower variance!</p>
<p>The policy gradient is defined over complete trajectories:</p>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, Q^{\pi_\theta}(s_t, a_t) ]
</span></p>
<p>However, <span class="math inline">\nabla_\theta \log \pi_\theta(s_t, a_t) \, Q^{\pi_\theta}(s_t, a_t)</span> now only depends on <span class="math inline">(s_t, a_t)</span>, not the future nor the past. Each step of the episode is now independent from each other (if we have the Markov property). We can then <strong>sample single transitions</strong> instead of complete episodes:</p>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q^{\pi_\theta}(s, a) ]
</span></p>
<p>Note that this is not true for <span class="math inline">\mathcal{J}(\theta)</span> directly, as the value of <span class="math inline">\mathcal{J}(\theta)</span> changes (computed over single transitions instead of complete episodes, so it is smaller), but it is true for its gradient (both go in the same direction)!</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Policy Gradient Theorem <span class="citation" data-cites="Sutton1999">(<a href="../references.html#ref-Sutton1999" role="doc-biblioref">Sutton et al., 1999</a>)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>For any MDP, the policy gradient is:</p>
<p><span class="math display">
    g = \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q^{\pi_\theta}(s, a) ]
</span></p>
</div>
</div>
</section>
<section id="policy-gradient-theorem-with-function-approximation" class="level3">
<h3 class="anchored" data-anchor-id="policy-gradient-theorem-with-function-approximation">Policy Gradient Theorem with function approximation</h3>
<p>Better yet, <span class="citation" data-cites="Sutton1999">(<a href="../references.html#ref-Sutton1999" role="doc-biblioref">Sutton et al., 1999</a>)</span> showed that we can replace the true Q-value <span class="math inline">Q^{\pi_\theta}(s, a)</span> by an estimate <span class="math inline">Q_\varphi(s, a)</span> as long as this one is unbiased:</p>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q_\varphi(s, a) ]
</span></p>
<p>We only need to have:</p>
<p><span class="math display">
    Q_\varphi(s, a) \approx Q^{\pi_\theta}(s, a) \; \forall s, a
</span></p>
<p>The approximated Q-values can for example minimize the <strong>mean square error</strong> with the true Q-values:</p>
<p><span class="math display">
    \mathcal{L}(\varphi) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[(Q^{\pi_\theta}(s, a) - Q_\varphi(s, a))^2]
</span></p>
</section>
<section id="actor-critic-architectures" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="actor-critic-architectures">Actor-critic architectures</h3>
<p>We obtain an <strong>actor-critic</strong> architecture:</p>
<ul>
<li>the <strong>actor</strong> <span class="math inline">\pi_\theta(s, a)</span> implements the policy and selects an action <span class="math inline">a</span> in a state <span class="math inline">s</span>.</li>
<li>the <strong>critic</strong> <span class="math inline">Q_\varphi(s, a)</span> estimates the value of that action and drives learning in the actor.</li>
</ul>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="../slides/img/policygradient.svg" class="img-fluid figure-img" style="width:90.0%"></p>
<p></p><figcaption class="figure-caption margin-caption">Actor-critic architecture for policy gradient.</figcaption><p></p>
</figure>
</div>
<p>But how to train the critic? We do not know <span class="math inline">Q^{\pi_\theta}(s, a)</span>. As always, we can estimate it through <strong>sampling</strong>:</p>
<ul>
<li><strong>Monte-Carlo</strong> critic: sampling the complete episode.</li>
</ul>
<p><span class="math display">
    \mathcal{L}(\varphi) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[(R(s, a) - Q_\varphi(s, a))^2]
</span></p>
<ul>
<li><strong>SARSA</strong> critic: sampling <span class="math inline">(s, a, r, s', a')</span> transitions.</li>
</ul>
<p><span class="math display">
    \mathcal{L}(\varphi) =  \mathbb{E}_{s, s' \sim \rho_\theta, a, a' \sim \pi_\theta}[(r + \gamma \, Q_\varphi(s', a') - Q_\varphi(s, a))^2]
</span></p>
<ul>
<li><strong>Q-learning</strong> critic: sampling <span class="math inline">(s, a, r, s')</span> transitions.</li>
</ul>
<p><span class="math display">
    \mathcal{L}(\varphi) =  \mathbb{E}_{s, s' \sim \rho_\theta, a \sim \pi_\theta}[(r + \gamma \, \max_{a'} Q_\varphi(s', a') - Q_\varphi(s, a))^2]
</span></p>
<p>As with REINFORCE, the PG actor suffers from the <strong>high variance</strong> of the Q-values. It is possible to use a <strong>baseline</strong> in the PG without introducing a bias:</p>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, (Q^{\pi_\theta}(s, a) -b)]
</span></p>
<p>In particular, the <strong>advantage actor-critic</strong> uses the value of a state as the baseline:</p>
<p><span class="math display">
\begin{aligned}
    \nabla_\theta \mathcal{J}(\theta) &amp;=  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, (Q^{\pi_\theta}(s, a) - V^{\pi_\theta}(s))] \\
    &amp;\\
    &amp;=  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, A^{\pi_\theta}(s, a)] \\
\end{aligned}
</span></p>
<p>The critic can either:</p>
<ul>
<li>learn to approximate both <span class="math inline">Q^{\pi_\theta}(s, a)</span> and <span class="math inline">V^{\pi_\theta}(s)</span> with two different NN (SAC).</li>
<li>replace one of them with a sampling estimate (A3C, DDPG)</li>
<li>learn the advantage <span class="math inline">A^{\pi_\theta}(s, a)</span> directly (GAE, PPO)</li>
</ul>
<p><strong>Policy Gradient methods</strong> can therefore take many forms :</p>
<p><span class="math display">
    \nabla_\theta J(\theta) =  \mathbb{E}_{s_t \sim \rho_\theta, a_t \sim \pi_\theta}[\nabla_\theta \log \pi_\theta (s_t, a_t) \, \psi_t ]
</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\psi_t = R_t</span> is the <em>REINFORCE</em> algorithm (MC sampling).</p></li>
<li><p><span class="math inline">\psi_t = R_t - b</span> is the <em>REINFORCE with baseline</em> algorithm.</p></li>
<li><p><span class="math inline">\psi_t = Q^\pi(s_t, a_t)</span> is the <em>policy gradient theorem</em>.</p></li>
<li><p><span class="math inline">\psi_t = A^\pi(s_t, a_t) = Q^\pi(s_t, a_t) - V^\pi(s_t)</span> is the <em>advantage actor-critic</em>.</p></li>
<li><p><span class="math inline">\psi_t = r_{t+1} + \gamma \, V^\pi(s_{t+1}) - V^\pi(s_t)</span> is the <em>TD actor-critic</em>.</p></li>
<li><p><span class="math inline">\psi_t = \sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \, V^\pi(s_{t+n}) - V^\pi(s_t)</span> is the <em>n-step advantage</em>.</p></li>
</ul>
<p>and many others…</p>
<p>The different variants of PG deal with the bias/variance trade-off.</p>
<p><span class="math display">
    \nabla_\theta J(\theta) =  \mathbb{E}_{s_t \sim \rho_\theta, a_t \sim \pi_\theta}[\nabla_\theta \log \pi_\theta (s_t, a_t) \, \psi_t ]
</span></p>
<ol type="1">
<li>The more <span class="math inline">\psi_t</span> relies on <strong>sampled rewards</strong> (e.g.&nbsp;<span class="math inline">R_t</span>), the more the gradient will be correct on average (small bias), but the more it will vary (high variance). This increases the sample complexity: we need to average more samples to correctly estimate the gradient.</li>
<li>The more <span class="math inline">\psi_t</span> relies on <strong>estimations</strong> (e.g.&nbsp;the TD error), the more stable the gradient (small variance), but the more incorrect it is (high bias). This can lead to suboptimal policies, i.e.&nbsp;local optima of the objective function.</li>
</ol>
<p>All the methods we will see in the rest of the course are attempts at finding the best trade-off.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-Mao2016" class="csl-entry" role="doc-biblioentry">
Mao, H., Alizadeh, M., Menache, I., and Kandula, S. (2016). Resource <span>Management</span> with <span>Deep Reinforcement Learning</span>. in <em>Proceedings of the 15th <span>ACM Workshop</span> on <span>Hot Topics</span> in <span>Networks</span> - <span>HotNets</span> ’16</em> (<span>Atlanta, GA, USA</span>: <span>ACM Press</span>), 50–56. doi:<a href="https://doi.org/10.1145/3005745.3005750">10.1145/3005745.3005750</a>.
</div>
<div id="ref-Sutton1999" class="csl-entry" role="doc-biblioentry">
Sutton, R. S., McAllester, D., Singh, S., and Mansour, Y. (1999). Policy gradient methods for reinforcement learning with function approximation. in <em>Proceedings of the 12th <span>International Conference</span> on <span>Neural Information Processing Systems</span></em> (<span>MIT Press</span>), 1057–1063. Available at: <a href="https://dl.acm.org/citation.cfm?id=3009806">https://dl.acm.org/citation.cfm?id=3009806</a>.
</div>
<div id="ref-Williams1992" class="csl-entry" role="doc-biblioentry">
Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. <em>Machine Learning</em> 8, 229–256.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../notes/3.2-BeyondDQN.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Beyond DQN</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/3.4-A3C.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">Advantage actor-critic (A2C, A3C)</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">Copyright 2022, Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de" class="email">julien.vitay@informatik.tu-chemnitz.de</a></div>
  </div>
</footer>



<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>