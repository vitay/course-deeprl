<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Deep Reinforcement Learning - 13&nbsp; Advantage actor-critic (A2C, A3C)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/3.5-DDPG.html" rel="next">
<link href="../notes/3.3-PG.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-title">Advantage actor-critic (A2C, A3C)</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../" class="sidebar-logo-link">
      <img src="../../slides/img/tuc-new.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Deep Reinforcement Learning</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Introduction</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.1-Introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.2-Math.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Math basics (optional)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Tabular RL</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.1-Bandits.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.2-MDP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Markov Decision Processes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.3-DP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Dynamic Programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.4-MC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Monte-Carlo (MC) methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.5-TD.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Temporal Difference learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.6-FA.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Function approximation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.7-NN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Model-free RL</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.1-DQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Q-Learning (DQN)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.2-BeyondDQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Beyond DQN</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.3-PG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Policy gradient (PG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.4-A3C.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Advantage actor-critic (A2C, A3C)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.5-DDPG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Deterministic Policy Gradient (DDPG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.6-PPO.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Natural gradients (TRPO, PPO)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.7-SAC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Maximum Entropy RL (SAC)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Model-based RL</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.1-MB.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Model-based RL</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.2-LearnedModels.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Learned world models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.3-AlphaGo.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">AlphaGo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.4-SR.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Successor representations</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">Outlook</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.1-Outlook.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Outlook</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">Exercises</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Content.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">List of exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/1-Python-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction To Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/2-Numpy-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Numpy and Matplotlib</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/3-Sampling-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Sampling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/4-Bandits-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/5-Bandits2-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits - part 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/6-DP-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Dynamic programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/7-Gym-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Gym environments</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/8-MonteCarlo-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Monte-Carlo control</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/9-TD-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Q-learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/10-Eligibilitytraces-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Eligibility traces</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/11-Keras-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Keras tutorial</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/12-DQN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">DQN</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#distributed-actor-critic" id="toc-distributed-actor-critic" class="nav-link active" data-scroll-target="#distributed-actor-critic">Distributed actor-critic</a></li>
  <li><a href="#a3c-asynchronous-advantage-actor-critic" id="toc-a3c-asynchronous-advantage-actor-critic" class="nav-link" data-scroll-target="#a3c-asynchronous-advantage-actor-critic">A3C: Asynchronous advantage actor-critic</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-title">Advantage actor-critic (A2C, A3C)</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>Slides: <a href="../slides/3.4-A3C.html" target="_blank">html</a> <a href="../slides/pdf/3.4-A3C.pdf" target="_blank">pdf</a></p>
<section id="distributed-actor-critic" class="level2">
<h2 class="anchored" data-anchor-id="distributed-actor-critic">Distributed actor-critic</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/89JH4CGD1Uo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>Let’s consider an <strong>n-step advantage actor-critic</strong> architecture where the Q-value of the action <span class="math inline">(s_t, a_t)</span> is approximated by the <strong>n-step return</strong>:</p>
<p><span class="math display">Q^{\pi_\theta}(s_t, a_t) \approx R_t^n =  \sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \, V_\varphi(s_{t+n})</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/nstep.png" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">n-step returns. Source: <span class="citation" data-cites="Sutton1998">(<a href="../references.html#ref-Sutton1998" role="doc-biblioref">Sutton and Barto, 1998</a>)</span>.</figcaption><p></p>
</figure>
</div>
<p>The <strong>actor</strong> <span class="math inline">\pi_\theta(s, a)</span> uses PG with baseline to learn the policy:</p>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{s_t \sim \rho_\theta, a_t \sim \pi_\theta}[\nabla_\theta \log \pi_\theta (s_t, a_t) \, (R_t^n - V_\varphi(s_t)) ]
</span></p>
<p>The <strong>critic</strong> <span class="math inline">V_\varphi(s)</span> approximates the value of each state by minimizing the mse:</p>
<p><span class="math display">
    \mathcal{L}(\varphi) =  \mathbb{E}_{s_t \sim \rho_\theta, a_t \sim \pi_\theta}[(R^n_t - V_\varphi(s_t))^2]
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/a2c.svg" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">Advantage actor-critic.</figcaption><p></p>
</figure>
</div>
<p>The advantage actor-critic is strictly <strong>on-policy</strong>:</p>
<ul>
<li>The critic <strong>must</strong> evaluate actions selected the current version of the actor <span class="math inline">\pi_\theta</span>, not an old version or another policy.</li>
<li>The actor must learn from the current value function <span class="math inline">V^{\pi_\theta} \approx V_\varphi</span>.</li>
</ul>
<p><span class="math display">\begin{cases}
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{s_t \sim \rho_\theta, a_t \sim \pi_\theta}[\nabla_\theta \log \pi_\theta (s_t, a_t) \, (R^n_t - V_\varphi(s_t)) ] \\
    \\
    \mathcal{L}(\varphi) =  \mathbb{E}_{s_t \sim \rho_\theta, a_t \sim \pi_\theta}[(R^n_t - V_\varphi(s_t))^2] \\
\end{cases}</span></p>
<p>We cannot use an <strong>experience replay memory</strong> to deal with the correlated inputs, as it is only for off-policy methods. We cannot either get an uncorrelated batch of transitions by acting <strong>sequentially</strong> with a single agent.</p>
<p>A simple solution is to have <strong>multiple actors</strong> with the same weights <span class="math inline">\theta</span> interacting <strong>in parallel</strong> with different copies of the environment: <strong>distributed learning</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/a2c-arch.svg" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Distributed learning. Source: <a href="https://ray.readthedocs.io/en/latest/rllib.html" class="uri">https://ray.readthedocs.io/en/latest/rllib.html</a></figcaption><p></p>
</figure>
</div>
<p>Each <strong>rollout worker</strong> (actor) starts an episode in a different state: at any point of time, the workers will be in <strong>uncorrelated states</strong>. From time to time, the workers all send their experienced transitions to the <strong>learner</strong> which updates the policy using a <strong>batch of uncorrelated transitions</strong>. After the update, the workers use the new policy.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Distributed RL
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Initialize global policy or value network <span class="math inline">\theta</span>.</p></li>
<li><p>Initialize <span class="math inline">N</span> copies of the environment in different states.</p></li>
<li><p><strong>while</strong> True:</p>
<ul>
<li><p><strong>for</strong> each worker in parallel:</p>
<ul>
<li>Copy the global network parameters <span class="math inline">\theta</span> to each worker:</li>
</ul>
<p><span class="math display">\theta_k \leftarrow \theta</span></p>
<ul>
<li><p>Initialize an empty transition buffer <span class="math inline">\mathcal{D}_k</span>.</p></li>
<li><p>Perform <span class="math inline">d</span> steps with the worker on its copy of the environment.</p></li>
<li><p>Append each transition <span class="math inline">(s, a, r, s')</span> to the transition buffer.</p></li>
</ul></li>
<li><p>join(): wait for each worker to terminate.</p></li>
<li><p>Gather the <span class="math inline">N</span> transition buffers into a single buffer <span class="math inline">\mathcal{D}</span>.</p></li>
<li><p>Update the global network on <span class="math inline">\mathcal{D}</span> to obtain new weights <span class="math inline">\theta</span>.</p></li>
</ul></li>
</ul>
</div>
</div>
<p>Distributed learning can be used for any deep RL algorithm, including DQN variants. Distributed DQN variants include GORILA, IMPALA, APE-X, R2D2.”All” you need is one (or more) GPU for training the global network and <span class="math inline">N</span> CPU cores for the workers. The workers fill the ERM much more quickly.</p>
<p>In practice, managing the communication between the workers and the global network through processes can be quite painful. There are some <strong>frameworks</strong> abstracting the dirty work, such as <strong>RLlib</strong> <a href="https://ray.readthedocs.io/en/latest/rllib.html" class="uri">https://ray.readthedocs.io/en/latest/rllib.html</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/rllib.svg" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption"><code>rllib</code> framework. Source: <a href="https://ray.readthedocs.io/en/latest/rllib.html" class="uri">https://ray.readthedocs.io/en/latest/rllib.html</a></figcaption><p></p>
</figure>
</div>
<p>Having multiple workers interacting with different environments is easy in simulation (Atari games). With physical environments, working in real time, it requires lots of money…</p>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/iaF43Ze1oeI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
</section>
<section id="a3c-asynchronous-advantage-actor-critic" class="level2">
<h2 class="anchored" data-anchor-id="a3c-asynchronous-advantage-actor-critic">A3C: Asynchronous advantage actor-critic</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/HRLXCmC0ooc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p><span class="citation" data-cites="Mnih2016">(<a href="../references.html#ref-Mnih2016" role="doc-biblioref">Mnih et al., 2016</a>)</span> proposed the <strong>A3C</strong> algorithm (asynchronous advantage actor-critic). The stochastic policy <span class="math inline">\pi_\theta</span> is produced by the <strong>actor</strong> with weights <span class="math inline">\theta</span> and learned using :</p>
<p><span class="math display">\nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{s_t \sim \rho_\theta, a_t \sim \pi_\theta}[\nabla_\theta \log \pi_\theta (s_t, a_t) \, (R^n_t - V_\varphi(s_t)) ]</span></p>
<p>The value of a state <span class="math inline">V_\varphi(s)</span> is produced by the <strong>critic</strong> with weights <span class="math inline">\varphi</span>, which minimizes the mse with the <strong>n-step return</strong>:</p>
<p><span class="math display">\mathcal{L}(\varphi) =  \mathbb{E}_{s_t \sim \rho_\theta, a_t \sim \pi_\theta}[(R^n_t - V_\varphi(s_t))^2]</span></p>
<p><span class="math display">R^n_t =  \sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \, V_\varphi(s_{t+n})</span></p>
<p>Both the actor and the critic are trained on batches of transitions collected using <strong>parallel workers</strong>. Two things are different from the general distributed approach: workers compute <strong>partial gradients</strong> and updates are <strong>asynchronous</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/a3c-parallel.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">A3C distributed architecture <span class="citation" data-cites="Mnih2016">(<a href="../references.html#ref-Mnih2016" role="doc-biblioref">Mnih et al., 2016</a>)</span>.</figcaption><p></p>
</figure>
</div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Worker
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>def</strong> worker(<span class="math inline">\theta</span>, <span class="math inline">\varphi</span>):</p>
<ul>
<li><p>Initialize empty transition buffer <span class="math inline">\mathcal{D}</span>. Initialize the environment to the <strong>last</strong> state visited by this worker.</p></li>
<li><p><strong>for</strong> <span class="math inline">n</span> steps:</p>
<ul>
<li>Select an action using <span class="math inline">\pi_\theta</span>, store the transition in the transition buffer.</li>
</ul></li>
<li><p><strong>for</strong> each transition in <span class="math inline">\mathcal{D}</span>:</p>
<ul>
<li>Compute the <strong>n-step return</strong> in each state</li>
</ul>
<p><span class="math display">R^n_t =  \sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \, V_\varphi(s_{t+n})</span></p></li>
<li><p>Compute <strong>policy gradient</strong> for the actor on the transition buffer:</p></li>
</ul>
<p><span class="math display">d\theta = \nabla_\theta \mathcal{J}(\theta) = \frac{1}{n} \sum_{t=1}^n \nabla_\theta \log \pi_\theta (s_t, a_t) \, (R^n_t - V_\varphi(s_t))</span></p>
<ul>
<li>Compute <strong>value gradient</strong> for the critic on the transition buffer:</li>
</ul>
<p><span class="math display">d\varphi = \nabla_\varphi \mathcal{L}(\varphi) = -\frac{1}{n} \sum_{t=1}^n (R^n_t - V_\varphi(s_t)) \, \nabla_\varphi V_\varphi(s_t)</span></p>
<ul>
<li><strong>return</strong> <span class="math inline">d\theta</span>, <span class="math inline">d\varphi</span></li>
</ul></li>
</ul>
</div>
</div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
A2C algorithm
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Initialize actor <span class="math inline">\theta</span> and critic <span class="math inline">\varphi</span>.</p></li>
<li><p>Initialize <span class="math inline">K</span> workers with a copy of the environment.</p></li>
<li><p><strong>for</strong> <span class="math inline">t \in [0, T_\text{total}]</span>:</p>
<ul>
<li><p><strong>for</strong> <span class="math inline">K</span> workers <strong>in parallel</strong>:</p>
<ul>
<li><span class="math inline">d\theta_k</span>, <span class="math inline">d\varphi_k</span> = worker(<span class="math inline">\theta</span>, <span class="math inline">\varphi</span>)</li>
</ul></li>
<li><p>join()</p></li>
<li><p>Merge all gradients:</p></li>
</ul>
<p><span class="math display">d\theta = \frac{1}{K} \sum_{i=1}^K d\theta_k \; ; \; d\varphi = \frac{1}{K} \sum_{i=1}^K d\varphi_k</span></p>
<ul>
<li>Update the actor and critic using gradient ascent/descent:</li>
</ul>
<p><span class="math display">\theta \leftarrow \theta + \eta \, d\theta \; ; \; \varphi \leftarrow \varphi - \eta \, d\varphi</span></p></li>
</ul>
</div>
</div>
<p>The previous algorithm depicts <strong>A2C</strong>, the synchronous version of A3C. A2C synchronizes the workers (threads), i.e.&nbsp;it waits for the <span class="math inline">K</span> workers to finish their job before merging the gradients and updating the global networks.</p>
<p>A3C is <strong>asynchronous</strong>:</p>
<ul>
<li>The partial gradients are applied to the global networks <strong>as soon as</strong> they are available.</li>
<li>No need to wait for all workers to finish their job.</li>
</ul>
<p>As the workers are not synchronized, this means that one worker could be copying the global networks <span class="math inline">\theta</span> and <span class="math inline">\varphi</span> <strong>while</strong> another worker is writing them. This is called a <strong>Hogwild!</strong> update <span class="citation" data-cites="Niu2011">(<a href="../references.html#ref-Niu2011" role="doc-biblioref">Niu et al., 2011</a>)</span>: no locks, no semaphores. Many workers can read/write the same data. It turns out NN are robust enough for this kind of updates.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
A3C: asynchronous updates
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Initialize actor <span class="math inline">\theta</span> and critic <span class="math inline">\varphi</span>.</p></li>
<li><p>Initialize <span class="math inline">K</span> workers with a copy of the environment.</p></li>
<li><p><strong>for</strong> <span class="math inline">K</span> workers <strong>in parallel</strong>:</p>
<ul>
<li><p><strong>for</strong> <span class="math inline">t \in [0, T_\text{total}]</span>:</p>
<ul>
<li><p>Copy the global networks <span class="math inline">\theta</span> and <span class="math inline">\varphi</span>.</p></li>
<li><p>Compute partial gradients:</p></li>
</ul>
<p><span class="math display">d\theta_k, d\varphi_k = \text{worker}(\theta, \varphi)</span></p>
<ul>
<li>Update the <strong>global</strong> actor and critic using the <strong>partial gradients</strong>:</li>
</ul>
<p><span class="math display">\theta \leftarrow \theta + \eta \, d\theta_k</span></p>
<p><span class="math display">\varphi \leftarrow \varphi - \eta \, d\varphi_k</span></p></li>
</ul></li>
</ul>
</div>
</div>
<p>A3C does not use an <em>experience replay memory</em> as DQN. Instead, it uses <strong>multiple parallel workers</strong> to distribute learning. Each worker has a copy of the actor and critic networks, as well as an instance of the environment. Weight updates are synchronized regularly though a <strong>master network</strong> using Hogwild!-style updates (every <span class="math inline">n=5</span> steps!). Because the workers learn different parts of the state-action space, the weight updates are not very correlated.</p>
<p>A3C still needs target networks to ensure stability. It works best on shared-memory systems (multi-core) as communication costs between GPUs are huge.</p>
<p>A3C set a new record for Atari games in 2016. The main advantage is that the workers gather experience in parallel: training is much faster than with DQN. LSTMs can be used to improve the performance.</p>
<p><img src="../slides/img/a3c-comparison.png" class="img-fluid" style="width:100.0%" data-fig-align="center" alt="A3C results (Mnih et al., 2016)."> <img src="../slides/img/a3c-time.png" class="img-fluid" style="width:70.0%" data-fig-align="center" alt="A3C results (Mnih et al., 2016)."></p>
<p>Learning is only marginally better with more threads:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/a3c-threads1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">A3C results <span class="citation" data-cites="Mnih2016">(<a href="../references.html#ref-Mnih2016" role="doc-biblioref">Mnih et al., 2016</a>)</span>.</figcaption><p></p>
</figure>
</div>
<p>but much faster!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/a3c-threads2.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">A3C results <span class="citation" data-cites="Mnih2016">(<a href="../references.html#ref-Mnih2016" role="doc-biblioref">Mnih et al., 2016</a>)</span>.</figcaption><p></p>
</figure>
</div>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/0xo1Ldx3L5Q" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/nMR5mjCFZCw" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/Ajjc08-iPx8" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>A3C came up in 2016. A lot of things happened since then…</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/rainbow-results1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">The Rainbow network combines all DQN improvements and outperforms each of them. Source: <span class="citation" data-cites="Hessel2017">(<a href="../references.html#ref-Hessel2017" role="doc-biblioref">Hessel et al., 2017</a>)</span>.</figcaption><p></p>
</figure>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-Hessel2017" class="csl-entry" role="doc-biblioentry">
Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., et al. (2017). Rainbow: <span>Combining Improvements</span> in <span>Deep Reinforcement Learning</span>. Available at: <a href="http://arxiv.org/abs/1710.02298">http://arxiv.org/abs/1710.02298</a>.
</div>
<div id="ref-Mnih2016" class="csl-entry" role="doc-biblioentry">
Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., et al. (2016). Asynchronous <span>Methods</span> for <span>Deep Reinforcement Learning</span>. in <em>Proc. <span>ICML</span></em> Available at: <a href="http://arxiv.org/abs/1602.01783">http://arxiv.org/abs/1602.01783</a>.
</div>
<div id="ref-Niu2011" class="csl-entry" role="doc-biblioentry">
Niu, F., Recht, B., Re, C., and Wright, S. J. (2011). <span>HOGWILD</span>!: <span>A Lock-Free Approach</span> to <span>Parallelizing Stochastic Gradient Descent</span>. in <em>Proc. <span>Advances</span> in <span>Neural Information Processing Systems</span></em>, 21–21. Available at: <a href="http://arxiv.org/abs/1106.5730">http://arxiv.org/abs/1106.5730</a>.
</div>
<div id="ref-Sutton1998" class="csl-entry" role="doc-biblioentry">
Sutton, R. S., and Barto, A. G. (1998). <em>Reinforcement <span>Learning</span>: <span>An</span> introduction</em>. <span>Cambridge, MA</span>: <span>MIT press</span>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../notes/3.3-PG.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Policy gradient (PG)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/3.5-DDPG.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">Deep Deterministic Policy Gradient (DDPG)</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">Copyright 2022, Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de" class="email">julien.vitay@informatik.tu-chemnitz.de</a></div>
  </div>
</footer>



<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>