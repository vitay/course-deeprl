<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Deep Reinforcement Learning - Successor representations</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/5.1-Outlook.html" rel="next">
<link href="../notes/4.3-AlphaGo.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notes/4.1-MB.html"><strong>Model-based RL</strong></a></li><li class="breadcrumb-item"><a href="../notes/4.4-SR.html"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Successor representations</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../notes/img/tuc-new.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Deep Reinforcement Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text"><strong>Introduction</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.1-Introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.2-Math.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Math basics (optional)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text"><strong>Tabular RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.1-Bandits.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.2-MDP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Markov Decision Processes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.3-DP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Dynamic Programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.4-MC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Monte-Carlo (MC) methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.5-TD.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Temporal Difference learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.6-FA.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Function approximation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.7-NN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text"><strong>Model-free RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.1-DQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Q-Learning (DQN)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.2-BeyondDQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Beyond DQN</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.3-PG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Policy gradient (PG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.4-A3C.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Advantage actor-critic (A2C, A3C)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.5-DDPG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Deterministic Policy Gradient (DDPG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.6-PPO.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Natural gradients (TRPO, PPO)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.7-SAC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Maximum Entropy RL (SAC)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text"><strong>Model-based RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.1-MB.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Model-based RL</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.2-LearnedModels.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Learned world models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.3-AlphaGo.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">AlphaGo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.4-SR.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Successor representations</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text"><strong>Outlook</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.1-Outlook.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Outlook</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text"><strong>Exercises</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Content.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">List of exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Installation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Python installation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/1-Python-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction To Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/2-Numpy-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Numpy and Matplotlib</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/3-Sampling-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Sampling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/4-Bandits-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/5-Bandits2-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits - part 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/6-DP-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Dynamic programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/7-Gym-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Gym environments</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/8-MonteCarlo-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Monte-Carlo control</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/9-TD-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Q-learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/10-Eligibilitytraces-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Eligibility traces</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/11-Keras-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Keras tutorial</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/12-DQN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">DQN</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/13-PPO-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">PPO</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#model-based-vs.-model-free" id="toc-model-based-vs.-model-free" class="nav-link active" data-scroll-target="#model-based-vs.-model-free">Model-based vs.&nbsp;Model-free</a>
  <ul class="collapse">
  <li><a href="#comparison" id="toc-comparison" class="nav-link" data-scroll-target="#comparison">Comparison</a></li>
  <li><a href="#goal-directed-learning-vs.-habit-formation" id="toc-goal-directed-learning-vs.-habit-formation" class="nav-link" data-scroll-target="#goal-directed-learning-vs.-habit-formation">Goal-directed learning vs.&nbsp;habit formation</a></li>
  </ul></li>
  <li><a href="#successor-representations" id="toc-successor-representations" class="nav-link" data-scroll-target="#successor-representations">Successor representations</a>
  <ul class="collapse">
  <li><a href="#principle-of-successor-representations" id="toc-principle-of-successor-representations" class="nav-link" data-scroll-target="#principle-of-successor-representations">Principle of successor representations</a></li>
  <li><a href="#learning-successor-representations" id="toc-learning-successor-representations" class="nav-link" data-scroll-target="#learning-successor-representations">Learning successor representations</a></li>
  </ul></li>
  <li><a href="#successor-features" id="toc-successor-features" class="nav-link" data-scroll-target="#successor-features">Successor features</a></li>
  <li><a href="#deep-successor-reinforcement-learning" id="toc-deep-successor-reinforcement-learning" class="nav-link" data-scroll-target="#deep-successor-reinforcement-learning">Deep Successor Reinforcement Learning</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Successor representations</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Slides: <a href="../slides/4.4-SR.html" target="_blank">html</a> <a href="../slides/pdf/4.4-SR.pdf" target="_blank">pdf</a></p>
<section id="model-based-vs.-model-free" class="level2">
<h2 class="anchored" data-anchor-id="model-based-vs.-model-free">Model-based vs.&nbsp;Model-free</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/jt9YvApme3Q" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<section id="comparison" class="level3">
<h3 class="anchored" data-anchor-id="comparison">Comparison</h3>
<p>Model-free methods use the <strong>reward prediction error</strong> (RPE) to update values:</p>
<p><span class="math display">
    \delta_t = r_{t+1} + \gamma \, V^\pi(s_{t+1}) - V^\pi(s_t)
</span></p>
<p><span class="math display">
    \Delta V^\pi(s_t) = \alpha \, \delta_t
</span></p>
<p>Encountered rewards propagate very slowly to all states and actions. If the environment changes (transition probabilities, rewards), they have to relearn everything. After training, selecting an action is very fast.</p>
<p>Model-based RL can learn very fast changes in the transition or reward distributions:</p>
<p><span class="math display">
    \Delta r(s_t, a_t, s_{t+1}) = \alpha \, (r_{t+1} - r(s_t, a_t, s_{t+1}))
</span></p>
<p><span class="math display">
    \Delta p(s' | s_t, a_t) = \alpha \, (\mathbb{I}(s_{t+1} = s') - p(s' | s_t, a_t))
</span></p>
<p>But selecting an action requires planning in the tree of possibilities (slow).</p>
<p>Relative advantages of MF and MB methods:</p>
<table class="table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Inference speed</th>
<th style="text-align: left;">Sample complexity</th>
<th style="text-align: left;">Optimality</th>
<th style="text-align: left;">Flexibility</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Model-free</td>
<td style="text-align: left;">fast</td>
<td style="text-align: left;">high</td>
<td style="text-align: left;">yes</td>
<td style="text-align: left;">no</td>
</tr>
<tr class="even">
<td style="text-align: left;">Model-based</td>
<td style="text-align: left;">slow</td>
<td style="text-align: left;">low</td>
<td style="text-align: left;">as good as the model</td>
<td style="text-align: left;">yes</td>
</tr>
</tbody>
</table>
<p>A trade-off would be nice… Most MB models in the deep RL literature are hybrid MB/MF models anyway.</p>
</section>
<section id="goal-directed-learning-vs.-habit-formation" class="level3">
<h3 class="anchored" data-anchor-id="goal-directed-learning-vs.-habit-formation">Goal-directed learning vs.&nbsp;habit formation</h3>
<p>Two forms of behavior are observed in the animal psychology literature:</p>
<ol type="1">
<li><strong>Goal-directed</strong> behavior learns Stimulus <span class="math inline">\rightarrow</span> Response <span class="math inline">\rightarrow</span> Outcome associations.</li>
<li><strong>Habits</strong> are developed by overtraining Stimulus <span class="math inline">\rightarrow</span> Response associations.</li>
</ol>
<p>The main difference is that habits are not influenced by <strong>outcome devaluation</strong>, i.e.&nbsp;when reard slose their value.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/rewarddevaluation.jpg" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Outcome devaluation. Credit: Bernard W. Balleine</figcaption>
</figure>
</div>
<p>The classical theory assigns MF to habits and MB to goal-directed, mostly because their sensitivity to outcome devaluation. The open question is the arbitration mechanism between these two segregated process: who takes control? Recent work suggests both systems are largely overlapping.</p>
</section>
</section>
<section id="successor-representations" class="level2">
<h2 class="anchored" data-anchor-id="successor-representations">Successor representations</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/Qt2Qtf_WbfM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<section id="principle-of-successor-representations" class="level3">
<h3 class="anchored" data-anchor-id="principle-of-successor-representations">Principle of successor representations</h3>
<p>Successor representations (SR, <span class="citation" data-cites="Dayan1993">(<a href="../references.html#ref-Dayan1993" role="doc-biblioref">Dayan, 1993</a>)</span>) have been introduced to combine MF and MB properties. Let’s split the definition of the value of a state:</p>
<p><span class="math display">
\begin{align}
    V^\pi(s) &amp;= \mathbb{E}_{\pi} [\sum_{k=0}^\infty \gamma^k \, r_{t+k+1} | s_t =s] \\
            &amp;\\
               &amp;= \mathbb{E}_{\pi} [\begin{bmatrix} 1 \\ \gamma \\ \gamma^2 \\ \ldots \\ \gamma^\infty \end{bmatrix} \times
                  \begin{bmatrix} \mathbb{I}(s_{t}) \\ \mathbb{I}(s_{t+1}) \\ \mathbb{I}(s_{t+2}) \\ \ldots \\ \mathbb{I}(s_{\infty}) \end{bmatrix}  \times
                  \begin{bmatrix} r_{t+1} \\ r_{t+2} \\ r_{t+3} \\ \ldots \\ r_{t+\infty} \end{bmatrix}
                | s_t =s]\\
\end{align}
</span></p>
<p>where <span class="math inline">\mathbb{I}(s_{t})</span> is 1 when the agent is in <span class="math inline">s_t</span> at time <span class="math inline">t</span>, 0 otherwise.</p>
<p>The left part corresponds to the <strong>transition dynamics</strong>: which states will be visited by the policy, discounted by <span class="math inline">\gamma</span>. The right part corresponds to the <strong>immediate reward</strong> in each visited state. Couldn’t we learn the transition dynamics and the reward distribution separately in a model-free manner?</p>
<p>SR rewrites the value of a state into an <strong>expected discounted future state occupancy</strong> <span class="math inline">M^\pi(s, s')</span> and an <strong>expected immediate reward</strong> <span class="math inline">r(s')</span> by summing over all possible states <span class="math inline">s'</span> of the MDP:</p>
<p><span class="math display">
\begin{align}
    V^\pi(s) &amp;= \mathbb{E}_{\pi} [\sum_{k=0}^\infty \gamma^k \, r_{t+k+1} | s_t =s] \\
            &amp;\\
               &amp;= \sum_{s' \in \mathcal{S}} \mathbb{E}_{\pi} [\sum_{k=0}^\infty \gamma^k \, \mathbb{I}(s_{t+k}=s') \times r_{t+k+1}  | s_t =s]\\
            &amp;\\
               &amp;\approx \sum_{s' \in \mathcal{S}} \mathbb{E}_{\pi} [\sum_{k=0}^\infty \gamma^k \, \mathbb{I}(s_{t+k}=s')  | s_t =s] \times \mathbb{E} [r_{t+1}  | s_{t}=s']\\
            &amp;\\
               &amp;\approx \sum_{s' \in \mathcal{S}} M^\pi(s, s') \times r(s')\\
\end{align}
</span></p>
<p>The underlying assumption is that the world dynamics are independent from the reward function (which does not depend on the policy). This allows to re-use knowledge about world dynamics in other contexts (e.g.&nbsp;a new reward function in the same environment): <strong>transfer learning</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/sr-transferlearning.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Transfer learning in Gridworld. Source: <a href="https://awjuliani.medium.com/the-present-in-terms-of-the-future-successor-representations-in-reinforcement-learning-316b78c5fa3" class="uri">https://awjuliani.medium.com/the-present-in-terms-of-the-future-successor-representations-in-reinforcement-learning-316b78c5fa3</a></figcaption>
</figure>
</div>
<p>What matters is the states that you will visit and how interesting they are, not the order in which you visit them. Knowing that being in the mensa will eventually get you some food is enough to know that being in the mensa is a good state: you do not need to remember which exact sequence of transitions will put food in your mouth.</p>
<p>SR algorithms must estimate two quantities:</p>
<ul>
<li>The <strong>expected immediate reward</strong> received after each state:</li>
</ul>
<p><span class="math display">r(s) = \mathbb{E} [r_{t+1} | s_t = s]</span></p>
<ul>
<li>The <strong>expected discounted future state occupancy</strong> (the <strong>SR</strong> itself):</li>
</ul>
<p><span class="math display">M^\pi(s, s') = \mathbb{E}_{\pi} [\sum_{k=0}^\infty \gamma^k \, \mathbb{I}(s_{t+k} = s') | s_t = s]</span></p>
<p>The value of a state <span class="math inline">s</span> is then computed with:</p>
<p><span class="math display">
    V^\pi(s) = \sum_{s' \in \mathcal{S}} M(s, s') \times r(s')
</span></p>
<p>what allows to infer the policy (e.g.&nbsp;using an actor-critic architecture). The immediate reward for a state can be estimated very quickly and flexibly after receiving each reward:</p>
<p><span class="math display">
    \Delta \, r(s_t) = \alpha \, (r_{t+1} - r(s_t))
</span></p>
<p>Imagine a very simple MDP with 4 states and a single deterministic action:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/sr-simplemdp.svg" class="img-fluid figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>The transition matrix <span class="math inline">\mathcal{P}^\pi</span> depicts the possible <span class="math inline">(s, s')</span> transitions:</p>
<p><span class="math display">\mathcal{P}^\pi = \begin{bmatrix}
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 1 \\
\end{bmatrix}
</span></p>
<p>The SR matrix <span class="math inline">M</span> also represents the future transitions discounted by <span class="math inline">\gamma</span>:</p>
<p><span class="math display">M = \begin{bmatrix}
1 &amp; \gamma &amp; \gamma^2 &amp; \gamma^3 \\
0 &amp; 1 &amp; \gamma &amp; \gamma^2 \\
0 &amp; 0 &amp; 1  &amp; \gamma\\
0 &amp; 0 &amp; 0 &amp; 1 \\
\end{bmatrix}
</span></p>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
SR matrix in a Tolman’s maze
</div>
</div>
<div class="callout-body-container callout-body">
<p><img src="../slides/img/sr-tolman.png" class="img-fluid" data-fig-align="center"> Source: <span class="citation" data-cites="Russek2017">(<a href="../references.html#ref-Russek2017" role="doc-biblioref">Russek et al., 2017</a>)</span></p>
<p>The SR represents whether a state can be reached soon from the current state (b) using the current policy. The SR depends on the policy: * A random agent will map the local neighborhood (c). * A goal-directed agent will have SR representations that follow the optimal path (d).</p>
<p>It is therefore different from the transition matrix, as it depends on behavior and rewards. The exact dynamics are lost compared to MB: it only represents whether a state is reachable, not how.</p>
</div>
</div>
<p>The SR matrix reflects the proximity between states depending on the transitions and the policy. it does not have to be a spatial relationship.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/schapiro1.png" class="img-fluid figure-img" style="width:60.0%"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/schapiro2.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption class="figure-caption">Probabilistic MDP and the corresponding SR matrix. Source: <span class="citation" data-cites="Stachenfeld2017">(<a href="../references.html#ref-Stachenfeld2017" role="doc-biblioref">Stachenfeld et al., 2017</a>)</span>.</figcaption>
</figure>
</div>
</section>
<section id="learning-successor-representations" class="level3">
<h3 class="anchored" data-anchor-id="learning-successor-representations">Learning successor representations</h3>
<p>How can we learn the SR matrix for all pairs of states?</p>
<p><span class="math display">
    M^\pi(s, s') = \mathbb{E}_{\pi} [\sum_{k=0}^\infty \gamma^k \, \mathbb{I}(s_{t+k} = s') | s_t = s]
</span></p>
<p>We first notice that the SR obeys a recursive Bellman-like equation:</p>
<p><span class="math display">\begin{aligned}
    M^\pi(s, s') &amp;= \mathbb{I}(s_{t} = s') + \mathbb{E}_{\pi} [\sum_{k=1}^\infty \gamma^k \, \mathbb{I}(s_{t+k} = s') | s_t = s] \\
            &amp;= \mathbb{I}(s_{t} = s') + \gamma \, \mathbb{E}_{\pi} [\sum_{k=0}^\infty \gamma^k \, \mathbb{I}(s_{t+k+1} = s') | s_t = s] \\
            &amp;= \mathbb{I}(s_{t} = s') + \gamma \, \mathbb{E}_{s_{t+1} \sim \mathcal{P}^\pi(s' | s)} [\mathbb{E}_{\pi} [\sum_{k=0}^\infty \gamma^k \, \mathbb{I}(s_{t+k} = s') | s_{t+1} = s] ]\\
            &amp;= \mathbb{I}(s_{t} = s') + \gamma \, \mathbb{E}_{s_{t+1} \sim \mathcal{P}^\pi(s' | s)} [M^\pi(s_{t+1}, s')]\\
\end{aligned}
</span></p>
<p>This is reminiscent of TDM: the remaining distance to the goal is 0 if I am already at the goal, or gamma the distance from the next state to the goal.</p>
<p>If we know the transition matrix for a fixed policy <span class="math inline">\pi</span>:</p>
<p><span class="math display">\mathcal{P}^\pi(s, s') = \sum_a \pi(s, a) \, p(s' | s, a)</span></p>
<p>we can obtain the SR directly with matrix inversion as we did in <strong>dynamic programming</strong>:</p>
<p><span class="math display">
    M^\pi = I + \gamma \, \mathcal{P}^\pi \times M^\pi
</span></p>
<p>so that:</p>
<p><span class="math display">
    M^\pi = (I - \gamma \, \mathcal{P}^\pi)^{-1}
</span></p>
<p>This DP approach is called <strong>model-based SR</strong> (MB-SR, <span class="citation" data-cites="Momennejad2017">(<a href="../references.html#ref-Momennejad2017" role="doc-biblioref">Momennejad et al., 2017</a>)</span>) as it necessitates to know the environment dynamics.</p>
<p>If we do not know the transition probabilities, we simply sample a single <span class="math inline">s_t, s_{t+1}</span> transition:</p>
<p><span class="math display">
    M^\pi(s_t, s') \approx \mathbb{I}(s_{t} = s') + \gamma \, M^\pi(s_{t+1}, s')
</span></p>
<p>We can define a <strong>sensory prediction error</strong> (SPE):</p>
<p><span class="math display">
    \delta^\text{SR}_t = \mathbb{I}(s_{t} = s') + \gamma \, M^\pi(s_{t+1}, s') - M(s_t, s')
</span></p>
<p>that is used to update an estimate of the SR:</p>
<p><span class="math display">
    \Delta M^\pi(s_t, s') = \alpha \, \delta^\text{SR}_t
</span></p>
<p>This is <strong>SR-TD</strong>, using a SPE instead of RPE, which learns only from transitions but ignores rewards.</p>
<p>The SPE has to be applied on ALL successor states <span class="math inline">s'</span> after a transition <span class="math inline">(s_t, s_{t+1})</span>:</p>
<p><span class="math display">
    M^\pi(s_t, \mathbf{s'}) = M^\pi(s_t, \mathbf{s'}) + \alpha \, (\mathbb{I}(s_{t}=\mathbf{s'}) + \gamma \, M^\pi(s_{t+1}, \mathbf{s'}) - M(s_t, \mathbf{s'}))
</span></p>
<p>Contrary to the RPE, the SPE is a <strong>vector</strong> of prediction errors, used to update one row of the SR matrix. The SPE tells how <strong>surprising</strong> a transition <span class="math inline">s_t \rightarrow s_{t+1}</span> is for the SR.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Summary
</div>
</div>
<div class="callout-body-container callout-body">
<p>The SR matrix represents the <strong>expected discounted future state occupancy</strong>:</p>
<p><span class="math display">M^\pi(s, s') = \mathbb{E}_{\pi} [\sum_{k=0}^\infty \gamma^k \, \mathbb{I}(s_{t+k} = s') | s_t = s]</span></p>
<p>It can be learned using a TD-like SPE from single transitions:</p>
<p><span class="math display">
    M^\pi(s_t, \mathbf{s'}) = M^\pi(s_t, \mathbf{s'}) + \alpha \, (\mathbb{I}(s_{t}=\mathbf{s'}) + \gamma \, M^\pi(s_{t+1}, \mathbf{s'}) - M(s_t, \mathbf{s'}))
</span></p>
<p>The immediate reward in each state can be learned <strong>independently from the policy</strong>:</p>
<p><span class="math display">
    \Delta \, r(s_t) = \alpha \, (r_{t+1} - r(s_t))
</span></p>
<p>The value <span class="math inline">V^\pi(s)</span> of a state is obtained by summing of all successor states:</p>
<p><span class="math display">
    V^\pi(s) = \sum_{s' \in \mathcal{S}} M(s, s') \times r(s')
</span></p>
<p>This critic can be used to train an <strong>actor</strong> <span class="math inline">\pi_\theta</span> using regular TD learning (e.g.&nbsp;A3C).</p>
</div>
</div>
<p>Note that it is straightforward to extend the idea of SR to state-action pairs:</p>
<p><span class="math display">M^\pi(s, a, s') = \mathbb{E}_{\pi} [\sum_{k=0}^\infty \gamma^k \, \mathbb{I}(s_{t+k} = s') | s_t = s, a_t = a]</span></p>
<p>allowing to estimate Q-values:</p>
<p><span class="math display">
    Q^\pi(s, a) = \sum_{s' \in \mathcal{S}} M(s, a, s') \times r(s')
</span></p>
<p>using SARSA or Q-learning-like SPEs:</p>
<p><span class="math display">
    \delta^\text{SR}_t = \mathbb{I}(s_{t} = s') + \gamma \, M^\pi(s_{t+1}, a_{t+1}, s') - M(s_t, a_{t}, s')
</span></p>
<p>depending on the choice of the next action <span class="math inline">a_{t+1}</span> (on- or off-policy).</p>
</section>
</section>
<section id="successor-features" class="level2">
<h2 class="anchored" data-anchor-id="successor-features">Successor features</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/3eOUgtU-3Vc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>The SR matrix associates each state to all others (<span class="math inline">N\times N</span> matrix):</p>
<ul>
<li>curse of dimensionality.</li>
<li>only possible for discrete state spaces.</li>
</ul>
<p>A better idea is to describe each state <span class="math inline">s</span> by a feature vector <span class="math inline">\phi(s) = [\phi_i(s)]_{i=1}^d</span> with less dimensions than the number of states. This feature vector can be constructed (see the lecture on function approximation) or learned by an autoencoder (latent representation).</p>
<p>The <strong>successor feature representation</strong> (SFR) represents the discounted probability of observing a feature <span class="math inline">\phi_j</span> after being in <span class="math inline">s</span>. Instead of predicting when the agent will see a cat after being in the current state <span class="math inline">s</span>, the SFR predicts when it will see eyes, ears or whiskers independently:</p>
<p><span class="math display">
    M^\pi_j(s) = M^\pi(s, \phi_j) = \mathbb{E}_{\pi} [\sum_{k=0}^\infty \gamma^k \, \mathbb{I}(\phi_j(s_{t+k})) | s_t = s, a_t = a]
</span></p>
<p>Linear SFR <span class="citation" data-cites="Gehring2015">(<a href="../references.html#ref-Gehring2015" role="doc-biblioref">Gehring, 2015</a>)</span> supposes that it can be linearly approximated from the features of the current state:</p>
<p><span class="math display">
    M^\pi_j(s) = M^\pi(s, \phi_j) = \sum_{i=1}^d m_{i, j} \, \phi_i(s)
</span></p>
<p>The value of a state is now defined as the sum over successor features of their immediate reward discounted by the SFR:</p>
<p><span class="math display">
    V^\pi(s) = \sum_{j=1}^d M^\pi_j(s) \, r(\phi_j) = \sum_{j=1}^d r(\phi_j) \, \sum_{i=1}^d m_{i, j} \, \phi_i(s)
</span></p>
<p>The SFR matrix <span class="math inline">M^\pi = [m_{i, j}]_{i, j}</span> associates each feature <span class="math inline">\phi_i</span> of the current state to all successor features <span class="math inline">\phi_j</span>: Knowing that I see a kitchen door in the current state, how likely will I see a food outcome in the near future?</p>
<p>Each successor feature <span class="math inline">\phi_j</span> is associated to an expected immediate reward <span class="math inline">r(\phi_j)</span>: A good state is a state where food features (high <span class="math inline">r(\phi_j)</span>) are likely to happen soon (high <span class="math inline">m_{i, j}</span>).</p>
<p>In matrix-vector form:</p>
<p><span class="math display">
    V^\pi(s) = \mathbf{r}^T \times M^\pi \times \phi(s)
</span></p>
<p>The reward vector <span class="math inline">\mathbf{r}</span> only depends on the features and can be learned independently from the policy, but can be made context-dependent: Food features can be made more important when the agent is hungry, less when thirsty.</p>
<p><strong>Transfer learning</strong> becomes possible in the same environment: * Different goals (searching for food or water, going to place A or B) only require different reward vectors. * The dynamics of the environment are stored in the SFR.</p>
<p>How can we learn the SFR matrix <span class="math inline">M^\pi</span>?</p>
<p><span class="math display">
    V^\pi(s) = \mathbf{r}^T \times M^\pi \times \phi(s)
</span></p>
<p>We only need to use the sensory prediction error for a transition between the feature vectors <span class="math inline">\phi(s_t)</span> and <span class="math inline">\phi(s_{t+1})</span>:</p>
<p><span class="math display">\delta_t^\text{SFR} = \phi(s_t) + \gamma \, M^\pi \times \phi(s_{t+1}) - M^\pi \times \phi(s_t)</span></p>
<p>and use it to update the whole matrix:</p>
<p><span class="math display">\Delta M^\pi = \delta_t^\text{SFR} \times \phi(s_t)^T</span></p>
<p>However, this linear approximation scheme only works for <strong>fixed</strong> feature representation <span class="math inline">\phi(s)</span>. We need to go deeper…</p>
</section>
<section id="deep-successor-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="deep-successor-reinforcement-learning">Deep Successor Reinforcement Learning</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/DSR.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Deep Successor Reinforcement Learning architecture. <span class="citation" data-cites="Kulkarni2016">(<a href="../references.html#ref-Kulkarni2016" role="doc-biblioref">Kulkarni et al., 2016</a>)</span>.</figcaption>
</figure>
</div>
<p>Each state <span class="math inline">s_t</span> is represented by a D-dimensional (D=512) vector <span class="math inline">\phi(s_t) = f_\theta(s_t)</span> which is the output of an encoder. A decoder <span class="math inline">g_{\hat{\theta}}</span> is used to provide a reconstruction loss, so <span class="math inline">\phi(s_t)</span> is a latent representation of an autoencoder:</p>
<p><span class="math display">\mathcal{L}_\text{reconstruction}(\theta, \hat{\theta}) = \mathbb{E}[(g_{\hat{\theta}}(\phi(s_t)) - s_t)^2]</span></p>
<p>The immediate reward <span class="math inline">R(s_t)</span> is linearly predicted from the feature vector <span class="math inline">\phi(s_t)</span> using a reward vector <span class="math inline">\mathbf{w}</span>.</p>
<p><span class="math display">R(s_t) = \phi(s_t)^T \times \mathbf{w}</span></p>
<p><span class="math display">\mathcal{L}_\text{reward}(\mathbf{w}, \theta) = \mathbb{E}[(r_{t+1} - \phi(s_t)^T \times \mathbf{w})^2]</span></p>
<p>The reconstruction loss is important, otherwise the latent representation <span class="math inline">\phi(s_t)</span> would be too reward-oriented and would not generalize. The reward function is learned on a single task, but it can fine-tuned on another task, with all other weights frozen.</p>
<p>For each available action <span class="math inline">a</span>, a DNN <span class="math inline">u_\alpha</span> predicts the future feature occupancy <span class="math inline">M(s, s', a)</span> for the current state:</p>
<p><span class="math display">m_{s_t a} = u_\alpha(s_t, a)</span></p>
<p>The Q-value of an action is simply the dot product between the SR of an action and the reward vector <span class="math inline">\mathbf{w}</span>:</p>
<p><span class="math display">Q(s_t, a) = \mathbf{w}^T \times m_{s_t a} </span></p>
<p>The selected action is <span class="math inline">\epsilon</span>-greedily selected around the greedy action:</p>
<p><span class="math display">a_t = \text{arg}\max_a Q(s_t, a)</span></p>
<p>The SR of each action is learned using the Q-learning-like SPE (with fixed <span class="math inline">\theta</span> and a target network <span class="math inline">u_{\alpha'}</span>):</p>
<p><span class="math display">\mathcal{L}^\text{SPE}(\alpha) = \mathbb{E}[\sum_a (\phi(s_t) + \gamma \, \max_{a'} u_{\alpha'}(s_{t+1}, a') - u_\alpha(s_t, a))^2]</span></p>
<p>The compound loss is used to train the complete network end-to-end <strong>off-policy</strong> using a replay buffer (DQN-like).</p>
<p><span class="math display">\mathcal{L}(\theta, \hat{\theta}, \mathbf{w}, \alpha) = \mathcal{L}_\text{reconstruction}(\theta, \hat{\theta}) + \mathcal{L}_\text{reward}(\mathbf{w}, \theta) + \mathcal{L}^\text{SPE}(\alpha)</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/DSR-algorithm.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Deep Successor Reinforcement Learning algorithm. <span class="citation" data-cites="Kulkarni2016">(<a href="../references.html#ref-Kulkarni2016" role="doc-biblioref">Kulkarni et al., 2016</a>)</span>.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/DSR-results.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Deep Successor Reinforcement Learning results. <span class="citation" data-cites="Kulkarni2016">(<a href="../references.html#ref-Kulkarni2016" role="doc-biblioref">Kulkarni et al., 2016</a>)</span>.</figcaption>
</figure>
</div>
<p>The interesting property is that you do not need rewards to learn: * A random agent can be used to learn the encoder and the SR, but <span class="math inline">\mathbf{w}</span> can be left untouched. * When rewards are introduced (or changed), only <span class="math inline">\mathbf{w}</span> has to be adapted, while DQN would have to re-learn all Q-values.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/DSR-results2.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Change in the value of distal rewards. <span class="citation" data-cites="Kulkarni2016">(<a href="../references.html#ref-Kulkarni2016" role="doc-biblioref">Kulkarni et al., 2016</a>)</span>.</figcaption>
</figure>
</div>
<p>This is the principle of <strong>latent learning</strong> in animal psychology: fooling around in an environment without a goal allows to learn the structure of the world, what can speed up learning when a task is introduced. The SR is a <strong>cognitive map</strong> of the environment: learning task-unspecific relationships.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>The same idea was published by three different groups at the same time (preprint in 2016, conference in 2017): <span class="citation" data-cites="Barreto2016">(<a href="../references.html#ref-Barreto2016" role="doc-biblioref">Barreto et al., 2016</a>)</span>, <span class="citation" data-cites="Kulkarni2016">(<a href="../references.html#ref-Kulkarni2016" role="doc-biblioref">Kulkarni et al., 2016</a>)</span>, <span class="citation" data-cites="Zhang2016">(<a href="../references.html#ref-Zhang2016" role="doc-biblioref">Zhang et al., 2016</a>)</span>. The <span class="citation" data-cites="Barreto2016">(<a href="../references.html#ref-Barreto2016" role="doc-biblioref">Barreto et al., 2016</a>)</span> is from Deepmind, so it tends to be cited more…</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Visual Semantic Planning using Deep Successor Representations
</div>
</div>
<div class="callout-body-container callout-body">
<p>See the paper: <span class="citation" data-cites="Zhu2017">(<a href="../references.html#ref-Zhu2017" role="doc-biblioref">Zhu et al., 2017</a>)</span></p>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/_2pYVw6ATKo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-Barreto2016" class="csl-entry" role="listitem">
Barreto, A., Dabney, W., Munos, R., Hunt, J. J., Schaul, T., van Hasselt, H., et al. (2016). Successor <span>Features</span> for <span>Transfer</span> in <span>Reinforcement Learning</span>. <a href="http://arxiv.org/abs/1606.05312">http://arxiv.org/abs/1606.05312</a>.
</div>
<div id="ref-Dayan1993" class="csl-entry" role="listitem">
Dayan, P. (1993). Improving <span>Generalization</span> for <span>Temporal Difference Learning</span>: <span>The Successor Representation</span>. <em>Neural Computation</em> 5, 613–624. doi:<a href="https://doi.org/10.1162/neco.1993.5.4.613">10.1162/neco.1993.5.4.613</a>.
</div>
<div id="ref-Gehring2015" class="csl-entry" role="listitem">
Gehring, C. A. (2015). Approximate <span>Linear Successor Representation</span>. in, 5. <a href="http://people.csail.mit.edu/gehring/publications/clement-gehring-rldm-2015.pdf">http://people.csail.mit.edu/gehring/publications/clement-gehring-rldm-2015.pdf</a>.
</div>
<div id="ref-Kulkarni2016" class="csl-entry" role="listitem">
Kulkarni, T. D., Saeedi, A., Gautam, S., and Gershman, S. J. (2016). Deep <span>Successor Reinforcement Learning</span>. <a href="http://arxiv.org/abs/1606.02396">http://arxiv.org/abs/1606.02396</a>.
</div>
<div id="ref-Momennejad2017" class="csl-entry" role="listitem">
Momennejad, I., Russek, E. M., Cheong, J. H., Botvinick, M. M., Daw, N. D., and Gershman, S. J. (2017). The successor representation in human reinforcement learning. <em>Nature Human Behaviour</em> 1, 680–692. doi:<a href="https://doi.org/10.1038/s41562-017-0180-8">10.1038/s41562-017-0180-8</a>.
</div>
<div id="ref-Russek2017" class="csl-entry" role="listitem">
Russek, E. M., Momennejad, I., Botvinick, M. M., Gershman, S. J., and Daw, N. D. (2017). Predictive representations can link model-based reinforcement learning to model-free mechanisms. <em>PLOS Computational Biology</em> 13, e1005768. doi:<a href="https://doi.org/10.1371/journal.pcbi.1005768">10.1371/journal.pcbi.1005768</a>.
</div>
<div id="ref-Stachenfeld2017" class="csl-entry" role="listitem">
Stachenfeld, K. L., Botvinick, M. M., and Gershman, S. J. (2017). The hippocampus as a predictive map. <em>Nature Neuroscience</em> 20, 1643–1653. doi:<a href="https://doi.org/10.1038/nn.4650">10.1038/nn.4650</a>.
</div>
<div id="ref-Zhang2016" class="csl-entry" role="listitem">
Zhang, J., Springenberg, J. T., Boedecker, J., and Burgard, W. (2016). Deep <span>Reinforcement Learning</span> with <span>Successor Features</span> for <span>Navigation</span> across <span>Similar Environments</span>. <a href="http://arxiv.org/abs/1612.05533">http://arxiv.org/abs/1612.05533</a>.
</div>
<div id="ref-Zhu2017" class="csl-entry" role="listitem">
Zhu, Y., Gordon, D., Kolve, E., Fox, D., Fei-Fei, L., Gupta, A., et al. (2017). Visual <span>Semantic Planning</span> using <span>Deep Successor Representations</span>. <a href="http://arxiv.org/abs/1705.08080">http://arxiv.org/abs/1705.08080</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../notes/4.3-AlphaGo.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">AlphaGo</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/5.1-Outlook.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">Outlook</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">Copyright Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de" class="email">julien.vitay@informatik.tu-chemnitz.de</a></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>