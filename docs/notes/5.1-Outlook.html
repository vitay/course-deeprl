<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Deep Reinforcement Learning - Outlook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../exercises/Content.html" rel="next">
<link href="../notes/4.4-SR.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notes/5.1-Outlook.html"><strong>Outlook</strong></a></li><li class="breadcrumb-item"><a href="../notes/5.1-Outlook.html"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Outlook</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../notes/img/tuc-new.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Deep Reinforcement Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text"><strong>Introduction</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.1-Introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1.2-Math.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Math basics (optional)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text"><strong>Tabular RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.1-Bandits.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.2-MDP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Markov Decision Processes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.3-DP.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Dynamic Programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.4-MC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Monte-Carlo (MC) methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.5-TD.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Temporal Difference learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.6-FA.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Function approximation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2.7-NN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text"><strong>Model-free RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.1-DQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Q-Learning (DQN)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.2-BeyondDQN.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Beyond DQN</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.3-PG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Policy gradient (PG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.4-A3C.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Advantage actor-critic (A2C, A3C)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.5-DDPG.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Deep Deterministic Policy Gradient (DDPG)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.6-PPO.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Natural gradients (TRPO, PPO)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3.7-SAC.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Maximum Entropy RL (SAC)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text"><strong>Model-based RL</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.1-MB.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Model-based RL</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.2-LearnedModels.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Learned world models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.3-AlphaGo.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">AlphaGo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/4.4-SR.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Successor representations</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text"><strong>Outlook</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/5.1-Outlook.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Outlook</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text"><strong>Exercises</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Content.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">List of exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/Installation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Python installation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/1-Python-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction To Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/2-Numpy-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Numpy and Matplotlib</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/3-Sampling-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Sampling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/4-Bandits-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/5-Bandits2-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Bandits - part 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/6-DP-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Dynamic programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/7-Gym-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Gym environments</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/8-MonteCarlo-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Monte-Carlo control</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/9-TD-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Q-learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/10-Eligibilitytraces-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Eligibility traces</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/11-Keras-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Keras tutorial</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/12-DQN-solution.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">DQN</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#limits-of-deep-reinforcement-learning" id="toc-limits-of-deep-reinforcement-learning" class="nav-link active" data-scroll-target="#limits-of-deep-reinforcement-learning">Limits of deep reinforcement learning</a>
  <ul class="collapse">
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#rl-libraries" id="toc-rl-libraries" class="nav-link" data-scroll-target="#rl-libraries">RL libraries</a></li>
  </ul></li>
  <li><a href="#inverse-rl---learning-the-reward-function" id="toc-inverse-rl---learning-the-reward-function" class="nav-link" data-scroll-target="#inverse-rl---learning-the-reward-function">Inverse RL - learning the reward function</a></li>
  <li><a href="#intrinsic-motivation-and-curiosity" id="toc-intrinsic-motivation-and-curiosity" class="nav-link" data-scroll-target="#intrinsic-motivation-and-curiosity">Intrinsic motivation and curiosity</a></li>
  <li><a href="#hierarchical-rl---learning-different-action-levels" id="toc-hierarchical-rl---learning-different-action-levels" class="nav-link" data-scroll-target="#hierarchical-rl---learning-different-action-levels">Hierarchical RL - learning different action levels</a></li>
  <li><a href="#meta-reinforcement-learning---rl2" id="toc-meta-reinforcement-learning---rl2" class="nav-link" data-scroll-target="#meta-reinforcement-learning---rl2">Meta Reinforcement learning - RL<span class="math inline">^2</span></a></li>
  <li><a href="#offline-rl" id="toc-offline-rl" class="nav-link" data-scroll-target="#offline-rl">Offline RL</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Outlook</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Slides: <a href="../slides/5.1-Outlook.html" target="_blank">html</a> <a href="../slides/pdf/5.1-Outlook.pdf" target="_blank">pdf</a></p>
<section id="limits-of-deep-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="limits-of-deep-reinforcement-learning">Limits of deep reinforcement learning</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/JDQdcTpHryM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<section id="overview" class="level3">
<h3 class="anchored" data-anchor-id="overview">Overview</h3>
<p><strong>Model-free methods</strong> (DQN, A3C, DDPG, PPO, SAC) are able to find optimal policies in complex MDPs by just <strong>sampling</strong> transitions. They suffer however from a high <strong>sample complexity</strong>, i.e.&nbsp;they need ridiculous amounts of samples to converge.</p>
<p><strong>Model-based methods</strong> (I2A, Dreamer, MuZero) use <strong>learned dynamics</strong> to predict the future and plan the consequences of an action. The sample complexity is lower, but learning a good model can be challenging. Inference times can be prohibitive.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/drl-overview.svg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Overview of deep RL methods. Source: <a href="https://github.com/avillemin/RL-Personnal-Notebook" class="uri">https://github.com/avillemin/RL-Personnal-Notebook</a></figcaption>
</figure>
</div>
<p>Deep RL is still very unstable. Depending on initialization, deep RL networks may or may not converge (30% of runs converge to a worse policy than a random agent). Careful optimization such as TRPO / PPO help, but not completely. You never know if failure is your fault (wrong network, bad hyperparameters, bug), or just bad luck.</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
Deep RL is popular because it's the only area in ML where it's socially acceptable to train on the test set.
</p>
— Jacob Andreas (<span class="citation" data-cites="jacobandreas">(<a href="../references.html#ref-jacobandreas" role="doc-biblioref"><strong>jacobandreas?</strong></a>)</span>) <a href="https://twitter.com/jacobandreas/status/924356906344267776?ref_src=twsrc%5Etfw">October 28, 2017</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>As it uses neural networks, deep RL <strong>overfits</strong> its training data, i.e.&nbsp;the environment it is trained on. If you change anything to the environment dynamics, you need to retrain from scratch. OpenAI Five collects 900 years of game experience per day on Dota 2: it overfits the game, it does not learn how to play. Modify the map a little bit and everything is gone (but see Meta RL - RL<span class="math inline">^2</span> later).</p>
<p>Classical methods sometimes still work better. Model Predictive Control (MPC) is able to control Mujoco robots much better than RL through classical optimization techniques (e.g.&nbsp;iterative LQR) while needing much less computations. If you have a good physics model, do not use DRL. Reserve it for unknown systems, or when using noisy sensors (images). Genetic algorithms (CMA-ES) sometimes give better results than RL to train policy networks.</p>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/uRVAX_sFT24" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>You cannot do that with deep RL (yet):</p>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/fRj34o4hN4I" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
</section>
<section id="rl-libraries" class="level3">
<h3 class="anchored" data-anchor-id="rl-libraries">RL libraries</h3>
<ul>
<li><code>keras-rl</code>: many deep RL algorithms implemented directly in keras: DQN, DDQN, DDPG, Continuous DQN (CDQN or NAF), Cross-Entropy Method (CEM)…</li>
</ul>
<p><a href="https://github.com/matthiasplappert/keras-rl" class="uri">https://github.com/matthiasplappert/keras-rl</a></p>
<ul>
<li><code>OpenAI Baselines</code> from OpenAI: A2C, ACER, ACKTR, DDPG, DQN, PPO, TRPO… Not maintained anymore.</li>
</ul>
<p><a href="https://github.com/openai/baselines" class="uri">https://github.com/openai/baselines</a></p>
<ul>
<li><code>Stable baselines</code> from Inria Flowers, a clean rewrite of OpenAI baselines also including SAC and TD3.</li>
</ul>
<p><a href="https://github.com/hill-a/stable-baselines" class="uri">https://github.com/hill-a/stable-baselines</a></p>
<ul>
<li><code>rlkit</code> from Vitchyr Pong (PhD student at Berkeley) with in particular model-based algorithms (TDM).</li>
</ul>
<p><a href="https://github.com/vitchyr/rlkit" class="uri">https://github.com/vitchyr/rlkit</a></p>
<ul>
<li><code>chainer-rl</code> implemented in Chainer: A3C, ACER, DQN, DDPG, PGT, PCL, PPO, TRPO.</li>
</ul>
<p><a href="https://github.com/chainer/chainerrl" class="uri">https://github.com/chainer/chainerrl</a></p>
<ul>
<li><code>RL Mushroom</code> is a very modular library based on Pytorch allowing to implement DQN and variants, DDPG, SAC, TD3, TRPO, PPO.</li>
</ul>
<p><a href="https://github.com/MushroomRL/mushroom-rl" class="uri">https://github.com/MushroomRL/mushroom-rl</a></p>
<ul>
<li><code>Tensorforce</code> implement in tensorflow: DQN and variants, A3C, DDPG, TRPO, PPO.</li>
</ul>
<p><a href="https://github.com/tensorforce/tensorforce" class="uri">https://github.com/tensorforce/tensorforce</a></p>
<ul>
<li><code>Tensorflow Agents</code> is officially supported by tensorflow: DQN, A3C, DDPG, TD3, PPO, SAC.</li>
</ul>
<p><a href="https://github.com/tensorflow/agents" class="uri">https://github.com/tensorflow/agents</a></p>
<ul>
<li><code>Coach</code> from Intel Nervana also provides many state-of-the-art algorithms.</li>
</ul>
<p><a href="https://github.com/NervanaSystems/coach" class="uri">https://github.com/NervanaSystems/coach</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/coach.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Deep RL algorithms available in Coach. Source: <a href="https://github.com/NervanaSystems/coach" class="uri">https://github.com/NervanaSystems/coach</a></figcaption>
</figure>
</div>
<ul>
<li><code>rllib</code> is part of the more global ML framework Ray, which also includes Tune for hyperparameter optimization.</li>
</ul>
<p>It has implementations in both tensorflow and Pytorch.</p>
<p>All major model-free algorithms are implemented (DQN, Rainbow, A3C, DDPG, PPO, SAC), including their distributed variants (Ape-X, IMPALA, TD3) but also model-based algorithms (Dreamer!)</p>
<p><a href="https://docs.ray.io/en/master/rllib.html" class="uri">https://docs.ray.io/en/master/rllib.html</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/rllib.svg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Architecture of rllib. Source: <a href="https://docs.ray.io/en/master/rllib.html" class="uri">https://docs.ray.io/en/master/rllib.html</a></figcaption>
</figure>
</div>
</section>
</section>
<section id="inverse-rl---learning-the-reward-function" class="level2">
<h2 class="anchored" data-anchor-id="inverse-rl---learning-the-reward-function">Inverse RL - learning the reward function</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/k-nr6Pwb9ds" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>RL is an optimization method: it maximizes the reward function that you provide it. If you do not design the reward function correctly, the agent may not do what you expect. In the Coast runners game, turbos provide small rewards but respawn very fast: it is more optimal to collect them repeatedly than to try to finish the race.</p>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/tlOIHko8ySg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>Defining the reward function that does what you want becomes an art. RL algorithms work better with dense rewards than sparse ones. It is tempting to introduce intermediary rewards. You end up covering so many special cases that it becomes unusable: Go as fast as you can but not in a curve, except if you are on a closed circuit but not if it rains…</p>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/8QnD8ZM0YCo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>In the OpenAI <strong>Lego stacking</strong> paper <span class="citation" data-cites="Popov2017">(<a href="../references.html#ref-Popov2017" role="doc-biblioref">Popov et al., 2017</a>)</span>, it was perhaps harder to define the reward function than to implement DDPG.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/lego_reward.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Lego stacking handmade reward function <span class="citation" data-cites="Popov2017">(<a href="../references.html#ref-Popov2017" role="doc-biblioref">Popov et al., 2017</a>)</span>.</figcaption>
</figure>
</div>
<p>The goal of <strong>inverse RL</strong> (see <span class="citation" data-cites="Arora2019">(<a href="../references.html#ref-Arora2019" role="doc-biblioref">Arora and Doshi, 2019</a>)</span> for a review) is to learn from <strong>demonstrations</strong> (e.g.&nbsp;from humans) which reward function is maximized. This is not <strong>imitation learning</strong>, where you try to learn and reproduce actions. The goal if to find a <strong>parametrized representation</strong> of the reward function:</p>
<p><span class="math display">\hat{r}(s) = \sum_{i=1}^K w_i \, \varphi_i(s)</span></p>
<p>When the reward function has been learned, you can train a RL algorithm to find the optimal policy.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/inverseRL.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">Inverse RL allows to learn from demonstrations. Source: <a href="http://www.miubiq.cs.titech.ac.jp/modeling-risk-anticipation-and-defensive-driving-on-residential-roads-using-inverse-reinforcement-learning/" class="uri">http://www.miubiq.cs.titech.ac.jp/modeling-risk-anticipation-and-defensive-driving-on-residential-roads-using-inverse-reinforcement-learning/</a></figcaption>
</figure>
</div>
</section>
<section id="intrinsic-motivation-and-curiosity" class="level2">
<h2 class="anchored" data-anchor-id="intrinsic-motivation-and-curiosity">Intrinsic motivation and curiosity</h2>
<p>One fundamental problem of RL is its dependence on the <strong>reward function</strong>. When rewards are <strong>sparse</strong>, the agent does not learn much (but see successor representations) unless its random exploration policy makes it discover rewards. The reward function is <strong>handmade</strong>, what is difficult in realistic complex problems.</p>
<p>Human learning does not (only) rely on maximizing rewards or achieving goals. Especially infants discover the world by <strong>playing</strong>, i.e.&nbsp;interacting with the environment out of <strong>curiosity</strong>.</p>
<blockquote class="blockquote">
<p>What happens if I do that? Oh, that’s fun.</p>
</blockquote>
<p>This called <strong>intrinsic motivation</strong>: we are motivated by understanding the world, not only by getting rewards. Rewards are internally generated.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/intrinsicmotivation.gif" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="figure-caption">In intrinsic motivation, rewards are generated internally depending on the achieved states. Source: <span class="citation" data-cites="Barto2013">(<a href="../references.html#ref-Barto2013" role="doc-biblioref">Barto, 2013</a>)</span>.</figcaption>
</figure>
</div>
<p>What is <strong>intrinsically</strong> rewarding / motivating / fun? Mostly what has <strong>unexpected</strong> consequences.</p>
<ul>
<li>If you can predict what is going to happen, it becomes boring.</li>
<li>If you cannot predict, you can become <strong>curious</strong> and try to <strong>explore</strong> that action.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/intrinsicreward.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Intrinsic rewards are defined by the ability to predict states. Source: <a href="https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa" class="uri">https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa</a></figcaption>
</figure>
</div>
<p>The <strong>intrinsic reward</strong> (IR) of an action is defined as the sensory prediction error:</p>
<p><span class="math display">
    \text{IR}(s_t, a_t, s_{t+1}) = || f(s_t, a_t) - s_{t+1}||
</span></p>
<p>where <span class="math inline">f(s_t, a_t)</span> is a <strong>forward model</strong> predicting the sensory consequences of an action. An agent maximizing the IR will tend to visit unknown / poorly predicted states (<strong>exploration</strong>).</p>
<p>Is it a good idea to predict frames directly? Frames are highly dimensional and there will always be a remaining error.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/intrinsicreward-hard.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Intrinsic rewards are defined by the ability to predict states. Source: <a href="https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa" class="uri">https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa</a></figcaption>
</figure>
</div>
<p>Moreover, they can be noisy and unpredictable, without being particularly interesting.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/leaves.gif" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="figure-caption">Falling leaves are hard to predict, but hardly interesting. Source: Giphy.</figcaption>
</figure>
</div>
<p>What can we do? As usual, predict in a latent space!</p>
<p>The intrinsic curiosity module (ICM, <span class="citation" data-cites="Pathak2017">(<a href="../references.html#ref-Pathak2017" role="doc-biblioref">Pathak et al., 2017</a>)</span>) learns to provide an intrinsic reward for a transition <span class="math inline">(s_t, a_t, s_{t+1})</span> by comparing the predicted latent representation <span class="math inline">\hat{\phi}(s_{t+1})</span> (using a <strong>forward</strong> model) to its “true” latent representation <span class="math inline">\phi(s_{t+1})</span>. The feature representation <span class="math inline">\phi(s_t)</span> is trained using an <strong>inverse model</strong> predicting the action leading from <span class="math inline">s_t</span> to <span class="math inline">s_{t+1}</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/icm.jpg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">intrinsic curiosity module. <span class="citation" data-cites="Pathak2017">(<a href="../references.html#ref-Pathak2017" role="doc-biblioref">Pathak et al., 2017</a>)</span></figcaption>
</figure>
</div>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/J3FHOyhUn3A" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Curiosity-driven RL on Atari games <span class="citation" data-cites="Burda2018">(<a href="../references.html#ref-Burda2018" role="doc-biblioref">Burda et al., 2018</a>)</span>:
</div>
</div>
<div class="callout-body-container callout-body">
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/l1FqtAHfJLI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
</div>
</div>
</section>
<section id="hierarchical-rl---learning-different-action-levels" class="level2">
<h2 class="anchored" data-anchor-id="hierarchical-rl---learning-different-action-levels">Hierarchical RL - learning different action levels</h2>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/O7cKkOzWn3s" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>In all previous RL methods, the action space is fixed. When you read a recipe, the actions are “Cut carrots”, “Boil water”, etc. But how do you perform these <strong>high-level actions</strong>? Break them into subtasks iteratively until you arrive to muscle activations. But it is not possible to learn to cook a boeuf bourguignon using muscle activations as actions.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/hierarchicalRL.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Hierarchical structure of preparing a boeuf bourguignon. Source: <a href="https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/" class="uri">https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/</a></figcaption>
</figure>
</div>
<p>Sub-policies (<strong>options</strong>) can be trained to solve simple tasks (going left, right, etc). A <strong>meta-learner</strong> or controller then learns to call each sub-policy when needed, at a much lower frequency <span class="citation" data-cites="Frans2017">(<a href="../references.html#ref-Frans2017" role="doc-biblioref">Frans et al., 2017</a>)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/MLSH.gif" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Meta Learning Shared Hierarchies <span class="citation" data-cites="Frans2017">(<a href="../references.html#ref-Frans2017" role="doc-biblioref">Frans et al., 2017</a>)</span>. Source: <a href="https://openai.com/blog/learning-a-hierarchy/" class="uri">https://openai.com/blog/learning-a-hierarchy/</a></figcaption>
</figure>
</div>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/zkJmH4NlzPs" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>Some additional references on Hierarchical Reinforcement Learning</p>
<ul>
<li><strong>MLSH:</strong> Frans, K., Ho, J., Chen, X., Abbeel, P., and Schulman, J. (2017). Meta Learning Shared Hierarchies. arXiv:1710.09767.</li>
<li><strong>FUN:</strong> Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver, D., et al.&nbsp;(2017). FeUdal Networks for Hierarchical Reinforcement Learning. arXiv:1703.01161</li>
<li><strong>Option-Critic architecture:</strong> Bacon, P.-L., Harb, J., and Precup, D. (2016). The Option-Critic Architecture. arXiv:1609.05140.</li>
<li><strong>HIRO:</strong> Nachum, O., Gu, S., Lee, H., and Levine, S. (2018). Data-Efficient Hierarchical Reinforcement Learning. arXiv:1805.08296.</li>
<li><strong>HAC:</strong> Levy, A., Konidaris, G., Platt, R., and Saenko, K. (2019). Learning Multi-Level Hierarchies with Hindsight. arXiv:1712.00948.</li>
<li><strong>Spinal-cortical:</strong> Heess, N., Wayne, G., Tassa, Y., Lillicrap, T., Riedmiller, M., and Silver, D. (2016). Learning and Transfer of Modulated Locomotor Controllers. arXiv:1610.05182.</li>
</ul>
</section>
<section id="meta-reinforcement-learning---rl2" class="level2">
<h2 class="anchored" data-anchor-id="meta-reinforcement-learning---rl2">Meta Reinforcement learning - RL<span class="math inline">^2</span></h2>
<p><strong>Meta learning</strong> is the ability to reuse skills acquired on a set of tasks to quickly acquire new (similar) ones (generalization).</p>
<p><img src="../slides/img/metalearning.png" class="img-fluid" style="width:100.0%" data-fig-align="center"> <img src="../slides/img/ml10.gif" class="img-fluid" style="width:100.0%" data-fig-align="center" alt="Meta Reinforcement learning. Source: https://meta-world.github.io/"></p>
<p>Meta RL is based on the idea of <strong>fast and slow</strong> learning: * Slow learning is the adaptation of weights in the NN. * Fast learning is the adaptation to changes in the environment.</p>
<p>A simple strategy developed concurrently by <span class="citation" data-cites="Wang2017a">(<a href="../references.html#ref-Wang2017a" role="doc-biblioref">Wang et al., 2017</a>)</span> and <span class="citation" data-cites="Duan2016a">(<a href="../references.html#ref-Duan2016a" role="doc-biblioref">Duan et al., 2016</a>)</span>is to have a model-free algorithm (e.g.&nbsp;A3C) integrate with a LSTM layer not only the current state <span class="math inline">s_t</span>, but also the previous action <span class="math inline">a_{t-1}</span> and reward <span class="math inline">r_t</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/metarl-lstm.png" class="img-fluid figure-img" style="width:30.0%"></p>
<figcaption class="figure-caption">Meta RL uses a LSTM layer to encode past actions and rewards in the state representation. Source: <span class="citation" data-cites="Wang2017a">(<a href="../references.html#ref-Wang2017a" role="doc-biblioref">Wang et al., 2017</a>)</span></figcaption>
</figure>
</div>
<p>The policy of the agent becomes <strong>memory-guided</strong>: it selects an action depending on what it did before, not only the state.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/RL_2.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Meta RL algorithms are trained on a set of similar MDPs. Source: <span class="citation" data-cites="Duan2016a">(<a href="../references.html#ref-Duan2016a" role="doc-biblioref">Duan et al., 2016</a>)</span></figcaption>
</figure>
</div>
<p>The algorithm is trained on a set of similar MDPs:</p>
<ol type="1">
<li>Select a MDP <span class="math inline">\mathcal{M}</span>.</li>
<li>Reset the internal state of the LSTM.</li>
<li>Sample trajectories and adapt the weights.</li>
<li>Repeat 1, 2 and 3.</li>
</ol>
<p>The meta RL can be be trained an a multitude of 2-armed bandits, each giving a reward of 1 with probability <span class="math inline">p</span> and <span class="math inline">1-p</span>. Left is a classical bandit algorithm, right is the meta bandit:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/metarl-twoarmed.gif" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">Classical bandit (left) and meta-bandit (right) learning a new two-armed bandit problem. Source: <a href="https://hackernoon.com/learning-policies-for-learning-policies-meta-reinforcement-learning-rl%C2%B2-in-tensorflow-b15b592a2ddf" class="uri">https://hackernoon.com/learning-policies-for-learning-policies-meta-reinforcement-learning-rl%C2%B2-in-tensorflow-b15b592a2ddf</a></figcaption>
</figure>
</div>
<p>The meta bandit has learned that the best strategy for any 2-armed bandit is to sample both actions randomly at the beginning and then stick to the best one. The meta bandit does not learn to solve each problem, it learns <strong>how</strong> to solve them.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="citation" data-cites="Belkhale2021">(<a href="../references.html#ref-Belkhale2021" role="doc-biblioref">Belkhale et al., 2021</a>)</span> <a href="https://sites.google.com/view/meta-rl-for-flight" class="uri">https://sites.google.com/view/meta-rl-for-flight</a></p>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/AP5FgKjFpvQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
</div>
</div>
<p>Additional references on meta RL:</p>
<ul>
<li><strong>Meta RL:</strong> Wang JX, Kurth-Nelson Z, Tirumala D, Soyer H, Leibo JZ, Munos R, Blundell C, Kumaran D, Botvinick M. (2016). Learning to reinforcement learn. arXiv:161105763.</li>
<li><strong>RL<span class="math inline">^2</span></strong> Duan Y, Schulman J, Chen X, Bartlett PL, Sutskever I, Abbeel P. 2016. RL<span class="math inline">^2</span>: Fast Reinforcement Learning via Slow Reinforcement Learning. arXiv:161102779.</li>
<li><strong>MAML:</strong> Finn C, Abbeel P, Levine S. (2017). Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. arXiv:170303400.</li>
<li><strong>PEARL:</strong> Rakelly K, Zhou A, Quillen D, Finn C, Levine S. (2019). Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables. arXiv:190308254.</li>
<li><strong>POET:</strong> Wang R, Lehman J, Clune J, Stanley KO. (2019). Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions. arXiv:190101753.</li>
<li><strong>MetaGenRL:</strong> Kirsch L, van Steenkiste S, Schmidhuber J. (2020). Improving Generalization in Meta Reinforcement Learning using Learned Objectives. arXiv:191004098.</li>
<li>Botvinick M, Ritter S, Wang JX, Kurth-Nelson Z, Blundell C, Hassabis D. (2019). Reinforcement Learning, Fast and Slow. Trends in Cognitive Sciences 23:408–422. doi:10.1016/j.tics.2019.02.006</li>
<li><a href="https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html" class="uri">https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html</a></li>
<li><a href="https://hackernoon.com/learning-policies-for-learning-policies-meta-reinforcement-learning-rl%C2%B2-in-tensorflow-b15b592a2ddf" class="uri">https://hackernoon.com/learning-policies-for-learning-policies-meta-reinforcement-learning-rl%C2%B2-in-tensorflow-b15b592a2ddf</a></li>
<li><a href="https://towardsdatascience.com/learning-to-learn-more-meta-reinforcement-learning-f0cc92c178c1" class="uri">https://towardsdatascience.com/learning-to-learn-more-meta-reinforcement-learning-f0cc92c178c1</a></li>
<li><a href="https://eng.uber.com/poet-open-ended-deep-learning/" class="uri">https://eng.uber.com/poet-open-ended-deep-learning/</a></li>
</ul>
</section>
<section id="offline-rl" class="level2">
<h2 class="anchored" data-anchor-id="offline-rl">Offline RL</h2>
<p>Even off-policy algorithms need to interact with the environment: the behavior policy is <span class="math inline">\epsilon</span>-soft around the learned policy.</p>
<p>Is it possible to learn purely <strong>offline</strong> from recorded transitions using another policy (experts)? Data efficiency. This would bring safety: the agent would not explore dangerous actions.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/offlinerl.gif" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Source: <a href="https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html" class="uri">https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html</a></figcaption>
</figure>
</div>
<p>D4RL (<a href="https://sites.google.com/view/d4rl/home" class="uri">https://sites.google.com/view/d4rl/home</a>) provides offline data recorded using expert policies to test offline algorithms.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/d4rl.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">https://ai.googleblog.com/2020/08/tackling-open-challenges-in-offline.html</figcaption>
</figure>
</div>
<p>As no exploration is allowed, the model is limited by the quality of the data: if the acquisition policy is random, there is not much to hope. If we have already a good policy, but slow or expensive to compute, we could try to transfer it to a fast neural network. If the policy is a human expert, it is called <strong>learning from demonstrations</strong> (lfd) or <strong>imitation learning</strong>.</p>
<p>The simplest approach to offline RL is <strong>behavioral cloning</strong>: simply supervised learning of <span class="math inline">(s, a)</span> pairs…</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/dave2-training.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption"><span class="citation" data-cites="Bojarski2016">Bojarski et al. (<a href="../references.html#ref-Bojarski2016" role="doc-biblioref">2016</a>)</span></figcaption>
</figure>
</div>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/qhUvQiKec2U" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p>The main problem in offline RL is the <strong>distribution shift</strong>: what if the trained policy assigns a non-zero probability to a <span class="math inline">(s, a)</span> pair that is <strong>outside</strong> the training data?</p>
<p>Most offline RL methods are <strong>conservative</strong> methods, which try to learn policies staying close to the known distribution of the data. See <span class="citation" data-cites="Levine2020">Levine et al. (<a href="../references.html#ref-Levine2020" role="doc-biblioref">2020</a>)</span> for a review. Examples:</p>
<ul>
<li>Batch-Contrained deep Q-learning (model-free) <span class="citation" data-cites="Fujimoto2019">(<a href="../references.html#ref-Fujimoto2019" role="doc-biblioref">Fujimoto et al., 2019</a>)</span></li>
<li>MOREL (model-based) <span class="citation" data-cites="Kidambi2021">(<a href="../references.html#ref-Kidambi2021" role="doc-biblioref">Kidambi et al., 2021</a>)</span></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/morel.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Source: <a href="https://kenshinhm.tistory.com/37" class="uri">https://kenshinhm.tistory.com/37</a></figcaption>
</figure>
</div>
<p>Transformers are the new SotA method to transform sequences into sequences. Why not sequences of states into sequences of actions?</p>
<p>The <strong>decision transformer</strong> <span class="citation" data-cites="Chen2021">(<a href="../references.html#ref-Chen2021" role="doc-biblioref">Chen et al., 2021</a>)</span> takes complete offline trajectories as inputs (s, a, r, s…) and predicts autoregressively the next action.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/decisiontranformer.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Source: <span class="citation" data-cites="Chen2021">Chen et al. (<a href="../references.html#ref-Chen2021" role="doc-biblioref">2021</a>)</span></figcaption>
</figure>
</div>
<p>However, transformers will mostly shine when used as World models… See <span class="citation" data-cites="Micheli2022">Micheli et al. (<a href="../references.html#ref-Micheli2022" role="doc-biblioref">2022</a>)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer-rl1.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">Source: <span class="citation" data-cites="Micheli2022">Micheli et al. (<a href="../references.html#ref-Micheli2022" role="doc-biblioref">2022</a>)</span></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../slides/img/transformer-rl2.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption class="figure-caption">Source: <span class="citation" data-cites="Micheli2022">Micheli et al. (<a href="../references.html#ref-Micheli2022" role="doc-biblioref">2022</a>)</span></figcaption>
</figure>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-Arora2019" class="csl-entry" role="listitem">
Arora, S., and Doshi, P. (2019). A <span>Survey</span> of <span>Inverse Reinforcement Learning</span>: <span>Challenges</span>, <span>Methods</span> and <span>Progress</span>. <a href="http://arxiv.org/abs/1806.06877">http://arxiv.org/abs/1806.06877</a>.
</div>
<div id="ref-Barto2013" class="csl-entry" role="listitem">
Barto, A. G. (2013). <span>“Intrinsic <span>Motivation</span> and <span>Reinforcement Learning</span>,”</span> in <em>Intrinsically <span>Motivated Learning</span> in <span>Natural</span> and <span>Artificial Systems</span></em>, eds. G. Baldassarre and M. Mirolli (<span>Berlin, Heidelberg</span>: <span>Springer</span>), 17–47. doi:<a href="https://doi.org/10.1007/978-3-642-32375-1_2">10.1007/978-3-642-32375-1_2</a>.
</div>
<div id="ref-Belkhale2021" class="csl-entry" role="listitem">
Belkhale, S., Li, R., Kahn, G., McAllister, R., Calandra, R., and Levine, S. (2021). Model-<span>Based Meta-Reinforcement Learning</span> for <span>Flight</span> with <span>Suspended Payloads</span>. <em>IEEE Robot. Autom. Lett.</em>, 1–1. doi:<a href="https://doi.org/10.1109/LRA.2021.3057046">10.1109/LRA.2021.3057046</a>.
</div>
<div id="ref-Bojarski2016" class="csl-entry" role="listitem">
Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., et al. (2016). End to <span>End Learning</span> for <span>Self-Driving Cars</span>. <a href="http://arxiv.org/abs/1604.07316">http://arxiv.org/abs/1604.07316</a>.
</div>
<div id="ref-Burda2018" class="csl-entry" role="listitem">
Burda, Y., Edwards, H., Pathak, D., Storkey, A., Darrell, T., and Efros, A. A. (2018). Large-<span>Scale Study</span> of <span>Curiosity-Driven Learning</span>. <a href="http://arxiv.org/abs/1808.04355">http://arxiv.org/abs/1808.04355</a>.
</div>
<div id="ref-Chen2021" class="csl-entry" role="listitem">
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., et al. (2021). Decision <span>Transformer</span>: <span>Reinforcement Learning</span> via <span>Sequence Modeling</span>. <a href="http://arxiv.org/abs/2106.01345">http://arxiv.org/abs/2106.01345</a>.
</div>
<div id="ref-Duan2016a" class="csl-entry" role="listitem">
Duan, Y., Schulman, J., Chen, X., Bartlett, P. L., Sutskever, I., and Abbeel, P. (2016). <span>RL</span>$2̂$: <span>Fast Reinforcement Learning</span> via <span>Slow Reinforcement Learning</span>. <a href="http://arxiv.org/abs/1611.02779">http://arxiv.org/abs/1611.02779</a>.
</div>
<div id="ref-Frans2017" class="csl-entry" role="listitem">
Frans, K., Ho, J., Chen, X., Abbeel, P., and Schulman, J. (2017). Meta <span>Learning Shared Hierarchies</span>. <a href="http://arxiv.org/abs/1710.09767">http://arxiv.org/abs/1710.09767</a>.
</div>
<div id="ref-Fujimoto2019" class="csl-entry" role="listitem">
Fujimoto, S., Meger, D., and Precup, D. (2019). Off-<span>Policy Deep Reinforcement Learning</span> without <span>Exploration</span>. in <em>Proceedings of the 36th <span>International Conference</span> on <span>Machine Learning</span></em> (<span>PMLR</span>), 2052–2062. <a href="https://proceedings.mlr.press/v97/fujimoto19a.html">https://proceedings.mlr.press/v97/fujimoto19a.html</a>.
</div>
<div id="ref-Kidambi2021" class="csl-entry" role="listitem">
Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T. (2021). <span>MOReL</span> : <span>Model-Based Offline Reinforcement Learning</span>. <a href="http://arxiv.org/abs/2005.05951">http://arxiv.org/abs/2005.05951</a>.
</div>
<div id="ref-Levine2020" class="csl-entry" role="listitem">
Levine, S., Kumar, A., Tucker, G., and Fu, J. (2020). Offline <span>Reinforcement Learning</span>: <span>Tutorial</span>, <span>Review</span>, and <span>Perspectives</span> on <span>Open Problems</span>. <a href="http://arxiv.org/abs/2005.01643">http://arxiv.org/abs/2005.01643</a>.
</div>
<div id="ref-Micheli2022" class="csl-entry" role="listitem">
Micheli, V., Alonso, E., and Fleuret, F. (2022). Transformers are <span>Sample Efficient World Models</span>. doi:<a href="https://doi.org/10.48550/arXiv.2209.00588">10.48550/arXiv.2209.00588</a>.
</div>
<div id="ref-Pathak2017" class="csl-entry" role="listitem">
Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T. (2017). Curiosity-driven <span>Exploration</span> by <span class="nocase">Self-supervised Prediction</span>. <a href="http://arxiv.org/abs/1705.05363">http://arxiv.org/abs/1705.05363</a>.
</div>
<div id="ref-Popov2017" class="csl-entry" role="listitem">
Popov, I., Heess, N., Lillicrap, T., Hafner, R., Barth-Maron, G., Vecerik, M., et al. (2017). Data-efficient <span>Deep Reinforcement Learning</span> for <span>Dexterous Manipulation</span>. <a href="http://arxiv.org/abs/1704.03073">http://arxiv.org/abs/1704.03073</a>.
</div>
<div id="ref-Wang2017a" class="csl-entry" role="listitem">
Wang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R., Kavukcuoglu, K., et al. (2017). Sample <span>Efficient Actor-Critic</span> with <span>Experience Replay</span>. <a href="http://arxiv.org/abs/1611.01224">http://arxiv.org/abs/1611.01224</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../notes/4.4-SR.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Successor representations</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../exercises/Content.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">List of exercises</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">Copyright Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de" class="email">julien.vitay@informatik.tu-chemnitz.de</a></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>