[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "This website contains the materials for the module Deep Reinforcement Learning taught by Dr. Julien Vitay at the Technische Universität Chemnitz, Faculty of Computer Science, Professorship for Artificial Intelligence.\nEach section/lecture is accompanied by a set of videos, the slides and a written version of the content. The videos are integrated in the lecture notes, but you can also access the complete playlist on Youtube.\nExercises are provided in the form of Jupyter notebooks, allowing to implement in Python at your own pace the algorithms seen in the lectures and to learn to use reinforcement learning libraries such as gym. A notebook to work on (locally or on Colab) and the solution are available in the Exercises section.\n\n\n\n\n\nIntroduction\n\nIntroduction\nMath basics (optional)\n\nTabular RL\n\nBandits\nMarkov Decision Processes\nDynamic Programming\nMonte Carlo control\nTemporal Difference\nFunction approximation\nDeep learning\n\nModel-free RL\n\nDeep Q-network\nBeyond DQN\nPolicy Gradient\nA2C / A3C\nDDPG\nTRPO / PPO\nSAC\n\nModel-based RL\n\nModel-based RL\nLearned models\nAlphaGo\nSuccessor representations\n\nOutlook\n\nOutlook\n\n\n\n\n\nNotebooks and videos are in the List of Exercises. Below are links to the rendered solutions.\n\n\n\nIntroduction to Python\nNumpy and Matplotlib\nSampling\nBandits - part 1\nBandits - part 2\nDynamic programming\n\n\n\nGym\nMonte Carlo control\nTemporal Difference\nEligibility traces\nKeras\nDQN\n\n\n\n\n\n\n\n\n(Sutton and Barto, 2017) Richard Sutton and Andrew Barto (2017). Reinforcement Learning: An Introduction. MIT press. http://incompleteideas.net/book/the-book-2nd.html\nCS294 course of Sergey Levine at Berkeley.\n\nhttp://rll.berkeley.edu/deeprlcourse/\n\nReinforcement Learning course by David Silver at UCL.\n\nhttp://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\n\n\n\n\nSutton, R. S., and Barto, A. G. (2017). Reinforcement Learning: An Introduction. 2nd ed. Cambridge, MA: MIT Press http://incompleteideas.net/book/the-book-2nd.html."
  },
  {
    "objectID": "notes/1.1-Introduction.html",
    "href": "notes/1.1-Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Slides: html pdf\nDeep reinforcement learning (deep RL or DRL) is the integration of deep learning methods, classically used in supervised or unsupervised learning contexts, with reinforcement learning (RL), a well-studied adaptive control method used in problems with delayed and partial feedback."
  },
  {
    "objectID": "notes/1.1-Introduction.html#history-of-rl",
    "href": "notes/1.1-Introduction.html#history-of-rl",
    "title": "Introduction",
    "section": "History of RL",
    "text": "History of RL\n\n\nEarly 20th century: animal behavior, psychology, operant conditioning\n\nIvan Pavlov, Edward Thorndike, B.F. Skinner\n\n1950s: optimal control, Markov Decision Process, dynamic programming\n\nRichard Bellman, Ronald Howard\n\n1970s: trial-and-error learning\n\nMarvin Minsky, Harry Klopf, Robert Rescorla, Allan Wagner\n\n1980s: temporal difference learning, Q-learning\n\nRichard Sutton, Andrew Barto, Christopher Watkins, Peter Dayan\n\n2013-now: deep reinforcement learning\n\nDeepmind (Mnih, Silver, Graves…)\nOpenAI (Sutskever, Schulman…)\nSergey Levine (Berkeley)\n\n\nReinforcement learning comes from animal behavior studies, especially operant conditioning / instrumental learning. Thorndike’s Law of Effect (1874–1949) suggested that behaviors followed by satisfying consequences tend to be repeated and those that produce unpleasant consequences are less likely to be repeated. Positive reinforcements (rewards) or negative reinforcements (punishments) can be used to modify behavior (Skinner’s box, 1936). This form of learning applies to all animals, including humans:\n\nTraining (animals, children…)\nAddiction, economics, gambling, psychological manipulation…\n\nBehaviorism: only behavior matters, not mental states.\n\nThe key concept of RL is trial and error learning. The agent (rat, robot, algorithm) tries out an action and observes the outcome.\n\nIf the outcome is positive (reward), the action is reinforced (more likely to occur again).\nIf the outcome is negative (punishment), the action will be avoided.\n\nAfter enough interactions, the agent has learned which action to perform in a given situation.\nRL is merely a formalization of the trial-and-error learning paradigm. The agent has to explore its environment via trial-and-error in order to gain knowledge. The biggest issue with this approach is that exploring large action spaces might necessitate a lot of trials (sample complexity). The modern techniques we will see in this course try to reduce the sample complexity."
  },
  {
    "objectID": "notes/1.1-Introduction.html#the-agent-environment-interface",
    "href": "notes/1.1-Introduction.html#the-agent-environment-interface",
    "title": "Introduction",
    "section": "The agent-environment interface",
    "text": "The agent-environment interface\n\n\n\n\nAgent-environment interface. Source: (Sutton and Barto, 2017).\n\n\nThe agent and the environment interact at discrete time steps: t=0, 1, … The agent observes its state at time t: s_t \\in \\mathcal{S}, produces an action at time t, depending on the available actions in the current state: a_t \\in \\mathcal{A}(s_t) and receives a reward according to this action at time t+1: r_{t+1} \\in \\Re. It then updates its state: s_{t+1} \\in \\mathcal{S}. The behavior of the agent is therefore is a sequence of state-action-reward-state (s, a, r, s') transitions.\n\n\n\nState-action-reward-state sequences. Source: (Sutton and Barto, 2017).\n\n\nSequences \\tau = (s_0, a_0, r_1, s_1, a_1, \\ldots, s_T) are called episodes, trajectories, histories or rollouts.\n\n\n\nAgent-environment interface for video games. Source: David Silver https://www.davidsilver.uk/teaching/.\n\n\nThe state s_t can relate to:\n\nthe environment state, i.e. all information external to the agent (position of objects, other agents, etc).\nthe internal state, information about the agent itself (needs, joint positions, etc).\n\nGenerally, the state represents all the information necessary to solve the task. The agent generally has no access to the states directly, but to observations o_t:\n\n    o_t = f(s_t)\n\nExample: camera inputs do not contain all the necessary information such as the agent’s position. Imperfect information define partially observable problems.\nWhat we search in RL is the optimal policy: which action a should the agent perform in a state s? The policy \\pi maps states into actions. It is defined as a probability distribution over states and actions:\n\n\\begin{align}\n    \\pi &: \\mathcal{S} \\times \\mathcal{A} \\rightarrow P(\\mathcal{S})\\\\\n    \\\\\n    & (s, a) \\rightarrow \\pi(s, a)  = P(a_t = a | s_t = s) \\\\\n\\end{align}\n\n\\pi(s, a) is the probability of selecting the action a in s. We have of course:\n\\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) = 1\nPolicies can be probabilistic / stochastic. Deterministic policies select a single action a^*in s:\n\\pi(s, a) = \\begin{cases} 1 \\; \\text{if} \\; a = a^* \\\\ 0 \\; \\text{if} \\; a \\neq a^* \\\\ \\end{cases}\nThe only teaching signal in RL is the reward function. The reward is a scalar value r_{t+1} provided to the system after each transition (s_t,a_t, s_{t+1}). Rewards can also be probabilistic (casino). The mathematical expectation of these rewards defines the expected reward of a transition:\n\n    r(s, a, s') = \\mathbb{E}_t [r_{t+1} | s_t = s, a_t = a, s_{t+1} = s']\n\nRewards can be:\n\ndense: a non-zero value is provided after each time step (easy).\nsparse: non-zero rewards are given very seldom (difficult).\n\nThe goal of the agent is to find a policy that maximizes the sum of future rewards at each timestep. The discounted sum of future rewards is called the return:\n\n    R_t = \\sum_{k=0}^\\infty \\gamma ^k \\, r_{t+k+1}\n\nRewards can be delayed w.r.t to an action: we care about all future rewards to select an action, not only the immediate ones. Example: in chess, the first moves are as important as the last ones in order to win, but they do not receive reward.\nThe expected return in a state s is called its value:\n\n    V^\\pi(s) = \\mathbb{E}_\\pi(R_t | s_t = s)\n\nThe value of a state defines how good it is to be in that state. If a state has a high value, it means we will be able to collect a lot of rewards on the long term and on average. Value functions are central to RL: if we know the value of all states, we can infer the policy. The optimal action is the one that leads to the state with the highest value. Most RL methods deal with estimating the value function from experience (trial and error).\nSimple maze\n\n\n\nSimple maze. Source: David Silver http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html.\n\n\nThe goal is to find a path from start to goal as fast as possible.\n\nStates: position in the maze (1, 2, 3…).\nActions: up, down, left, right.\nRewards: -1 for each step until the exit.\n\nThe value of each state indicates how good it is to be in that state. It can be learned by trial-and-error given a policy.\n\n\n\nValue of each state. Source: David Silver http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html.\n\n\nWhen the value of all states is known, we can infer the optimal policy by choosing actions leading to the states with the highest value.\n\n\n\nOptimal policy. Source: David Silver http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs we will see, the story is actually much more complicated…"
  },
  {
    "objectID": "notes/1.1-Introduction.html#difference-between-supervised-and-reinforcement-learning",
    "href": "notes/1.1-Introduction.html#difference-between-supervised-and-reinforcement-learning",
    "title": "Introduction",
    "section": "Difference between supervised and reinforcement learning",
    "text": "Difference between supervised and reinforcement learning\nSupervised learning\n\nCorrect input/output samples are provided by a superviser (training set).\nLearning is driven by prediction errors, the difference between the prediction and the target.\nFeedback is instantaneous: the target is immediately known.\nTime does not matter: training samples are randomly sampled from the training set.\n\nReinforcement learning\n\nBehavior is acquired through trial and error, no supervision.\nReinforcements (rewards or punishments) change the probability of selecting particular actions.\nFeedback is delayed: which action caused the reward? Credit assignment.\nTime matters: as behavior gets better, the observed data changes."
  },
  {
    "objectID": "notes/1.1-Introduction.html#applications-of-tabular-rl",
    "href": "notes/1.1-Introduction.html#applications-of-tabular-rl",
    "title": "Introduction",
    "section": "Applications of tabular RL",
    "text": "Applications of tabular RL\n\n\nPendulum\n\n\n\nCartpole\n\n\n\nCartpole before training. Source: https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288.\n\n\n\n\n\nCartpole after training. Source: https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288.\n\n\n\n\n\nBackgammon\nTD-Gammon (Tesauro, 1995) was one of the first AI to beat human experts at a complex game, Backgammon.\n\n\n\nBackgammon. Source: (Tesauro, 1995).\n\n\n\n\n\nTD-Gammon. Source: (Tesauro, 1995)."
  },
  {
    "objectID": "notes/1.1-Introduction.html#deep-reinforcement-learning-drl",
    "href": "notes/1.1-Introduction.html#deep-reinforcement-learning-drl",
    "title": "Introduction",
    "section": "Deep Reinforcement Learning (DRL)",
    "text": "Deep Reinforcement Learning (DRL)\n\n\n\nIn deep RL, the policy is approximated by a deep neural network.\n\n\nClassical tabular RL was limited to toy problems, with few states and actions. It is only when coupled with deep neural networks that interesting applications of RL became possible. Deepmind (now Google) started the deep RL hype in 2013 by learning to solve 50+ Atari games with a CNN, the deep Q-network (DQN) (Mnih et al., 2013).\n\n\n\nArchitecture of the deep Q-network. Source: (Mnih et al., 2013).\n\n\n\nDeep RL methods we since then improved and applied to a variety of control tasks, including simulated cars:\n\nor Parkour:\n\nOne very famous success of deep RL is when AlphaGo managed to beat Lee Sedol at the ancient game of Go:\n\nDeepRL has since been applied to real-world robotics:\n\n\nor even autonomous driving (https://wayve.ai/blog/learning-to-drive-in-a-day-with-reinforcement-learning):\n\nIt is also used for more complex video games, such as DotA II:\n\nor Starcraft II (AlphaStar, https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii)\n\n\n\n\n\nDeep RL is gaining a lot of importance in AI research, with lots of applications in control: video games, robotics, industrial applications… It may be AI’s best shot at producing intelligent behavior, as it does not rely on annotated data. A lot of problems have to be solved before becoming as mainstream as deep learning.\n\nSample complexity is often prohibitive.\nEnergy consumption and computing power simply crazy (AlphaGo: 1 MW, Dota2: 800 petaflop/s-days)\nThe correct reward function is hard to design, ethical aspects. (inverse RL)\nHard to incorporate expert knowledge. (model-based RL)\nLearns single tasks, does not generalize (hierarchical RL, meta-learning)"
  },
  {
    "objectID": "notes/1.1-Introduction.html#suggested-readings",
    "href": "notes/1.1-Introduction.html#suggested-readings",
    "title": "Introduction",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nSutton and Barto (1998, 2017). Reinforcement Learning: An Introduction. MIT Press.\n\nhttp://incompleteideas.net/sutton/book/the-book.html\n\nSzepesvari (2010). Algorithms for Reinforcement Learning. Morgan and Claypool.\n\nhttp://www.ualberta.ca/∼szepesva/papers/RLAlgsInMDPs.pdf\n\nCS294 course of Sergey Levine at Berkeley.\n\nhttp://rll.berkeley.edu/deeprlcourse/\n\nReinforcement Learning course by David Silver at UCL.\n\nhttps://www.davidsilver.uk/teaching/\n\n\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., et al. (2013). Playing Atari with Deep Reinforcement Learning. http://arxiv.org/abs/1312.5602.\n\n\nSutton, R. S., and Barto, A. G. (2017). Reinforcement Learning: An Introduction. 2nd ed. Cambridge, MA: MIT Press http://incompleteideas.net/book/the-book-2nd.html.\n\n\nTesauro, G. (1995). “TD-Gammon: A Self-Teaching Backgammon Program,” in Applications of Neural Networks, ed. A. F. Murray (Boston, MA: Springer US), 267–285. doi:10.1007/978-1-4757-2379-3_11."
  },
  {
    "objectID": "notes/1.2-Math.html",
    "href": "notes/1.2-Math.html",
    "title": "Math basics (optional)",
    "section": "",
    "text": "Slides: html pdf\nThis chapter is not part of the course itself (there will not be questions at the exam on basic mathematics) but serves as a reminder of the important mathematical notions that are needed to understand this course. Students who have studied mathematics as a major can safely skip this part, as there is nothing fancy (although the section on information theory could be worth a read).\nIt is not supposed to replace any course in mathematics (we won’t show any proof and will skip what we do not need) but rather to provide a high-level understanding of the most important concepts and set the notations. Nothing should be really new to you, but it may be useful to have everything summarized at the same place.\nReferences: Part I of (Goodfellow et al., 2016). Any mathematics textbook can be used in addition."
  },
  {
    "objectID": "notes/1.2-Math.html#linear-algebra",
    "href": "notes/1.2-Math.html#linear-algebra",
    "title": "Math basics (optional)",
    "section": "Linear algebra",
    "text": "Linear algebra\n\nSeveral mathematical objects are manipulated in linear algebra:\n\nScalars x are 0-dimensional values (single numbers, so to speak). They can either take real values (x \\in \\Re, e.g. x = 1.4573, floats in CS) or natural values (x \\in \\mathbb{N}, e.g. x = 3, integers in CS).\nVectors \\mathbf{x} are 1-dimensional arrays of length d. The bold notation \\mathbf{x} will be used in this course, but you may also be accustomed to the arrow notation \\overrightarrow{x} used on the blackboard. When using real numbers, the vector space with d dimensions is noted \\Re^d, so we can note \\mathbf{x} \\in \\Re^d. Vectors are typically represented vertically to outline their d elements x_1, x_2, \\ldots, x_d:\n\n\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix}\n\nMatrices A are 2-dimensional arrays of size (or shape) m \\times n (m rows, n columns, A \\in \\Re^{m \\times n}). They are represented by a capital letter to distinguish them from scalars (classically also in bold \\mathbf{A} but not here). The element a_{ij} of a matrix A is the element on the i-th row and j-th column.\n\nA = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix}\n\n\nTensors \\mathcal{A} are arrays with more than two dimensions. We will not really do math on these objects, but they are useful internally (hence the name of the tensorflow library).\n\n\nVectors\nA vector can be thought of as the coordinates of a point in an Euclidean space (such the 2D space), relative to the origin. A vector space relies on two fundamental operations, which are that:\n\nVectors can be added:\n\n\\mathbf{x} + \\mathbf{y} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix} + \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_d \\end{bmatrix} = \\begin{bmatrix} x_1 + y_1 \\\\ x_2 + y_2 \\\\ \\vdots \\\\ x_d + y_d \\end{bmatrix}\n\n\nVectors can be multiplied by a scalar:\n\na \\, \\mathbf{x} = a \\, \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix} = \\begin{bmatrix} a \\, x_1 \\\\ a \\, x_2 \\\\ \\vdots \\\\ a \\, x_d \\end{bmatrix}\n\n\n\n\nVector spaces allow additions of vectors. Source: https://mathinsight.org/image/vector_2d_add\n\n\nThese two operations generate a lot of nice properties (see https://en.wikipedia.org/wiki/Vector_space for a full list), including:\n\nassociativity: \\mathbf{x} + (\\mathbf{y} + \\mathbf{z}) = (\\mathbf{x} + \\mathbf{y}) + \\mathbf{z}\ncommutativity: \\mathbf{x} + \\mathbf{y} = \\mathbf{y} + \\mathbf{x}\nthe existence of a zero vector \\mathbf{x} + \\mathbf{0} = \\mathbf{x}\ninversion: \\mathbf{x} + (-\\mathbf{x}) = \\mathbf{0}\ndistributivity: a \\, (\\mathbf{x} + \\mathbf{y}) = a \\, \\mathbf{x} + a \\, \\mathbf{y}\n\nVectors have a norm (or length) ||\\mathbf{x}||. The most intuitive one (if you know the Pythagoras theorem) is the Euclidean norm or L^2-norm, which sums the square of each element:\n||\\mathbf{x}||_2 = \\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\nOther norms exist, distinguished by the subscript. The L^1-norm (also called Taxicab or Manhattan norm) sums the absolute value of each element:\n||\\mathbf{x}||_1 = |x_1| + |x_2| + \\ldots + |x_d|\nThe p-norm generalizes the Euclidean norm to other powers p:\n||\\mathbf{x}||_p = (|x_1|^p + |x_2|^p + \\ldots + |x_d|^p)^{\\frac{1}{p}}\nThe infinity norm (or maximum norm) L^\\infty returns the maximum element of the vector:\n||\\mathbf{x}||_\\infty = \\max(|x_1|, |x_2|, \\ldots, |x_d|)\nOne important operation for vectors is the dot product (also called scalar product or inner product) between two vectors:\n\\langle \\mathbf{x} \\cdot \\mathbf{y} \\rangle = \\langle \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix} \\cdot \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_d \\end{bmatrix} \\rangle = x_1 \\, y_1 + x_2 \\, y_2 + \\ldots + x_d \\, y_d\n\nThe dot product basically sums one by one the product of the elements of each vector. The angular brackets are sometimes omitted (\\mathbf{x} \\cdot \\mathbf{y}) but we will use them in this course for clarity.\nOne can notice immediately that the dot product is symmetric:\n\\langle \\mathbf{x} \\cdot \\mathbf{y} \\rangle = \\langle \\mathbf{y} \\cdot \\mathbf{x} \\rangle\nand linear:\n\\langle (a \\, \\mathbf{x} + b\\, \\mathbf{y}) \\cdot \\mathbf{z} \\rangle = a\\, \\langle \\mathbf{x} \\cdot \\mathbf{z} \\rangle + b \\, \\langle \\mathbf{y} \\cdot \\mathbf{z} \\rangle\nThe dot product is an indirect measurement of the angle \\theta between two vectors:\n\\langle \\mathbf{x} \\cdot \\mathbf{y} \\rangle = ||\\mathbf{x}||_2 \\, ||\\mathbf{y}||_2 \\, \\cos(\\theta)\n\n\n\nThe dot product between two vectors is proportional to the cosine of the angle between the two vectors. Source: https://mathinsight.org/image/dot_product_projection_unit_vector\n\n\nIf you normalize the two vectors by dividing them by their norm (which is a scalar), we indeed have the cosine of the angle between them: The higher the normalized dot product, the more the two vectors point towards the same direction (cosine distance between two vectors).\n\\langle \\displaystyle\\frac{\\mathbf{x}}{||\\mathbf{x}||_2} \\cdot \\frac{\\mathbf{y}}{||\\mathbf{y}||_2} \\rangle =  \\cos(\\theta)\n\n\nMatrices\nMatrices are derived from vectors, so most of the previous properties will be true. Let’s consider this 4x3 matrix:\nA = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\na_{41} & a_{42} & a_{43} \\\\\n\\end{bmatrix}\n\nEach column of the matrix is a vector with 4 elements:\n\\mathbf{a}_1 = \\begin{bmatrix}\na_{11} \\\\\na_{21} \\\\\na_{31} \\\\\na_{41} \\\\\n\\end{bmatrix} \\qquad\n\\mathbf{a}_2 = \\begin{bmatrix}\na_{12} \\\\\na_{22} \\\\\na_{32} \\\\\na_{42} \\\\\n\\end{bmatrix} \\qquad\n\\mathbf{a}_3 = \\begin{bmatrix}\na_{13} \\\\\na_{23} \\\\\na_{33} \\\\\na_{43} \\\\\n\\end{bmatrix} \\qquad\n\nA m \\times n matrix is therefore a collection of n vectors of size m put side by side column-wise:\nA = \\begin{bmatrix}\n\\mathbf{a}_1 & \\mathbf{a}_2 & \\mathbf{a}_3\\\\\n\\end{bmatrix}\n\nSo all properties of the vector spaces (associativity, commutativity, distributivity) also apply to matrices, as additions and multiplications with a scalar are defined.\n\\alpha \\, A + \\beta \\, B = \\begin{bmatrix}\n\\alpha\\, a_{11} + \\beta \\, b_{11} & \\alpha\\, a_{12} + \\beta \\, b_{12} & \\alpha\\, a_{13} + \\beta \\, b_{13} \\\\\n\\alpha\\, a_{21} + \\beta \\, b_{21} & \\alpha\\, a_{22} + \\beta \\, b_{22} & \\alpha\\, a_{23} + \\beta \\, b_{23} \\\\\n\\alpha\\, a_{31} + \\beta \\, b_{31} & \\alpha\\, a_{32} + \\beta \\, b_{32} & \\alpha\\, a_{33} + \\beta \\, b_{33} \\\\\n\\alpha\\, a_{41} + \\beta \\, b_{41} & \\alpha\\, a_{42} + \\beta \\, b_{42} & \\alpha\\, a_{43} + \\beta \\, b_{43} \\\\\n\\end{bmatrix}\n\n\n\n\n\n\n\nNote\n\n\n\nBeware, you can only add matrices of the same dimensions m\\times n. You cannot add a 2\\times 3 matrix to a 5 \\times 4 one.\n\n\nThe transpose A^T of a m \\times n matrix A is a n \\times m matrix, where the row and column indices are swapped:\nA = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix}, \\qquad\nA^T = \\begin{bmatrix}\na_{11} & a_{21} & \\cdots & a_{m1} \\\\\na_{12} & a_{22} & \\cdots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\cdots & a_{mn} \\\\\n\\end{bmatrix}\n\nThis is also true for vectors, which become horizontal after transposition:\n\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix}, \\qquad\n\\mathbf{x}^T = \\begin{bmatrix} x_1 & x_2 & \\ldots & x_d \\end{bmatrix}\n\nA very important operation is the matrix multiplication. If A is a m\\times n matrix and B a n \\times p matrix:\n\nA=\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix},\\quad\nB=\\begin{bmatrix}\nb_{11} & b_{12} & \\cdots & b_{1p} \\\\\nb_{21} & b_{22} & \\cdots & b_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{n1} & b_{n2} & \\cdots & b_{np} \\\\\n\\end{bmatrix}\n\nwe can multiply them to obtain a m \\times p matrix:\n\nC = A \\times B =\\begin{bmatrix}\nc_{11} & c_{12} & \\cdots & c_{1p} \\\\\nc_{21} & c_{22} & \\cdots & c_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nc_{m1} & c_{m2} & \\cdots & c_{mp} \\\\\n\\end{bmatrix}\n\nwhere each element c_{ij} is the dot product of the ith row of A and jth column of B:\nc_{ij} = \\langle A_{i, :} \\cdot B_{:, j} \\rangle = a_{i1}b_{1j} + a_{i2}b_{2j} +\\cdots + a_{in}b_{nj}= \\sum_{k=1}^n a_{ik}b_{kj}\n\n\n\n\n\n\n\nNote\n\n\n\nn, the number of columns of A and rows of B, must be the same!\n\n\n\n\n\nThe element c_{ij} of C = A \\times B is the dot product between the ith row of A and the jth column of B. Source: CC BY-NC-SA; Marcia Levitus\n\n\nThinking of vectors as n \\times 1 matrices, we can multiply a matrix m \\times n with a vector:\n\n\\mathbf{y} = A \\times \\mathbf{x} = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix} \\times \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix}\n\nThe result \\mathbf{y} is a vector of size m. In that sense, a matrix A can transform a vector of size n into a vector of size m: A represents a projection from \\Re^n to \\Re^m.\n\n\n\nA 2 \\times 3 projection matrix allows to project any 3D vector onto a 2D plane. This is for example what happens inside a camera. Source: https://en.wikipedia.org/wiki/Homogeneous_coordinate\n\n\nNote that the dot product between two vectors of size n is the matrix multiplication between the transpose of the first vector and the second one:\n\\mathbf{x}^T \\times \\mathbf{y} = \\begin{bmatrix} x_1 & x_2 & \\ldots & x_n \\end{bmatrix} \\times \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = x_1 \\, y_1 + x_2 \\, y_2 + \\ldots + x_n \\, y_n = \\langle \\mathbf{x} \\cdot \\mathbf{y} \\rangle\nSquare matrices of size n \\times n can be inverted. The inverse A^{-1} of a matrix A is defined by:\nA \\times A^{-1} = A^{-1} \\times A = I\nwhere I is the identity matrix (a matrix with ones on the diagonal and 0 otherwise). Not all matrices have an inverse (those who don’t are called singular or degenerate). There are plenty of conditions for a matrix to be invertible (for example its determinant is non-zero, see https://en.wikipedia.org/wiki/Invertible_matrix), but they will not matter in this course. Non-square matrices are generally not invertible, but see the pseudoinverse (https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse).\nMatrix inversion allows to solve linear systems of equations. Given the problem:\n\n\\begin{cases}\n    a_{11} \\, x_1 + a_{12} \\, x_2 + \\ldots + a_{1n} \\, x_n = b_1 \\\\\n    a_{21} \\, x_1 + a_{22} \\, x_2 + \\ldots + a_{2n} \\, x_n = b_2 \\\\\n    \\ldots \\\\\n    a_{n1} \\, x_1 + a_{n2} \\, x_2 + \\ldots + a_{nn} \\, x_n = b_n \\\\\n\\end{cases}\n\nwhich is equivalent to:\nA \\times \\mathbf{x} = \\mathbf{b}\nwe can multiply both sides to the left with A^{-1} (if it exists) and obtain:\n\\mathbf{x} = A^{-1} \\times \\mathbf{b}"
  },
  {
    "objectID": "notes/1.2-Math.html#calculus",
    "href": "notes/1.2-Math.html#calculus",
    "title": "Math basics (optional)",
    "section": "Calculus",
    "text": "Calculus\n\n\nFunctions\nA univariate function f associates to any real number x \\in \\Re (or a subset of \\Re called the support of the function) another (unique) real number f(x):\n\n\\begin{align}\nf\\colon \\quad \\Re &\\to \\Re\\\\\nx &\\mapsto f(x)\n\\end{align}\n\n\n\n\nExample of univariate function, here the quadratic function f(x) = x^2 - 2 \\, x + 1.\n\n\nA multivariate function f associates to any vector \\mathbf{x} \\in \\Re^n (or a subset) a real number f(\\mathbf{x}):\n\n\\begin{align}\nf\\colon \\quad \\Re^n &\\to \\Re\\\\\n\\mathbf{x} &\\mapsto f(\\mathbf{x})\n\\end{align}\n\nThe variables of the function are the elements of the vector. For low-dimensional vector spaces, it is possible to represent each element explicitly, for example:\n\n\\begin{align}\nf\\colon \\quad\\Re^3 &\\to \\Re\\\\\nx, y, z &\\mapsto f(x, y, z),\\end{align}\n\n\n\n\nExample of a multivariate function f(x_1, x_2) mapping \\Re^2 to \\Re. Source: https://en.wikipedia.org/wiki/Function_of_several_real_variables\n\n\nVector fields associate to any vector \\mathbf{x} \\in \\Re^n (or a subset) another vector (possibly of different size):\n\n\\begin{align}\n\\overrightarrow{f}\\colon \\quad \\Re^n &\\to \\Re^m\\\\\n\\mathbf{x} &\\mapsto \\overrightarrow{f}(\\mathbf{x}),\\end{align}\n\n\n\n\nVector field associating to each point of \\Re^2 another vector. Source: https://en.wikipedia.org/wiki/Vector_field\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe matrix-vector multiplication \\mathbf{y} = A \\times \\mathbf{x} is a linear vector field, mapping any vector \\mathbf{x} into another vector \\mathbf{y}.\n\n\n\n\nDifferentiation\n\nDerivatives\nDifferential calculus deals with the derivative of a function, a process called differentiation.\nThe derivative f'(x) or \\displaystyle\\frac{d f(x)}{dx} of a univariate function f(x) is defined as the local slope of the tangent to the function for a given value of x:\nf'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}\nThe line passing through the points (x, f(x)) and (x + h, f(x + h)) becomes tangent to the function when h becomes very small:\n\n\n\nThe derivative of the function f(x) can be approximated by the slope of the line passing through (x, f(x)) and (x + h, f(x + h)) when h becomes very small.\n\n\nThe sign of the derivative tells you how the function behaves locally:\n\nIf the derivative is positive, increasing a little bit x increases the function f(x), so the function is locally increasing.\nIf the derivative is negative, increasing a little bit x decreases the function f(x), so the function is locally decreasing.\n\nIt basically allows you to measure the local influence of x on f(x): if I change a little bit the value x, what happens to f(x)? This will be very useful in machine learning.\nA special case is when the derivative is equal to 0 in x. x is then called an extremum (or optimum) of the function, i.e. it can be a maximum or minimum.\n\n\n\n\n\n\nNote\n\n\n\nIf you differentiate f'(x) itself, you obtain the second-order derivative f''(x). You can repeat that process and obtain higher order derivatives.\nFor example, if x(t) represents the position x of an object depending on time t, the first-order derivative x'(t) denotes the speed of the object and the second-order derivative x''(t) its acceleration.\n\n\nYou can tell whether an extremum is a maximum or a minimum by looking at its second-order derivative:\n\nIf f''(x) > 0, the extremum is a minimum.\nIf f''(x) < 0, the extremum is a maximum.\nIf f''(x) = 0, the extremum is a saddle point.\n\n\n\n\nQuadratic functions have only one extremum (here a minimum in -1), as their derivative is linear and is equal to zero for only one value.\n\n\nThe derivative of a multivariate function f(\\mathbf{x}) is a vector of partial derivatives called the gradient of the function \\nabla_\\mathbf{x} \\, f(\\mathbf{x}):\n\n    \\nabla_\\mathbf{x} \\, f(\\mathbf{x}) = \\begin{bmatrix}\n        \\displaystyle\\frac{\\partial f(\\mathbf{x})}{\\partial x_1} \\\\\n        \\displaystyle\\frac{\\partial f(\\mathbf{x})}{\\partial x_2} \\\\\n        \\ldots \\\\\n        \\displaystyle\\frac{\\partial f(\\mathbf{x})}{\\partial x_n} \\\\\n    \\end{bmatrix}\n\nThe subscript to the \\nabla operator denotes with respect to (w.r.t) which variable the differentiation is done.\nA partial derivative w.r.t. to particular variable (or element of the vector) is simply achieved by differentiating the function while considering all other variables to be constant. For example the function:\nf(x, y) = x^2 + 3 \\, x \\, y + 4 \\, x \\, y^2 - 1\ncan be partially differentiated w.r.t. x and y as:\n\\begin{cases}\n\\displaystyle\\frac{\\partial f(x, y)}{\\partial x} = 2 \\, x + 3\\, y + 4 \\, y^2 \\\\\n\\\\\n\\displaystyle\\frac{\\partial f(x, y)}{\\partial y} = 3 \\, x + 8\\, x \\, y\n\\end{cases}\n\nThe gradient can be generalized to vector fields, where the Jacobian or Jacobi matrix is a matrix containing all partial derivatives.\n\nJ = \\begin{bmatrix}\n    \\dfrac{\\partial \\mathbf{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\mathbf{f}}{\\partial x_n} \\end{bmatrix}\n= \\begin{bmatrix}\n    \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\\n    \\vdots & \\ddots & \\vdots\\\\\n    \\dfrac{\\partial f_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_m}{\\partial x_n} \\end{bmatrix}\n\n\n\nAnalytical properties\nThe analytical form of the derivative of most standard mathematical functions is known. The following table lists the most useful ones in this course:\n\n\n\nFunction f(x)\nDerivative f'(x)\n\n\n\n\n x\n 1\n\n\nx^p\n p \\, x^{p-1}\n\n\n\\displaystyle\\frac{1}{x}\n- \\displaystyle\\frac{1}{x^2}\n\n\n e^x\ne^x\n\n\n\\ln x\n \\displaystyle\\frac{1}{x}\n\n\n\nDifferentiation is linear, which means that if we define the function:\nh(x) = a \\, f(x) + b \\, g(x)\nits derivative is:\nh'(x) = a \\, f'(x) + b \\, g'(x)\nA product of functions can also be differentiated analytically:\n(f(x) \\times g(x))' = f'(x) \\times g(x) + f(x) \\times g'(x)\n\n\n\n\n\n\nExample\n\n\n\nf(x) = x^2 \\, e^x\nf'(x) = 2 \\, x \\, e^x + x^2 \\cdot e^x\n\n\n\n\nChain rule\nA very important concept for neural networks is the chain rule, which tells how to differentiate function compositions (functions of a function) of the form:\n(f \\circ g) (x) = f(g(x))\nThe derivative of f \\circ g is:\n(f \\circ g)' (x) = (f' \\circ g) (x) \\times g'(x)\nThe chain rule may be more understandable using Leibniz’s notation:\n\\frac{d f \\circ g (x)}{dx} = \\frac{d f (g (x))}{d g(x)} \\times \\frac{d g (x)}{dx}\nBy posing y = g(x) as an intermediary variable, it becomes:\n\\frac{d f(y)}{dx} = \\frac{d f(y)}{dy} \\times \\frac{dy}{dx}\n\n\n\n\n\n\nExample\n\n\n\nThe function :\nh(x) = \\frac{1}{2 \\, x + 1}\nis the function composition of g(x) = 2 \\, x + 1 and f(x) = \\displaystyle\\frac{1}{x}, whose derivatives are known:\ng'(x) = 2 f'(x) = -\\displaystyle\\frac{1}{x^2}\nIts derivative according to the chain rule is:\nh'(x) = f'(g(x)) \\times g'(x) = -\\displaystyle\\frac{1}{(2 \\, x + 1)^2} \\times 2\n\n\nThe chain rule also applies to partial derivatives:\n\n    \\displaystyle\\frac{\\partial f \\circ g (x, y)}{\\partial x} = \\frac{\\partial f \\circ g (x, y)}{\\partial g (x, y)} \\times \\frac{\\partial g (x, y)}{\\partial x}\n\nand gradients:\n\n    \\nabla_\\mathbf{x} \\, f \\circ g (\\mathbf{x}) = \\nabla_{g(\\mathbf{x})} \\, f \\circ g (\\mathbf{x}) \\times \\nabla_\\mathbf{x} \\, g (\\mathbf{x})\n\n\n\n\nIntegration\nThe opposite operation of differentation is integration. Given a function f(x), we search a function F(x) whose derivative is f(x):\nF'(x) = f(x)\nThe integral of f is noted:\nF(x) = \\int f(x) \\, dx\ndx being an infinitesimal interval (similar h in the definition of the derivative). There are tons of formal definitions of integrals (Riemann, Lebesgue, Darboux…) and we will not get into details here as we will not use integrals a lot.\nThe most important to understand for now is maybe that the integral of a function is the area under the curve. The area under the curve of a function f on the interval [a, b] is:\n\\mathcal{S} = \\int_a^b f(x) \\, dx\n\n\n\nThe integral of f on [a, b] is the area of the surface between the function and the x-axis. Note that it can become negative when the function is mostly negative on [a, b]. Source: https://www.math24.net/riemann-sums-definite-integral/\n\n\nOne way to approximate this surface is to split the interval [a, b] into n intervals of width dx with the points x_1, x_2, \\ldots, x_n. This defines n rectangles of width dx and height f(x_i), so their surface is f(x_i) \\, dx. The area under the curve can then be approximated by the sum of the surfaces of all these rectangles.\n\n\n\nThe interval[a, b] can be split in n small intervals of width dx, defining n rectangles whose sum is close to the area under the curve. Source: https://www.math24.net/riemann-sums-definite-integral/\n\n\nWhen n \\to \\infty, or equivalently dx \\to 0, the sum of these rectangular areas (called the Riemann sum) becomes exactly the area under the curve. This is the definition of the definite integral:\n\\int_a^b f(x) \\, dx = \\lim_{dx \\to 0} \\sum_{i=1}^n f(x_i) \\, dx\nVery roughly speaking, the integral can be considered as the equivalent of a sum for continuous functions."
  },
  {
    "objectID": "notes/1.2-Math.html#probability-theory",
    "href": "notes/1.2-Math.html#probability-theory",
    "title": "Math basics (optional)",
    "section": "Probability theory",
    "text": "Probability theory\n\n\nDiscrete probability distributions\nLet’s note X a discrete random variable with n realizations (or outcomes) x_1, \\ldots, x_n.\n\nA coin has two outcomes: head and tails.\nA dice has six outcomes: 1, 2, 3, 4, 5, 6.\n\nThe probability that X takes the value x_i is defined in the frequentist sense by the relative frequency of occurrence, i.e. the proportion of samples having the value x_i, when the total number N of samples tends to infinity:\n\n    P(X = x_i) = \\frac{\\text{Number of favorable cases}}{\\text{Total number of samples}}\n\nThe set of probabilities \\{P(X = x_i)\\}_{i=1}^n define the probability distribution for the random variable (or probability mass function, pmf). By definition, we have 0 \\leq P(X = x_i) \\leq 1 and the probabilities have to respect:\n\n    \\sum_{i=1}^n P(X = x_i) = 1\n\nAn important metric for a random variable is its mathematical expectation or expected value, i.e. its “mean” realization weighted by the probabilities:\n\n    \\mathbb{E}[X] = \\sum_{i=1}^n P(X = x_i) \\, x_i\n\nThe expectation does not even need to be a valid realization:\n\n    \\mathbb{E}[\\text{Coin}] = \\frac{1}{2} \\, 0 + \\frac{1}{2} \\, 1 = 0.5\n\n\n    \\mathbb{E}[\\text{Dice}] = \\frac{1}{6} \\, (1 + 2 + 3 + 4 + 5 + 6) = 3.5\n\nWe can also compute the mathematical expectation of functions of a random variable:\n\n    \\mathbb{E}[f(X)] = \\sum_{i=1}^n P(X = x_i) \\, f(x_i)\n\nThe variance of a random variable is the squared deviation around the mean:\n\n    \\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\sum_{i=1}^n P(X = x_i) \\, (x_i - \\mathbb{E}[X])^2\n\nVariance of a coin:\n\n    \\text{Var}(\\text{Coin}) = \\frac{1}{2} \\, (0 - 0.5)^2 + \\frac{1}{2} \\, (1 - 0.5)^2 = 0.25\n\nVariance of a dice:\n\n    \\text{Var}(\\text{Dice}) = \\frac{1}{6} \\, ((1-3.5)^2 + (2-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (5-3.5)^2 + (6-3.5)^2) = \\frac{105}{36}\n\n\n\nContinuous probability distributions\nContinuous random variables can take infinitely many values in a continuous interval, e.g. \\Re or some subset. The closed set of values they can take is called the support \\mathcal{D}_X of the probability distribution. The probability distribution is described by a probability density function (pdf) f(x).\n\n\n\nNormal distributions are continuous distributions. The area under the curve is always 1.\n\n\nThe pdf of a distribution must be positive (f(x) \\geq 0 \\, \\forall x \\in \\mathcal{D}_X) and its integral (area under the curve) must be equal to 1:\n\n    \\int_{x \\in \\mathcal{D}_X} f(x) \\, dx = 1\n\nThe pdf does not give the probability of taking a particular value x (it is 0), but allows to get the probability that a value lies in a specific interval:\n\n    P(a \\leq X \\leq b) = \\int_{a}^b f(x) \\, dx\n\nOne can however think of the pdf as the likelihood that a value x comes from that distribution.\nFor continuous distributions, the mathematical expectation is now defined by an integral instead of a sum:\n\n    \\mathbb{E}[X] = \\int_{x \\in \\mathcal{D}_X} f(x) \\, x \\, dx\n\nthe variance also:\n\n    \\text{Var}(X) = \\int_{x \\in \\mathcal{D}_X} f(x) \\, (x - \\mathbb{E}[X])^2 \\, dx\n\nor a function of the random variable:\n\n    \\mathbb{E}[g(X)] = \\int_{x \\in \\mathcal{D}_X} f(x) \\, g(x) \\, dx\n\nNote that the expectation operator is linear:\n\n    \\mathbb{E}[a \\, X + b \\, Y] = a \\, \\mathbb{E}[X] + b \\, \\mathbb{E}[Y]\n\nbut not the variance, even when the distributions are independent:\n\n    \\text{Var}[a \\, X + b \\, Y] = a^2 \\, \\text{Var}[X] + b^2 \\, \\text{Var}[Y]\n\n\n\nStandard distributions\nProbability distributions can in principle have any form: f(x) is unknown. However, specific parameterized distributions can be very useful: their pmf/pdf is fully determined by a couple of parameters.\n\nThe Bernouilli distribution is a binary (discrete, 0 or 1) distribution with a parameter p specifying the probability to obtain the outcome 1 (e.g. a coin):\n\n\n    P(X = 1) = p \\; \\text{and} \\; P(X=0) = 1 - p\n P(X=x) = p^x \\, (1-p)^{1-x} \\mathbb{E}[X] = p\n\nThe Multinouilli or categorical distribution is a discrete distribution with k realizations (e.g. a dice). Each realization x_i is associated with a parameter p_i >0 representing its probability. We have \\sum_i p_i = 1.\n\nP(X = x_i) = p_i\n\nThe uniform distribution has an equal and constant probability of returning values between a and b, never outside this range. It is parameterized by the start of the range a and the end of the range b. Its support is [a, b]. The pdf of the uniform distribution \\mathcal{U}(a, b) is defined on [a, b] as:\n\n\n    f(x; a, b) = \\frac{1}{b - a}\n\n\nThe normal distribution is the most frequently encountered continuous distribution. It is parameterized by two parameters: the mean \\mu and the variance \\sigma^2 (or standard deviation \\sigma). Its support is \\Re. The pdf of the normal distribution \\mathcal{N}(\\mu, \\sigma) is defined on \\Re as:\n\n\n    f(x; \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\,\\pi\\,\\sigma^2}} \\, e^{-\\displaystyle\\frac{(x - \\mu)^2}{2\\,\\sigma^2}}\n\n\nThe exponential distribution is the probability distribution of the time between events in a Poisson point process, i.e., a process in which events occur continuously and independently at a constant average rate. It is parameterized by one parameter: the rate \\lambda. Its support is \\Re^+ (x > 0). The pdf of the exponential distribution is defined on \\Re^+ as:\n\n\n    f(x; \\lambda) = \\lambda \\, e^{-\\lambda \\, x}\n\n\n\nJoint and conditional probabilities\nLet’s now suppose that we have two random variables X and Y with different probability distributions P(X) and P(Y). The joint probability P(X, Y) denotes the probability of observing the realizations x and y at the same time:\nP(X=x, Y=y)\nIf the random variables are independent, we have:\nP(X=x, Y=y) = P(X=x) \\, P(Y=y)\nIf you know the joint probability, you can compute the marginal probability distribution of each variable:\nP(X=x) = \\sum_y P(X=x, Y=y)\nThe same is true for continuous probability distributions:\n\n    f(x) = \\int f(x, y) \\, dy\n\nSome useful information between two random variables is the conditional probability. P(X=x | Y=y) is the conditional probability that X=x, given that Y=y is observed.\n\nY=y is not random anymore: it is a fact (at least theoretically).\nYou wonder what happens to the probability distribution of X now that you know the value of Y.\n\nConditional probabilities are linked to the joint probability by:\n\n    P(X=x | Y=y) = \\frac{P(X=x, Y=y)}{P(Y=y)}\n\nIf X and Y are independent, we have P(X=x | Y=y) = P(X=x) (knowing Y does not change anything to the probability distribution of X). We can use the same notation for the complete probability distributions:\n\n    P(X | Y) = \\frac{P(X, Y)}{P(Y)}\n\nExample\n\n\n\nSource: https://www.elevise.co.uk/g-e-m-h-5-u.html.\n\n\nYou ask 50 people whether they like cats or dogs:\n\n18 like both cats and dogs.\n21 like only dogs.\n5 like only cats.\n6 like none of them.\n\nWe consider loving cats and dogs as random variables (and that our sample size is big enough to use probabilities…). Among the 23 who love cats, which proportion also loves dogs?\n\n\n\n\n\n\nAnswer\n\n\n\nWe have P(\\text{dog}) = \\displaystyle\\frac{18+21}{50}= \\displaystyle\\frac{39}{50} and P(\\text{cat}) = \\displaystyle\\frac{18+5}{50} = \\frac{23}{50}.\nThe joint probability of loving both cats and dogs is P(\\text{cat}, \\text{dog}) = \\displaystyle\\frac{18}{50}.\nThe conditional probability of loving dogs given one loves cats is:\nP(\\text{dog} | \\text{cat}) = \\displaystyle\\frac{P(\\text{cat}, \\text{dog})}{P(\\text{cat})} = \\frac{\\frac{18}{50}}{\\frac{23}{50}} = \\frac{18}{23}\n\n\n\n\nBayes’ rule\nNoticing that the definition of conditional probabilities is symmetric:\n\n    P(X, Y) = P(X | Y) \\, P(Y) = P(Y | X) \\, P(X)\n\nwe can obtain the Bayes’ rule:\n\n    P(Y | X) = \\frac{P(X|Y) \\, P(Y)}{P(X)}\n\nIt is very useful when you already know P(X|Y) and want to obtain P(Y|X) (Bayesian inference).\n\nP(Y | X) is called the posterior probability.\nP(X | Y) is called the likelihood.\nP(Y) is called the prior probability (belief).\nP(X) is called the model evidence or marginal likelihood.\n\nExample\nLet’s consider a disease D (binary random variable) and a medical test T (also binary). The disease affects 10% of the general population:\nP(D=1)= 0.1 \\qquad \\qquad P(D=0)=0.9\nWhen a patient has the disease, the test is positive 80% of the time (true positives):\nP(T=1 | D=1) = 0.8 \\qquad \\qquad P(T=0 | D=1) = 0.2\nWhen a patient does not have the disease, the test is still positive 10% of the time (false positives):\nP(T=1 | D=0) = 0.1 \\qquad \\qquad P(T=0 | D=0) = 0.9\nGiven that the test is positive, what is the probability that the patient is ill?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\\begin{aligned}\n    P(D=1|T=1) &= \\frac{P(T=1 | D=1) \\, P(D=1)}{P(T=1)} \\\\\n               &\\\\\n               &= \\frac{P(T=1 | D=1) \\, P(D=1)}{P(T=1 | D=1) \\, P(D=1) + P(T=1 | D=0) \\, P(D=0)} \\\\\n               &\\\\\n               &= \\frac{0.8 \\times 0.1}{0.8 \\times 0.1 + 0.1 \\times 0.9} \\\\\n               &\\\\\n               & = 0.47 \\\\\n\\end{aligned}"
  },
  {
    "objectID": "notes/1.2-Math.html#statistics",
    "href": "notes/1.2-Math.html#statistics",
    "title": "Math basics (optional)",
    "section": "Statistics",
    "text": "Statistics\n\n\nMonte Carlo sampling\nRandom sampling or Monte Carlo sampling (MC) consists of taking N samples x_i out of the distribution X (discrete or continuous) and computing the sample average:\n\n    \\mathbb{E}[X] = \\mathbb{E}_{x \\sim X} [x] \\approx \\frac{1}{N} \\, \\sum_{i=1}^N x_i\n\n\n\n\nSamples taken from a normal distribution will mostly be around the mean.\n\n\nMore samples will be obtained where f(x) is high (x is probable), so the average of the sampled data will be close to the expected value of the distribution.\nLaw of big numbers\n\nAs the number of identically distributed, randomly generated variables increases, their sample mean (average) approaches their theoretical mean.\n\nMC estimates are only correct when:\n\nthe samples are i.i.d (independent and identically distributed):\n\nindependent: the samples must be unrelated with each other.\nidentically distributed: the samples must come from the same distribution X.\n\nthe number of samples is large enough. Usually N > 30 for simple distributions.\n\nOne can estimate any function of the random variable with random sampling:\n\n    \\mathbb{E}[f(X)] = \\mathbb{E}_{x \\sim X} [f(x)] \\approx \\frac{1}{N} \\, \\sum_{i=1}^N f(x_i)\n\n\n\n\nSampling can be used to estimate \\pi: when sampling x and y uniformly in [0, 1], the proportion of points with a norm smaller than tends to \\pi/4. Source: https://towardsdatascience.com/an-overview-of-monte-carlo-methods-675384eb1694\n\n\n\n\nCentral limit theorem\nSuppose we have an unknown distribution X with expected value \\mu = \\mathbb{E}[X] and variance \\sigma^2. We can take randomly N samples from X to compute the sample average:\n\n    S_N = \\frac{1}{N} \\, \\sum_{i=1}^N x_i\n\nThe Central Limit Theorem (CLT) states that:\n\nThe distribution of sample averages is normally distributed with mean \\mu and variance \\frac{\\sigma^2}{N}.\n\nS_N \\sim \\mathcal{N}(\\mu, \\frac{\\sigma}{\\sqrt{N}})\nIf we perform the sampling multiple times, even with few samples, the average of the sampling averages will be very close to the expected value. The more samples we get, the smaller the variance of the estimates. Although the distribution X can be anything, the sampling averages are normally distributed.\n\n\n\nSource: https://en.wikipedia.org/wiki/Central_limit_theorem\n\n\n\n\nEstimators\nCLT shows that the sampling average is an unbiased estimator of the expected value of a distribution:\n\\mathbb{E}[S_N] = \\mathbb{E}[X]\nAn estimator is a random variable used to measure parameters of a distribution (e.g. its expectation). The problem is that estimators can generally be biased.\nTake the example of a thermometer M measuring the temperature T. T is a random variable (normally distributed with \\mu=20 and \\sigma=10) and the measurements M relate to the temperature with the relation:\n\n    M = 0.95 \\, T + 0.65\n\n\n\n\nLeft: measurement as a function of the temperature. Right: distribution of temperature.\n\n\nThe thermometer is not perfect, but do random measurements allow us to estimate the expected value of the temperature?\nWe could repeatedly take 100 random samples of the thermometer and see how the distribution of sample averages look like:\n\n\n\n\n\nBut, as the expectation is linear, we actually have:\n\n    \\mathbb{E}[M] = \\mathbb{E}[0.95 \\, T + 0.65] = 0.95 \\, \\mathbb{E}[T] + 0.65 = 19.65 \\neq \\mathbb{E}[T]\n\nThe thermometer is a biased estimator of the temperature.\nLet’s note \\theta a parameter of a probability distribution X that we want to estimate (it does not have to be its mean). An estimator \\hat{\\theta} is a random variable mapping the sample space of X to a set of sample estimates.\n\nThe bias of an estimator is the mean error made by the estimator:\n\n\n    \\mathcal{B}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta} - \\theta] = \\mathbb{E}[\\hat{\\theta}] - \\theta\n\n\nThe variance of an estimator is the deviation of the samples around the expected value:\n\n\n    \\text{Var}(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}] )^2]\n\nIdeally, we would like estimators with:\n\nlow bias: the estimations are correct on average (= equal to the true parameter).\nlow variance: we do not need many estimates to get a correct estimate (CLT: \\frac{\\sigma}{\\sqrt{N}})\n\n\n\n\n\n\nUnfortunately, the perfect estimator does not exist in practice. One usually talks of a bias/variance trade-off: if you have a small bias, you will have a high variance, or vice versa. In machine learning, bias corresponds to underfitting, variance to overfitting."
  },
  {
    "objectID": "notes/1.2-Math.html#information-theory",
    "href": "notes/1.2-Math.html#information-theory",
    "title": "Math basics (optional)",
    "section": "Information theory",
    "text": "Information theory\n\n\nEntropy\nInformation theory (a field founded by Claude Shannon) asks how much information is contained in a probability distribution. Information is related to surprise or uncertainty: are the outcomes of a random variable surprising?\n\nAlmost certain outcomes (P \\sim 1) are not surprising because they happen all the time.\nAlmost impossible outcomes (P \\sim 0) are very surprising because they are very rare.\n\n\n\n\nSelf-information.\n\n\nA useful measurement of how surprising is an outcome x is the self-information:\n\n    I (x) = - \\log P(X = x)\n\nDepending on which log is used, self-information has different units, but it is just a rescaling, the base never matters:\n\n\\log_2: bits or shannons.\n\\log_e = \\ln: nats.\n\nThe entropy (or Shannon entropy) of a probability distribution is the expected value of the self-information of its outcomes:\n\n    H(X) = \\mathbb{E}_{x \\sim X} [I(x)] = \\mathbb{E}_{x \\sim X} [- \\log P(X = x)]\n\nIt measures the uncertainty, randomness or information content of the random variable.\nIn the discrete case:\n\n    H(X) = - \\sum_x P(x) \\, \\log P(x)\n\nIn the continuous case:\n\n    H(X) = - \\int_x f(x) \\, \\log f(x) \\, dx\n\nThe entropy of a Bernouilli variable is maximal when both outcomes are equiprobable. If a variable is deterministic, its entropy is minimal and equal to zero.\n\n\n\nThe entropy of a Bernouilli distribution is maximal when the two outcomes are equiprobable.\n\n\nThe joint entropy of two random variables X and Y is defined by:\n\n    H(X, Y) = \\mathbb{E}_{x \\sim X, y \\sim Y} [- \\log P(X=x, Y=y)]\n\nThe conditional entropy of two random variables X and Y is defined by:\n\n    H(X | Y) = \\mathbb{E}_{x \\sim X, y \\sim Y} [- \\log P(X=x | Y=y)]  = \\mathbb{E}_{x \\sim X, y \\sim Y} [- \\log \\frac{P(X=x , Y=y)}{P(Y=y)}]\n\nIf the variables are independent, we have:\n\n    H(X, Y) = H(X) + H(Y)\n \n    H(X | Y) = H(X)\n\nBoth are related by:\n\n    H(X | Y) = H(X, Y) - H(Y)\n\nThe equivalent of Bayes’ rule is:\n\n    H(Y |X) = H(X |Y) + H(Y) - H(X)\n\n\n\nMutual Information, cross-entropy and Kullback-Leibler divergence\nThe most important information measurement between two variables is the mutual information MI (or information gain):\n\n    I(X, Y) = H(X) - H(X | Y) = H(Y) - H(Y | X)\n\nIt measures how much information the variable X holds on Y:\n\nIf the two variables are independent, the MI is 0 : X is as random, whether you know Y or not.\n\n\n        I (X, Y) = 0\n\n\nIf the two variables are dependent, knowing Y gives you information on X, which becomes less random, i.e. less uncertain / surprising.\n\n\n        I (X, Y) > 0\n\nIf you can fully predict X when you know Y, it becomes deterministic (H(X|Y)=0) so the mutual information is maximal (I(X, Y) = H(X)).\nThe cross-entropy between two distributions X and Y is defined as:\n\n    H(X, Y) = \\mathbb{E}_{x \\sim X}[- \\log P(Y=x)]\n\n\n\n\n\n\n\nNote\n\n\n\nBeware that the notation H(X, Y) is the same as the joint entropy, but it is a different concept!\n\n\n\n\n\nThe cross-entropy measures the overlap between two probability distributions.\n\n\nThe cross-entropy measures the negative log-likelihood that a sample x taken from the distribution X could also come from the distribution Y. More exactly, it measures how many bits of information one would need to distinguish the two distributions X and Y.\n\n    H(X, Y) = \\mathbb{E}_{x \\sim X}[- \\log P(Y=x)]\n\nIf the two distributions are the same almost anywhere, one cannot distinguish samples from the two distributions, the cross-entropy is the same as the entropy of X. If the two distributions are completely different, one can tell whether a sample Z comes from X or Y, the cross-entropy is higher than the entropy of X.\nIn practice, the Kullback-Leibler divergence \\text{KL}(X ||Y) is a better measurement of the similarity (statistical distance) between two probability distributions:\n\n    \\text{KL}(X ||Y) = \\mathbb{E}_{x \\sim X}[- \\log \\frac{P(Y=x)}{P(X=x)}]\n\nIt is linked to the cross-entropy by:\n\n    \\text{KL}(X ||Y) = H(X, Y) - H(X)\n\nIf the two distributions are the same almost anywhere, the KL divergence is zero. If the two distributions are different, the KL divergence is positive. Minimizing the KL between two distributions is the same as making the two distributions “equal”. But remember: the KL is not a metric, as it is not symmetric.\n\n\n\n\n\n\nNote\n\n\n\nRefer https://towardsdatascience.com/entropy-cross-entropy-and-kl-divergence-explained-b09cdae917a for nice visual explanations of the cross-entropy.\n\n\n\n\n\n\nGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press http://www.deeplearningbook.org."
  },
  {
    "objectID": "notes/2.1-Bandits.html",
    "href": "notes/2.1-Bandits.html",
    "title": "Bandits",
    "section": "",
    "text": "Slides: html pdf"
  },
  {
    "objectID": "notes/2.1-Bandits.html#n-armed-bandits",
    "href": "notes/2.1-Bandits.html#n-armed-bandits",
    "title": "Bandits",
    "section": "n-armed bandits",
    "text": "n-armed bandits\n\nRL evaluates actions through trial-and-error rather than comparing its predictions to the correct actions. This is called evaluative feedback, as the feedback depends completely on the action taken. Oppositely, supervised learning is a form of instructive feedback, as the targets (labels / ground truth) do not depend at all on the prediction.\nEvaluative feedback indicates how good the action is, but not whether it is the best or worst action possible. Two forms of RL learning can be distinguished:\n\nAssociative learning: inputs are mapped to the best possible outputs (general RL).\nNon-associative learning: finds one best output, regardless of the current state or past history (bandits).\n\nThe n-armed bandit (or multi-armed bandit) is a non-associative evaluative feedback procedure. Learning and action selection take place in the same single state, while the n actions have different reward distributions. The goal is to find out through trial and error which action provides the most reward on average.\n\n\n\n10-armed bandit with the true value of each action.\n\n\nWe have the choice between N different actions (a_1, ..., a_N). Each action a taken at time t provides a reward r_t drawn from the action-specific probability distribution r(a). The mathematical expectation of that distribution is the expected reward, called the true value of the action Q^*(a).\n\n    Q^*(a) = \\mathbb{E} [r(a)]\n\nThe reward distribution also has a variance: we usually ignore it in RL, as all we care about is the optimal action (but see distributional RL later).\na^* = \\text{argmax}_a Q^*(a)\nIf we take the optimal action an infinity of times, we maximize the reward intake on average. The question is how to find out the optimal action through trial and error, i.e. without knowing the exact reward distribution r(a).\nWe only have access to samples of r(a) by taking the action a at time t (a trial, play or step).\nr_t \\sim r(a)\nThe received rewards r_t vary around the true value over time. We need to build estimates Q_t(a) of the value of each action based on the samples. These estimates will be very wrong at the beginning, but should get better over time."
  },
  {
    "objectID": "notes/2.1-Bandits.html#sampling-based-evaluation",
    "href": "notes/2.1-Bandits.html#sampling-based-evaluation",
    "title": "Bandits",
    "section": "Sampling-based evaluation",
    "text": "Sampling-based evaluation\n\nThe expectation of the reward distribution can be approximated by the mean of its samples (sampling average):\n\n    \\mathbb{E} [r(a)] \\approx  \\frac{1}{N} \\sum_{t=1}^N r_t |_{a_t = a}\n\nSuppose that the action a had been selected t times, producing rewards\n\n    (r_1, r_2, ..., r_t)\n\nThe estimated value of action a at play t is then:\n\n    Q_t (a) = \\frac{r_1 + r_2 + ... + r_t }{t}\n\nOver time, the estimated action-value converges to the true action-value:\n\n   \\lim_{t \\to \\infty} Q_t (a) = Q^* (a)\n\n\n\n\nThe sampling average can correctly approximate the true value of an action given enough samples.\n\n\nThe drawback of maintaining the mean of the received rewards is that it consumes a lot of memory:\n\n    Q_t (a) = \\frac{r_1 + r_2 + ... + r_t }{t} = \\frac{1}{t} \\, \\sum_{i=1}^{t} r_i\n\nIt is possible to update an estimate of the mean in an online or incremental manner:\n\n\\begin{aligned}\n    Q_{t+1}(a) &= \\frac{1}{t+1} \\, \\sum_{i=1}^{t+1} r_i \\\\\n            &= \\frac{1}{t+1} \\, (r_{t+1} + \\sum_{i=1}^{t} r_i )\\\\\n            &= \\frac{1}{t+1} \\, (r_{t+1} + t \\,  Q_{t}(a) ) \\\\\n            &= \\frac{1}{t+1} \\, (r_{t+1} + (t + 1) \\,  Q_{t}(a) - Q_t(a))\n\\end{aligned}\n\nThe estimate at time t+1 depends on the previous estimate at time t and the last reward r_{t+1}:\n\n    Q_{t+1}(a) = Q_t(a) + \\frac{1}{t+1} \\, (r_{t+1} - Q_t(a))\n\nThe problem with the exact mean is that it is only exact when the reward distribution is stationary, i.e. when the probability distribution does not change over time. If the reward distribution is non-stationary, the \\frac{1}{t+1} term will become very small and prevent rapid updates of the mean.\n\n\n\nThe sampling average have problems when the reward distribution is non-stationary.\n\n\nThe solution is to replace \\frac{1}{t+1} with a fixed parameter called the learning rate (or step size) \\alpha:\n\n\\begin{aligned}\n    Q_{t+1}(a) & = Q_t(a) + \\alpha \\, (r_{t+1} - Q_t(a)) \\\\\n                & \\\\\n                & = (1 - \\alpha) \\, Q_t(a) + \\alpha \\, r_{t+1}\n\\end{aligned}\n\nThe computed value is called a moving average (or sliding average), as if one used only a small window of the past history.\n\n    Q_{t+1}(a) = Q_t(a) + \\alpha \\, (r_{t+1} - Q_t(a))\n\nor:\n\n    \\Delta Q(a) = \\alpha \\, (r_{t+1} - Q_t(a))\n\n\n\n\nThe moving average is much more flexible for non-stationary distributions.\n\n\nThe moving average adapts very fast to changes in the reward distribution and should be used in non-stationary problems. It is however not exact and sensible to noise. Choosing the right value for \\alpha can be difficult.\nThe form of this update rule is very important to remember:\n\n    \\text{new estimate} = \\text{current estimate} + \\alpha \\, (\\text{target} - \\text{current estimate})\n\nEstimates following this update rule track the mean of their sampled target values. \\text{target} - \\text{current estimate} is the prediction error between the target and the estimate."
  },
  {
    "objectID": "notes/2.1-Bandits.html#action-selection",
    "href": "notes/2.1-Bandits.html#action-selection",
    "title": "Bandits",
    "section": "Action selection",
    "text": "Action selection\n\n\nGreedy action selection\nLet’s suppose we have formed reasonable estimates of the Q-values Q_t(a) at time t. Which action should we do next? If we select the next action a_{t+1} randomly (random agent), we do not maximize the rewards we receive, but we can continue learning the Q-values. Choosing the action to perform next is called action selection and several schemes are possible.\n\n\n\nGreedy action selection.\n\n\nThe greedy action is the action whose estimated value is maximal at time t based on our current estimates:\n\n    a^*_t = \\text{argmax}_{a} Q_t(a)\n\nIf our estimates Q_t are correct (i.e. close from Q^*), the greedy action is the optimal action and we maximize the rewards on average. If our estimates are wrong, the agent will perform sub-optimally.\nThis defines the greedy policy, where the probability of taking the greedy action is 1 and the probability of selecting another action is 0:\n\n    \\pi(a) = \\begin{cases} 1 \\; \\text{if} \\; a = a_t^* \\\\ 0 \\; \\text{otherwise.} \\end{cases}\n\nThe greedy policy is deterministic: the action taken is always the same for a fixed Q_t.\n\n\nExploration-exploitation dilemma\n\n\n\nGreedy action selection only works when the estimates are good enough.\n\n\n\n\n\nEstimates are initially bad (e.g. 0 here), so an action is sampled randomly and a reward is received.\n\n\n\n\n\nThe Q-value of that action becomes positive, so it becomes the greedy action.\n\n\n\n\n\nGreedy action selection will always select that action, although the second one would have been better.\n\n\nThis exploration-exploitation dilemma is the hardest problem in RL:\n\nExploitation is using the current estimates to select an action: they might be wrong!\nExploration is selecting non-greedy actions in order to improve their estimates: they might not be optimal!\n\nOne has to balance exploration and exploitation over the course of learning:\n\nMore exploration at the beginning of learning, as the estimates are initially wrong.\nMore exploitation at the end of learning, as the estimates get better.\n\n\n\n\\epsilon-greedy action selection\n\n\n\n\\epsilon-greedy action selection\n\n\n\\epsilon-greedy action selection ensures a trade-off between exploitation and exploration. The greedy action is selected with probability 1 - \\epsilon (with 0 < \\epsilon <1), the others with probability \\epsilon:\n\n    \\pi(a) = \\begin{cases} 1 - \\epsilon \\; \\text{if} \\; a = a_t^* \\\\ \\frac{\\epsilon}{|\\mathcal{A}| - 1} \\; \\text{otherwise.} \\end{cases}\n\nThe parameter \\epsilon controls the level of exploration: the higher \\epsilon, the more exploration. One can set \\epsilon high at the beginning of learning and progressively reduce it to exploit more. However, it chooses equally among all actions: the worst action is as likely to be selected as the next-to-best action.\n\n\nSoftmax action selection\n\n\n\nSoftmax action selection\n\n\nSoftmax action selection defines the probability of choosing an action using all estimated value. It represents the policy using a Gibbs (or Boltzmann) distribution:\n\n    \\pi(a) = \\dfrac{\\exp \\dfrac{Q_t(a)}{\\tau}}{ \\displaystyle\\sum_{a'} \\exp \\dfrac{Q_t(a')}{\\tau}}\n\nwhere \\tau is a positive parameter called the temperature.\n\n\n\n\n\nJust as \\epsilon, the temperature \\tau controls the level of exploration:\n\nHigh temperature causes the actions to be nearly equiprobable (random agent).\nLow temperature causes the greediest actions only to be selected (greedy agent).\n\n\n\nExample of action selection for the 10-armed bandit\nProcedure:\n\nN = 10 possible actions with Q-values Q^*(a_1), ... , Q^*(a_{10}) randomly chosen in \\mathcal{N}(0, 1).\nEach reward r_t is drawn from a normal distribution \\mathcal{N}(Q^*(a), 1) depending on the selected action.\nEstimates Q_t(a) are initialized to 0.\nThe algorithms run for 1000 plays, and the results are averaged 200 times.\n\nGreedy action selection\nGreedy action selection allows to get rid quite early of the actions with negative rewards. However, it may stick with the first positive action it founds, probably not the optimal one. The more actions you have, the more likely you will get stuck in a suboptimal policy.\n\n\n\nGreedy action selection on a 10-armed bandit for 200 plays.\n\n\n\\epsilon-greedy action selection\n\\epsilon-greedy action selection continues to explore after finding a good (but often suboptimal) action. It is not always able to recognize the optimal action (it depends on the variance of the rewards).\n\n\n\n\\epsilon-greedy action selection on a 10-armed bandit for 200 plays.\n\n\nSoftmax action selection\nSoftmax action selection explores more consistently the available actions. The estimated Q-values are much closer to the true values than with (\\epsilon-)greedy methods.\n\n\n\nSoftmax action selection on a 10-armed bandit for 200 plays.\n\n\nComparison\n\n\n\nAverage reward per step.\n\n\n\n\n\nFrequency of selection of the optimal action.\n\n\nThe greedy method learns faster at the beginning, but get stuck in the long-term by choosing suboptimal actions (50% of trials). \\epsilon-greedy methods perform better on the long term, because they continue to explore. High values for \\epsilon provide more exploration, hence find the optimal action earlier, but also tend to deselect it more often: with a limited number of plays, it may collect less reward than smaller values of \\epsilon.\n\n\n\nAverage reward per step.\n\n\n\n\n\nFrequency of selection of the optimal action.\n\n\nThe softmax does not necessarily find a better solution than \\epsilon-greedy, but it tends to find it faster (depending on \\epsilon or \\tau), as it does not lose time exploring obviously bad solutions. \\epsilon-greedy or softmax methods work best when the variance of rewards is high. If the variance is zero (always the same reward value), the greedy method would find the optimal action more rapidly: the agent only needs to try each action once.\n\n\nExploration schedule\nA useful technique to cope with the exploration-exploitation dilemma is to slowly decrease the value of \\epsilon or \\tau with the number of plays. This allows for more exploration at the beginning of learning and more exploitation towards the end. It is however hard to find the right decay rate for the exploration parameters.\n\n\n\nExploration parameters such as \\epsilon or \\tau can be scheduled to allow more exploration at the beginning of learning.\n\n\nThe performance is worse at the beginning, as the agent explores with a high temperature. But as the agent becomes greedier and greedier, the performance become more optimal than with a fixed temperature.\n\n\n\nAverage reward per step.\n\n\n\n\n\nFrequency of selection of the optimal action.\n\n\n\n\nOptimistic initial values\nThe problem with online evaluation is that it depends a lot on the initial estimates Q_0.\n\nIf the initial estimates are already quite good (expert knowledge), the Q-values will converge very fast.\nIf the initial estimates are very wrong, we will need a lot of updates to correctly estimate the true values.\n\n\n\\begin{aligned}\n    &Q_{t+1}(a) = (1 - \\alpha) \\, Q_t(a) + \\alpha \\, r_{t+1}  \\\\\n    &\\\\\n    & \\rightarrow Q_1(a) = (1 - \\alpha) \\, Q_0(a) + \\alpha \\, r_1 \\\\\n    & \\rightarrow Q_2(a) = (1 - \\alpha) \\, Q_1(a) + \\alpha \\, r_2 = (1- \\alpha)^2 \\, Q_0(a) + (1-\\alpha)\\alpha \\, r_1 + \\alpha r_2 \\\\\n\\end{aligned}\n\nThe influence of Q_0 on Q_t fades quickly with (1-\\alpha)^t, but that can be lost time or lead to a suboptimal policy. However, we can use this at our advantage with optimistic initialization.\nBy choosing very high initial values for the estimates (they can only decrease), one can ensure that all possible actions will be selected during learning by the greedy method, solving the exploration problem. This leads however to an overestimation of the value of other actions.\n\n\n\nOptimistic initialization.\n\n\n\n\nReinforcement comparison\n\nActions followed by large rewards should be made more likely to recur, whereas actions followed by small rewards should be made less likely to recur. But what is a large/small reward? Is a reward of 5 large or small? Reinforcement comparison methods only maintain a preference p_t(a) for each action, which is not exactly its Q-value. The preference for an action is updated after each play, according to the update rule:\n\n    p_{t+1}(a_t) =    p_{t}(a_t) + \\beta \\, (r_t - \\tilde{r}_t)\n\nwhere \\tilde{r}_t is the moving average of the recently received rewards (regardless the action):\n\n    \\tilde{r}_{t+1} =  \\tilde{r}_t + \\alpha \\, (r_t - \\tilde{r}_t)\n\n\nIf an action brings more reward than usual (good surprise), we increase the preference for that action.\nIf an action brings less reward than usual (bad surprise), we decrease the preference for that action.\n\n\\beta > 0 and 0 < \\alpha < 1 are two constant parameters.\nThe preferences can be used to select the action using the softmax method just like the Q-values (without temperature):\n\n    \\pi_t (a) = \\dfrac{\\exp p_t(a)}{ \\displaystyle\\sum_{a'} \\exp p_t(a')}\n\n\n\n\nReinforcement comparison.\n\n\nReinforcement comparison can be very effective, as it does not rely only on the rewards received, but also on their comparison with a baseline, the average reward. This idea is at the core of actor-critic architectures which we will see later. The initial average reward \\tilde{r}_{0} can be set optimistically to encourage exploration.\n\n\n\nAverage reward per step.\n\n\n\n\n\nFrequency of selection of the optimal action.\n\n\n\n\nGradient bandit algorithm\nOne can even go further than reinforcement comparison: Instead of only increasing the preference for the executed action if it brings more reward than usual, we could also decrease the preference for the other actions. The preferences are used to select an action a_t via softmax:\n\n    \\pi_t (a) = \\dfrac{\\exp p_t(a)}{ \\displaystyle\\sum_{a'} \\exp p_t(a')}\n\nThe update rule for the action taken a_t is:\n\n    p_{t+1}(a_t) =    p_{t}(a_t) + \\beta \\, (r_t - \\tilde{r}_t) \\, (1 - \\pi_t(a_t))\n\nThe update rule for the other actions a a \\neq a_t is:\n\n    p_{t+1}(a) =    p_{t}(a) - \\beta \\, (r_t - \\tilde{r}_t) \\, \\pi_t(a)\n\nThe reward baseline is updated with:\n\n    \\tilde{r}_{t+1} =  \\tilde{r}_t + \\alpha \\, (r_t - \\tilde{r}_t)\n\nThe preference can increase become quite high, making the policy greedy towards the end. No need for a temperature parameter!\n\n\n\nReinforcement comparison.\n\n\nGradient bandit is not always better than reinforcement comparison, but learns initially faster (depending on the parameters \\alpha and \\beta).\n\n\n\nAverage reward per step.\n\n\n\n\n\nFrequency of selection of the optimal action.\n\n\n\n\nUpper-Confidence-Bound action selection\nIn the previous methods, exploration is controlled by an external parameter (\\epsilon or \\tau) which is global to each action an must be scheduled. A much better approach would be to decide whether to explore an action based on the uncertainty about its Q-value:\n\nIf we are certain about the value of an action, there is no need to explore it further, we only have to exploit it if it is good.\n\nThe central limit theorem tells us that the variance of a sampling estimator decreases with the number of samples:\n\nThe distribution of sample averages is normally distributed with mean \\mu and variance \\frac{\\sigma^2}{N}.\n\nS_N \\sim \\mathcal{N}(\\mu, \\frac{\\sigma}{\\sqrt{N}})\nThe more you explore an action a, the smaller the variance of Q_t(a), the more certain you are about the estimation, the less you need to explore it.\nUpper-Confidence-Bound (UCB) action selection is a greedy action selection method that uses an exploration bonus:\n\n    a^*_t = \\text{argmax}_{a} \\left( Q_t(a) + c \\, \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right)\n\nQ_t(a) is the current estimated value of a and N_t(a) is the number of times the action a has already been selected. It realizes a balance between trusting the estimates Q_t(a) and exploring uncertain actions which have not been explored much yet.\nThe term \\sqrt{\\frac{\\ln t}{N_t(a)}} is an estimate of the variance of Q_t(a). The sum of both terms is an upper-bound of the true value \\mu + \\sigma. When an action has not been explored much yet, the uncertainty term will dominate and the action be explored, although its estimated value might be low. When an action has been sufficiently explored, the uncertainty term goes to 0 and we greedily follow Q_t(a). The exploration-exploitation trade-off is automatically adjusted by counting visits to an action.\n\n\n\nUpper-Confidence-Bound action selection.\n\n\nThe “smart” exploration of UCB allows to find the optimal action faster.\n\n\n\nAverage reward per step.\n\n\n\n\n\nFrequency of selection of the optimal action.\n\n\n\n\nSummary of evaluative feedback methods\nGreedy, \\epsilon-greedy, softmax, reinforcement comparison, gradient bandit and UCB all have their own advantages and disadvantages depending on the type of problem: stationary or not, high or low reward variance, etc…\nThese simple techniques are the most useful ones for bandit-like problems: more sophisticated ones exist, but they either make too restrictive assumptions, or are computationally intractable.\nTake home messages:\n\nRL tries to estimate values based on sampled rewards.\nOne has to balance exploitation and exploration throughout learning with the right action selection scheme.\nMethods exploring more find better policies, but are initially slower."
  },
  {
    "objectID": "notes/2.1-Bandits.html#contextual-bandits",
    "href": "notes/2.1-Bandits.html#contextual-bandits",
    "title": "Bandits",
    "section": "Contextual bandits",
    "text": "Contextual bandits\n\nIn contextual bandits, the obtained rewards do not only depend on the action a, but also on the state or context s:\n\n    r_{t+1} \\sim r(s, a)\n\nFor example, the n-armed bandit could deliver rewards with different probabilities depending on who plays, the time of the year or the availability of funds in the casino. The problem is simply to estimate Q(s, a) instead of Q(a)…\n\n\n\nContextual bandits are an intermediary problem between bandits and the full RL setup, as rewards depend on the state, but the state is not influenced by the action. Source: https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c\n\n\nContextual bandits are for example useful for Recommender systems: actions are the display of an advertisement, the context (or state) represents the user features / identity (possibly learned by an autoencoder), and the reward represents whether the user clicks on the ad or not.\n\n\n\nContextual bandits are used in recommender systems. Source: https://aws.amazon.com/blogs/machine-learning/power-contextual-bandits-using-continual-learning-with-amazon-sagemaker-rl/\n\n\nSome efficient algorithms have been developed recently, for example (Agarwal et al., 2014), but we will not go into details in this course.\n\n\n\n\nAgarwal, A., Hsu, D., Kale, S., Langford, J., Li, L., and Schapire, R. E. (2014). Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits. in Proceedings of the 31 st International Conference on Machine Learning (Beijing, China), 9. https://arxiv.org/abs/1402.0555."
  },
  {
    "objectID": "notes/2.2-MDP.html",
    "href": "notes/2.2-MDP.html",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "Slides: html pdf"
  },
  {
    "objectID": "notes/2.2-MDP.html#from-finite-state-machines-to-markov-decision-process",
    "href": "notes/2.2-MDP.html#from-finite-state-machines-to-markov-decision-process",
    "title": "Markov Decision Processes",
    "section": "From Finite State Machines to Markov Decision Process",
    "text": "From Finite State Machines to Markov Decision Process\n\nThe kind of task that can be solved by RL is called a Markov Decision Process (MDP). For a MDP, the environment is fully observable, i.e. the current state s_t completely characterizes the process at time t. Actions a_t provoke transitions between two states s_t and s_{t+1}, according to transition probabilities. A reward r_{t+1} is (probabilistically) associated to each transition.\n\n\n\nAgent-environment interface for video games. Source: David Silver https://www.davidsilver.uk/teaching/\n\n\n\n\n\n\n\n\nNote\n\n\n\nn-armed bandits are MDPs with only one state.\n\n\n\nFinite State Machine (FSM)\nA finite state machine (or finite state automaton) is a mathematical model of computation. A FSM can only be in a single state s at any given time. Transitions between states are governed by external inputs, when some condition is met.\n\n\n\nFinite state machine. Source: https://web.stanford.edu/class/archive/cs/cs103/cs103.1142/button-fsm/\n\n\nA FSM is fully defined by:\n\nThe state set \\mathcal{S} = \\{ s_i\\}_{i=1}^N.\nIts initial state S_0.\nA list of conditions for each transition.\n\nA FSM is usually implemented by a series of if/then/else statements:\n\nif state == “hover” and press == true:\n\nstate = “pressed”\n\nelif …\n\n\n\nMarkov Chain (MC)\n\n\n\nMarkov chain. Credit: David Silver https://www.davidsilver.uk/teaching/\n\n\nA first-order Markov Chain (or Markov process) is a stochastic process generated by a FSM, where transitions between states are governed by state transition probabilities.\nA Markov Chain is defined by:\n\nThe state set \\mathcal{S} = \\{ s_i\\}_{i=1}^N.\nThe state transition probability function:\n\n\n\\begin{aligned}\n    \\mathcal{P}: \\mathcal{S} \\rightarrow & P(\\mathcal{S}) \\\\\n    p(s' | s) & =  P (s_{t+1} = s' | s_t = s) \\\\\n\\end{aligned}\n\n\nMarkov property\nWhen the states have the Markov property, the state transition probabilities fully describe the MC. The Markov property states that:\n\nThe future is independent of the past given the present.\n\nFormally, the state s_t (state at time t) is Markov (or Markovian) if and only if:\n\n    P(s_{t+1} | s_t) = P(s_{t+1} | s_t, s_{t-1}, \\ldots, s_0)\n\nThe knowledge of the current state s_t is enough to predict in which state s_{t+1} the system will be at the next time step. We do not need the whole history \\{s_0, s_1, \\ldots, s_t\\} of the system to predict what will happen.\n\n\n\n\n\n\nNote\n\n\n\nIf we need only s_{t-1} and s_t to predict s_{t+1}, we have a second-order Markov chain.\n\n\nFor example, the probability 0.8 of transitioning from “Class 2” to “Class 3” is the same regardless we were in “Class 1” or “Pub” before. If this is not the case, the states are not Markov, and this is not a Markov chain. We would need to create two distinct states:\n\n“Class 2 coming from Class 1”\n“Class 2 coming from the pub”\n\nSingle video frames are not Markov states: you cannot generally predict what will happen based on a single image. A simple solution is to stack or concatenate multiple frames: By measuring the displacement of the ball between two consecutive frames, we can predict where it is going. One can also learn state representations containing the history using recurrent neural networks (see later).\n\n\nState transition matrix\nSupposing that the states have the Markov property, the transitions in the system can be summarized by the state transition matrix \\mathcal{P}:\n\n\n\nState transition matrix. Credit: David Silver https://www.davidsilver.uk/teaching/\n\n\nEach element of the state transition matrix corresponds to p(s' | s). Each row of the state transition matrix sums to 1:\n\\sum_{s'} p(s' | s)  = 1\nThe tuple <\\mathcal{S}, \\mathcal{P}> fully describes the Markov chain.\n\n\n\nMarkov Reward Process (MRP)\nA Markov Reward Process is a Markov Chain where each transition is associated with a scalar reward r, coming from some probability distribution.\n\n\n\nMarkov Reward Process. Credit: David Silver https://www.davidsilver.uk/teaching/\n\n\nA Markov Reward Process is defined by the tuple <\\mathcal{S}, \\mathcal{P}, \\mathcal{R}, \\gamma>.\n\nThe finite state set \\mathcal{S}.\nThe state transition probability function: \n\\begin{aligned}\n  \\mathcal{P}: \\mathcal{S} \\rightarrow & P(\\mathcal{S}) \\\\\n  p(s' | s) & =  P (s_{t+1} = s' | s_t = s) \\\\\n\\end{aligned}\n\nThe expected reward function: \n\\begin{aligned}\n  \\mathcal{R}: \\mathcal{S} \\times \\mathcal{S} \\rightarrow & \\Re \\\\\n  r(s, s') &=  \\mathbb{E} (r_{t+1} | s_t = s, s_{t+1} = s') \\\\\n\\end{aligned}\n\nThe discount factor \\gamma \\in [0, 1].\n\nAs with n-armed bandits, we only care about the expected reward received during a transition s \\rightarrow s' (on average), but the actual reward received r_{t+1} may vary around the expected value.\nr(s, s') =  \\mathbb{E} (r_{t+1} | s_t = s, s_{t+1} = s')\nThe main difference with n-armed bandits is that the MRP will be in a sequence of states (possibly infinite):\ns_0 \\rightarrow s_1 \\rightarrow s_2  \\rightarrow \\ldots \\rightarrow s_T\nand collect a sequence of reward samples:\nr_1 \\rightarrow r_2 \\rightarrow r_3  \\rightarrow \\ldots \\rightarrow r_{T}\nIn a MRP, we are interested in estimating the return R_t, i.e. the discounted sum of future rewards after the step t:\n\n    R_t = r_{t+1} + \\gamma \\, r_{t+2} + \\gamma^2 \\, r_{t+3} + \\ldots = \\sum_{k=0}^\\infty \\gamma^k \\, r_{t+k+1}\n\nOf course, you never know the return at time t: transitions and rewards are probabilistic, so the received rewards in the future are not exactly predictable at t. R_t is therefore purely theoretical: RL is all about estimating the return.\nThe discount factor (or discount rate, or discount) \\gamma \\in [0, 1] is a very important parameter in RL: It defines the present value of future rewards. Receiving 10 euros now has a higher value than receiving 10 euros in ten years, although the reward is the same: you do not have to wait.\nThe value of receiving a reward r after k+1 time steps is \\gamma^k \\, r. Immediate rewards are better than delayed rewards. When \\gamma < 1, \\gamma^k tends to 0 when k goes to infinity: this makes sure that the return is always finite. This is particularly important when the MRP is cyclic / periodic. If all sequences terminate at some time step T, we can set \\gamma= 1.\n\n\nMarkov Decision Process (MDP)\nA Markov Decision Process is a MRP where transitions are influenced by actions a \\in \\mathcal{A}.\n\n\n\nMarkov decision process. Credit: David Silver https://www.davidsilver.uk/teaching/\n\n\nA finite MDP is defined by the tuple <\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma>:\n\nThe finite state set \\mathcal{S}.\nThe finite action set \\mathcal{A}.\nThe state transition probability function:\n\n\n\\begin{aligned}\n    \\mathcal{P}: \\mathcal{S} \\times \\mathcal{A} \\rightarrow & P(\\mathcal{S}) \\\\\n    p(s' | s, a) & =  P (s_{t+1} = s' | s_t = s, a_t = a) \\\\\n\\end{aligned}\n\n\nThe expected reward function:\n\n\n\\begin{aligned}\n    \\mathcal{R}: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow & \\Re \\\\\n    r(s, a, s') &=  \\mathbb{E} (r_{t+1} | s_t = s, a_t = a, s_{t+1} = s') \\\\\n\\end{aligned}\n\n\nThe discount factor \\gamma \\in [0, 1].\n\nWhy do we need transition probabilities in RL?\n p(s' | s, a) =  P (s_{t+1} = s' | s_t = s, a_t = a)\nSome RL tasks are deterministic: an action a in a state s always leads to the state s' (board games, video games…). Others are stochastic: the same action a can lead to different states s': Casino games (throwing a dice, etc), two-opponent games (the next state depends on what the other player chooses), uncertainty (shoot at basketball, slippery wheels, robotic grasping)…\nFor a transition (s, a, s'), the received reward can be also stochastic: casino games (armed bandit), incomplete information, etc. Most of the problems we will see in this course have deterministic rewards, but we only care about expectations anyway.\n\nMarkov property\nThe state of the agent at step t refers to whatever information is available about its environment or its own “body”. The state can include immediate “sensations”, highly processed sensations, and structures built up over time from sequences of sensations. A state should summarize all past sensations so as to retain all essential information, i.e. it should have the Markov Property:\n\\begin{aligned}\n     P( s_{t+1} = s, r_{t+1} = r & | s_t, a_t, r_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = P( s_{t+1} = s, r_{t+1} = r | s_t, a_t ) \\\\\n     &\\text{for all s, r, and past histories} \\quad (s_{t}, a_{t}, ..., s_0, a_0)\n\\end{aligned}\n\nThis means that the current state representation s contains enough information to predict the probability of arriving in the next state s' given the chosen action a. When the Markovian property is not met, we have a Partially-Observable Markov Decision Process (POMDP).\nIn a POMDP, the agent does not have access to the true state s_t of the environment, but only observations o_t. Observations are partial views of the state, without the Markov property. The dynamics of the environment (transition probabilities, reward expectations) only depend on the state, not the observations. The agent can only make decisions (actions) based on the sequence of observations, as it does not have access to the state directly (Plato’s cavern).\nIn a POMDP, the state s_t of the agent is implicitly the concatenation of the past observations and actions:\n\ns_t = (o_0, a_0, o_1, a_1, \\ldots, a_{t-1}, o_t)\n\nUnder conditions, this inferred state can have the Markov property and the POMDP is solvable.\n\n\nReturns\nSuppose the sequence of rewards obtained after step t (after being in state s_t and choosing action a_t) is:\n r_{t+1}, r_{t+2}, r_{t+3}, ... \nWhat we want to maximize is the return (reward-to-go) at each time step t, i.e. the sum of all future rewards:\n\n    R_t = r_{t+1} + \\gamma \\, r_{t+2} +  \\gamma^2 \\, r_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k \\, r_{t+k+1}\n\nMore generally, for a trajectory (episode) \\tau = (s_0, a_0, r_1, s_1, a_1, \\ldots, s_T), one can define its return as:\n R(\\tau) = \\sum_{t=0}^{T} \\gamma^t \\, r_{t+1} \nFor episodic tasks (which break naturally into finite episodes of length T, e.g. plays of a game, trips through a maze), the return is always finite and easy to compute at the end of the episode. The discount factor can be set to 1:\n\n    R_t = \\sum_{k=0}^{T} r_{t+k+1}\n\nFor continuing tasks (which can not be split into episodes), the return could become infinite if \\gamma = 1. The discount factor has to be smaller than 1.\n\n    R_t = \\sum_{k=0}^{\\infty} \\gamma^k \\, r_{t+k+1}\n\nThe discount rate \\gamma determines the relative importance of future rewards for the behavior:\n\nif \\gamma is close to 0, only the immediately available rewards will count: the agent is greedy or myopic.\nif \\gamma is close to 1, even far-distance rewards will be taken into account: the agent is farsighted.\n\n\n\n\n\n\n\nWhy the reward on the long term?\n\n\n\n\n\n\n\n\nSelecting the action a_1 in s_1 does not bring reward immediately (r_1 = 0) but allows to reach s_5 in the future and get a reward of 10. Selecting a_2 in s_1 brings immediately a reward of 1, but that will be all. a_1 is better than a_2, because it will bring more reward on the long term.\nWhen selecting a_1 in s_1, the discounted return is:\n\n    R = 0 + \\gamma \\, 0 + \\gamma^2 \\, 0 + \\gamma^3 \\, 10 + \\ldots = 10 \\, \\gamma^3\n\nwhile it is R= 1 for the action a_2.\nFor small values of \\gamma (e.g. 0.1), 10\\, \\gamma^3 becomes smaller than one, so the action a_2 leads to a higher discounted return. The discount rate \\gamma changes the behavior of the agent. It is usually taken somewhere between 0.9 and 0.999.\n\n\n\n\nExample: the cartpole balancing task\n\n\n\nCartpole balancing task\n\n\n\nState: Position and velocity of the cart, angle and speed of the pole.\nActions: Commands to the motors for going left or right.\nReward function: Depends on whether we consider the task as episodic or continuing.\n\nThe problem can be viewed both as an episodic or continuing task:\n\nEpisodic task where episode ends upon failure:\n\nreward = +1 for every step before failure, 0 at failure.\nreturn = number of steps before failure.\n\nContinuing task with discounted return:\n\nreward = -1 at failure, 0 otherwise.\nreturn = - \\gamma^k for k steps before failure.\n\n\nIn both cases, the goal is to maximize the return by maintaining the pole vertical as long as possible.\n\n\nExample: the recycling robot\n\n\n\nRecycling robot. Source: (Sutton and Barto, 1998)\n\n\nAt each step, the recycling robot has to decide whether it should:\n\nactively search for a can,\nwait for someone to bring it a can, or\ngo to home base and recharge.\n\nSearching is better (more reward) but runs down the battery (probability 1-\\alpha to empty the battery): if the robot runs out of power while searching, he has to be rescued (which leads to punishment and should be avoided). Decisions must be made on basis of the current energy level: high, low. This will be the state of the robot. The return is the number of cans collected on the long term.\n\n\\mathcal{S} = \\{ \\text{high}, \\text{low} \\}\n\\mathcal{A}(\\text{high} ) = \\{ \\text{search}, \\text{wait} \\}\n\\mathcal{A}(\\text{low} ) = \\{ \\text{search}, \\text{wait}, \\text{recharge} \\}\nR^{\\text{search}} = expected number of cans while searching.\nR^{\\text{wait}} = expected number of cans while waiting.\nR^{\\text{search}} > R^{\\text{wait}}\n\nThe MDP is fully described by the following table:\n\n\n\n\n\n\n\n\n\n\ns\ns'\na\np(s' / s, a)\nr(s, a, s')\n\n\n\n\nhigh\nhigh\nsearch\n\\alpha\n\\mathcal{R}^\\text{search}\n\n\nhigh\nlow\nsearch\n1 - \\alpha\n\\mathcal{R}^\\text{search}\n\n\nlow\nhigh\nsearch\n1 - \\beta\n-3\n\n\nlow\nlow\nsearch\n\\beta\n\\mathcal{R}^\\text{search}\n\n\nhigh\nhigh\nwait\n1\n\\mathcal{R}^\\text{wait}\n\n\nhigh\nlow\nwait\n0\n\\mathcal{R}^\\text{wait}\n\n\nlow\nhigh\nwait\n0\n\\mathcal{R}^\\text{wait}\n\n\nlow\nlow\nwait\n1\n\\mathcal{R}^\\text{wait}\n\n\nlow\nhigh\nrecharge\n1\n0\n\n\nlow\nlow\nrecharge\n0\n0\n\n\n\n\n\nThe policy\nThe probability that an agent selects a particular action a in a given state s is called the policy \\pi.\n\n\\begin{align}\n    \\pi &: \\mathcal{S} \\times \\mathcal{A} \\rightarrow P(\\mathcal{S})\\\\\n    (s, a) &\\rightarrow \\pi(s, a)  = P(a_t = a | s_t = s) \\\\\n\\end{align}\n\nThe policy can be deterministic (one action has a probability of 1, the others 0) or stochastic. The goal of an agent is to find a policy that maximizes the sum of received rewards on the long term, i.e. the return R_t at each each time step. This policy is called the optimal policy \\pi^*.\n\n    \\mathcal{J}(\\pi) = \\mathbb{E}_{\\rho_\\pi} [R_t] \\qquad\n    \\pi^* = \\text{argmax} \\, \\mathcal{J}(\\pi)\n\n\n\nGoal of Reinforcement Learning\nRL is an adaptive optimal control method for Markov Decision Processes using (sparse) rewards as a partial feedback. At each time step t, the agent observes its Markov state s_t \\in \\mathcal{S}, produces an action a_t \\in \\mathcal{A}(s_t), receives a reward according to this action r_{t+1} \\in \\Re and updates its state: s_{t+1} \\in \\mathcal{S}.\nThe agent generates trajectories \\tau = (s_0, a_0, r_1, s_1, a_1, \\ldots, s_T) depending on its policy \\pi(s ,a).\nThe return of a trajectory is the (discounted) sum of rewards accumulated during the sequence:\n R(\\tau) = \\sum_{t=0}^{T} \\gamma^t \\, r_{t+1} \nThe goal is to find the optimal policy \\pi^* (s, a) that maximizes in expectation the return of each possible trajectory under that policy:\n\n    \\mathcal{J}(\\pi) = \\mathbb{E}_{\\tau \\sim \\rho_\\pi} [R(\\tau)] \\qquad\n    \\pi^* = \\text{argmax} \\, \\mathcal{J}(\\pi)"
  },
  {
    "objectID": "notes/2.2-MDP.html#bellman-equations",
    "href": "notes/2.2-MDP.html#bellman-equations",
    "title": "Markov Decision Processes",
    "section": "Bellman equations",
    "text": "Bellman equations\n\n\nValue Functions\nA central notion in RL is to estimate the value (or utility) of every state and action of the MDP. The value of a state V^{\\pi} (s) is the expected return when starting from that state and thereafter following the agent’s current policy \\pi.\nThe state-value function V^{\\pi} (s) of a state s given the policy \\pi is defined as the mathematical expectation of the return after that state:\n  V^{\\pi} (s) = \\mathbb{E}_{\\rho_\\pi} ( R_t | s_t = s) = \\mathbb{E}_{\\rho_\\pi} ( \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} |s_t=s ) \nThe mathematical expectation operator \\mathbb{E}(\\cdot) is indexed by \\rho_\\pi, the probability distribution of states achievable with \\pi.\nSeveral trajectories are possible after the state s:\n\nThe state transition probability function p(s' | s, a) leads to different states s', even if the same actions are taken.\nThe expected reward function r(s, a, s') provides stochastic rewards, even if the transition (s, a, s') is the same.\nThe policy \\pi itself is stochastic.\n\nOnly rewards that are obtained using the policy \\pi should be taken into account, not the complete distribution of states and rewards.\nThe value of a state is not intrinsic to the state itself, it depends on the policy: One could be in a state which is very close to the goal (only one action left to win game), but if the policy is very bad, the “good” action will not be chosen and the state will have a small value.\nThe value of taking an action a in a state s under policy \\pi is the expected return starting from that state, taking that action, and thereafter following the following \\pi. The action-value function for a state-action pair (s, a) under the policy \\pi (or Q-value) is defined as:\n\n\\begin{align}\n    Q^{\\pi} (s, a)  & = \\mathbb{E}_{\\rho_\\pi} ( R_t | s_t = s, a_t =a) \\\\\n                    & = \\mathbb{E}_{\\rho_\\pi} ( \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} |s_t=s, a_t=a) \\\\\n\\end{align}\n\nState- and action-value functions are linked to each other. The value of a state V^{\\pi}(s) depends on the value Q^{\\pi} (s, a) of the action that will be chosen by the policy \\pi in s:\n\n    V^{\\pi}(s) = \\mathbb{E}_{a \\sim \\pi(s,a)} [Q^{\\pi} (s, a)] = \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, Q^{\\pi} (s, a)\n\nIf the policy \\pi is deterministic (the same action is chosen every time), the value of the state is the same as the value of that action (same expected return). If the policy \\pi is stochastic (actions are chosen with different probabilities), the value of the state is the expectation (weighted average) of the value of the actions. If the Q-values are known, the V-values can be found easily.\nWe can note that the return at time t depends on the immediate reward r_{t+1} and the return at the next time step t+1:\n\n\\begin{aligned}\n    R_t &= r_{t+1} + \\gamma \\, r_{t+2} +  \\gamma^2  \\, r_{t+3} + \\dots + \\gamma^k \\, r_{t+k+1} + \\dots \\\\\n        &= r_{t+1} + \\gamma \\, ( r_{t+2} +  \\gamma \\, r_{t+3} + \\dots + \\gamma^{k-1} \\, r_{t+k+1} + \\dots) \\\\\n        &= r_{t+1} + \\gamma \\,  R_{t+1} \\\\\n\\end{aligned}\n\nWhen taking the mathematical expectation of that identity, we obtain:\n\n    \\mathbb{E}_{\\rho_\\pi}[R_t] = r(s_t, a_t, s_{t+1}) + \\gamma \\, \\mathbb{E}_{\\rho_\\pi}[R_{t+1}]\n\nIt becomes clear that the value of an action depends on the immediate reward received just after the action, as well as the value of the next state:\n\n        Q^{\\pi}(s_t, a_t) = r(s_t, a_t, s_{t+1}) + \\gamma \\,  V^{\\pi} (s_{t+1})\n\nBut that is only for a fixed (s_t, a_t, s_{t+1}) transition. Taking transition probabilities into account, one can obtain the Q-values when the V-values are known:\n\n        Q^{\\pi}(s, a) = \\mathbb{E}_{s' \\sim p(s'|s, a)} [ r(s, a, s') + \\gamma \\, V^{\\pi} (s') ] = \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{\\pi} (s') ]\n\nThe value of an action depends on:\n\nthe states s' one can arrive after the action (with a probability p(s' | s, a)).\nthe value of that state V^{\\pi} (s'), weighted by \\gamma as it is one step in the future.\nthe reward received immediately after taking that action r(s, a, s') (as it is not included in the value of s').\n\n\n\nBellman equations\nA fundamental property of value functions used throughout reinforcement learning is that they satisfy a particular recursive relationship:\n\n\\begin{aligned}\n        V^{\\pi}(s)  &= \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, Q^{\\pi} (s, a)\\\\\n                    &= \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{\\pi} (s') ]\n\\end{aligned}\n\nThis equation is called the Bellman equation for V^{\\pi}. It expresses the relationship between the value of a state and the value of its successors, depending on the dynamics of the MDP (p(s' | s, a) and r(s, a, s')) and the current policy \\pi. The interesting property of the Bellman equation for RL is that it admits one and only one solution V^{\\pi}(s).\nThe same recursive relationship stands for Q^{\\pi}(s, a):\n\n\\begin{aligned}\n        Q^{\\pi}(s, a)  &= \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{\\pi} (s') ] \\\\\n                    &=  \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, \\sum_{a' \\in \\mathcal{A}(s')} \\pi(s', a') \\, Q^{\\pi} (s', a')]\n\\end{aligned}\n\nwhich is called the Bellman equation for Q^{\\pi}.\nThe following backup diagrams denote these recursive relationships.\n\n\n\nBackup diagrams of the Bellman equations. Left: the value of a state s depends on the policy \\pi and the value of the succeeding states s'. Right: the value of an action depends on the value of the action that can be taken in the next states. Source: (Sutton and Barto, 1998)"
  },
  {
    "objectID": "notes/2.2-MDP.html#bellman-optimality-equations",
    "href": "notes/2.2-MDP.html#bellman-optimality-equations",
    "title": "Markov Decision Processes",
    "section": "Bellman optimality equations",
    "text": "Bellman optimality equations\n\n\nOptimal policy\nThe optimal policy is the policy that gathers the maximum of reward on the long term. Value functions define a partial ordering over policies:\n\na policy \\pi is better than another policy \\pi' if its expected return is greater or equal than that of \\pi' for all states s.\n\n\n        \\pi \\geq \\pi' \\Leftrightarrow V^{\\pi}(s) \\geq V^{\\pi'}(s) \\quad \\forall s \\in \\mathcal{S}\n\nThere exists at least one policy that is better than all the others: this is the optimal policy \\pi^*. We note V^*(s) and Q^*(s, a) the optimal value of the different states and actions under \\pi^*.\n\n   V^* (s) = \\max_{\\pi} V^{\\pi}(s) \\quad \\forall s \\in \\mathcal{S}\n\n\n    Q^* (s, a) = \\max_{\\pi} Q^{\\pi}(s, a) \\quad \\forall s \\in \\mathcal{S}, \\quad \\forall a \\in \\mathcal{A}\n\nWhen the policy is optimal \\pi^*, the link between the V and Q values is even easier. The V and Q values are maximal for the optimal policy: there is no better alternative.\n\n\n\nBackup diagrams of the Bellman optimality equations. Source: (Sutton and Barto, 1998)\n\n\nThe optimal action a^* to perform in the state s is the one with the highest optimal Q-value Q^*(s, a).\n\n    a^* = \\text{argmax}_a \\, Q^*(s, a)\n\nBy definition, this action will bring the maximal return when starting in s.\n\n    Q^*(s, a) = \\mathbb{E}_{\\rho_{\\pi^*}} [R_t]\n\nThe optimal policy is greedy with respect to Q^*(s, a), i.e. deterministic.\n\n    \\pi^*(s, a) = \\begin{cases}\n                1 \\; \\text{if} \\; a = a^* \\\\\n                0 \\; \\text{otherwise.}\n                \\end{cases}\n\n\n\nBellman optimality equations\nAs the optimal policy is deterministic, the optimal value of a state is equal to the value of the optimal action:\n\n    V^* (s)  = \\max_{a \\in \\mathcal{A}(s)} Q^{\\pi^*} (s, a)\n\nThe expected return after being in s is the same as the expected return after being in s and choosing the optimal action a^*, as this is the only action that can be taken. This allows to find the Bellman optimality equation for V^*:\n\n    V^* (s)  = \\max_{a \\in \\mathcal{A}(s)} \\sum_{s' \\in \\mathcal{S}}  p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{*} (s') ]\n\nThe same Bellman optimality equation stands for Q^*:\n\n    Q^* (s, a)  = \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s')  + \\gamma \\max_{a' \\in \\mathcal{A}(s')} Q^* (s', a') ]\n\nThe optimal value of (s, a) depends on the optimal action in the next state s'.\nThe Bellman optimality equations for V^* form a system of equations:\n\nIf there are N states s, there are N Bellman equations with N unknowns V^*(s).\n\n\n    V^* (s)  = \\max_{a \\in \\mathcal{A}(s)} \\sum_{s' \\in \\mathcal{S}}  p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{*} (s') ]\n\nIf the dynamics of the environment are known (p(s' | s, a) and r(s, a, s')), then in principle one can solve this system of equations using linear algebra. For finite MDPs, the Bellman optimality equation for V^* has a unique solution (one and only one): This is the principle of dynamic programming.\nThe same is true for the Bellman optimality equation for Q^*: If there are N states and M actions available, there are N\\times M equations with N\\times M unknowns Q^*(s, a).\n\n    Q^* (s, a)  = \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s')  + \\gamma \\max_{a' \\in \\mathcal{A}(s')} Q^* (s', a') ]\n\nV^* and Q^* are interdependent: one needs only to compute one of them.\nV^* (s)  = \\max_{a \\in \\mathcal{A}(s)} \\, Q^{*} (s, a)\nQ^* (s, a)  = \\sum_{s' \\in \\mathcal{S}} \\, p(s' | s, a) \\, [r(s, a, s') + \\gamma V^*(s') ] \nIf you only have V^*(s), you need to perform a one-step-ahead search using the dynamics of the MDP:\n\n    Q^* (s, a)  = \\sum_{s' \\in \\mathcal{S}} \\, p(s' | s, a) \\, [r(s, a, s') + \\gamma V^*(s') ]\n\nand then select the optimal action with the highest Q^*-value. Using the V^*(s) values is called model-based: you need to know the model of the environment to act, at least locally.\nIf you have all Q^*(s, a), the optimal policy is straightforward:\n\n    \\pi^*(s, a) = \\begin{cases}\n                1 \\; \\text{if} \\; a = \\text{argmax}_a \\, Q^*(s, a) \\\\\n                0 \\; \\text{otherwise.}\n                \\end{cases}\n\nFinding Q^* makes the selection of optimal actions easy:\n\nno need to iterate over all actions and to know the dynamics p(s' | s, a) and r(s, a, s').\nfor any state s, it can simply find the action that maximizes Q^*(s,a).\n\nThe action-value function effectively caches the results of all one-step-ahead searches into a single value: model-free. At the cost of representing a function of all state-action pairs, the optimal action-value function allows optimal actions to be selected without having to know anything about the environment’s dynamics. But there are N \\times M equations to solve instead of just N…\nFinding an optimal policy by solving the Bellman optimality equations requires the following:\n\naccurate knowledge of environment dynamics p(s' | s, a) and r(s, a, s') for all transitions;\nenough memory and time to do the computations;\nthe Markov property.\n\nHow much space and time do we need? A solution requires an exhaustive search, looking ahead at all possibilities, computing their probabilities of occurrence and their desirability in terms of expected rewards. The number of states is often huge or astronomical (e.g., Go has about 10^{170} states). Dynamic programming solves exactly the Bellman equations; Monte-Carlo and temporal-difference methods approximate them."
  },
  {
    "objectID": "notes/2.2-MDP.html#generalized-policy-iteration",
    "href": "notes/2.2-MDP.html#generalized-policy-iteration",
    "title": "Markov Decision Processes",
    "section": "Generalized Policy Iteration",
    "text": "Generalized Policy Iteration\n\n\n\nGeneralized Policy Iteration. Source: (Sutton and Barto, 1998)\n\n\nRL algorithms iterate over two steps:\n\nPolicy evaluation\n\nFor a given policy \\pi, the value of all states V^\\pi(s) or all state-action pairs Q^\\pi(s, a) is calculated, either based on:\n\nthe Bellman equations (Dynamic Programming)\nsampled experience (Monte-Carlo and Temporal Difference)\n\n\nPolicy improvement\n\nFrom the current estimated values V^\\pi(s) or Q^\\pi(s, a), a new better policy \\pi is derived.\n\n\nAfter enough iterations, the policy converges to the optimal policy (if the states are Markov).\n\n\n\n\n\n\nDifferent notations in RL\n\n\n\nNotations can vary depending on the source. The ones used in this course use what you can read in most modern deep RL papers (Deepmind, OpenAI), but beware that you can encounter G_t for the return…\n\n\n\n\n\n\n\n\n\n\nThis course\nSutton and Barto 1998\nSutton and Barto 2017\n\n\n\n\nCurrent state\ns_t\ns_t\nS_t\n\n\nSelected action\na_t\na_t\nA_t\n\n\nSampled reward\nr_{t+1}\nr_{t+1}\nR_{t+1}\n\n\nTransition probability\np(s' / s,a)\n\\mathcal{P}_{ss'}^a\np(s'/s, a)\n\n\nExpected reward\nr(s,a, s')\n\\mathcal{R}_{ss'}^a\nr(s, a, s')\n\n\nReturn\nR_t\nR_t\nG_t\n\n\nState value function\nV^\\pi(s)\nV^\\pi(s)\nv_\\pi(s)\n\n\nAction value function\nQ^\\pi(s, a)\nQ^\\pi(s, a)\nq_\\pi(s, a)\n\n\n\n\n\n\n\n\n\nSutton, R. S., and Barto, A. G. (1998). Reinforcement Learning: An introduction. Cambridge, MA: MIT press."
  },
  {
    "objectID": "notes/2.3-DP.html",
    "href": "notes/2.3-DP.html",
    "title": "Dynamic Programming",
    "section": "",
    "text": "Slides: html pdf"
  },
  {
    "objectID": "notes/2.3-DP.html#policy-iteration",
    "href": "notes/2.3-DP.html#policy-iteration",
    "title": "Dynamic Programming",
    "section": "Policy iteration",
    "text": "Policy iteration\n\n\nPolicy evaluation\nRemember the Bellman equation for the state s and a fixed policy \\pi:\n\n      V^{\\pi} (s)  = \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{\\pi} (s') ]\n\nLet’s note \\mathcal{P}_{ss'}^\\pi the transition probability between s and s' (dependent on the policy \\pi) and \\mathcal{R}_{s}^\\pi the expected reward in s (also dependent):\n\n  \\mathcal{P}_{ss'}^\\pi = \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, p(s' | s, a)\n\n\n  \\mathcal{R}_{s}^\\pi = \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} \\, p(s' | s, a) \\ r(s, a, s')\n\nThe Bellman equation becomes:\n\n      V^{\\pi} (s)  = \\mathcal{R}_{s}^\\pi + \\gamma \\, \\sum_{s' \\in \\mathcal{S}} \\, \\mathcal{P}_{ss'}^\\pi \\, V^{\\pi} (s')\n\nAs we have a fixed policy during the evaluation (Markov Reward Process), the Bellman equation is simplified.\nLet’s now put the Bellman equations in a matrix-vector form.\n\n      V^{\\pi} (s)  = \\mathcal{R}_{s}^\\pi + \\gamma \\, \\sum_{s' \\in \\mathcal{S}} \\, \\mathcal{P}_{ss'}^\\pi \\, V^{\\pi} (s')\n\nWe first define the vector of state values \\mathbf{V}^\\pi:\n\n  \\mathbf{V}^\\pi = \\begin{bmatrix}\n      V^\\pi(s_1) \\\\ V^\\pi(s_2) \\\\ \\vdots \\\\ V^\\pi(s_n) \\\\\n  \\end{bmatrix}\n\nand the vector of expected reward \\mathbf{R}^\\pi:\n\n  \\mathbf{R}^\\pi = \\begin{bmatrix}\n      \\mathcal{R}^\\pi(s_1) \\\\ \\mathcal{R}^\\pi(s_2) \\\\ \\vdots \\\\ \\mathcal{R}^\\pi(s_n) \\\\\n  \\end{bmatrix}\n\nThe state transition matrix \\mathcal{P}^\\pi is defined as:\n\n  \\mathcal{P}^\\pi = \\begin{bmatrix}\n      \\mathcal{P}_{s_1 s_1}^\\pi & \\mathcal{P}_{s_1 s_2}^\\pi & \\ldots & \\mathcal{P}_{s_1 s_n}^\\pi \\\\\n      \\mathcal{P}_{s_2 s_1}^\\pi & \\mathcal{P}_{s_2 s_2}^\\pi & \\ldots & \\mathcal{P}_{s_2 s_n}^\\pi \\\\\n      \\vdots & \\vdots & \\vdots & \\vdots \\\\\n      \\mathcal{P}_{s_n s_1}^\\pi & \\mathcal{P}_{s_n s_2}^\\pi & \\ldots & \\mathcal{P}_{s_n s_n}^\\pi \\\\\n  \\end{bmatrix}\n\nYou can simply check that:\n\n  \\begin{bmatrix}\n      V^\\pi(s_1) \\\\ V^\\pi(s_2) \\\\ \\vdots \\\\ V^\\pi(s_n) \\\\\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n      \\mathcal{R}^\\pi(s_1) \\\\ \\mathcal{R}^\\pi(s_2) \\\\ \\vdots \\\\ \\mathcal{R}^\\pi(s_n) \\\\\n  \\end{bmatrix}\n  + \\gamma \\, \\begin{bmatrix}\n      \\mathcal{P}_{s_1 s_1}^\\pi & \\mathcal{P}_{s_1 s_2}^\\pi & \\ldots & \\mathcal{P}_{s_1 s_n}^\\pi \\\\\n      \\mathcal{P}_{s_2 s_1}^\\pi & \\mathcal{P}_{s_2 s_2}^\\pi & \\ldots & \\mathcal{P}_{s_2 s_n}^\\pi \\\\\n      \\vdots & \\vdots & \\vdots & \\vdots \\\\\n      \\mathcal{P}_{s_n s_1}^\\pi & \\mathcal{P}_{s_n s_2}^\\pi & \\ldots & \\mathcal{P}_{s_n s_n}^\\pi \\\\\n  \\end{bmatrix} \\times \\begin{bmatrix}\n      V^\\pi(s_1) \\\\ V^\\pi(s_2) \\\\ \\vdots \\\\ V^\\pi(s_n) \\\\\n  \\end{bmatrix}\n\nleads to the same equations as:\n\n      V^{\\pi} (s)  = \\mathbf{R}_{s}^\\pi + \\gamma \\, \\sum_{s' \\in \\mathcal{S}} \\, \\mathcal{P}_{ss'}^\\pi \\, V^{\\pi} (s')\n\nfor all states s.\nThe Bellman equations for all states s can therefore be written with a matrix-vector notation as:\n\n  \\mathbf{V}^\\pi = \\mathbf{R}^\\pi + \\gamma \\, \\mathcal{P}^\\pi \\, \\mathbf{V}^\\pi\n\nor:\n\n  (\\mathbb{I} - \\gamma \\, \\mathcal{P}^\\pi ) \\times \\mathbf{V}^\\pi = \\mathbf{R}^\\pi\n\nwhere \\mathbb{I} is the identity matrix. If we know \\mathcal{P}^\\pi and \\mathbf{R}^\\pi (dynamics of the MDP for the policy \\pi), we can simply obtain the state values with a matrix inversion:\n\n  \\mathbf{V}^\\pi = (\\mathbb{I} - \\gamma \\, \\mathcal{P}^\\pi )^{-1} \\times \\mathbf{R}^\\pi\n\nDone! But, if we have n states, the matrix \\mathcal{P}^\\pi has n^2 elements. Inverting \\mathbb{I} - \\gamma \\, \\mathcal{P}^\\pi requires at least \\mathcal{O}(n^{2.37}) operations. Forget it if you have more than a thousand states (1000^{2.37} \\approx 13 million operations). In dynamic programming, we will therefore use iterative methods to estimate \\mathbf{V}^\\pi with (hopefully) less operations.\n\n\nIterative policy evaluation\n\nThe idea of iterative policy evaluation (IPE) is to consider a sequence of consecutive state-value functions which should converge from initially wrong estimates V_0(s) towards the real state-value function V^{\\pi}(s).\n\n      V_0 \\rightarrow V_1 \\rightarrow V_2 \\rightarrow \\ldots \\rightarrow V_k \\rightarrow V_{k+1} \\rightarrow \\ldots \\rightarrow V^\\pi\n\n\n\n\nBackup diagram of iterative policy evaluation. Credit: David Silver.\n\n\nThe value function at step k+1 V_{k+1}(s) is computed using the previous estimates V_{k}(s) and the Bellman equation transformed into an update rule. In vector notation:\n\n  \\mathbf{V}_{k+1} = \\mathbf{R}^\\pi + \\gamma \\, \\mathcal{P}^\\pi \\, \\mathbf{V}_k\n\nLet’s start with dummy (e.g. random) initial estimates V_0(s) for the value of every state s. We can obtain new estimates V_1(s) which are slightly less wrong by applying once the Bellman operator:\n\n     V_{1} (s) \\leftarrow \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V_0 (s') ] \\quad \\forall s \\in \\mathcal{S}\n\nBased on these estimates V_1(s), we can obtain even better estimates V_2(s) by applying again the Bellman operator:\n\n     V_{2} (s) \\leftarrow \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V_1 (s') ] \\quad \\forall s \\in \\mathcal{S}\n\nGenerally, state-value function estimates are improved iteratively through:\n\n     V_{k+1} (s) \\leftarrow \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V_k (s') ] \\quad \\forall s \\in \\mathcal{S}\n\nV_\\infty = V^{\\pi} is a fixed point of this update rule because of the uniqueness of the solution to the Bellman equation.\nThe Bellman operator \\mathcal{T}^\\pi is a mapping between two vector spaces:\n\n  \\mathcal{T}^\\pi (\\mathbf{V}) = \\mathbf{R}^\\pi + \\gamma \\, \\mathcal{P}^\\pi \\, \\mathbf{V}\n\nIf you apply repeatedly the Bellman operator on any initial vector \\mathbf{V}_0, it converges towards the solution of the Bellman equations \\mathbf{V}^\\pi. Mathematically speaking, \\mathcal{T}^\\pi is a \\gamma-contraction, i.e. it makes value functions closer by at least \\gamma:\n\n  || \\mathcal{T}^\\pi (\\mathbf{V}) - \\mathcal{T}^\\pi (\\mathbf{U})||_\\infty \\leq \\gamma \\, ||\\mathbf{V} - \\mathbf{U} ||_\\infty\n\nThe contraction mapping theorem ensures that \\mathcal{T}^\\pi converges to an unique fixed point: this proves the existence and uniqueness of the solution of the Bellman equations.\nIterative Policy Evaluation relies on full backups: it backs up the value of ALL possible successive states into the new value of a state.\n\n     V_{k+1} (s) \\leftarrow \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V_k (s') ] \\quad \\forall s \\in \\mathcal{S}\n\nThe backups are synchronous: all states are backed up in parallel.\n\n  \\mathbf{V}_{k+1} = \\mathbf{R}^\\pi + \\gamma \\, \\mathcal{P}^\\pi \\, \\mathbf{V}_k\n\nThe termination of iterative policy evaluation has to be controlled by hand, as the convergence of the algorithm is only at the limit. It is good practice to look at the variations on the values of the different states, and stop the iteration when this variation falls below a predefined threshold.\n\n\n\n\n\n\nIterative Policy Evaluation\n\n\n\n\nFor a fixed policy \\pi, initialize V(s)=0 \\; \\forall s \\in \\mathcal{S}.\nwhile not converged:\n\nfor all states s:\n\nV_\\text{target}(s) = \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V (s') ]\n\n\\delta =0\nfor all states s:\n\n\\delta = \\max(\\delta, |V(s) - V_\\text{target}(s)|)\nV(s) = V_\\text{target}(s)\n\nif \\delta < \\delta_\\text{threshold}:\n\nconverged = True\n\n\n\n\n\n\n\nPolicy improvement\nThe goal of finding the value function for a given policy \\pi is to help improving this policy. For each state s, we would like to know if we should deterministically choose an action a \\neq \\pi(s) or not in order to improve the policy.\nThe value of an action a in the state s for the policy \\pi is given by:\n\n     Q^{\\pi} (s, a) = \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s') + \\gamma \\, V^{\\pi}(s') ]\n\nf the Q-value of an action a is higher than the one currently selected by the deterministic policy:\nQ^{\\pi} (s, a) > Q^{\\pi} (s, \\pi(s)) = V^{\\pi}(s)\nthen it is better to select a once in s and thereafter follow \\pi. If there is no better action, we keep the previous policy for this state. This corresponds to a greedy action selection over the Q-values, defining a deterministic policy \\pi(s):\n\\pi(s) \\leftarrow \\text{argmax}_a Q^{\\pi} (s, a) = \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s') + \\gamma \\, V^{\\pi}(s') ]\nAfter the policy improvement, the Q-value of each deterministic action \\pi(s) has increased or stayed the same.\n\\text{argmax}_a Q^{\\pi} (s, a) = \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s') + \\gamma \\, V^{\\pi}(s') ] \\geq Q^\\pi(s, \\pi(s))\nThis defines an improved policy \\pi', where all states and actions have a higher value than previously. Greedy action selection over the state value function implements policy improvement:\n\\pi' \\leftarrow \\text{Greedy}(V^\\pi)\n\n\n\n\n\n\nGreedy policy improvement\n\n\n\n\nfor each state s \\in \\mathcal{S}:\n\n\\pi(s) \\leftarrow \\text{argmax}_a \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s') + \\gamma \\, V^{\\pi}(s') ]\n\n\n\n\n\n\nPolicy iteration\nOnce a policy \\pi has been improved using V^{\\pi} to yield a better policy \\pi', we can then compute V^{\\pi'} and improve it again to yield an even better policy \\pi''.\nThe algorithm policy iteration successively uses policy evaluation and policy improvement to find the optimal policy.\n\n  \\pi_0 \\xrightarrow[]{E} V^{\\pi_0} \\xrightarrow[]{I} \\pi_1 \\xrightarrow[]{E} V^{\\pi^1} \\xrightarrow[]{I}  ... \\xrightarrow[]{I} \\pi^* \\xrightarrow[]{E} V^{*}\n\n\n\n\nPolicy Iteration. Source: (Sutton and Barto, 1998)\n\n\nThe optimal policy being deterministic, policy improvement can be greedy over the state values. If the policy does not change after policy improvement, the optimal policy has been found.\n\n\n\n\n\n\nPolicy iteration\n\n\n\n\nInitialize a deterministic policy \\pi(s) and set V(s)=0 \\; \\forall s \\in \\mathcal{S}.\nwhile \\pi is not optimal:\n\nwhile not converged: # Policy evaluation\n\nfor all states s:\n\nV_\\text{target}(s) = \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V (s') ]\n\nfor all states s:\n\nV(s) = V_\\text{target}(s)\n\n\nfor each state s \\in \\mathcal{S}: # Policy improvement\n\n\\pi(s) \\leftarrow \\text{argmax}_a \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s') + \\gamma \\, V^{\\pi}(s') ]\n\nif \\pi has not changed: break"
  },
  {
    "objectID": "notes/2.3-DP.html#value-iteration",
    "href": "notes/2.3-DP.html#value-iteration",
    "title": "Dynamic Programming",
    "section": "Value iteration",
    "text": "Value iteration\n\nPolicy iteration can converge in a surprisingly small number of iterations. One drawback of policy iteration is that it uses a full policy evaluation, which can be computationally exhaustive as the convergence of V_k is only at the limit and the number of states can be huge.\nThe idea of value iteration is to interleave policy evaluation and policy improvement, so that the policy is improved after each iteration of policy evaluation, not after complete convergence. As policy improvement returns a deterministic greedy policy, updating of the value of a state is then simpler:\n\n        V_{k+1}(s) = \\max_a \\sum_{s'} p(s' | s,a) [r(s, a, s') + \\gamma \\, V_k(s') ]\n\nNote that this is equivalent to turning the Bellman optimality equation into an update rule. Value iteration converges to V^*, faster than policy iteration, and should be stopped when the values do not change much anymore.\n\n\n\n\n\n\nValue iteration\n\n\n\n\nInitialize a deterministic policy \\pi(s) and set V(s)=0 \\; \\forall s \\in \\mathcal{S}.\nwhile not converged:\n\nfor all states s:\n\nV_\\text{target}(s) = \\max_a \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V (s') ]\n\n\\delta = 0\nfor all states s:\n\n\\delta = \\max(\\delta, |V(s) - V_\\text{target}(s)|)\nV(s) = V_\\text{target}(s)\n\nif \\delta < \\delta_\\text{threshold}:\n\nconverged = True"
  },
  {
    "objectID": "notes/2.3-DP.html#comparison-of-policy--and-value-iteration",
    "href": "notes/2.3-DP.html#comparison-of-policy--and-value-iteration",
    "title": "Dynamic Programming",
    "section": "Comparison of Policy- and Value-iteration",
    "text": "Comparison of Policy- and Value-iteration\nFull policy-evaluation backup\n\n    V_{k+1} (s) \\leftarrow \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V_k (s') ]\n\n\n\n\nBackup diagram of policy evaluation.\n\n\nFull value-iteration backup\n\n    V_{k+1} (s) \\leftarrow \\max_{a \\in \\mathcal{A}(s)} \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V_k (s') ]\n\n\n\n\nBackup diagram of value evaluation."
  },
  {
    "objectID": "notes/2.3-DP.html#asynchronous-dynamic-programming",
    "href": "notes/2.3-DP.html#asynchronous-dynamic-programming",
    "title": "Dynamic Programming",
    "section": "Asynchronous dynamic programming",
    "text": "Asynchronous dynamic programming\nSynchronous DP requires exhaustive sweeps of the entire state set (synchronous backups).\n\nwhile not converged:\n\nfor all states s:\n\nV_\\text{target}(s) = \\max_a \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V (s') ]\n\nfor all states s:\n\nV(s) = V_\\text{target}(s)\n\n\n\nAsynchronous DP updates instead each state independently and asynchronously (in-place):\n\nwhile not converged:\n\nPick a state s randomly (or following a heuristics).\nUpdate the value of this state.\n\n\n    V(s) =  \\max_a \\,  \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V (s') ]\n  \n\nWe must still ensure that all states are visited, but their frequency and order is irrelevant."
  },
  {
    "objectID": "notes/2.3-DP.html#curse-of-dimensionality",
    "href": "notes/2.3-DP.html#curse-of-dimensionality",
    "title": "Dynamic Programming",
    "section": "Curse of dimensionality",
    "text": "Curse of dimensionality\nPolicy-iteration and value-iteration consist of alternations between policy evaluation and policy improvement, although at different frequencies. This principle is called Generalized Policy Iteration (GPI). Finding an optimal policy is polynomial in the number of states and actions: \\mathcal{O}(n^2 \\, m) (n is the number of states, m the number of actions). However, the number of states is often astronomical, e.g., often growing exponentially with the number of state variables (what Bellman called “the curse of dimensionality”) In practice, classical DP can only be applied to problems with a few millions of states.\n\n\n\nCurse of dimensionality. Source: https://medium.com/diogo-menezes-borges/give-me-the-antidote-for-the-curse-of-dimensionality-b14bce4bf4d2\n\n\nIf one variable can be represented by 5 discrete values:\n\n2 variables necessitate 25 states,\n3 variables need 125 states, and so on…\n\nThe number of states explodes exponentially with the number of dimensions of the problem…\n\n\n\n\nSutton, R. S., and Barto, A. G. (1998). Reinforcement Learning: An introduction. Cambridge, MA: MIT press."
  },
  {
    "objectID": "notes/2.4-MC.html",
    "href": "notes/2.4-MC.html",
    "title": "Monte-Carlo (MC) methods",
    "section": "",
    "text": "Slides: html pdf"
  },
  {
    "objectID": "notes/2.4-MC.html#monte-carlo-policy-evaluation",
    "href": "notes/2.4-MC.html#monte-carlo-policy-evaluation",
    "title": "Monte-Carlo (MC) methods",
    "section": "Monte-Carlo policy evaluation",
    "text": "Monte-Carlo policy evaluation\n\nThe value of a state s is defined as the mathematical expectation of the return obtained after that state and thereafter following the policy \\pi:\nV^{\\pi} (s) = \\mathbb{E}_{\\rho_\\pi} ( R_t | s_t = s) = \\mathbb{E}_{\\rho_\\pi} ( \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} | s_t = s )\nMonte-Carlo methods (MC) approximate this mathematical expectation by sampling M trajectories \\tau_i starting from s and computing the sampling average of the obtained returns:\n\n    V^{\\pi}(s) = \\mathbb{E}_{\\rho_\\pi} (R_t | s_t = s) \\approx \\frac{1}{M} \\sum_{i=1}^M R(\\tau_i)\n\nIf you have enough trajectories, the sampling average is an unbiased estimator of the value function. The advantage of Monte-Carlo methods is that they require only experience, not the complete dynamics p(s' | s,a) and r(s, a, s').\nThe idea of MC policy evaluation is to repeatedly sample episodes starting from each possible state s_0 and maintain a running average of the obtained returns for each state:\n\n\n\n\n\n\nMonte-Carlo policy evaluation of state values\n\n\n\n\nwhile True:\n\nStart from an initial state s_0.\nGenerate a sequence of transitions according to the current policy \\pi until a terminal state s_T is reached.\n\n\n      \\tau = (s_o, a_o, r_ 1, s_1, a_1, \\ldots, s_T)\n  \n\nCompute the return R_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} for all encountered states s_0, s_1, \\ldots, s_T.\nUpdate the estimated state value V(s_t) of all encountered states using the obtained return:\n\n\n      V(s_t) \\leftarrow V(s_t) + \\alpha \\, (R_t - V(s_t))\n  \n\n\n\nThe same method can be used to estimate Q-values.\n\n\n\n\n\n\nOnline Monte-Carlo policy evaluation of action values\n\n\n\n\nwhile True:\n\nStart from an initial state s_0.\nGenerate a sequence of transitions according to the current policy \\pi until a terminal state s_T is reached.\n\n\n      \\tau = (s_o, a_o, r_ 1, s_1, a_1, \\ldots, s_T)\n  \n\nCompute the return R_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} for all encountered state-action pairs (s_0, a_0), (s_1, a_1), \\ldots, (s_{T-1}, a_{T-1}).\nUpdate the estimated action value Q(s_t, a_t) of all encountered state-action pairs using the obtained return:\n\n\n      Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\, (R_t - Q(s_t, a_t))\n  \n\n\n\nThere are much more values to estimate (one per state-action pair), but the policy will be easier to derive."
  },
  {
    "objectID": "notes/2.4-MC.html#monte-carlo-policy-improvement",
    "href": "notes/2.4-MC.html#monte-carlo-policy-improvement",
    "title": "Monte-Carlo (MC) methods",
    "section": "Monte-Carlo policy improvement",
    "text": "Monte-Carlo policy improvement\nAfter each episode, the state or action values of the visited (s, a) pairs have changed, so the current policy might not be optimal anymore. As in DP, the policy can then be improved in a greedy manner:\n\\begin{aligned}\n        \\pi'(s) & = \\text{argmax}_a Q(s, a)\\\\\n        &\\\\\n        & = \\text{argmax}_a \\sum_{s' \\in \\mathcal{S}} p(s'|s, a) \\, [r(s, a, s') + \\gamma \\, V(s')] \\\\\n\\end{aligned}\n\nEstimating the Q-values allows to act greedily, while estimating the V-values still requires the dynamics p(s' | s,a) and r(s, a, s')."
  },
  {
    "objectID": "notes/2.4-MC.html#on-policy-monte-carlo-control",
    "href": "notes/2.4-MC.html#on-policy-monte-carlo-control",
    "title": "Monte-Carlo (MC) methods",
    "section": "On-policy Monte-Carlo control",
    "text": "On-policy Monte-Carlo control\nMonte-Carlo control alternates between MC policy evaluation and policy improvement until the optimal policy is found: generalized policy iteration (GPI).\n\n\n\n\n\n\nPrinciple of Monte-Carlo control\n\n\n\n\nwhile True:\n\nSelect an initial state s_0.\nGenerate a sequence of transitions according to the current policy \\pi until a terminal state s_T is reached.\n\n\n      \\tau = (s_o, a_o, r_ 1, s_1, a_1, \\ldots, s_T)\n  \n\nCompute the return R_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} of all encountered state-action pairs.\nUpdate the estimated action value Q_k(s_t, a_t) of all encountered state-action pairs:\n\n\n      Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\, (R_t - Q(s_t, a_t))\n  \n\nFor each state s_t in the episode, improve the policy greedily:\n\n\n      \\pi(s_t, a) = \\begin{cases}\n                      1\\; \\text{if} \\; a = \\text{argmax} \\, Q(s_t, a) \\\\\n                      0 \\; \\text{otherwise.} \\\\\n                    \\end{cases}\n  \n\n\n\nThe problem with MC control is that we need a policy to generate the sample episodes, but it is that policy that we want to learn. We have the same exploration/exploitation problem as in bandits:\n\nIf I trust my estimates too much (exploitation), I may miss more interesting solutions by keeping generating the same episodes.\nIf I act randomly (exploration), I will find more interesting solutions, but I won’t keep doing them.\n\nExploitation is using the current estimated values to select the greedy action: The estimated values represent how good we think an action is, so we have to use this value to update the policy.\nExploration is executing non-greedy actions to try to reduce our uncertainty about the true values: The values are only estimates: they may be wrong so we can not trust them completely.\nIf you only exploit your estimates, you may miss interesting solutions. If you only explore, you do not use what you know: you act randomly and do not obtain as much reward as you could. You can’t exploit all the time; you can’t explore all the time. You can never stop exploring; but you can reduce it if your performance is good enough.\nAn easy solution to ensure exploration is to assume exploring starts, where every state-action pair has a non-zero probability to be selected as the start of an episode.\nExploration can be ensured by forcing the learned policy to be stochastic, aka \\epsilon-soft.\n\n\\epsilon-Greedy action selection randomly selects non-greedy actions with a small probability \\epsilon:\n\n\n    \\pi(s, a) = \\begin{cases} 1 - \\epsilon \\; \\text{if} \\; a = \\text{argmax}\\, Q(s, a) \\\\ \\frac{\\epsilon}{|\\mathcal{A}| - 1} \\; \\text{otherwise.} \\end{cases}\n\n\nSoftmax action selection uses a Gibbs (or Boltzmann) distribution to represent the probability of choosing the action a in state s:\n\n\n    \\pi(s, a) = \\frac{\\exp Q(s, a) / \\tau}{ \\sum_b \\exp Q(s, b) / \\tau}\n\n\\epsilon-greedy choses non-greedy actions randomly, while softmax favors the best alternatives.\nIn on-policy control methods, the learned policy has to be \\epsilon-soft, which means all actions have a probability of at least \\frac{\\epsilon}{|\\mathcal{A}|} to be visited. \\epsilon-greedy and softmax policies meet this criteria. Each sample episode is generated using this policy, which ensures exploration, while the control method still converges towards the optimal \\epsilon-policy.\n\n\n\n\n\n\nOn-policy Monte-Carlo control\n\n\n\n\nwhile True:\n\nGenerate an episode \\tau = (s_0, a_0, r_1, \\ldots, s_T) using the current stochastic policy \\pi.\nFor each state-action pair (s_t, a_t) in the episode, update the estimated Q-value:\n\n\n      Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\, (R_t - Q(s_t, a_t))\n  \n\nFor each state s_t in the episode, improve the policy (e.g. \\epsilon-greedy):\n\n\n      \\pi(s_t, a) = \\begin{cases}\n                      1 - \\epsilon \\; \\text{if} \\; a = \\text{argmax}\\, Q(s, a) \\\\\n                      \\frac{\\epsilon}{|\\mathcal{A(s_t)}-1|} \\; \\text{otherwise.} \\\\\n                    \\end{cases}\n  \n\n\n\nAnother option to ensure exploration is to generate the sample episodes using a policy b(s, a) different from the current policy \\pi(s, a) of the agent. The behavior policy b(s, a) used to generate the episodes is only required to select at least occasionally the same actions as the learned policy \\pi(s, a) (coverage assumption):\n \\pi(s,a) > 0 \\Rightarrow b(s,a) > 0\nThere are mostly two choices regarding the behavior policy:\n\nAn \\epsilon-soft behavior policy over the Q-values as in on-policy MC is often enough, while a deterministic (greedy) policy can be learned implicitly.\nThe behavior policy could also come from expert knowledge, i.e. known episodes from the MDP generated by somebody else (human demonstrator, classical algorithm).\n\nBut are we mathematically allowed to do this?"
  },
  {
    "objectID": "notes/2.4-MC.html#importance-sampling",
    "href": "notes/2.4-MC.html#importance-sampling",
    "title": "Monte-Carlo (MC) methods",
    "section": "Importance sampling",
    "text": "Importance sampling\n\nWe search for the optimal policy that maximizes in expectation the return of each trajectory (episode) possible under the learned policy \\pi:\n\\mathcal{J}(\\pi) = \\mathbb{E}_{\\tau \\sim \\rho_\\pi} [R(\\tau)]\n\\rho_\\pi denotes the probability distribution of trajectories achievable using the policy \\pi. If we generate the trajectories from the behavior policy b(s, a), we end up maximizing something else:\n\\mathcal{J}'(\\pi) = \\mathbb{E}_{\\tau \\sim \\rho_b} [R(\\tau)]\nThe policy that maximizes \\mathcal{J}'(\\pi) is not the optimal policy of the MDP.\n\n\n\nYou cannot use samples from one distribution to estimate something about another.\n\n\nIf you try to estimate a parameter of a random distribution \\pi using samples of another distribution b, the sample average will have a strong bias. We need to correct the samples from b in order to be able to estimate the parameters of \\pi correctly: this is called importance sampling (IS).\nWe want to estimate the expected return of the trajectories generated by the policy \\pi:\n\\mathcal{J}(\\pi) = \\mathbb{E}_{\\tau \\sim \\rho_\\pi} [R(\\tau)]\nWe start by using the definition of the mathematical expectation:\n\\mathcal{J}(\\pi) = \\int_\\tau \\rho_\\pi(\\tau) \\, R(\\tau) \\, d\\tau\nThe expectation is the integral over all possible trajectories of their return R(\\tau), weighted by the likelihood \\rho_\\pi(\\tau) that a trajectory \\tau is generated by the policy \\pi.\n\n\n\nRL maximizes the expected return of the trajectories generated by the policy.\n\n\nThe trick is to introduce the behavior policy b in what we want to estimate:\n\\mathcal{J}(\\pi) = \\int_\\tau \\frac{\\rho_b(\\tau)}{\\rho_b(\\tau)} \\, \\rho_\\pi(\\tau) \\, R(\\tau) \\, d\\tau\n\\rho_b(\\tau) is the likelihood that a trajectory \\tau is generated by the behavior policy b. We shuffle a bit the terms:\n\\mathcal{J}(\\pi) = \\int_\\tau \\rho_b(\\tau) \\, \\frac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)} \\,  R(\\tau) \\, d\\tau\nand notice that it has the form of an expectation over trajectories generated by b:\n\\mathcal{J}(\\pi) = \\mathbb{E}_{\\tau \\sim \\rho_b} [\\frac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)} \\, R(\\tau)]\nThis means that we can sample trajectories from b, but we need to correct the observed return by the importance sampling weight \\frac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)}.\nThe importance sampling weight corrects the mismatch between \\pi and b.\n\n\n\nImportance sampling gives more weight to samples that are likely under the distribution to estimate.\n\n\nIf the two distributions are the same (on-policy), the IS weight is 1, there is no need to correct the returns. If a sample is likely under b but not under \\pi, we should not care about its return: \\frac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)} << 1. If a sample is likely under \\pi but not much under b, we increase its importance in estimating the return: \\frac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)} >> 1. The sampling average of the corrected samples will be closer from the true estimate (unbiased).\nHow do we compute these probability distributions \\rho_\\pi(\\tau) and \\rho_b(\\tau) for a trajectory \\tau? A trajectory \\tau is a sequence of state-action transitions (s_0, a_0, s_1, a_1, \\ldots, s_T) whose probability depends on:\n\nthe probability of choosing an action a_t in state s_t: the policy \\pi(s, a).\nthe probability of arriving in the state s_{t+1} from the state s_t with the action a_t: the transition probability p(s_{t+1} | s_t, a_t).\n\nThe likelihood of a trajectory \\tau = (s_0, a_0, s_1, a_1, \\ldots, s_T) under a policy \\pi depends on the policy and the transition probabilities (Markov property):\n\n    \\rho_\\pi(\\tau) = p_\\pi(s_0, a_0, s_1, a_1, \\ldots, s_T) = p(s_0) \\, \\prod_{t=0}^{T-1} \\pi_\\theta(s_t, a_t) \\, p(s_{t+1} | s_t, a_t)\n\np(s_0) is the probability of starting an episode in s_0, we do not have control over it.\nWhat is interesting is that the transition probabilities disappear when calculating the importance sampling weight:\n\n    \\rho_{0:T-1} = \\frac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)} = \\frac{p_0 (s_0) \\, \\prod_{t=0}^{T-1} \\pi(s_t, a_t) p(s_{t+1} | s_t, a_t)}{p_0 (s_0) \\, \\prod_{t=0}^T b(s_t, a_t) p(s_{t+1} | s_t, a_t)} = \\frac{\\prod_{t=0}^{T-1} \\pi(s_t, a_t)}{\\prod_{t=0}^T b(s_t, a_t)} = \\prod_{t=0}^{T-1} \\frac{\\pi(s_t, a_t)}{b(s_t, a_t)}\n\nThe importance sampling weight is simply the product over the length of the episode of the ratio between \\pi(s_t, a_t) and b(s_t, a_t)."
  },
  {
    "objectID": "notes/2.4-MC.html#off-policy-monte-carlo-control",
    "href": "notes/2.4-MC.html#off-policy-monte-carlo-control",
    "title": "Monte-Carlo (MC) methods",
    "section": "Off-policy Monte-Carlo control",
    "text": "Off-policy Monte-Carlo control\nIn off-policy MC control, we generate episodes using the behavior policy b and update greedily the learned policy \\pi. For the state s_t, the obtained returns just need to be weighted by the relative probability of occurrence of the rest of the episode following the policies \\pi and b:\n\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(s_k, a_k)}{b(s_k, a_k)}\nV^\\pi(s_t) = \\mathbb{E}_{\\tau \\sim \\rho_b} [\\rho_{t:T-1} \\, R_t]\nThis gives us the updates:\n\n    V(s_t) = V(s_t) + \\alpha  \\, \\rho_{t:T-1} \\, (R_t - V(s_t))\n\nand:\n\n    Q(s_t, a_t) = Q(s_t, a_t) + \\alpha  \\, \\rho_{t:T-1} \\, (R_t - Q(s_t, a_t))\n\nUnlikely episodes under \\pi are barely used for learning, likely ones are used a lot.\n\n\n\n\n\n\nOff-policy Monte-Carlo control\n\n\n\n\nwhile True:\n\nGenerate an episode \\tau = (s_0, a_0, r_1, \\ldots, s_T) using the behavior policy b.\nFor each state-action pair (s_t, a_t) in the episode, update the estimated Q-value:\n\n\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(s_k, a_k)}{b(s_k, a_k)}\n\n      Q(s_t, a_t) = Q(s_t, a_t) + \\alpha  \\, \\rho_{t:T-1} \\, (R_t - Q(s_t, a_t))\n  \n\nFor each state s_t in the episode, update the learned deterministic policy (greedy):\n\n\n      \\pi(s_t, a) = \\begin{cases}\n                      1\\; \\text{if} \\; a = \\text{argmax} \\, Q(s_t, a) \\\\\n                      0 \\; \\text{otherwise.} \\\\\n                    \\end{cases}\n  \n\n\n\n\nProblem 1: if the learned policy is greedy, the IS weight becomes quickly 0 for a non-greedy action a_t:\n\n\\pi(s_t, a_t) = 0 \\rightarrow \\rho_{0:T-1} = \\prod_{k=0}^{T-1} \\frac{\\pi(s_k, a_k)}{b(s_k, a_k)} = 0\nOff-policy MC control only learns from the last greedy actions, what is slow at the beginning.\nSolution: \\pi and b should not be very different. Usually \\pi is greedy and b is a softmax (or \\epsilon-greedy) over it.\n\nProblem 2: if the learned policy is stochastic, the IS weights can quickly vanish to 0 or explode to infinity:\n\n\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(s_k, a_k)}{b(s_k, a_k)}\nIf \\dfrac{\\pi(s_k, a_k)}{b(s_k, a_k)} is smaller than 1, the products go to 0. If it is bigger than 1, it grows to infinity.\nSolution: one can normalize the IS weight between different episodes (see Sutton and Barto) or clip it (e.g. restrict it to [0.9, 1.1], see PPO later in this course).\nThe main advantage of off-policy strategies is that you can learn from other’s actions, you don’t have to rely on your initially wrong policies to discover the solution by chance. Example: learning to play chess by studying thousands/millions of plays by chess masters. In a given state, only a subset of the possible actions are actually executed by experts: the others may be too obviously wrong. The exploration is then guided by this expert knowledge, not randomly among all possible actions.\nOff-policy methods greatly reduce the number of transitions needed to learn a policy: very stupid actions are not even considered, but the estimation policy learns an optimal strategy from the “classical” moves. Drawback: if a good move is not explored by the behavior policy, the learned policy will never try it.\n\n\n\n\n\n\nProperties of Monte-Carlo methods\n\n\n\n\nMonte-Carlo evaluation estimates value functions via sampling of entire episodes.\nMonte-Carlo control (evaluation-improvement) is a generalized policy iteration method.\nMC for action values is model-free: you do not need to know p(s' | s, a) to learn the optimal policy, you just sample transitions (trial and error).\nMC only applies to episodic tasks: as you learn at the end of an episode, it is not possible to learn continuing tasks.\nMC suffers from the exploration-exploitation problem:\n\non-policy MC learns a stochastic policy (\\epsilon-greedy, softmax) to ensure exploration.\noff-policy MC learns a greedy policy, but explores via a behavior policy (importance sampling).\n\nMonte-Carlo methods have:\n\na small bias: with enough sampled episodes, the estimated values converge to the true values.\na huge variance: the slightest change of the policy can completely change the episode and its return. You will need a lot of samples to form correct estimates: sample complexity."
  },
  {
    "objectID": "notes/2.5-TD.html",
    "href": "notes/2.5-TD.html",
    "title": "Temporal Difference learning",
    "section": "",
    "text": "Slides: html pdf"
  },
  {
    "objectID": "notes/2.5-TD.html#temporal-difference-algorithms",
    "href": "notes/2.5-TD.html#temporal-difference-algorithms",
    "title": "Temporal Difference learning",
    "section": "Temporal Difference algorithms",
    "text": "Temporal Difference algorithms\n\nMC methods wait until the end of the episode to compute the obtained return, and update the estimates of all encountered states:\n\n    V(s_t) = V(s_t) + \\alpha (R_t - V(s_t))\n\nIf the episode is very long, learning might be very slow. If the task is continuing, it is impossible. Considering that the return at time t is the immediate reward plus the return in the next step:\n\n    R_t = r_{t+1} + \\gamma \\,  R_{t+1}\n\nwe could replace R_{t+1} by an estimate, which is the value of the next state V^\\pi(s_{t+1}) = \\mathbb{E}_\\pi [R_{t+1} | s_{t+1}=s]:\nR_t \\approx r_{t+1} + \\gamma \\,  V^\\pi(s_{t+1})\nTemporal-Difference (TD) methods simply replace the actual return by an estimation in the update rule:\n\n    V(s_t) = V(s_t) + \\alpha \\, (r_{t+1} + \\gamma \\, V(s_{t+1}) - V(s_t))\n\nwhere r_{t+1} + \\gamma\\, V(s_{t+1}) is a sampled estimate of the return.\n\n\n\nTD replaces R_{t+1} with an estimate V(s_{t+1}). Adapted from (Sutton and Barto, 1998).\n\n\nThe quantity\n\\delta_t = r_{t+1} + \\gamma \\, V(s_{t+1}) - V(s_t)\nis called equivalently the reward prediction error (RPE), the TD error or the advantage of the action a_t. It is the difference between the estimated return in state s_t V(s_t) and the actual return r_{t+1} + \\gamma \\, V(s_{t+1}), computed with an estimation.\nIf \\delta_t > 0, it means that we received more reward r_{t+1} than expected, or that we arrived in a state s_{t+1} that is better than expected: we should increase the value of s_t as we underestimate it. If \\delta_t < 0, we should decrease the value of s_t as we overestimate it.\nThe learning procedure in TD is then possible after each transition: the backup diagram is limited to only one state and its follower.\n\n\n\n\n\n\nTD(0) policy evaluation\n\n\n\n\nwhile True:\n\nStart from an initial state s_0.\nforeach step t of the episode:\n\nSelect a_t using the current policy \\pi in state s_t.\nApply a_t, observe r_{t+1} and s_{t+1}.\nCompute the TD error:\n\n\\delta_t = r_{t+1} + \\gamma \\, V(s_{t+1}) - V(s_t)\n\nUpdate the state-value function of s_t:\n\n\n      V(s_t) = V(s_t) + \\alpha \\, \\delta_t\n  \n\nif s_{t+1} is terminal: break\n\n\n\n\n\nTD learns from experience in a fully incremental manner. It does not need to wait until the end of an episode. It is therefore possible to learn continuing tasks. TD converges to V^{\\pi} if the step-size parameter \\alpha is small enough.\nThe TD error is used to evaluate the policy:\n\n    V(s_t) = V(s_t) + \\alpha \\, (r_{t+1} + \\gamma \\, V(s_{t+1}) - V(s_t)) = V(s_t) + \\alpha \\, \\delta_t\n\nThe estimates converge to:\nV^\\pi(s) = \\mathbb{E}_\\pi [r(s, a, s') + \\gamma \\, V^\\pi(s')]\nBy using an estimate of the return R_t instead of directly the return as in MC, we increase the bias (estimates are always wrong, especially at the beginning of learning) but we reduce the variance: only r(s, a, s') is stochastic, not the value function V^\\pi. We can therefore expect less optimal solutions, but we will also need less samples: better sample efficiency than MC but worse convergence (suboptimal).\nQ-values can be estimated in the same way:\n\n    Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\, (r_{t+1} + \\gamma \\, Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))\n\nLike for MC, the exploration/exploitation trade-off has to be managed: what is the next action a_{t+1}? There are therefore two classes of TD control algorithms: on-policy (SARSA) and off-policy (Q-learning).\n\nSARSA: On-policy TD control\nSARSA (state-action-reward-state-action) updates the value of a state-action pair by using the predicted value of the next state-action pair according to the current policy.\n\n\n\nState-action-reward-state-action transitions. Source: (Sutton and Barto, 1998).\n\n\nWhen arriving in s_{t+1} from (s_t, a_t), we already sample the next action:\na_{t+1} \\sim \\pi(s_{t+1}, a)\nWe can now update the value of (s_t, a_t):\n\n    Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\, (r_{t+1} + \\gamma \\, Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))\n\nThe next action a_{t+1} will have to be executed next: SARSA is on-policy. You cannot change your mind and execute another a_{t+1}. The learned policy must be \\epsilon-soft (stochastic) to ensure exploration. SARSA converges to the optimal policy if \\alpha is small enough and if \\epsilon (or \\tau) slowly decreases to 0.\n\n\n\n\n\n\nSARSA: On-policy TD control\n\n\n\n\nwhile True:\n\nStart from an initial state s_0 and select a_0 using the current policy \\pi.\nforeach step t of the episode:\n\nApply a_{t}, observe r_{t+1} and s_{t+1}.\nSelect a_{t+1} using the current stochastic policy \\pi.\nUpdate the action-value function of (s_t, a_t):\n\n Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\, (r_{t+1} + \\gamma \\, Q(s_{t+1}, a_{t+1})  - Q(s_t, a_t)) \n\nImprove the stochastic policy, e.g:\n\n\n      \\pi(s_t, a) = \\begin{cases}\n                      1 - \\epsilon \\; \\text{if} \\; a = \\text{argmax} \\, Q(s_t, a) \\\\\n                      \\frac{\\epsilon}{|\\mathcal{A}(s_t) -1|} \\; \\text{otherwise.} \\\\\n                    \\end{cases}\n  \n\nif s_{t+1} is terminal: break\n\n\n\n\n\n\n\nQ-learning: Off-policy TD control\nQ-learning directly approximates the optimal action-value function Q^* independently of the current policy, using the greedy action in the next state.\nQ(s_t, a_t) = Q(s_t, a_t) + \\alpha \\, (r_{t+1} + \\gamma \\, \\max_a Q(s_{t+1}, a) - Q(s_t, a_t))\nThe next action a_{t+1} can be generated by a behavior policy: Q-learning is off-policy, but the learned policy can be deterministic. The behavior policy can be an \\epsilon-soft policy derived from Q or expert knowledge. The behavior policy only needs to visit all state-action pairs during learning to ensure optimality.\n\n\n\n\n\n\nQ-learning: Off-policy TD control\n\n\n\n\nwhile True:\n\nStart from an initial state s_0.\nforeach step t of the episode:\n\nSelect a_{t} using the behavior policy b (e.g. derived from \\pi).\nApply a_t, observe r_{t+1} and s_{t+1}.\nUpdate the action-value function of (s_t, a_t):\n\nQ(s_t, a_t) = Q(s_t, a_t) + \\alpha \\, (r_{t+1} + \\gamma \\, \\max_a Q(s_{t+1}, a) - Q(s_t, a_t))\n\nImprove greedily the learned policy:\n\n\\pi(s_t, a) = \\begin{cases}\n                  1\\; \\text{if} \\; a = \\text{argmax} \\, Q(s_t, a) \\\\\n                  0 \\; \\text{otherwise.} \\\\\n                \\end{cases}\n  \n\nif s_{t+1} is terminal: break\n\n\n\n\n\nIn off-policy Monte-Carlo, Q-values are estimated using the return of the rest of the episode on average:\nQ^\\pi(s, a) = \\mathbb{E}_{\\tau \\sim \\rho_b}[\\rho_{0:T-1} \\, R(\\tau) | s_0 = s, a_0=a]\nAs the rest of the episode is generated by b, we need to correct the returns using the importance sampling weight. In Q-learning, Q-values are estimated using other estimates:\nQ^\\pi(s, a) = \\mathbb{E}_{s_t \\sim \\rho_b, a_t \\sim b}[ r_{t+1} + \\gamma \\, \\max_a Q^\\pi(s_{t+1}, a) | s_t = s, a_t=a]\nAs we only sample transitions using b and not episodes, there is no need to correct the returns: the returns use estimates Q^\\pi, which depend on \\pi and not b. The immediate reward r_{t+1} is stochastic, but is the same whether you sample a_t from \\pi or from b.\n\n\n\n\n\n\nTemporal Difference learning\n\n\n\n\nTemporal Difference allow to learn Q-values from single transitions instead of complete episodes.\nMC methods can only be applied to episodic problems, while TD works for continuing tasks.\nMC and TD methods are model-free: you do not need to know anything about the environment (p(s' |s, a) and r(s, a, s')) to learn.\nThe exploration-exploitation dilemma must be dealt with:\n\nOn-policy TD (SARSA) follows the learned stochastic policy.\n\n\n      Q(s, a) = Q(s, a) + \\alpha \\, (r(s, a, s') + \\gamma \\, Q(s', a') - Q(s, a))\n  \n\nOff-policy TD (Q-learning) follows a behavior policy and learns a deterministic policy.\n\n\n      Q(s, a) = Q(s, a) + \\alpha \\, (r(s, a, s') + \\gamma \\, \\max_a Q(s', a) - Q(s, a))\n  \nTD uses bootstrapping like DP: it uses other estimates to update one estimate.\nQ-learning is the go-to method in tabular RL."
  },
  {
    "objectID": "notes/2.5-TD.html#actor-critic-methods",
    "href": "notes/2.5-TD.html#actor-critic-methods",
    "title": "Temporal Difference learning",
    "section": "Actor-critic methods",
    "text": "Actor-critic methods\n\nActor-critic methods are TD methods that have a separate memory structure to explicitly represent the policy independent of the value function. The policy \\pi is implemented by the actor, because it is used to select actions. The estimated values V(s) are implemented by the critic, because it criticizes the actions made by the actor. Learning is always on-policy: the critic must learn about and critique whatever policy is currently being followed by the actor.\n\n\n\nActor-critic architecture. Source: (Sutton and Barto, 1998).\n\n\nThe critic computes the TD error or 1-step advantage after each transition (s_t, a_t, r_{t+1}, s_{t+1}):\n\\delta_t = r_{t+1} + \\gamma \\, V(s_{t+1}) - V(s_t)\nThis scalar signal is the output of the critic and drives learning in both the actor and the critic. It tells us how good the action a_t was compared to our expectation V(s_t).\nWhen the advantage \\delta_t > 0, this means that the action lead to a better reward or a better state than what was expected by V(s_t), which is a good surprise, so the action should be reinforced (selected again) and the value of that state increased.\nWhen \\delta_t < 0, this means that the previous estimation of (s_t, a_t) was too high (bad surprise), so the action should be avoided in the future and the value of the state reduced.\nThe critic is updated using this scalar signal:\n V(s_t) \\leftarrow V(s_t) + \\alpha \\, \\delta_t\nThe actor is updated according to this TD error signal. For example a softmax actor over preferences:\np(s_t, a_t) \\leftarrow p(s_t, a_t) + \\beta \\, \\delta_t\n\\pi(s, a) = \\frac{\\exp{p(s, a)}}{\\sum_b \\exp{p(s, b)}}\nWhen \\delta_t >0, the preference is increased, so the probability of selecting it again increases. When \\delta_t <0, the preference is decreased, so the probability of selecting it again decreases. This is the equivalent of reinforcement comparison for bandits.\n\n\n\n\n\n\nActor-critic algorithm with preferences\n\n\n\n\nStart in s_0. Initialize the preferences p(s,a) for each state action pair and the critic V(s) for each state.\nforeach step t:\n\nSelect a_t using the actor \\pi in state s_t:\n\n\\pi(s_t, a) = \\frac{\\exp{p(s, a)}}{\\sum_b \\exp{p(s, b)}}\n\nApply a_t, observe r_{t+1} and s_{t+1}.\nCompute the TD error in s_t using the critic:\n\n\n      \\delta_t = r_{t+1} + \\gamma \\, V(s_{t+1}) - V(s_t)\n  \n\nUpdate the actor:\n\n\n      p(s_t, a_t) \\leftarrow p(s_t, a_t) + \\beta \\, \\delta_t\n  \n\nUpdate the critic:\n\n\n      V(s_t) \\leftarrow V(s_t) + \\alpha \\, \\delta_t\n  \n\n\n\nThe advantage of the separation between the actor and the critic is that now the actor can take any form (preferences, linear approximation, deep networks). It requires minimal computation in order to select the actions, in particular when the action space is huge or even continuous. It can learn stochastic policies, which is particularly useful in non-Markov problems.\nIt is obligatory to learn on-policy: the critic must evaluate the actions taken by the current actor and the actor must learn from the current critic, not “old” V-values.\n\n\n\n\n\n\nValue-based vs. policy-based algorithms\n\n\n\n\nValue-based methods use value estimates Q_t(s, a) to infer a policy:\n\nOn-policy methods learn and use a stochastic policy to explore.\nOff-policy methods learn a deterministic policy but use a (stochastic) behavior policy to explore.\n\nPolicy-based methods directly learn the policy \\pi_t(s, a) (actor) using preferences or function approximators.\n\nA critic learning values is used to improve the policy w.r.t a performance baseline.\nActor-critic architectures are strictly on-policy.\n\n\n\n\n\n\nBandits\nMDP\n\n\n\n\nValue-based\n\n\n\n\n\\qquadOn-policy\n\\epsilon-greedy, softmax\nSARSA\n\n\n\\qquadOff-policy\ngreedy\nQ-learning\n\n\nPolicy-based\n\n\n\n\n\\qquadOn-policy\nReinforcement comparison\nActor-critic"
  },
  {
    "objectID": "notes/2.5-TD.html#eligibility-traces-and-advantage-estimation",
    "href": "notes/2.5-TD.html#eligibility-traces-and-advantage-estimation",
    "title": "Temporal Difference learning",
    "section": "Eligibility traces and advantage estimation",
    "text": "Eligibility traces and advantage estimation\n\nMC has high variance, zero bias: it has good convergence properties and we are more likely to find the optimal policy. It is not very sensitive to initial estimates, and very simple to understand and use.\nTD has low variance, some bias, so it is usually more sample efficient than MC. TD(0) converges to V^\\pi(s) (but not always with function approximation). The policy might be suboptimal. It is more sensitive to initial values (bootstrapping).\n\n\n\nGridworld environment. Source: (Sutton and Barto, 1998).\n\n\nWhen the reward function is sparse (e.g. only at the end of a game), only the last action, leading to that reward, will be updated the first time in TD.\n\n    Q(s, a) = Q(s, a) + \\alpha \\, (r(s, a, s') + \\gamma \\, \\max_a Q(s', a) - Q(s, a))\n\nThe previous actions, which were equally important in obtaining the reward, will only be updated the next time they are visited. This makes learning very slow: if the path to the reward has n steps, you will need to repeat the same episode at least n times to learn the Q-value of the first action.\n\nn-step returns\n\n\n\nn-step returns. Source: (Sutton and Barto, 1998).\n\n\nOptimally, we would like a trade-off between:\n\nTD (only one state/action is updated each time, small variance but significant bias)\nMonte-Carlo (all states/actions in an episode are updated, no bias but huge variance).\n\nIn n-step TD prediction, the next n rewards are used to estimate the return, the rest is approximated. The n-step return is the discounted sum of the n next rewards is computed as in MC plus the predicted value at step t+n which replaces the rest as in TD.\n\n    R^n_t = \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\,  V(s_{t+n})\n\nWe can update the value of the state with this n-step return:\n\n    V(s_t) = V(s_t) + \\alpha \\, (R^n_t - V (s_t))\n\nThe n-step advantage at time t is:\n\nA^n_t = \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\,  V(s_{t+n}) - V (s_t)\n\nIt is easy to check that the TD error is the 1-step advantage:\n\n    \\delta_t = A^1_t = r_{t+1} + \\gamma \\, V(s_{t+1}) - V(s_t)\n\nAs you use more “real” rewards, you reduce the bias of Q-learning. As you use estimates for the rest of the episode, you reduce the variance of MC methods. But how to choose n?\n\n\nEligibility traces\nOne solution is to average the n-step returns, using a discount factor \\lambda :\nR^\\lambda_t = (1  - \\lambda) \\, \\sum_{n=1}^\\infty \\lambda^{n-1} \\, R^n_t\n\n\n\n\\lambda-returns are averages of all n-step returns. Source: (Sutton and Barto, 1998).\n\n\nThe term 1- \\lambda is there to ensure that the coefficients \\lambda^{n-1} sum to one.\n\\sum_{n=1}^\\infty \\lambda^{n-1} = \\dfrac{1}{1 - \\lambda}\nEach reward r_{t+k+1} will count multiple times in the \\lambda-return. Distant rewards are discounted by \\lambda^k in addition to \\gamma^k.\n\n\n\n\\lambda controls the importance of large n-step returns. Source: (Sutton and Barto, 1998).\n\n\nLarge n-step returns (MC) should not have as much importance as small ones (TD), as they have a high variance.\nTo understand the role of \\lambda, let’s split the infinite sum w.r.t the end of the episode at time T. n-step returns with n \\geq T all have a MC return of R_t:\nR^\\lambda_t = (1  - \\lambda) \\, \\sum_{n=1}^{T-t-1} \\lambda^{n-1} \\, R^n_t + \\lambda^{T-t-1} \\, R_t\n\\lambda controls the bias-variance trade-off:\n\nIf \\lambda=0, the \\lambda-return is equal to R^1_t = r_{t+1} + \\gamma \\, V(s_{t+1}), i.e. TD: high bias, low variance.\nIf \\lambda=1, the \\lambda-return is equal to R_t = \\sum_{k=0}^{\\infty} \\gamma^{k} \\, r_{t+k+1}, i.e. MC: low bias, high variance.\n\nThis forward view of eligibility traces implies to look at all future rewards until the end of the episode to perform a value update. This prevents online learning using single transitions.\n\n\n\nForward view of eligibility traces. Source: (Sutton and Barto, 1998).\n\n\nAnother view on eligibility traces is that the TD reward prediction error at time t is sent backwards in time:\n\n\\delta_t = r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)\n\n\n\n\nBackward view of eligibility traces. Source: (Sutton and Barto, 1998).\n\n\nEvery state s previously visited during the episode will be updated proportionally to the current TD error and an eligibility trace e_t(s):\n\n    V(s) \\leftarrow V(s) + \\alpha \\, \\delta_t \\, e_t(s)\n\nThe eligibility trace defines since how long the state was visited:\n\n    e_t(s) = \\begin{cases}\n                \\gamma \\, \\lambda \\, e_{t-1}(s) \\qquad\\qquad \\text{if} \\quad s \\neq s_t \\\\\n                e_{t-1}(s) + 1 \\qquad \\text{if} \\quad s = s_t \\\\\n            \\end{cases}\n\n\n\n\nUpdating of eligibility traces. Source: (Sutton and Barto, 1998).\n\n\n\\lambda defines how important is a future TD error for the current state.\n\n\n\n\n\n\nTD(\\lambda) algorithm: policy evaluation\n\n\n\n\nforeach step t of the episode:\n\nSelect a_t using the current policy \\pi in state s_t, observe r_{t+1} and s_{t+1}.\nCompute the TD error in s_t:\n\n\n      \\delta_t = r_{t+1} + \\gamma \\, V_k(s_{t+1}) - V_k(s_t)\n  \n\nIncrement the trace of s_t:\n\n\n      e_{t+1}(s_t) = e_t(s_t) + 1\n  \n\nforeach state s \\in [s_o, \\ldots, s_t] in the episode:\n\nUpdate the state value function:\n\n\n      V_{k+1}(s) = V_k(s) + \\alpha \\, \\delta_t \\, e_t(s)\n  \n\nDecay the eligibility trace:\n\n\n      e_{t+1}(s) = \\lambda \\, \\gamma \\, e_t(s)\n  \nif s_{t+1} is terminal: break\n\n\n\n\nThe backward view of eligibility traces can be applied on single transitions, given we know the history of visited states and maintain a trace for each of them. Eligibility traces are a very useful way to speed learning up in TD methods and control the bias/variance trade-off. This modification can be applied to all TD methods: TD(\\lambda) for states, SARSA(\\lambda) and Q(\\lambda) for actions.\nThe main drawback is that we need to keep a trace for ALL possible state-action pairs: memory consumption. Clever programming can limit this issue. The value of \\lambda has to be carefully chosen for the problem: perhaps initial actions are random and should not be reinforced. If your problem is not strictly Markov (POMDP), eligibility traces can help as they update the history!\n\n\nGeneralized advantage estimation (GAE)\nThe n-step advantage at time t:\n\nA^n_t = \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\,  V(s_{t+n}) - V (s_t)\n\ncan be written as function of the TD error of the next n transitions:\n\n    A^{n}_t = \\sum_{l=0}^{n-1} \\gamma^l \\, \\delta_{t+l}\n\n\n\n\n\n\n\nProof with n=2:\n\n\n\n\\begin{aligned}\nA^2_t &= r_{t+1} + \\gamma \\, r_{t+2} + \\gamma^2 \\, V(s_{t+2}) - V(s_{t}) \\\\\n&\\\\\n&= (r_{t+1} - V(s_t)) + \\gamma \\, (r_{t+2} + \\gamma \\, V(s_{t+2}) ) \\\\\n&\\\\\n&= (r_{t+1} + \\gamma \\, V(s_{t+1}) - V(s_t)) + \\gamma \\, (r_{t+2} + \\gamma \\, V(s_{t+2}) - V(s_{t+1})) \\\\\n&\\\\\n&= \\delta_t + \\gamma \\, \\delta_{t+1}\n\\end{aligned}\n\n\n\nThe n-step advantage realizes a bias/variance trade-off, but which value of n should we choose?\n\nA^n_t = \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\,  V(s_{t+n}) - V (s_t)\n\nSchulman et al. (2015) (Schulman et al., 2015) proposed a generalized advantage estimate (GAE) A_t^{\\text{GAE}(\\gamma, \\lambda)} summing all possible n-step advantages with a discount parameter \\lambda:\nA_t^{\\text{GAE}(\\gamma, \\lambda)} = (1 - \\lambda) \\sum_{n=1}^\\infty \\lambda^n \\, A^n_t\nThis is just a forward eligibility trace over distant n-step advantages: the 1-step advantage is more important the the 1000-step advantage (too much variance). We can show that the GAE can be expressed as a function of the future 1-step TD errors:\nA_t^{\\text{GAE}(\\gamma, \\lambda)} = \\sum_{k=0}^\\infty (\\gamma \\, \\lambda)^k \\, \\delta_{t+k}\nThe parameter \\lambda controls the bias-variance trade-off.\n\nWhen \\lambda=0, the generalized advantage is the TD error:\n\nA_t^{\\text{GAE}(\\gamma, 0)} = r_{t+1} + \\gamma \\, V(s_{t+1}) - V(s_t)  = \\delta_{t}\n\nWhen \\lambda=1, the generalized advantage is the MC advantage:\n\nA_t^{\\text{GAE}(\\gamma, 1)} = \\sum_{k=0}^\\infty \\gamma^k \\, r_{t+k+1} - V(s_t) = R_t - V(s_t)\nAny value in between controls the bias-variance trade-off: from the high bias / low variance of TD to the small bias / high variance of MC. In practice, it leads to a better estimation than n-step advantages, but is more computationally expensive.\n\n\n\n\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015). Trust Region Policy Optimization. in Proceedings of the 31 st International Conference on Machine Learning, 1889–1897. http://proceedings.mlr.press/v37/schulman15.html.\n\n\nSutton, R. S., and Barto, A. G. (1998). Reinforcement Learning: An introduction. Cambridge, MA: MIT press."
  },
  {
    "objectID": "notes/2.6-FA.html",
    "href": "notes/2.6-FA.html",
    "title": "Function approximation",
    "section": "",
    "text": "Slides: html pdf"
  },
  {
    "objectID": "notes/2.6-FA.html#limits-of-tabular-rl",
    "href": "notes/2.6-FA.html#limits-of-tabular-rl",
    "title": "Function approximation",
    "section": "Limits of tabular RL",
    "text": "Limits of tabular RL\n\nAll the methods seen so far belong to tabular RL. Q-learning necessitates to store in a Q-table one Q-value per state-action pair (s, a).\n\n\n\nTabular Q-learning. Source: https://towardsdatascience.com/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677\n\n\nIf a state has never been visited during learning, the Q-values will still be at their initial value (0.0), no policy can be derived.\n\n\n\nIn high-dimensional state spaces (e.g. images), tabular RL cannot generalize between close states.\n\n\nSimilar states likely have the same optimal action: we want to be able to generalize the policy between states. For most realistic problems, the size of the Q-table becomes quickly untractable. If you use black-and-white 256x256 images as inputs, you have 2^{256 * 256} = 10^{19728} possible states! Tabular RL is therefore limited to toy problems.\nTabular RL also only works for small discrete action spaces. Robots have continuous action spaces, where the actions are changes in joint angles or torques. A joint angle could for example take any value in [0, \\pi]. A solution would be to discretize the action space (one action per degree), but we would fall into the curse of dimensionality.\n\n\n\nCurse of dimensionality. Source: https://medium.com/diogo-menezes-borges/give-me-the-antidote-for-the-curse-of-dimensionality-b14bce4bf4d2\n\n\nThe more degrees of freedom, the more discrete actions, the more entries in the Q-table… Tabular RL cannot deal with continuous action spaces, unless we approximate the policy with an actor-critic architecture."
  },
  {
    "objectID": "notes/2.6-FA.html#function-approximation",
    "href": "notes/2.6-FA.html#function-approximation",
    "title": "Function approximation",
    "section": "Function approximation",
    "text": "Function approximation\n\n\nFeature vectors\nLet’s represent a state s by a vector of d features \\phi(s) = [\\phi_1(s), \\phi_2(s), \\ldots, \\phi_d(s)]^T. For the cartpole, the feature vector would be:\n \\phi(s) = \\begin{bmatrix}x \\\\ \\dot{x} \\\\ \\theta \\\\ \\dot{\\theta} \\end{bmatrix}\nwhere x is the position, \\theta the angle, \\dot{x} and \\dot{\\theta} their derivatives. We are able to represent any state s of the cartpole problem using these four variables.\nFor more complex problems, the feature vector should include all the necessary information (Markov property). Example of breakout:\n\n    \\phi(s) = \\begin{bmatrix}\n        x \\, \\text{position of the paddle} \\\\\n        x \\, \\text{position of the ball} \\\\\n        y \\, \\text{position of the ball} \\\\\n        x \\, \\text{speed of the ball} \\\\\n        y \\, \\text{speed of the position} \\\\\n        \\text{presence of brick 1} \\\\\n        \\text{presence of brick 2} \\\\\n        \\vdots \\\\\n    \\end{bmatrix}\n\nIn deep RL, we will learn these feature vectors, but let’s suppose for now that we have them.\nNote that we can always fall back to the tabular case using one-hot encoding of the states:\n\n\\phi(s_1) = \\begin{bmatrix}1\\\\0\\\\0\\\\ \\ldots\\\\ 0\\end{bmatrix} \\qquad\n\\phi(s_2) = \\begin{bmatrix}0\\\\1\\\\0\\\\ \\ldots\\\\ 0\\end{bmatrix}\\qquad\n\\phi(s_3) = \\begin{bmatrix}0\\\\0\\\\1\\\\ \\ldots\\\\ 0\\end{bmatrix} \\qquad \\ldots\n\nBut the idea is that we can represent states with much less values than the number of states:\nd \\ll |\\mathcal{S}|\nWe can also represent continuous state spaces with feature vectors, as in cartpole.\n\n\nState value approximation\nIn state value approximation, we want to approximate the state value function V^\\pi(s) with a parameterized function V_\\varphi(s):\nV_\\varphi(s) \\approx V^\\pi(s)\n\n\n\nState value approximation.\n\n\nThe parameterized function can have any form. Its has a set of parameters \\varphi used to transform the feature vector \\phi(s) into an approximated value V_\\varphi(s).\nThe simplest function approximator (FA) is the linear approximator.\n\n\n\nLinear state value approximation.\n\n\nThe approximated value is a linear combination of the features:\nV_\\varphi(s) = \\sum_{i=1}^d w_i \\, \\phi_i(s) = \\mathbf{w}^T \\times \\phi(s)\nThe weight vector \\mathbf{w} = [w_1, w_2, \\ldots, w_d]^Tis the set of parameters \\varphi of the function. A linear approximator is a single artificial neuron (linear regression) without a bias.\nRegardless the form of the function approximator, we want to find the parameters \\varphi making the approximated values V_\\varphi(s) as close as possible from the true values V^\\pi(s) for all states s.\nThis is a regression problem, so we want to minimize the mean-square error between the two quantities:\n \\min_\\varphi \\mathcal{L}(\\varphi) = \\mathbb{E}_{s \\in \\mathcal{S}} [ (V^\\pi(s) - V_\\varphi(s))^2]\nThe loss function \\mathcal{L}(\\varphi) is minimal when the predicted values are close to the true ones on average over the state space.\nLet’s suppose for now that we know the true state values V^\\pi(s) for all states and that the parametrized function is differentiable. We can find the minimum of the loss function by applying gradient descent (GD) iteratively:\n\n    \\Delta \\varphi = - \\eta \\, \\nabla_\\varphi \\mathcal{L}(\\varphi)\n\n\\nabla_\\varphi \\mathcal{L}(\\varphi) is the gradient of the loss function w.r.t to the parameters \\varphi.\n\n    \\nabla_\\varphi \\mathcal{L}(\\varphi) = \\begin{bmatrix}\n        \\frac{\\partial \\mathcal{L}(\\varphi)}{\\partial \\varphi_1} \\\\\n        \\frac{\\partial \\mathcal{L}(\\varphi)}{\\partial \\varphi_2} \\\\\n        \\ldots \\\\\n        \\frac{\\partial \\mathcal{L}(\\varphi)}{\\partial \\varphi_K} \\\\\n    \\end{bmatrix}\n\nWhen applied repeatedly, GD converges to a local minimum of the loss function.\nIn order to minimize the mean square error, we will iteratively modify the parameters \\varphi according to:\n\n\\begin{aligned}\n    \\Delta \\varphi = \\varphi_{k+1} - \\varphi_n & = - \\eta \\, \\nabla_\\varphi \\mathcal{L}(\\varphi) \\\\\n    &\\\\\n    & = - \\eta \\, \\nabla_\\varphi \\mathbb{E}_{s \\in \\mathcal{S}} [ (V^\\pi(s) - V_\\varphi(s))^2] \\\\\n    &\\\\\n    & = \\mathbb{E}_{s \\in \\mathcal{S}} [- \\eta \\, \\nabla_\\varphi  (V^\\pi(s) - V_\\varphi(s))^2] \\\\\n    &\\\\\n    & = \\mathbb{E}_{s \\in \\mathcal{S}} [\\eta \\,  (V^\\pi(s) - V_\\varphi(s)) \\, \\nabla_\\varphi V_\\varphi(s)] \\\\\n\\end{aligned}\n\nAs it would be too slow to compute the expectation on the whole state space (batch algorithm), we will sample the quantity:\n\\delta_\\varphi = \\eta \\,  (V^\\pi(s) - V_\\varphi(s)) \\, \\nabla_\\varphi V_\\varphi(s)\nand update the parameters with stochastic gradient descent (SGD).\nIf we sample K states s_i from the state space, we get:\n\n    \\Delta \\varphi = \\eta \\,  \\frac{1}{K} \\sum_{k=1}^K (V^\\pi(s_k) - V_\\varphi(s_k)) \\, \\nabla_\\varphi V_\\varphi(s_k)\n\nWe can also sample a single state s (online algorithm):\n\n    \\Delta \\varphi = \\eta \\, (V^\\pi(s) - V_\\varphi(s)) \\, \\nabla_\\varphi V_\\varphi(s)\n\nUnless stated otherwise, we will sample single states in this section, but beware that the parameter updates will be noisy (high variance).\nThe approximated value is a linear combination of the features:\nV_\\varphi(s) = \\sum_{i=1}^d w_i \\, \\phi_i(s) = \\mathbf{w}^T \\times \\phi(s)\nThe weights are updated using stochastic gradient descent:\n\n    \\Delta \\mathbf{w} = \\eta \\, (V^\\pi(s) - V_\\varphi(s)) \\, \\phi(s)\n\nThis is the delta learning rule of linear regression and classification, with \\phi(s) being the input vector and V^\\pi(s) - V_\\varphi(s) the prediction error.\nThe rule can be used with any function approximator, we only need to be able to differentiate it:\n\n    \\Delta \\varphi = \\eta \\, (V^\\pi(s) - V_\\varphi(s)) \\, \\nabla_\\varphi V_\\varphi(s)\n\nThe problem is that we do not know V^\\pi(s), as it is what we are trying to estimate. We can replace V^\\pi(s) by a sampled estimate using Monte-Carlo or TD:\n\nMonte-Carlo function approximation:\n\n\n    \\Delta \\varphi = \\eta \\, (R_t - V_\\varphi(s)) \\, \\nabla_\\varphi V_\\varphi(s)\n\n\nTemporal Difference function approximation:\n\n\n    \\Delta \\varphi = \\eta \\, (r_{t+1} + \\gamma \\, V_\\varphi(s') - V_\\varphi(s)) \\, \\nabla_\\varphi V_\\varphi(s)\n\n\n\n\n\n\n\nGradient Monte Carlo Algorithm for value estimation\n\n\n\n\nInitialize the parameter \\varphi to 0 or randomly.\nwhile not converged:\n\nGenerate an episode according to the current policy \\pi until a terminal state s_T is reached.\n\n\n      \\tau = (s_o, a_o, r_ 1, s_1, a_1, \\ldots, s_T)\n  \n\nFor all encountered states s_0, s_1, \\ldots, s_{T-1}:\n\nCompute the return R_t = \\sum_k \\gamma^k r_{t+k+1} .\nUpdate the parameters using function approximation:\n\n\n     \\Delta \\varphi = \\eta \\, (R_t - V_\\varphi(s_t)) \\, \\nabla_\\varphi V_\\varphi(s_t)\n\n\n\n\n\n\n\n\n\n\n\nSemi-gradient Temporal Difference Algorithm for value estimation\n\n\n\n\nInitialize the parameter \\varphi to 0 or randomly.\nwhile not converged:\n\nStart from an initial state s_0.\nforeach step t of the episode:\n\nSelect a_t using the current policy \\pi in state s_t.\nObserve r_{t+1} and s_{t+1}.\nUpdate the parameters using function approximation:\n\n\n      \\Delta \\varphi = \\eta \\, (r_{t+1} + \\gamma \\, V_\\varphi(s_{t+1}) - V_\\varphi(s_t)) \\, \\nabla_\\varphi V_\\varphi(s_t)\n  \n\nif s_{t+1} is terminal: break\n\n\n\n\n\nAs in tabular RL, Gradient Monte-Carlo has no bias (real returns) but a high variance. Semi-gradient TD has less variance, but a significant bias as V_\\varphi(s_{t+1}) is initially wrong. You can never trust these estimates completely.\nNote that for Temporal Difference, we actually want to minimize the TD reward-prediction error for all states, i.e. the surprise:\n\\mathcal{L}(\\varphi) = \\mathbb{E}_{s \\in \\mathcal{S}} [ (r_{t+1} + \\gamma \\, V_\\varphi(s') - V_\\varphi(s))^2]= \\mathbb{E}_{s \\in \\mathcal{S}} [ \\delta_t^2]\n\n\nAction value approximation\nQ-values can be approximated by a parameterized function Q_\\theta(s, a) in the same manner. There are basically two options for the structure of the function approximator:\n\nThe FA takes a feature vector for both the state s and the action a (which can be continuous) as inputs, and outputs a single Q-value Q_\\theta(s ,a).\n\n\n\n\nAction value approximation for a single action.\n\n\n\nThe FA takes a feature vector for the state s as input, and outputs one Q-value Q_\\theta(s ,a) per possible action (the action space must be discrete).\n\n\n\n\nAction value approximation for all actions.\n\n\nIn both cases, we minimize the mse between the true value Q^\\pi(s, a) and the approximated value Q_\\theta(s, a).\n\n\n\n\n\n\nQ-learning with function approximation\n\n\n\n\nInitialize the parameters \\theta.\nwhile True:\n\nStart from an initial state s_0.\nforeach step t of the episode:\n\nSelect a_{t} using the behavior policy b (e.g. derived from \\pi).\nTake a_t, observe r_{t+1} and s_{t+1}.\nUpdate the parameters \\theta:\n\n\\Delta \\theta = \\eta \\, (r_{t+1} + \\gamma \\, \\max_a Q_\\theta(s_{t+1}, a) - Q_\\theta(s_t, a_t)) \\, \\nabla_\\theta Q_\\theta(s_t, a_t)\n\nImprove greedily the learned policy:\n\n\\pi(s_t, a) = \\text{Greedy}(Q_\\theta(s_t, a))\n\nif s_{t+1} is terminal: break"
  },
  {
    "objectID": "notes/2.6-FA.html#feature-construction",
    "href": "notes/2.6-FA.html#feature-construction",
    "title": "Function approximation",
    "section": "Feature construction",
    "text": "Feature construction\n\n\nLinear features\nBefore we dive into deep RL (i.e. RL with non-linear FA), let’s see how we can design good feature vectors for linear function approximation. The problem with deep NN is that they need a lot of samples to converge, what worsens the fundamental problem of RL: sample efficiency. By engineering the right features, we could use linear approximators, which converge much faster. The convergence of linear FA is guaranteed, not (always) non-linear ones.\nWhy do we need to choose features? For the cartpole, the feature vector \\phi(s) could be:\n \\phi(s) = \\begin{bmatrix}x \\\\ \\dot{x} \\\\ \\theta \\\\ \\dot{\\theta} \\end{bmatrix}\nwhere x is the position, \\theta the angle, \\dot{x} and \\dot{\\theta} their derivatives. Can we predict the value of a state linearly?\nV_\\varphi(s) = \\sum_{i=1}^d w_i \\, \\phi_i(s) = \\mathbf{w}^T \\times \\phi(s)\nThis answer is no, as a high angular velocity \\dot{\\theta} is good when the pole is horizontal (going up) but bad if the pole is vertical (will not stop). The value would depends linearly on something like \\dot{\\theta} \\, \\sin \\theta, which is a non-linear combination of features.\nLet’s suppose we have a simple problem where the state s is represented by two continuous variables x and y. The true value function V^\\pi(s) is a non-linear function of x and y.\n\n\n\nState values V^\\pi(s) for a two-dimensional state space.\n\n\nIf we apply linear FA directly on the feature vector [x, y], we catch the tendency of V^\\pi(s) but we make a lot of bad predictions: high bias (underfitting).\n\n\n\nLinear approximation of the state value function.\n\n\n\n\nPolynomial features\nTo introduce non-linear relationships between continuous variables, a simple method is to construct the feature with polynomials of the variables.\nExample with polynomials of order 2:\n\n    \\phi(s) = \\begin{bmatrix}1 & x & y & x\\, y & x^2 & y^2 \\end{bmatrix}^T\n\nWe transform the two input variables x and y into a vector with 6 elements. The 1 (order 0) is there to learn the offset / bias.\nExample with polynomials of order 3:\n\n    \\phi(s) = \\begin{bmatrix}1 & x & y & x\\, y & x^2 & y^2 & x^2 \\, y & x \\, y^2 & x^3 & y^3\\end{bmatrix}^T\n\nWe then just need to apply linear FA on these feature vectors (polynomial regression).\n\n    V_\\varphi(s) = w_0 + w_1 \\, x + w_2 \\, y + w_3 \\, x \\, y + w_4 \\, x^2 + w_5 \\, y^2 + \\ldots\n\n\n\n\nPolynomial approximation of the state value function with order 2.\n\n\n\n\n\nPolynomial approximation of the state value function with order 6.\n\n\nThe higher the degree of the polynomial, the better the fit, but the number of features grows exponentially. This adds to the computational complexity and leads to overfitting: if we only sample some states, high-order polynomials will not interpolate correctly.\n\n\nFourier transforms\nInstead of approximating a state variable x by a polynomial:\n\n    V_\\varphi(s) = w_0 + w_1 \\, x + w_2 \\, x^2 + w_3 \\, x^3 + \\ldots\n\nwe could also use its Fourier decomposition (here DCT, discrete cosine transform):\n\n    V_\\varphi(s) = w_0 + w_1 \\, \\cos(\\pi \\, x) + w_2 \\, \\cos( 2 \\, \\pi \\, x) + w_3 \\, \\cos(3 \\, \\pi \\, x) + \\ldots\n\nThe Fourier theorem tells us that, if we take enough frequencies, we can reconstruct the signal V_\\varphi(s) perfectly.\n\n\n\nFourier transform in 1D. Source: (Sutton and Barto, 1998).\n\n\nIt is just a change of basis, the problem stays a linear regression to find w_0, w_1, w_2, etc.\nFourier transforms can be applied on multivariate functions as well.\n\n\n\nFourier transform in 2D. Source: (Sutton and Barto, 1998).\n\n\n\n\n\nComparison of polynomial and Fourier features. Source: (Sutton and Barto, 1998).\n\n\nA Fourier basis tends to work better than a polynomial basis. The main problem is that the number of features increases very fast with the number of input dimensions and the desired precision (higher-order polynomials, more frequencies).\n\n\nDiscrete coding\nAn obvious solution for continuous state variables is to discretize the input space. The input space is divided into a grid of non-overlapping tiles.\n\n\n\nLinear approximation of the state value function using discrete coding.\n\n\nThe feature vector is a binary vector with a 1 when the input is inside a tile, 0 otherwise.\n\\phi(s) = \\begin{bmatrix}0 & 0 & \\ldots & 0 & 1 & 0 & \\ldots & 0 \\\\ \\end{bmatrix}^T\nThis ensures generalization inside a tile: you only need a couple of samples inside a tile to know the mean value of all the states. Drawbacks: the value function is step-like (discontinuous), the correct size of a tile is not known, we fall into the curse of dimensionality.\n\n\nCoarse coding\nA more efficient solution is coarse coding. The tiles (rectangles, circles, or what you need) need to overlap.\n\n\n\nCoarse coding uses overlapping tiles. Source: (Sutton and Barto, 1998).\n\n\nA state s is encoded by a binary vector, but with several 1, for each tile it belongs.\n\\phi(s) = \\begin{bmatrix}0 & 1 & 0 & \\ldots & 1 & 1 & 0 & \\ldots & 0 \\\\ \\end{bmatrix}^T\nThis allows generalization inside a tile, but also across tiles. The size and shape of the “receptive field” influences the generalization properties.\n\n\n\nThe overlap between tiles defines the generalization. Source: (Sutton and Barto, 1998).\n\n\n\n\nTile coding\nA simple way to ensure that tiles overlap is to use several regular grids with an offset. Each tiling will be coarse, but the location of a state will be quite precise as it may belong to many tiles.\n\n\n\nTile coding. Source: (Sutton and Barto, 1998).\n\n\nThis helps against the curse of dimensionality: high precision, but the number of tiles does not grow exponentially.\n\n\nRadial-basis functions (RBF)\nThe feature vector in tile coding is a binary vector: there will be discontinuous jumps in the approximated value function when moving between tiles. We can use radial-basis functions (RBF) such as Gaussians to map the state space.\n\n\n\nRadial-basis functions.\n\n\nWe set a set of centers \\{c_i\\}_{i=1}^K in the input space on a regular grid (or randomly). Each element of the feature vector will be a Gaussian function of the distance between the state s and one center:\n\\phi_i(s) = \\exp \\frac{-(s - c_i)^2}{2\\, \\sigma_i^2}\nThe approximated value function now represents continuously the states:\nV_\\varphi(s) = \\sum_{i=1}^d w_i \\, \\phi_i(s) = \\sum_{i=1}^d w_i \\, \\exp \\frac{-(s - c_i)^2}{2\\, \\sigma_i^2}\nIf you have enough centers and they overlap sufficiently, you can even decode the original state perfectly:\n\\hat{s} = \\sum_{i=1}^d \\phi_i(s) \\, c_i\n\n\n\n\n\n\nSummary of function approximation\n\n\n\n\n\n\n\n\nIn FA, we project the state information into a feature space to get a better representation. We then apply a linear approximation algorithm to estimate the value function:\nV_\\varphi(s) = \\mathbf{w}^T \\, \\phi(s)\nThe linear FA is trained using some variant of gradient decent:\n\\Delta \\mathbf{w} = \\eta \\, (V^\\pi(s) - V_\\varphi(s)) \\, \\phi(s)\nDeep neural networks are the most powerful function approximators in supervised learning. Do they also work with RL?\n\n\n\n\n\n\nSutton, R. S., and Barto, A. G. (1998). Reinforcement Learning: An introduction. Cambridge, MA: MIT press."
  },
  {
    "objectID": "notes/2.7-NN.html",
    "href": "notes/2.7-NN.html",
    "title": "Deep learning",
    "section": "",
    "text": "Slides: html pdf"
  },
  {
    "objectID": "notes/2.7-NN.html#feedforward-neural-networks",
    "href": "notes/2.7-NN.html#feedforward-neural-networks",
    "title": "Deep learning",
    "section": "Feedforward neural networks",
    "text": "Feedforward neural networks\n\nAn artificial neural network (ANN) is a cascade of fully-connected (FC) layers of artificial neurons.\n\n\n\nShallow vs. deep neural networks.\n\n\nEach layer k transforms an input vector \\mathbf{h}_{k-1} into an output vector \\mathbf{h}_{k} using a weight matrix W_k, a bias vector \\mathbf{b}_k and an activation function f().\n\\mathbf{h}_{k} = f(W_k \\times \\mathbf{h}_{k-1} + \\mathbf{b}_k)\nOverall, ANNs are non-linear parameterized function estimators from the inputs \\mathbf{x} to the outputs \\mathbf{y} with parameters \\theta (all weight matrices and biases).\n\\mathbf{y} = F_\\theta (\\mathbf{x})\nANNs can be used for both regression (continuous outputs) and classification (discrete outputs) tasks. In supervised learning, we have a fixed training set \\mathcal{D} of N samples (\\mathbf{x}_t, \\mathbf{t}_i), where t_i is the desired output or target.\n\nRegression:\n\nThe output layer uses a linear activation function: f(x) = x\nThe network minimizes the mean square error (mse) of the model on the training set:\n\n\\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\in \\mathcal{D}} [ ||\\mathbf{t} - \\mathbf{y}||^2 ]\nClassification:\n\nThe output layer uses the softmax operator to produce a probabilty distribution: y_j = \\frac{e^{z_j}}{\\sum_k e^{z_k}}\nThe network minimizes the cross-entropy or negative log-likelihood of the model on the training set:\n\n\\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\in \\mathcal{D}} [ - \\mathbf{t} \\, \\log \\mathbf{y} ]\n\nThe cross-entropy between two probability distributions X and Y measures their similarity:\n\n    H(X, Y) = \\mathbb{E}_{x \\sim X}[- \\log P(Y=x)]\n\nIt measures whether samples from X are likely under Y? Minimizing the cross-entropy makes the two distributions equal almost anywhere.\n\n\n\nThe cross-entropy measures the similarity of the two probability distributions.\n\n\nIn supervised learning, the targets \\mathbf{t} are fixed one-hot encoded vectors.\n\\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\in \\mathcal{D}} [ - \\sum_j t_j \\, \\log y_{j} ]\nBut it could be any target distribution.\n\n\n\nCross-entropy between multinomial distributions.\n\n\nIn both cases, we want to minimize the loss variant by applying Stochastic Gradient Descent (SGD) or a variant (Adam, RMSprop).\n\n    \\Delta \\theta = - \\eta \\, \\nabla_\\theta \\mathcal{L}(\\theta)\n\nThe question is how to compute the gradient of the loss function w.r.t the parameters \\theta. For both the mse and cross-entropy loss functions, we have:\n\\nabla_\\theta \\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [- (\\mathbf{t} - \\mathbf{y}) \\, \\nabla_\\theta \\, \\mathbf{y}]\nThere is an algorithm to compute efficiently the gradient of the output w.r.t the parameters: backpropagation (see Neurocomputing). In deep RL, we do not care about backprop: tensorflow or pytorch do it for us.\n\n\n\nPrinciple of deep reinforcement learning.\n\n\nThere are three aspects to consider when building a neural network:\n\nArchitecture: how many layers, what type of layers, how many neurons, etc.\n\nTask-dependent: each RL task will require a different architecture. Not our focus.\n\nLoss function: what should the network do?\n\nCentral to deep RL!\n\nUpdate rule how should we update the parameters \\theta to minimize the loss function? SGD, backprop.\n\nNot really our problem, but see natural gradients later."
  },
  {
    "objectID": "notes/2.7-NN.html#convolutional-neural-networks",
    "href": "notes/2.7-NN.html#convolutional-neural-networks",
    "title": "Deep learning",
    "section": "Convolutional neural networks",
    "text": "Convolutional neural networks\n\nWhen using images as inputs, fully-connected networks (FCN) would have too many weights: slow learning and overfitting.\n\n\n\nFully-connected layers necessitate too many parameters on images.\n\n\nConvolutional layers reduce the number of weights by reusing weights at different locations. This the principle of a convolution, which leads to fast and efficient learning.\n\n\n\nConvolutional layers share weights on the image.\n\n\nA convolutional layer extracts features of its inputs. d filters are defined with very small sizes (3x3, 5x5…). Each filter is convoluted over the input image (or the previous layer) to create a feature map. The set of d feature maps becomes a new 3D structure: a tensor. If the input image is 32x32x3, the resulting tensor will be 32x32xd. The convolutional layer has only very few parameters: each feature map has 3x3 values in the filter and a bias, i.e. 10 parameters. The convolution operation is differentiable: backprop will work.\n\n\n\nConvolutional layer. Source: http://cs231n.github.io/convolutional-networks/\n\n\nThe number of elements in a convolutional layer is still too high. We need to reduce the spatial dimension of a convolutional layer by downsampling it. For each feature, a max-pooling layer takes the maximum value of a feature for each subregion of the image (mostly 2x2). Pooling allows translation invariance: the same input pattern will be detected whatever its position in the input image. Max-pooling is also differentiable.\n\n\n\nConvolutional layer. Source: http://cs231n.github.io/convolutional-networks/\n\n\nA convolutional neural network (CNN) is a cascade of convolution and pooling operations, extracting layer by layer increasingly complex features. The spatial dimensions decrease after each pooling operation, but the number of extracted features increases after each convolution. One usually stops when the spatial dimensions are around 7x7. The last layers are fully connected. Can be used for regression and classification depending on the output layer and the loss function. Training a CNN uses backpropagation all along: the convolution and pooling operations are differentiable.\n\n\n\nConvolutional neural network.\n\n\nThe only thing we need to know is that CNNs are non-linear function approximators that work well with images.\n\\mathbf{y} = F_\\theta (\\mathbf{x})\nThe convolutional layers extract complex features from the images through learning. The last FC layers allow to approximate values (regression) or probability distributions (classification)."
  },
  {
    "objectID": "notes/2.7-NN.html#autoencoders",
    "href": "notes/2.7-NN.html#autoencoders",
    "title": "Deep learning",
    "section": "Autoencoders",
    "text": "Autoencoders\n\nThe problem with FCN and CNN is that they extract features in supervised learning tasks: Need for a lot of annotated data (image, label). Autoencoders allows unsupervised learning, as they only need inputs (images). Their task is to reconstruct the input:\n\\mathbf{y} = \\mathbf{\\tilde{x}} \\approx \\mathbf{x}\nThe reconstruction loss is simply the mse between the input and its reconstruction.\n\n    \\mathcal{L}_\\text{autoencoder}(\\theta) = \\mathbb{E}_{\\mathbf{x} \\in \\mathcal{D}} [ ||\\mathbf{\\tilde{x}} - \\mathbf{x}||^2 ]\n\nApart from the loss function, they are trained as regular NNs. Autoencoders consists of:\n\nthe encoder: from the input \\mathbf{x} to the latent space \\mathbf{z}.\nthe decoder: from the latent space \\mathbf{z} to the reconstructed input \\mathbf{\\tilde{x}}.\n\n\n\n\nAutoencoder. Source: https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html\n\n\nThe latent space \\mathbf{z} is a compressed representation (bottleneck) of the inputs \\mathbf{x}. It has to learn to compress efficiently the inputs without losing too much information, in order to reconstruct the inputs: Dimensionality reduction, unsupervised feature extraction.\nIn deep RL, we can construct the feature vector with an autoencoder. The autoencoder can be trained offline with a random agent or online with the current policy (auxiliary loss).\n\n\n\nAutoencoders can be used in RL to find a feature space where linear FA can apply easily."
  },
  {
    "objectID": "notes/2.7-NN.html#recurrent-neural-networks",
    "href": "notes/2.7-NN.html#recurrent-neural-networks",
    "title": "Deep learning",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\nFCN, CNN and AE are feedforward neural networks: they transform an input \\mathbf{x} into an output \\mathbf{y}:\n\\mathbf{y} = F_\\theta(\\mathbf{x})\nIf you present a sequence of inputs \\mathbf{x}_0, \\mathbf{x}_1, \\ldots, \\mathbf{x}_t to a feedforward network, the outputs will be independent from each other:\n\\mathbf{y}_0 = F_\\theta(\\mathbf{x}_0) \\mathbf{y}_1 = F_\\theta(\\mathbf{x}_1) \\dots \\mathbf{y}_t = F_\\theta(\\mathbf{x}_t)\nThe output \\mathbf{y}_t does not depend on the history of inputs \\mathbf{x}_0, \\mathbf{x}_1, \\ldots, \\mathbf{x}_{t-1}. This not always what you want. If your inputs are frames of a video, the correct response at time t might also depend on previous frames. The task of the NN could be to explain what happens at each frame. As we saw, a single frame is often not enough to predict the future (Markov property).\n\n\n\nRecurrent neural network. Source: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n\n\nA recurrent neural network (RNN) uses it previous output as an additional input (context). All vectors have a time index t denoting the time at which this vector was computed. The input vector at time t is \\mathbf{x}_t, the output vector is \\mathbf{h}_t:\n\n    \\mathbf{h}_t = f(W_x \\times \\mathbf{x}_t + W_h \\times \\mathbf{h}_{t-1} + \\mathbf{b})\n\nThe input \\mathbf{x}_t and previous output \\mathbf{h}_{t-1} are multiplied by learnable weights:\n\nW_x is the input weight matrix.\nW_h is the recurrent weight matrix.\n\nThis is equivalent to a deep neural network taking the whole history \\mathbf{x}_0, \\mathbf{x}_1, \\ldots, \\mathbf{x}_t as inputs, but reusing weights between two time steps. The weights are trainable using backpropagation through time (BPTT). A RNN can learn the temporal dependencies between inputs.\n\n\n\nRecurrent neural network, unrolled. Source: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n\n\nA popular variant of RNN is LSTM (long short-term memory). In addition to the input \\mathbf{x}_t and output \\mathbf{h}_t, it also has a state (or memory or context) \\mathbf{C}_t which is maintained over time. It also contains three multiplicative gates:\n\nThe input gate controls which inputs should enter the memory.\nThe forget gate controls which memory should be forgotten.\nThe output gate controls which part of the memory should be used to produce the output.\n\n\n\n\nLSTM cell. Source: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n\n\nAn obvious use case of RNNs in deep RL is for POMDP (partially observable MDP). If the individual states s_t do not have the Markov property, the output of a LSTM does: The output of the RNN is a representation of the complete history s_0, s_1, \\ldots, s_t. We can apply RL on the output of a RNN and solve POMDPs for free!\n\n\n\nLSTM layers help solving POMDP by concatenating and compressing the history. Source: https://deepmind.com/blog/article/capture-the-flag-science"
  },
  {
    "objectID": "notes/3.1-DQN.html",
    "href": "notes/3.1-DQN.html",
    "title": "Deep Q-Learning (DQN)",
    "section": "",
    "text": "Slides: html pdf"
  },
  {
    "objectID": "notes/3.1-DQN.html#value-based-deep-rl",
    "href": "notes/3.1-DQN.html#value-based-deep-rl",
    "title": "Deep Q-Learning (DQN)",
    "section": "Value-based deep RL",
    "text": "Value-based deep RL\n\nThe basic idea in value-based deep RL is to approximate the Q-values in each possible state, using a deep neural network with free parameters \\theta:\nQ_\\theta(s, a) \\approx Q^\\pi(s, a) = \\mathbb{E}_\\pi (R_t | s_t=s, a_t=a)\nThe Q-values now depend on the parameters \\theta of the DNN. The derived policy \\pi_\\theta uses for example an \\epsilon-greedy or softmax action selection scheme over the estimated Q-values:\n\n    \\pi_\\theta(s, a) \\leftarrow \\text{Softmax} (Q_\\theta(s, a))\n\nThere are two possibilities to approximate Q-values Q_\\theta(s, a):\n\nThe DNN approximates the Q-value of a single (s, a) pair. The action space can be continuous.\n\n\n\n\nAction value approximation for a single action.\n\n\n\nThe DNN approximates the Q-value of all actions a in a state s. The action space must be discrete (one output neuron per action).\n\n\n\n\nAction value approximation for all actions.\n\n\nWe could simply adapt Q-learning with FA to the DNN:\n\n\n\n\n\n\nNaive deep Q-learning with function approximation\n\n\n\n\nInitialize the deep neural network with parameters \\theta.\nStart from an initial state s_0.\nfor t \\in [0, T_\\text{total}]:\n\nSelect a_{t} using a softmax over the Q-values Q_\\theta(s_t, a).\nTake a_t, observe r_{t+1} and s_{t+1}.\nUpdate the parameters \\theta by minimizing the loss function:\n\n\\mathcal{L}(\\theta) = (r_{t+1} + \\gamma \\, \\max_{a'} Q_\\theta(s_{t+1}, a') - Q_\\theta(s_t, a_t))^2\n\n\n\nThis naive approach will not work: DNNs cannot learn from single examples (online learning = instability). DNNs require stochastic gradient descent (SGD):\n\n    \\mathcal{L}(\\theta) = E_\\mathcal{D} (||\\textbf{t} - \\textbf{y}||^2) \\approx \\frac{1}{K} \\sum_{i=1}^K ||\\textbf{t}_i - \\textbf{y}_i||^2\n\nThe loss function is estimated by sampling a minibatch of K i.i.d samples from the training set to compute the loss function and update the parameters \\theta. This is necessary to avoid local minima of the loss function. Although Q-learning can learn from single transitions, it is not possible using DNN. Why not using the last K transitions to train the network? We could store them in a transition buffer and train the network on it.\n\n\n\n\n\n\nNaive deep Q-learning with a transition buffer\n\n\n\n\nInitialize the deep neural network with parameters \\theta.\nInitialize an empty transition buffer \\mathcal{D} of size K: \\{(s_k, a_k, r_k, s'_k)\\}_{k=1}^K.\nfor t \\in [0, T_\\text{total}]:\n\nSelect a_{t} using a softmax over the Q-values Q_\\theta(s_t, a).\nTake a_t, observe r_{t+1} and s_{t+1}.\nStore (s_t, a_t, r_{t+1}, s_{t+1}) in the transition buffer.\nEvery K steps:\n\nUpdate the parameters \\theta using the transition buffer:\n\n\\mathcal{L}(\\theta) = \\frac{1}{K} \\, \\sum_{k=1}^K (r_k + \\gamma \\, \\max_{a'} Q_\\theta(s'_k, a') - Q_\\theta(s_k, a_k))^2\n\nEmpty the transition buffer.\n\n\n\n\n\n\nCorrelated inputs\nUnfortunately, this does not work either. The last K transitions (s, a, r, s') are not i.i.d (independent and identically distributed). The transition (s_{t+1}, a_{t+1}, r_{t+2}, s_{t+2}) depends on (s_{t}, a_{t}, r_{t+1}, s_{t+1}) by definition, i.e. the transitions are correlated. Even worse, when playing video games, successive frames will be very similar or even identical.\n\n\n\nSuccessive video frames are extremely correlated.\n\n\nThe actions are also correlated: you move the paddle to the left for several successive steps.\nFeeding transitions sequentially to a DNN is the same as giving all MNIST 0’s to a DNN, then all 1’s, etc… It does not work.\n\n\n\nCorrelated vs. uniformaly sampled MNIST digits.\n\n\nIn SL, we have all the training data before training: it is possible to get i.i.d samples by shuffling the training set between two epochs. In RL, we create the “training set” (transitions) during training: the samples are not i.i.d as we act sequentially over time.\n\n\nNon-stationary targets\nIn SL, the targets \\mathbf{t} do not change over time: an image of a cat stays an image of a cat throughout learning.\n\\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\sim \\mathcal{D}} [||\\mathbf{t} - F_\\theta(\\mathbf{x})||^2]\nThe problem is said stationary, as the distribution of the data does not change over time.\nIn RL, the targets t = r + \\gamma \\, \\max_{a'} Q_\\theta(s', a') do change over time:\n\nQ_\\theta(s', a') depends on \\theta, so after one optimization step, all targets have changed!\nAs we improve the policy over training, we collect higher returns.\n\n\\mathcal{L}(\\theta) = \\mathbb{E}_{s, a \\sim \\pi_\\theta} [(r + \\gamma \\, \\max_{a'} Q_\\theta(s', a') - Q_\\theta(s, a))^2]\nNeural networks do not like this at all. After a while, they give up and settle on a suboptimal policy.\n\n\n\nSupervised learning has stationary targets, not RL. Learning is much less efficient and optimal in RL."
  },
  {
    "objectID": "notes/3.1-DQN.html#deep-q-network-dqn",
    "href": "notes/3.1-DQN.html#deep-q-network-dqn",
    "title": "Deep Q-Learning (DQN)",
    "section": "Deep Q-network (DQN)",
    "text": "Deep Q-network (DQN)\n\nNon-linear approximators never really worked with RL before 2013 because of:\n\nThe correlation between successive inputs or outputs.\nThe non-stationarity of the problem.\n\nThese two problems are very bad for deep networks, which end up overfitting the learned episodes or not learning anything at all. Deepmind researchers (Mnih et al., 2013) proposed to use two classical ML tricks to overcome these problems:\n\nexperience replay memory.\ntarget networks.\n\n\nExperience replay memory\nTo avoid correlation between samples, (Mnih et al. 2015) proposed to store the (s, a, r, s') transitions in a huge experience replay memory or replay buffer \\mathcal{D} (e.g. 1 million transitions).\n\n\n\nExperience replay memory / replay buffer.\n\n\nWhen the buffer is full, we simply overwrite old transitions. The Q-learning update is only applied on a random minibatch of those past experiences, not the last transitions. This ensure the independence of the samples (non-correlated samples).\n\n\n\n\n\n\nNaive deep Q-learning with experience replay memory\n\n\n\n\nInitialize value network Q_{\\theta}.\nInitialize experience replay memory \\mathcal{D} of maximal size N.\nfor t \\in [0, T_\\text{total}]:\n\nSelect an action a_t based on Q_\\theta(s_t, a), observe s_{t+1} and r_{t+1}.\nStore (s_t, a_t, r_{t+1}, s_{t+1}) in the experience replay memory.\nEvery T_\\text{train} steps:\n\nSample a minibatch \\mathcal{D}_s randomly from \\mathcal{D}.\nFor each transition (s_k, a_k, r_k, s'_k) in the minibatch:\n\nCompute the target value t_k = r_k + \\gamma \\, \\max_{a'} Q_{\\theta}(s'_k, a')\n\nUpdate the value network Q_{\\theta} on \\mathcal{D}_s to minimize:\n\n\\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathcal{D}_s}[(t_k - Q_\\theta(s_k, a_k))^2]\n\n\n\n\nBut wait! The samples of the minibatch are still not i.i.d, as they are not identically distributed:\n\nSome samples were generated with a very old policy \\pi_{\\theta_0}.\nSome samples have been generated recently by the current policy \\pi_\\theta.\n\nThe samples of the minibatch do not come from the same distribution, so this should not work, except if you use an off-policy algorithm, such as Q-learning!\nQ^\\pi(s, a) = \\mathbb{E}_{s_t \\sim \\rho_b, a_t \\sim b}[ r_{t+1} + \\gamma \\, \\max_a Q^\\pi(s_{t+1}, a) | s_t = s, a_t=a]\nIn Q-learning, you can take samples from any behavior policy b, as long as the coverage assumption stands:\n \\pi(s,a) > 0 \\Rightarrow b(s,a) > 0\nHere, the behavior policy b is a kind of “superset” of all past policies \\pi used to fill the ERM, so it “covers” the current policy.\nb = \\{\\pi_{\\theta_0}, \\pi_{\\theta_1}, \\ldots, \\pi_{\\theta_t}\\}\nSamples from b are i.i.d, so Q-learning is going to work.\n\n\n\n\n\n\nNote\n\n\n\nIt is not possible to use an experience replay memory with on-policy algorithms.\nQ^\\pi(s, a) = \\mathbb{E}_{s_t \\sim \\rho_\\pi, a_t \\sim \\pi}[ r_{t+1} + \\gamma \\, Q^\\pi(s_{t+1}, a_{t+1}) | s_t = s, a_t=a]\na_{t+1} \\sim \\pi_\\theta would not be the same between \\pi_{\\theta_0} (which generated the sample) and \\pi_{\\theta_t} (the current policy).\n\n\n\n\nTarget network\nThe second problem when using DNN for RL is that the target is non-stationary, i.e. it changes over time: as the network becomes better, the Q-values have to increase.\nIn DQN, the target for the update is not computed from the current deep network \\theta:\n\n    r + \\gamma \\, \\max_{a'} Q_\\theta(s', a')\n\nbut from a target network \\theta´ updated only every few thousands of iterations.\n\n    r + \\gamma \\, \\max_{a'} Q_{\\theta'}(s', a')\n\n\\theta' is simply a copy of \\theta from the past.\n\n\n\nTarget network.\n\n\nThe DQN loss function becomes:\n\n    \\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [(r + \\gamma \\, \\max_{a'} Q_{\\theta'}(s', a')) - Q_\\theta(s, a))^2]\n\nThis allows the target r + \\gamma \\, \\max_{a'} Q_{\\theta'}(s', a') to be stationary between two updates. It leaves time for the trained network to catch up with the targets.\n\n\n\nThe target network keeps the target constant long enough for the DNN to catch up.\n\n\nThe target network is updated by simply replacing the parameters \\theta' with the current trained parameters \\theta:\n\\theta' \\leftarrow \\theta\nThe value network \\theta basically learns using an older version of itself…\n\n\nDQN algorithm\n\n\n\n\n\n\nDQN: Deep Q-network algorithm\n\n\n\n\nInitialize value network Q_{\\theta} and target network Q_{\\theta'}.\nInitialize experience replay memory \\mathcal{D} of maximal size N.\nfor t \\in [0, T_\\text{total}]:\n\nSelect an action a_t based on Q_\\theta(s_t, a), observe s_{t+1} and r_{t+1}.\nStore (s_t, a_t, r_{t+1}, s_{t+1}) in the experience replay memory.\nEvery T_\\text{train} steps:\n\nSample a minibatch \\mathcal{D}_s randomly from \\mathcal{D}.\nFor each transition (s_k, a_k, r_k, s'_k) in the minibatch:\n\nCompute the target value t_k = r_k + \\gamma \\, \\max_{a'} Q_{\\theta'}(s'_k, a') using the target network.\n\nUpdate the value network Q_{\\theta} on \\mathcal{D}_s to minimize:\n\n\\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathcal{D}_s}[(t_k - Q_\\theta(s_k, a_k))^2]\nEvery T_\\text{target} steps:\n\nUpdate target network: \\theta' \\leftarrow \\theta.\n\n\n\n\n\nThe deep network can be anything. Deep RL is only about defining the loss function adequately. For pixel-based problems (e.g. video games), convolutional neural networks (without max-pooling) are the weapon of choice.\n\n\n\nArchitecture of DQN (Mnih et al., 2013).\n\n\nWhy no max-pooling? The goal of max-pooling is to get rid of the spatial information in the image. For object recognition, you do not care whether the object is in the center or on the side of the image. Max-pooling brings spatial invariance. In video games, you want to keep the spatial information: the optimal action depends on where the ball is relative to the paddle.\nAre individual frames good representations of states? Using video frames as states breaks the Markov property: the speed and direction of the ball is a very relevant information for the task, but not contained in a single frame. This characterizes a Partially-observable Markov Decision Process (POMDP).\nThe simple solution retained in the original DQN paper is to stack the last four frames to form the state representation. Having the previous positions of the ball, the network can learn to infer its direction of movement.\n\n\n\n\n\n\nDQN code in Keras\n\n\n\n\nCreating the CNN in keras / tensorflow / pytorch is straightforward:\n\nmodel = Sequential()\n\nmodel.add(Input((4, 84, 84)))\n\nmodel.add(Conv2D(16, (8, 8), strides=(4, 4)), activation='relu'))\n\nmodel.add(Conv2D(32, (4, 4), strides=(2, 2), activation='relu'))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(256, activation='relu'))\n\nmodel.add(Dense(nb_actions, activation='linear'))\n\noptimizer = RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)\n\nmodel.compile(optimizer, loss='mse')\n\nEach step of the algorithm follows the GPI approach:\n\ndef q_iteration(env, model, state, memory):\n    # Choose the action with epsilon-greedy\n    if np.random.random() < epsilon:\n        action = env.action_space.sample()\n    else:\n        # Predict the Q-values for the current state and take the greedy action\n        values = model.predict([state])[0]\n        action = values.argmax()\n\n    # Play one game iteration\n    new_state, reward, done, _ = env.step(action)\n\n    # Append the transition to the replay buffer \n    memory.add(state, action, new_state, reward, done)\n\n    # Sample a minibatch from the memory and fit the DQN\n    s, a, r, s_, d = memory.sample_batch(32)\n    fit_batch(model, s, a, r, s_, d)\n\nThe only slight difficulty is actually to compute the targets for learning:\n\ndef fit_batch(model, states, actions, rewards, next_states, dones)\n\n    # Predict the Q-values in the current state\n    Q_values = model.predict(states)\n    \n    # Predict the Q-values in the next state using the target model\n    next_Q_value = target_model.predict(next_states).max(axis=1)\n    \n    # Terminal states have a value of 0\n    next_Q_value[dones] = 0.0\n    \n    # Compute the target\n    targets = Q_values.copy()\n    for i in range(batch_size):\n        targets[i, actions[i]] = rewards[i] + self.gamma * next_Q_value[i]\n        \n    # Train the model on the minibatch\n    self.model.fit(states, targets, epochs=1, batch_size=batch_size, verbose=0)\n\n\n\n\nDQN results\nDQN was trained using 50M frames (38 days of game experience) per game. Replay buffer of 1M frames. Action selection: \\epsilon-greedy with \\epsilon = 0.1 and annealing. Optimizer: RMSprop with a batch size of 32.\nThe DQN network was trained to solve 49 different Atari 2600 games with the same architecture and hyperparameters. In most of the games, the network reaches super-human performance. Some games are still badly performed (e.g. Montezuma’s revenge), as they require long-term planning. It was the first RL algorithm able to learn different tasks (no free lunch theorem). The 2015 paper in Nature started the hype for deep RL.\n\n\n\nTraining curves of DQN (Mnih et al., 2013).\n\n\n\n\n\nPerformance of DQN on Atari games (Mnih et al., 2013)."
  },
  {
    "objectID": "notes/3.1-DQN.html#dqn-variants",
    "href": "notes/3.1-DQN.html#dqn-variants",
    "title": "Deep Q-Learning (DQN)",
    "section": "DQN variants",
    "text": "DQN variants\n\n\nDouble DQN\nQ-learning methods, including DQN, tend to overestimate Q-values, especially for the non-greedy actions:\nQ_\\theta(s, a) > Q^\\pi(s, a)\nThis does not matter much in action selection, as we apply \\epsilon-greedy or softmax on the Q-values anyway, but it may make learning slower (sample complexity) and less optimal.\n\n\n\nQ-learning methods overstimate Q-values (van Hasselt et al., 2015).\n\n\nTo avoid optimistic estimations, the target is computed by both the value network \\theta and the target network \\theta':\n\nAction selection: The next greedy action a^* is calculated by the value network \\theta (current policy):\n\na^* =\\text{argmax}_{a'} Q_{\\theta}(s', a')\n\nAction evaluation: Its Q-value for the target is calculated using the target network \\theta' (older values):\n\nt = r + \\gamma \\, Q_{\\theta'}(s´, a^*)\nThis gives the following loss function for double DQN (DDQN, (van Hasselt et al., 2015)):\n\n    \\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [(r + \\gamma \\, Q_{\\theta'}(s´, \\text{argmax}_{a'} Q_{\\theta}(s', a')) - Q_\\theta(s, a))^2]\n\n\n\n\nEstimated Q-values by Double DQN compared to DQN and the ground truth (van Hasselt et al., 2015).\n\n\n\n\nPrioritized Experience Replay\nThe experience replay memory or replay buffer is used to store the last 1M or so transitions (s, a, r, s'). The learning algorithm uniformly samples a minibatch of size K to update its parameters.\nNot all transitions are interesting:\n\nSome transitions were generated by a very old policy, the current policy won’t take them anymore.\nSome transitions are already well predicted: the TD error is small, there is nothing to learn from.\n\n\\delta_t = r_{t+1} + \\gamma \\, \\max_{a'} Q_\\theta(s_{t+1}, a_{t+1}) - Q_\\theta(s_t, a_t) \\approx 0\nThe experience replay memory makes learning very slow, as we need a lot of samples to learn something useful: high sample complexity. We need a smart mechanism to preferentially pick the transitions that will boost learning the most, without introducing a bias.\nPrioritized sweeping is actually a quite old idea (Moore and Atkeson, 1993). The idea of prioritized experience replay (PER, (Schaul et al., 2015)) is to sample in priority those transitions whose TD error is the highest:\n\\delta_t = r_{t+1} + \\gamma \\, \\max_{a'} Q_\\theta(s_{t+1}, a_{t+1}) - Q_\\theta(s_t, a_t)\nIn practice, we insert the transition (s, a, r, s', \\delta) into the replay buffer. To create a minibatch, the sampling algorithm select a transition k based on the probability:\nP(k) = \\frac{(|\\delta_k| + \\epsilon)^\\alpha}{\\sum_k (|\\delta_k| + \\epsilon)^\\alpha}\n\\epsilon is a small parameter ensuring that transition with no TD error still get sampled from time to time. \\alpha allows to change the behavior from uniform sampling (\\alpha=0, as in DQN) to fully prioritized sampling (\\alpha=1). \\alpha should be annealed from 0 to 1 during training. Think of it as a “kind of” softmax over the TD errors. After the samples have been used for learning, their TD error \\delta is updated in the PER.\nThe main drawback is that inserting and sampling can be computationally expensive is we simply sort the transitions based on (|\\delta_k| + \\epsilon)^\\alpha:\n\nInsertion: \\mathcal{O}(N \\, \\log N).\nSampling: \\mathcal{O}(N).\n\n\n\n\nSorting transitions w.r.t their advantage is expensive. Source: https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/\n\n\nUsing binary sumtrees instead of a linear queue, prioritized experience replay can be made efficient in both insertion (\\mathcal{O}(\\log N)) and sampling (\\mathcal{O}(1)).\n\n\n\nSumtrees allow efficient insertion and sampling of the PER. Source: https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682\n\n\n\n\n\nDQN with PER outperforms DQN (Schaul et al., 2015).\n\n\n\n\n\nDQN with PER outperforms DQN (Schaul et al., 2015).\n\n\n\n\nDueling networks\nDQN and its variants learn to predict directly the Q-value of each available action.\n\n\n\nDQN predicts directly the Q-values. Source: (Wang et al., 2016).\n\n\nThere are several problems with predicting Q-values with a DNN:\n\nThe Q-values can take high values, especially with different values of \\gamma.\nThe Q-values have a high variance, between the minimum and maximum returns obtained during training.\nFor a transition (s_t, a_t, s_{t+1}), a single Q-value is updated, not all actions in s_t.\n\n\n\n\nThe variance of the Q-values between good and bad states is high.\n\n\nThe exact Q-values of all actions are not equally important.\n\nIn bad states (low V^\\pi(s)), you can do whatever you want, you will lose.\nIn neutral states, you can do whatever you want, nothing happens.\nIn good states (high V^\\pi(s)), you need to select the right action to get rewards, otherwise you lose.\n\nAn important notion is the advantage A^\\pi(s, a) of an action:\n\n    A^\\pi(s, a) = Q^\\pi(s, a) - V^\\pi(s)\n\nIt tells how much return can be expected by taking the action a in the state s, compared to what is usually obtained in s with the current policy. If a policy \\pi is deterministic and always selects a^* in s, we have:\n\n    A^\\pi(s, a^*) = 0\n \n    A^\\pi(s, a \\neq a^*) < 0\n\nThis is particularly true for the optimal policy. But if we have separate estimates V_\\varphi(s) and Q_\\theta(s, a), some actions may have a positive advantage. Advantages have less variance than Q-values.\n\n\n\nThe variance of the advantages is much lower.\n\n\nIn dueling networks (Wang et al., 2016), the network is forced to decompose the estimated Q-value Q_\\theta(s, a) into a state value V_\\alpha(s) and an advantage function A_\\beta(s, a):\n\n    Q_\\theta(s, a) = V_\\alpha(s) + A_\\beta(s, a)\n\n\n\n\nDueling DQN decomposes the Q-values as the sum of the V-value and the advantage of the action. Source: (Wang et al., 2016).\n\n\nThe parameters \\alpha and \\beta are just two shared subparts of the NN \\theta. The loss function\n\\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [(r + \\gamma \\, Q_{\\theta'}(s´, \\text{argmax}_{a'} Q_{\\theta}(s', a')) - Q_\\theta(s, a))^2]\nis exactly the same as in (D)DQN: only the internal structure of the NN changes.\nThe Q-values are the sum of two functions:\n\n    Q_\\theta(s, a) = V_\\alpha(s) + A_\\beta(s, a)\n\nHowever, this sum is unidentifiable:\n\n\\begin{aligned}\nQ_\\theta(s, a) = 10 & = 1 + 9 \\\\\n                    & = 2 + 8 \\\\\n                    & = 3 + 7 \\\\\n\\end{aligned}\n\nTo constrain the sum, (Wang et al. 2016) propose that the greedy action w.r.t the advantages should have an advantage of 0:\n\n    Q_\\theta(s, a) = V_\\alpha(s) + (A_\\beta(s, a) - \\max_{a'} A_\\beta(s, a'))\n\nThis way, there is only one solution to the addition. The operation is differentiable, so backpropagation will work. (Wang et al. 2016) show that subtracting the mean advantage works better in practice:\n\n    Q_\\theta(s, a) = V_\\alpha(s) + (A_\\beta(s, a) - \\frac{1}{|\\mathcal{A}|} \\, \\sum_{a'} A_\\beta(s, a'))\n\n\n\n\nDueling DQN network improves over double DQN with PER on most Atari games. Source: (Wang et al., 2016).\n\n\n\n\n\n\n\n\nSummary of DQN algorithms\n\n\n\n\n\n\nArchitecture of DQN (Mnih et al., 2013).\n\n\nDQN and its early variants (double duelling DQN with PER) are an example of value-based deep RL. The value Q_\\theta(s, a) of each possible action in a given state is approximated by a convolutional neural network. The NN has to minimize the mse between the predicted Q-values and the target value corresponding to the Bellman equation:\n\\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [(r + \\gamma \\, Q_{\\theta'}(s´, \\text{argmax}_{a'} Q_{\\theta}(s', a')) - Q_\\theta(s, a))^2]\nThe use of an experience replay memory and of target networks allows to stabilize learning and avoid suboptimal policies. The main drawback of DQN is sample complexity: it needs huge amounts of experienced transitions to find a correct policy. The sample complexity come from the deep network itself (gradient descent is iterative and slow), but also from the ERM: it contains 1M transitions, most of which are outdated. Value-based algorithms only work for small and discrete action spaces (one output neuron per action).\n\n\n\n\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., et al. (2013). Playing Atari with Deep Reinforcement Learning. http://arxiv.org/abs/1312.5602.\n\n\nMoore, A. W., and Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning with less data and less time. Mach Learn 13, 103–130. doi:10.1007/BF00993104.\n\n\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. (2015). Prioritized Experience Replay. http://arxiv.org/abs/1511.05952.\n\n\nvan Hasselt, H., Guez, A., and Silver, D. (2015). Deep Reinforcement Learning with Double Q-learning. http://arxiv.org/abs/1509.06461.\n\n\nWang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and de Freitas, N. (2016). Dueling Network Architectures for Deep Reinforcement Learning. http://arxiv.org/abs/1511.06581."
  },
  {
    "objectID": "notes/3.2-BeyondDQN.html",
    "href": "notes/3.2-BeyondDQN.html",
    "title": "Beyond DQN",
    "section": "",
    "text": "Slides: html pdf"
  },
  {
    "objectID": "notes/3.2-BeyondDQN.html#distributional-learning-categorical-dqn",
    "href": "notes/3.2-BeyondDQN.html#distributional-learning-categorical-dqn",
    "title": "Beyond DQN",
    "section": "Distributional learning : Categorical DQN",
    "text": "Distributional learning : Categorical DQN\n\n\nWhy learning distributions of returns?\nUntil now, we have only cared about the expectation of the returns, i.e. their mean value:\nV^\\pi(s) = \\mathbb{E}_\\pi [R_t | s_t = s]\nQ^\\pi(s, a) = \\mathbb{E}_\\pi [R_t | s_t = s, a_t = a]\nWe select actions with the highest expected return, which makes sense on the long term.\nSuppose we have two actions a_1 and a_2, which provide different returns with the same probability:\n\nR(a_1) = \\{100, 200\\}\nR(a_2) = \\{-100, 400\\}\n\nTheir Q-value is the same: Q(a_1) = Q(a_2) = 150, so if you play them an infinity of times, they are both optimal. But suppose that, after learning, you can only try a single action. Which one do you chose? RL does not distinguish safe from risky actions.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\n\n\nThe trip by train from Leipzig to Chemnitz takes 1 hour if everything goes well. Once a week on average, the train will get stuck on the way for 30 minutes. The expected duration of the trip is 1h + 1/5*30 = 1h06. But in practice it takes either 1h or 1h30, never 1h06. If driving by car always takes 1h15, it might be worth it if you have an urgent appointment that day.\n\n\n\n\nCategorical learning\nThe idea of distributional RL is to learn the distribution of returns \\mathcal{Z}^\\pi directly instead of its expectation:\nR_t \\sim \\mathcal{Z}^\\pi(s_t, a_t)\n\n\n\nDistribution of returns for a given state.\n\n\nNote that we can always obtain the Q-values back:\nQ^\\pi(s, a) = \\mathbb{E}_\\pi [\\mathcal{Z}^\\pi(s, a)]\nIn categorical DQN (Bellemare et al., 2017), they model the distribution of returns as a discrete probability distribution: the categorical or multinouilli distribution. One first needs to identify the minimum and maximum returns R_\\text{min} and R_\\text{max} possible in the problem. One then splits the range [R_\\text{min}, R_\\text{max}] in n discrete bins centered on the atoms \\{z_i\\}_{i=1}^N.\nThe probability that the return obtained the action (s, a) lies in the bin of the atom z_i is noted p_i(s, a). It can be approximated by a neural network F with parameters \\theta, using a softmax output layer:\n\n    p_i(s, a; \\theta) = \\frac{\\exp F_i(s, a; \\theta)}{\\sum_{j=1}^{n} \\exp F_j(s, a; \\theta)}\n\nThe n probabilities \\{p_i(s, a; \\theta)\\}_{i=1}^N completely define the parameterized distribution \\mathcal{Z}_\\theta(s, a).\n\\mathcal{Z}_\\theta(s, a) = \\sum_a p_i(s, a; \\theta) \\,\\delta_{z_i}\nwhere \\delta_{z_i}is a Dirac distribution centered on the atom z_i. The Q-value of an action can be obtained by:\n\n    Q_\\theta(s, a) = \\mathbb{E} [\\mathcal{Z}_\\theta(s, a)] = \\sum_{i=1}^{n} p_i(s, a; \\theta) \\, z_i\n\n\n\n\nCategorical DQN outputs the distribution of returns for each action using a softmax output layer. Source: https://physai.sciencesconf.org/data/pages/distributional_RL_Remi_Munos.pdf\n\n\nThe only thing we need is a neural network \\theta returning for each action a in the state s a discrete probability distribution \\mathcal{Z}_\\theta(s, a) instead of a single Q-value Q_\\theta(s, a). The NN uses a softmax activation function for each action. Action selection is similar to DQN: we first compute the Q_\\theta(s, a) and apply greedy / \\epsilon-greedy / softmax over the actions.\n\n    Q_\\theta(s, a) = \\sum_{i=1}^{n} p_i(s, a; \\theta) \\, z_i\n\nThe number n of atoms for each action should be big enough to represent the range of returns. A number that works well with Atari games is n=51: Categorical DQN is often noted C51.\n\n\nDistributional Bellman target\nHow do we learn the distribution of returns \\mathcal{Z}_\\theta(s, a) of parameters \\{p_i(s, a; \\theta)\\}_{i=1}^N? In Q-learning, we minimize the mse between the prediction Q_\\theta(s, a) and the target:\n\\mathcal{T} \\, Q_\\theta(s, a) = r + \\gamma \\, Q_\\theta(s', a')\nwhere \\mathcal{T} is the Bellman operator.\n\\min_\\theta (\\mathcal{T} \\, Q_\\theta(s, a) - Q_\\theta(s, a))^2\nWe do the same here: we apply the Bellman operator on the distribution \\mathcal{Z}_\\theta(s, a).\n\\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) = r(s, a) + \\gamma \\, \\mathcal{Z}_\\theta(s', a')\nWe then minimize the statistical “distance” between the distributions \\mathcal{Z}_\\theta(s, a) and \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a).\n\\min_\\theta \\text{KL}(\\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) || \\mathcal{Z}_\\theta(s, a))\nLet’s note P^\\pi \\, \\mathcal{Z} the return distribution of the greedy action in the next state \\mathcal{Z}_\\theta(s', a').\nMultiplying the returns by the discount factor \\gamma < 1 shrinks the return distribution (its support gets smaller). The atoms z_i of \\mathcal{Z}_\\theta(s', a') now have the position \\gamma \\, z_i, but the probabilities stay the same.\n\n\n\nMultiplying the returns by \\gamma < 1 shrinks the supports of the distribution. Source: https://physai.sciencesconf.org/data/pages/distributional_RL_Remi_Munos.pdf\n\n\nAdding a reward r translates the distribution. The probabilities do not change, but the new position of the atoms is:\nz'_i = r + \\gamma \\, z_i\n\n\n\nAdding the reward translates the distribution. Source: https://physai.sciencesconf.org/data/pages/distributional_RL_Remi_Munos.pdf\n\n\nBut now we have a problem: the atoms z'_i of \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) do not match with the atoms z_i of \\mathcal{Z}_\\theta(s, a). We need to interpolate the target distribution to compare it with the predicted distribution.\nWe need to apply a projection \\Phi so that the bins of \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) are the same as the ones of \\mathcal{Z}_\\theta(s, a). The formula sounds complicated, but it is basically a linear interpolation:\n\n    (\\Phi \\, \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a))_i = \\sum_{j=1}^n \\big [1 - \\frac{| [\\mathcal{T}\\, z_j]_{R_\\text{min}}^{R_\\text{max}} - z_i|}{\\Delta z} \\big ]_0^1 \\, p_j (s', a'; \\theta)\n\n\n\n\nThe Bellman target distribution \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) must be projected to match the support of \\mathcal{Z}_\\theta(s, a). Source: https://physai.sciencesconf.org/data/pages/distributional_RL_Remi_Munos.pdf\n\n\nWe now have two distributions \\mathcal{Z}_\\theta(s, a) and \\Phi \\, \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a) sharing the same support. We now want to have the prediction \\mathcal{Z}_\\theta(s, a) close from the target \\Phi \\, \\mathcal{T} \\, \\mathcal{Z}_\\theta(s, a). These are probability distributions, not numbers, so we cannot use the mse. We instead minimize the Kullback-Leibler (KL) divergence between the two distributions.\n\n\n\n\n\n\nKullback-Leibler (KL) divergence\n\n\n\nLet’s consider a parameterized discrete distribution X_\\theta and a discrete target distribution T. The KL divergence between the two distributions is:\n\\text{KL}(X_\\theta || T) = \\mathbb{E}_T [- T \\, \\log \\, \\frac{X_\\theta}{T}]\nIt can be rewritten as the sum of the cross-entropy and the entropy of T:\n\\text{KL}(X_\\theta || T) = \\mathbb{E}_T [- T \\, \\log \\, X_\\theta + T \\, \\log T] = H(X_\\theta, T) - H(T)\nAs T does not depend on \\theta, the gradient of the KL divergence w.r.t to \\theta is the same as the gradient of the cross-entropy.\n\\nabla_\\theta \\, \\text{KL}(X_\\theta || T) = \\nabla_\\theta \\,  \\mathbb{E}_T [- T \\, \\log \\, X_\\theta]\nMinimizing the KL divergence is the same as minimizing the cross-entropy. Neural networks with a softmax output layer and the cross-entropy loss function can do that.\n\n\n\n\nCategorical DQN\nThe categorical DQN algorithm follows the main lines of DQN, with the additional step of prohecting the distributions:\n\n\n\n\n\n\nCategorical DQN\n\n\n\n\nInitialize distributional value network Z_{\\theta} and target network Z_{\\theta'}.\nInitialize experience replay memory \\mathcal{D} of maximal size N.\nfor t \\in [0, T_\\text{total}]:\n\nSelect an action a_t based on Q_\\theta(s_t, a), observe s_{t+1} and r_{t+1}.\nStore (s_t, a_t, r_{t+1}, s_{t+1}) in the experience replay memory.\nEvery T_\\text{train} steps:\n\nSample a minibatch \\mathcal{D}_s randomly from \\mathcal{D}.\nFor each transition (s_k, a_k, r_k, s'_k) in the minibatch:\n\nSelect the greedy action in the next state using the target network:\n\na'_k = \\text{argmax}_a \\, Q_{\\theta'}(s'_k, a) = \\text{argmax}_a \\, \\mathbb{E}[Z_{\\theta'}(s'_k, a)]\n\nApply the Bellman operator on the distribution of the next greedy action:\n\nTZ_k = r_k + \\gamma \\, Z_{\\theta'}(s'_k, a'_k)\n\nProject this distribution to the support of Z_\\theta(s_k, a_k).\n\n\\mathbf{t}_k = \\text{Projection}(TZ_k, Z_\\theta(s_k, a_k))\nUpdate the value network Q_{\\theta} on \\mathcal{D}_s to minimize the cross-entropy:\n\n\\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathcal{D}_s}[ - \\mathbf{t}_k \\, \\log Z_\\theta(s_k, a_k)]\n\n\n\n\nIn practice, the computation of the cross-entropy loss is described in (Bellemare et al., 2017):\n\n\n\nComputation of the cross-entropy loss in (Bellemare et al., 2017).\n\n\nHaving the full distribution of returns allow to deal with uncertainty. For certain actions in critical states, one could get a high return (killing an enemy) or no return (death). The distribution reflects that the agent is not certain of the goodness of the action. Expectations would not provide this information.\n\n\n\nThe distribution of returns for each action allows to estimate the uncertainty. Source: https://deepmind.com/blog/article/going-beyond-average-reinforcement-learning\n\n\n\n\n\nC51 outperforms both DQN and humans on Atari games. Source (Bellemare et al., 2017).\n\n\n\n\n\nC51 outperforms double DQN on most games. Source (Bellemare et al., 2017).\n\n\n\n\nOther variants of distributional learning include:\n\nQR-DQN Distributional Reinforcement Learning with Quantile Regression (Dabney et al., 2017).\nIQN Implicit Quantile Networks for Distributional Reinforcement Learning (Dabney et al., 2018).\nThe Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning (Gruslys et al., 2017)."
  },
  {
    "objectID": "notes/3.2-BeyondDQN.html#noisy-dqn",
    "href": "notes/3.2-BeyondDQN.html#noisy-dqn",
    "title": "Beyond DQN",
    "section": "Noisy DQN",
    "text": "Noisy DQN\n\nDQN and its variants rely on \\epsilon-greedy action selection over the Q-values to explore. The exploration parameter \\epsilon is annealed during training to reach a final minimal value. It is preferred to softmax action selection, where \\tau scales with the unknown Q-values. The problem is that it is a global exploration mechanism: well-learned states do not need as much exploration as poorly explored ones.\n\n\n\nThe exploration parameter \\epsilon or \\tau is annealed during learning to solve the exploration/exploitation trade-off. Source: https://www.researchgate.net/publication/334741451/figure/fig2/AS:786038515589120@1564417594220/Epsilon-greedy-method-At-each-step-a-random-number-is-generated-by-the-model-If-the_W640.jpg\n\n\n\\epsilon-greedy and softmax add exploratory noise to the output of DQN: The Q-values predict a greedy action, but another action is taken. What about adding noise to the parameters (weights and biases) of the DQN, what would change the greedy action everytime? Controlling the level of noise inside the neural network indirectly controls the exploration level.\n\n\n\nInstead of adding noise to the output (greedy action), we could add noise to the parameters of the neural network. Source: https://openai.com/blog/better-exploration-with-parameter-noise/\n\n\n\n\n\n\n\n\nNote\n\n\n\nA very similar idea was proposed by OpenAI at the same ICLR conference: (Plappert et al., 2018)\n\n\nParameter noise builds on the idea of Bayesian deep learning. Instead of learning a single value of the parameters:\ny = \\theta_1 \\, x + \\theta_0\nwe learn the distribution of the parameters, for example by assuming they come from a normal distribution:\n\\theta \\sim \\mathcal{N}(\\mu_\\theta, \\sigma_\\theta^2)\nFor each new input, we sample a value for the parameter:\n\\theta = \\mu_\\theta + \\sigma_\\theta \\, \\epsilon\nwith \\epsilon \\sim \\mathcal{N}(0, 1) a random variable. The prediction y will vary for the same input depending on the variances:\ny = (\\mu_{\\theta_1} + \\sigma_{\\theta_1} \\, \\epsilon_1) \\, x + \\mu_{\\theta_0} + \\sigma_{\\theta_0} \\, \\epsilon_0\nThe mean and variance of each parameter can be learned through backpropagation!\n\n\n\nBayesian deep learning learns a distribution of weights. Source: https://ericmjl.github.io/bayesian-deep-learning-demystified\n\n\nAs the random variables \\epsilon_i \\sim \\mathcal{N}(0, 1) are not correlated with anything, the variances \\sigma_\\theta^2 should decay to 0. The variances \\sigma_\\theta^2 represent the uncertainty about the prediction y.\nApplied to DQN, this means that a state which has not been visited very often will have a high uncertainty: The predicted Q-values will change a lot between two evaluations, so the greedy action might change: exploration. Conversely, a well-explored state will have a low uncertainty: The greedy action stays the same: exploitation.\nNoisy DQN (Fortunato et al., 2017) uses greedy action selection over noisy Q-values. The level of exploration is learned by the network on a per-state basis. No need for scheduling! Parameter noise improves the performance of \\epsilon-greedy-based methods, including DQN, dueling DQN, A3C, DDPG (see later), etc.\n\n\n\nNoisy networks outperform their \\epsilon-soft variants. Source: (Fortunato et al., 2017)."
  },
  {
    "objectID": "notes/3.2-BeyondDQN.html#rainbow-network",
    "href": "notes/3.2-BeyondDQN.html#rainbow-network",
    "title": "Beyond DQN",
    "section": "Rainbow network",
    "text": "Rainbow network\n\nWe have seen various improvements over a few years (2013-2017):\n\nOriginal DQN (Mnih et al., 2013)\n\n\\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [(r + \\gamma \\, Q_{\\theta'}(s´, \\text{argmax}_{a'} Q_{\\theta'}(s', a')) - Q_\\theta(s, a))^2]\n\nDouble DQN (van Hasselt et al., 2015)\n\n\\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [(r + \\gamma \\, Q_{\\theta'}(s´, \\text{argmax}_{a'} Q_{\\theta}(s', a')) - Q_\\theta(s, a))^2]\n\nPrioritized Experience Replay (Schaul et al., 2015)\n\nP(k) = \\frac{(|\\delta_k| + \\epsilon)^\\alpha}{\\sum_k (|\\delta_k| + \\epsilon)^\\alpha}\n\nDueling DQN (Wang et al., 2016)\n\nQ_\\theta(s, a) = V_\\alpha(s) + A_\\beta(s, a)\n\nCategorical DQN (Bellemare et al., 2017)\n\n\\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathcal{D}_s}[ - \\mathbf{t}_k \\, \\log Z_\\theta(s_k, a_k)]\n\nNoisyNet (Fortunato et al., 2017)\n\n\\theta = \\mu_\\theta + \\sigma_\\theta \\, \\epsilon\nWhich of these improvements should we use?\n\n\n\nThe Rainbow network combines all DQN improvements and outperforms each of them. Source: (Hessel et al., 2017).\n\n\nAnswer: all of them. The rainbow network (Hessel et al., 2017) combines :\n\ndouble dueling DQN with PER.\ncategorical learning of return distributions.\nparameter noise for exploration.\nn-step return (n=3) for the bias/variance trade-off:\n\nR_t = \\sum_{k=0}^{n-1} \\gamma^k \\, r_{t+k+1} + \\gamma^n \\max_a Q(s_{t+n}, a)\nand outperforms any of the single improvements.\n\n\n\nAblation studies on the Rainbow network. Source: (Hessel et al., 2017).\n\n\nMost of these mechanisms are necessary to achieve optimal performance (ablation studies). n-step returns, PER and distributional learning are the most critical. Interestingly, double Q-learning does not have a huge effect on the Rainbow network: The other mechanisms (especially distributional learning) already ensure that Q-values are not over-estimated.\nYou can find good implementations of Rainbow DQN on all major frameworks, for example on rllib:\nhttps://docs.ray.io/en/latest/rllib-algorithms.html#deep-q-networks-dqn-rainbow-parametric-dqn"
  },
  {
    "objectID": "notes/3.2-BeyondDQN.html#distributed-learning",
    "href": "notes/3.2-BeyondDQN.html#distributed-learning",
    "title": "Beyond DQN",
    "section": "Distributed learning",
    "text": "Distributed learning\n\n\nGorila - General Reinforcement Learning Architecture\nThe DQN value network Q_\\theta(s, a) has two jobs:\n\nactor: it interacts with the environment to sample (s, a, r, s') transitions.\nlearner: it learns from minibatches out of the replay memory.\n\n\n\n\nThe DQN value network is both an actor and a learner, as it needs to sequentially select actions and learn from the replay memory. Source: (Nair et al., 2015).\n\n\nThe weights of the value network lie on the same CPU/GPU, so the two jobs have to be done sequentially: computational bottleneck. DQN cannot benefit from parallel computing: multi-core CPU, clusters of CPU/GPU, etc.\nThe Gorila framework (Nair et al., 2015) splits DQN into multiple actors and multiple learners. Each actor (or worker) interacts with its copy of the environment and stores transitions in a distributed replay buffer. Each learner samples minibatches from the replay buffer and computes gradients w.r.t the DQN loss. The parameter server (master network) applies the gradients on the parameters and frequently synchronizes the actors and learners.\n\n\n\nThe Gorila framework uses multiple actors to collect transitions and store them in the ERM. The learners sample the ERM and compute the gradients of the DQN loss function. The parameter servers collect the gradients and synchronize the parameters of the actors and learners. Source: (Nair et al., 2015).\n\n\nGorila allows to train DQN on parallel hardware (e.g. clusters of GPU) as long as the environment can be copied (simulation).\n\n\n\nThe Gorila framework allows to slightly improve the performance of DQN on Atari games… Source: (Nair et al., 2015).\n\n\nThe final performance is not incredibly better than single-GPU DQN, but obtained much faster in wall-clock time (2 days instead of 12-14 days on a single GPU).\n\n\n\n… but that performance is achieved in a much smaller wall-clock time. Source: (Nair et al., 2015).\n\n\n\n\nApe-X\nWith more experience, Deepmind realized that a single learner is better. Distributed SGD (computing gradients with different learners) is not very efficient. What matters is collecting transitions very quickly (multiple workers) but using prioritized experience replay to learn from the most interesting ones.\n\n\n\n\n\nUsing 360 workers (1 per CPU core), Ape-X (Horgan et al., 2018) reaches super-human performance for a fraction of the wall-clock training time.\n\n\n\n\n\nThe multiple parallel workers can collect much more frames, leading to the better performance. The learner uses n-step returns and the double dueling DQN network architecture, so it is not much different from Rainbow DQN internally."
  },
  {
    "objectID": "notes/3.2-BeyondDQN.html#recurrent-dqn",
    "href": "notes/3.2-BeyondDQN.html#recurrent-dqn",
    "title": "Beyond DQN",
    "section": "Recurrent DQN",
    "text": "Recurrent DQN\n\n\nDRQN: Deep Recurrent Q-network\nAtari games are POMDP: each frame is a partial observation, not a Markov state. One cannot infer the velocity of the ball from a single frame.\n\n\n\nMost Atari games have frames which do not respect the Markov property. Source: (Hausknecht and Stone, 2015).\n\n\nThe trick used by DQN and its variants is to stack the last four frames and provide them as inputs to the CNN. The last 4 frames have (almost) the Markov property.\nThe alternative is to use a recurrent neural network (e.g. LSTM) to encode the history of single frames.\n\n    \\mathbf{h}_t = f(W_x \\times \\mathbf{x}_t + W_h \\times \\mathbf{h}_{t-1} + \\mathbf{b})\n\nThe output at time t depends on the whole history of inputs (\\mathbf{x}_0, \\mathbf{x}_1, \\ldots, \\mathbf{x}_t).\nUsing the output of a LSTM as a state, we make sure that we have the Markov property, RL will work:\n\n    P(\\mathbf{h}_{t+1} | \\mathbf{h}_t) = P(\\mathbf{h}_{t+1} | \\mathbf{h}_t, \\mathbf{h}_{t-1}, \\ldots, \\mathbf{h}_0)\n\n\n\n\nValue-based network with a LSTM layer before the Q-value output layer. Source: https://blog.acolyer.org/2016/11/23/playing-fps-games-with-deep-reinforcement-learning/\n\n\nFor the neural network, it is just a matter of adding a LSTM layer before the output layer. The convolutional layers are feature extractors for the LSTM layer. The loss function does not change: backpropagation (through time) all along.\n\\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [(r + \\gamma \\, Q_{\\theta'}(s´, \\text{argmax}_{a'} Q_{\\theta}(s', a')) - Q_\\theta(s, a))^2]\n\n\n\nDRQN architecture (Hausknecht and Stone, 2015).\n\n\nThe only problem is that RNNs are trained using truncated backpropagation through time (BPTT). One needs to provide a partial history of T = 10 inputs to the network in order to learn one output:\n(\\mathbf{x}_{t-T}, \\mathbf{x}_{t-T+1}, \\ldots, \\mathbf{x}_t)\nThe experience replay memory should therefore not contain single transitions (s_t, a_t, r_{t+1}, s_{t+1}), but a partial history of transitions.\n(s_{t-T}, a_{t-T}, r_{t-T+1}, s_{t-T+1}, \\ldots, s_t, a_t, r_{t+1}, s_{t+1})\nUsing a LSTM layer helps on certain games, where temporal dependencies are longer that 4 frames, but impairs on others.\n\n\n\nDRQN performance compared to DQN (Hausknecht and Stone, 2015).\n\n\nBeware: LSTMs are extremely slow to train (but not to use). Stacking frames is still a reasonable option in many cases.\n\n\n\nTraining and inference times of DRQN compared to DQN (Hausknecht and Stone, 2015).\n\n\n\n\nR2D2: Recurrent Replay Distributed DQN\nR2D2 (Kapturowski et al., 2019) builds on Ape-X and DRQN:\n\ndouble dueling DQN with n-step returns (n=5) and prioritized experience replay.\n256 actors, 1 learner.\n1 LSTM layer after the convolutional stack.\n\nIn addition to solving practical problems with LSTMs (initial state at the beginning of an episode), it became the state of the art on Atari-57 until November 2019…\n\n\n\n\n\n\n\n\n\nBellemare, M. G., Dabney, W., and Munos, R. (2017). A Distributional Perspective on Reinforcement Learning. http://arxiv.org/abs/1707.06887.\n\n\nDabney, W., Ostrovski, G., Silver, D., and Munos, R. (2018). Implicit Quantile Networks for Distributional Reinforcement Learning. http://arxiv.org/abs/1806.06923.\n\n\nDabney, W., Rowland, M., Bellemare, M. G., and Munos, R. (2017). Distributional Reinforcement Learning with Quantile Regression. http://arxiv.org/abs/1710.10044.\n\n\nFortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., et al. (2017). Noisy Networks for Exploration. http://arxiv.org/abs/1706.10295.\n\n\nGruslys, A., Dabney, W., Azar, M. G., Piot, B., Bellemare, M., and Munos, R. (2017). The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning. http://arxiv.org/abs/1704.04651.\n\n\nHausknecht, M., and Stone, P. (2015). Deep Recurrent Q-Learning for Partially Observable MDPs. http://arxiv.org/abs/1507.06527.\n\n\nHessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., et al. (2017). Rainbow: Combining Improvements in Deep Reinforcement Learning. http://arxiv.org/abs/1710.02298.\n\n\nHorgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., van Hasselt, H., et al. (2018). Distributed Prioritized Experience Replay. http://arxiv.org/abs/1803.00933.\n\n\nKapturowski, S., Ostrovski, G., Quan, J., Munos, R., and Dabney, W. (2019). Recurrent experience replay in distributed reinforcement learning. in, 19. https://openreview.net/pdf?id=r1lyTjAqYX.\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., et al. (2013). Playing Atari with Deep Reinforcement Learning. http://arxiv.org/abs/1312.5602.\n\n\nNair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De Maria, A., et al. (2015). Massively Parallel Methods for Deep Reinforcement Learning. https://arxiv.org/pdf/1507.04296.pdf.\n\n\nPlappert, M., Houthooft, R., Dhariwal, P., Sidor, S., Chen, R. Y., Chen, X., et al. (2018). Parameter Space Noise for Exploration. http://arxiv.org/abs/1706.01905.\n\n\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. (2015). Prioritized Experience Replay. http://arxiv.org/abs/1511.05952.\n\n\nvan Hasselt, H., Guez, A., and Silver, D. (2015). Deep Reinforcement Learning with Double Q-learning. http://arxiv.org/abs/1509.06461.\n\n\nWang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and de Freitas, N. (2016). Dueling Network Architectures for Deep Reinforcement Learning. http://arxiv.org/abs/1511.06581."
  },
  {
    "objectID": "notes/3.3-PG.html",
    "href": "notes/3.3-PG.html",
    "title": "Policy gradient (PG)",
    "section": "",
    "text": "Slides: html pdf"
  },
  {
    "objectID": "notes/3.3-PG.html#policy-search",
    "href": "notes/3.3-PG.html#policy-search",
    "title": "Policy gradient (PG)",
    "section": "Policy Search",
    "text": "Policy Search\n\nLearning directly the Q-values in value-based methods (DQN) suffers from many problems:\n\nThe Q-values are unbounded: they can take any value (positive or negative), so the output layer must be linear.\nThe Q-values have a high variability: some (s,a) pairs have very negative values, others have very positive values. Difficult to learn for a NN.\nWorks only for small discrete action spaces: need to iterate over all actions to find the greedy action.\n\nInstead of learning the Q-values, one could approximate directly the policy \\pi_\\theta(s, a) with a neural network. \\pi_\\theta(s, a) is called a parameterized policy: it depends directly on the parameters \\theta of the NN. For discrete action spaces, the output of the NN can be a softmax layer, directly giving the probability of selecting an action. For continuous action spaces, the output layer can directly control the effector (joint angles). Parameterized policies can represent continuous policies and avoid the curse of dimensionality.\n\n\n\nPolicy search methods learn directly the policy.\n\n\nPolicy search methods aim at maximizing directly the expected return over all possible trajectories (episodes) \\tau = (s_0, a_0, \\dots, s_T, a_T)\n\n    \\mathcal{J}(\\theta) = \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[R(\\tau)] = \\int_{\\tau} \\rho_\\theta(\\tau) \\; R(\\tau) \\; d\\tau\n\nAll trajectories \\tau selected by the policy \\pi_\\theta should be associated with a high expected return R(\\tau) in order to maximize this objective function. \\rho_\\theta is the space of trajectories possible under \\pi_\\theta. This means that the optimal policy should only select actions that maximizes the expected return: exactly what we want.\n\n\n\nPolicy search maximizes the return of the trajectories generated by the policy.\n\n\nThe objective function is however not model-free, as the probability of a trajectory does depend on the environments dynamics:\n\n    \\rho_\\theta(\\tau) = p_\\theta(s_0, a_0, \\ldots, s_T, a_T) = p_0 (s_0) \\, \\prod_{t=0}^T \\pi_\\theta(s_t, a_t) \\, p(s_{t+1} | s_t, a_t)\n\nThe objective function is furthermore not computable:\n\nAn infinity of possible trajectories to integrate if the action space is continuous.\nEven if we sample trajectories, we would need a huge number of them to correctly estimate the objective function (sample complexity) because of the huge variance of the returns.\n\n\n    \\mathcal{J}(\\theta) = \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[R(\\tau)] \\approx \\frac{1}{M} \\, \\sum_{i=1}^M R(\\tau_i)\n\nAll we need to find is a computable gradient \\nabla_\\theta \\mathcal{J}(\\theta) to apply gradient ascent and backpropagation.\n\n    \\Delta \\theta = \\eta \\, \\nabla_\\theta \\mathcal{J}(\\theta)\n\nPolicy Gradient (PG) methods only try to estimate this gradient, but do not care about the objective function itself…\n\n    g = \\nabla_\\theta \\mathcal{J}(\\theta)\n\nIn particular, any function \\mathcal{J}'(\\theta) whose gradient is locally the same (or has the same direction) will do:\n\\mathcal{J}'(\\theta) = \\alpha \\, \\mathcal{J}(\\theta) + \\beta \\; \\Rightarrow \\; \\nabla_\\theta \\mathcal{J}'(\\theta) \\propto \\nabla_\\theta \\mathcal{J}(\\theta)  \\; \\Rightarrow \\; \\Delta \\theta = \\eta \\, \\nabla_\\theta \\mathcal{J}'(\\theta)\nThis is called surrogate optimization: we actually want to maximize \\mathcal{J}(\\theta) but we cannot compute it. We instead create a surrogate objective \\mathcal{J}'(\\theta) which is locally the same as \\mathcal{J}(\\theta) and tractable.\n\n\n\nPolicy gradient methods only care about the gradient of the objective function. Source: https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/."
  },
  {
    "objectID": "notes/3.3-PG.html#reinforce",
    "href": "notes/3.3-PG.html#reinforce",
    "title": "Policy gradient (PG)",
    "section": "REINFORCE",
    "text": "REINFORCE\n\n\nREINFORCE algorithm\nThe REINFORCE algorithm (Williams, 1992) proposes an unbiased estimate of the policy gradient:\n\n    \\nabla_\\theta \\mathcal{J}(\\theta) = \\nabla_\\theta \\int_\\tau \\rho_\\theta (\\tau) \\, R(\\tau) \\, d\\tau =  \\int_\\tau (\\nabla_\\theta \\rho_\\theta (\\tau)) \\, R(\\tau) \\, d\\tau\n\nby noting that the return of a trajectory does not depend on the weights \\theta (the agent only controls its actions, not the environment).\nWe now use the log-trick, a simple identity based on the fact that:\n\n    \\frac{d \\log f(x)}{dx} = \\frac{f'(x)}{f(x)}\n\nto rewrite the policy gradient of a single trajectory:\n\n    \\nabla_\\theta \\rho_\\theta (\\tau) = \\rho_\\theta (\\tau) \\, \\nabla_\\theta \\log \\rho_\\theta (\\tau)\n\nThe policy gradient becomes:\n\n    \\nabla_\\theta \\mathcal{J}(\\theta) =  \\int_\\tau \\rho_\\theta (\\tau) \\, \\nabla_\\theta \\log \\rho_\\theta (\\tau) \\, R(\\tau) \\, d\\tau\n\nwhich now has the form of a mathematical expectation:\n\n    \\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[ \\nabla_\\theta \\log \\rho_\\theta (\\tau) \\, R(\\tau) ]\n\nThe advantage of REINFORCE is that it is model-free:\n\n    \\rho_\\theta(\\tau) = p_\\theta(s_0, a_0, \\ldots, s_T, a_T) = p_0 (s_0) \\, \\prod_{t=0}^T \\pi_\\theta(s_t, a_t) p(s_{t+1} | s_t, a_t)\n\n\n    \\log \\rho_\\theta(\\tau) = \\log p_0 (s_0) + \\sum_{t=0}^T \\log \\pi_\\theta(s_t, a_t) + \\sum_{t=0}^T \\log p(s_{t+1} | s_t, a_t)\n\n\n    \\nabla_\\theta \\log \\rho_\\theta(\\tau) = \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t)\n\nThe transition dynamics p(s_{t+1} | s_t, a_t) disappear from the gradient. The Policy Gradient does not depend on the dynamics of the environment:\n\n    \\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, R(\\tau) ]\n\nThe REINFORCE algorithm is a policy-based variant of Monte-Carlo control:\n\n\n\n\n\n\nREINFORCE algorithm (Williams, 1992).\n\n\n\n\nwhile not converged:\n\nSample M trajectories \\{\\tau_i\\} using the current policy \\pi_\\theta and observe the returns \\{R(\\tau_i)\\}.\nEstimate the policy gradient as an average over the trajectories:\n\n\n     \\nabla_\\theta \\mathcal{J}(\\theta) \\approx \\frac{1}{M} \\sum_{i=1}^M \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, R(\\tau_i)\n  \n\nUpdate the policy using gradient ascent:\n\n\n      \\theta \\leftarrow \\theta + \\eta \\, \\nabla_\\theta \\mathcal{J}(\\theta)\n  \n\n\n\nAdvantages\n\nThe policy gradient is model-free.\nWorks with partially observable problems (POMDP): as the return is computed over complete trajectories, it does not matter whether the states are Markov or not.\n\nInconvenients\n\nOnly for episodic tasks.\nThe gradient has a high variance: returns may change a lot during learning.\nIt has therefore a high sample complexity: we need to sample many episodes to correctly estimate the policy gradient.\nStrictly on-policy: trajectories must be frequently sampled and immediately used to update the policy.\n\n\n\nREINFORCE with baseline\nTo reduce the variance of the estimated gradient, a baseline is often subtracted from the return:\n\n    \\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, (R(\\tau) - b) ]\n\nAs long as the baseline b is independent from \\theta, it does not introduce a bias:\n\n\\begin{aligned}\n    \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[\\nabla_\\theta \\log \\rho_\\theta (\\tau) \\, b ] & = \\int_\\tau \\rho_\\theta (\\tau) \\nabla_\\theta \\log \\rho_\\theta (\\tau) \\, b \\, d\\tau \\\\\n    & = \\int_\\tau \\nabla_\\theta  \\rho_\\theta (\\tau) \\, b \\, d\\tau \\\\\n    &= b \\, \\nabla_\\theta \\int_\\tau \\rho_\\theta (\\tau) \\, d\\tau \\\\\n    &=  b \\, \\nabla_\\theta 1 \\\\\n    &= 0\n\\end{aligned}\n\nA simple baseline that reduces the variance of the returns is a moving average of the returns obtained during all episodes:\nb = \\alpha \\, R(\\tau) + (1 - \\alpha) \\, b\nThis is similar to reinforcement comparison for bandits, except we compute the mean return instead of the mean reward. A trajectory \\tau should be reinforced if it brings more return than average.\n(Williams, 1992) showed that the best baseline (the one that reduces the variance the most) is actually:\n\n    b = \\frac{\\mathbb{E}_{\\tau \\sim \\rho_\\theta}[(\\nabla_\\theta \\log \\rho_\\theta (\\tau))^2 \\, R(\\tau)]}{\\mathbb{E}_{\\tau \\sim \\rho_\\theta}[(\\nabla_\\theta \\log \\rho_\\theta (\\tau))^2]}\n\nbut it is complex to compute. In practice, a baseline that works well is the value of the encountered states:\n\n    \\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, (R(\\tau) - V^\\pi(s_t)) ]\n\nR(\\tau) - V^\\pi(s_t) becomes the advantage of the action a_t in s_t: how much return does it provide compared to what can be expected in s_t generally. As in dueling networks, it reduces the variance of the returns. Problem: the value of each state has to be learned separately (see actor-critic architectures).\n\n\n\n\n\n\nApplication of REINFORCE to resource management\n\n\n\n\n\n\n\n\nREINFORCE with baseline can be used to allocate resources (CPU cores, memory, etc) when scheduling jobs on a cloud of compute servers. In DeepRM (Mao et al., 2016), the policy is approximated by a shallow NN (one hidden layer with 20 neurons). The state space is the current occupancy of the cluster as well as the job waiting list. The action space is sending a job to a particular resource. The reward is the negative job slowdown: how much longer the job needs to complete compared to the optimal case. DeepRM outperforms all alternative job schedulers."
  },
  {
    "objectID": "notes/3.3-PG.html#policy-gradient",
    "href": "notes/3.3-PG.html#policy-gradient",
    "title": "Policy gradient (PG)",
    "section": "Policy Gradient",
    "text": "Policy Gradient\n\n\nPolicy Gradient theorem\nThe REINFORCE gradient estimate is the following:\n\n    \\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, R(\\tau) ] =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[\\sum_{t=0}^T (\\nabla_\\theta \\log \\pi_\\theta(s_t, a_t)) \\, (\\sum_{t'=0}^T \\gamma^{t'} \\, r_{t'+1}) ]\n\nFor each state-action pair (s_t, a_t) encountered during the episode, the gradient of the log-policy is multiplied by the complete return of the episode:\nR(\\tau) = \\sum_{t'=0}^T \\gamma^{t'} \\, r_{t'+1}\nThe causality principle states that rewards obtained before time t are not caused by that action. The policy gradient can be rewritten as:\n\n    \\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, (\\sum_{t'=t}^T \\gamma^{t' - t} \\, r_{t'+1}) ] =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, R_t ]\n\nThe return at time t (reward-to-go) multiplies the gradient of the log-likelihood of the policy(the score) for each transition in the episode:\n\n    \\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, R_t ]\n\nAs we have:\nQ^\\pi(s, a) = \\mathbb{E}_\\pi [R_t | s_t =s; a_t =a]\nwe can replace R_t with Q^{\\pi_\\theta}(s_t, a_t) without introducing any bias:\n\n    \\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, Q^{\\pi_\\theta}(s_t, a_t) ]\n\nThis is true on average (no bias if the Q-value estimates are correct) and has a much lower variance!\nThe policy gradient is defined over complete trajectories:\n\n    \\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, Q^{\\pi_\\theta}(s_t, a_t) ]\n\nHowever, \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t) \\, Q^{\\pi_\\theta}(s_t, a_t) now only depends on (s_t, a_t), not the future nor the past. Each step of the episode is now independent from each other (if we have the Markov property). We can then sample single transitions instead of complete episodes:\n\n    \\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q^{\\pi_\\theta}(s, a) ]\n\nNote that this is not true for \\mathcal{J}(\\theta) directly, as the value of \\mathcal{J}(\\theta) changes (computed over single transitions instead of complete episodes, so it is smaller), but it is true for its gradient (both go in the same direction)!\n\n\n\n\n\n\nPolicy Gradient Theorem (Sutton et al., 1999)\n\n\n\nFor any MDP, the policy gradient is:\n\n    g = \\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q^{\\pi_\\theta}(s, a) ]\n\n\n\n\n\nPolicy Gradient Theorem with function approximation\nBetter yet, (Sutton et al., 1999) showed that we can replace the true Q-value Q^{\\pi_\\theta}(s, a) by an estimate Q_\\varphi(s, a) as long as this one is unbiased:\n\n    \\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q_\\varphi(s, a) ]\n\nWe only need to have:\n\n    Q_\\varphi(s, a) \\approx Q^{\\pi_\\theta}(s, a) \\; \\forall s, a\n\nThe approximated Q-values can for example minimize the mean square error with the true Q-values:\n\n    \\mathcal{L}(\\varphi) =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[(Q^{\\pi_\\theta}(s, a) - Q_\\varphi(s, a))^2]\n\n\n\nActor-critic architectures\nWe obtain an actor-critic architecture:\n\nthe actor \\pi_\\theta(s, a) implements the policy and selects an action a in a state s.\nthe critic Q_\\varphi(s, a) estimates the value of that action and drives learning in the actor.\n\n\n\n\nActor-critic architecture for policy gradient.\n\n\nBut how to train the critic? We do not know Q^{\\pi_\\theta}(s, a). As always, we can estimate it through sampling:\n\nMonte-Carlo critic: sampling the complete episode.\n\n\n    \\mathcal{L}(\\varphi) =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[(R(s, a) - Q_\\varphi(s, a))^2]\n\n\nSARSA critic: sampling (s, a, r, s', a') transitions.\n\n\n    \\mathcal{L}(\\varphi) =  \\mathbb{E}_{s, s' \\sim \\rho_\\theta, a, a' \\sim \\pi_\\theta}[(r + \\gamma \\, Q_\\varphi(s', a') - Q_\\varphi(s, a))^2]\n\n\nQ-learning critic: sampling (s, a, r, s') transitions.\n\n\n    \\mathcal{L}(\\varphi) =  \\mathbb{E}_{s, s' \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[(r + \\gamma \\, \\max_{a'} Q_\\varphi(s', a') - Q_\\varphi(s, a))^2]\n\nAs with REINFORCE, the PG actor suffers from the high variance of the Q-values. It is possible to use a baseline in the PG without introducing a bias:\n\n    \\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, (Q^{\\pi_\\theta}(s, a) -b)]\n\nIn particular, the advantage actor-critic uses the value of a state as the baseline:\n\n\\begin{aligned}\n    \\nabla_\\theta \\mathcal{J}(\\theta) &=  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, (Q^{\\pi_\\theta}(s, a) - V^{\\pi_\\theta}(s))] \\\\\n    &\\\\\n    &=  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, A^{\\pi_\\theta}(s, a)] \\\\\n\\end{aligned}\n\nThe critic can either:\n\nlearn to approximate both Q^{\\pi_\\theta}(s, a) and V^{\\pi_\\theta}(s) with two different NN (SAC).\nreplace one of them with a sampling estimate (A3C, DDPG)\nlearn the advantage A^{\\pi_\\theta}(s, a) directly (GAE, PPO)\n\nPolicy Gradient methods can therefore take many forms :\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) \\, \\psi_t ]\n\nwhere:\n\n\\psi_t = R_t is the REINFORCE algorithm (MC sampling).\n\\psi_t = R_t - b is the REINFORCE with baseline algorithm.\n\\psi_t = Q^\\pi(s_t, a_t) is the policy gradient theorem.\n\\psi_t = A^\\pi(s_t, a_t) = Q^\\pi(s_t, a_t) - V^\\pi(s_t) is the advantage actor-critic.\n\\psi_t = r_{t+1} + \\gamma \\, V^\\pi(s_{t+1}) - V^\\pi(s_t) is the TD actor-critic.\n\\psi_t = \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V^\\pi(s_{t+n}) - V^\\pi(s_t) is the n-step advantage.\n\nand many others…\nThe different variants of PG deal with the bias/variance trade-off.\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) \\, \\psi_t ]\n\n\nThe more \\psi_t relies on sampled rewards (e.g. R_t), the more the gradient will be correct on average (small bias), but the more it will vary (high variance). This increases the sample complexity: we need to average more samples to correctly estimate the gradient.\nThe more \\psi_t relies on estimations (e.g. the TD error), the more stable the gradient (small variance), but the more incorrect it is (high bias). This can lead to suboptimal policies, i.e. local optima of the objective function.\n\nAll the methods we will see in the rest of the course are attempts at finding the best trade-off.\n\n\n\n\nMao, H., Alizadeh, M., Menache, I., and Kandula, S. (2016). Resource Management with Deep Reinforcement Learning. in Proceedings of the 15th ACM Workshop on Hot Topics in Networks - HotNets ’16 (Atlanta, GA, USA: ACM Press), 50–56. doi:10.1145/3005745.3005750.\n\n\nSutton, R. S., McAllester, D., Singh, S., and Mansour, Y. (1999). Policy gradient methods for reinforcement learning with function approximation. in Proceedings of the 12th International Conference on Neural Information Processing Systems (MIT Press), 1057–1063. https://dl.acm.org/citation.cfm?id=3009806.\n\n\nWilliams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8, 229–256."
  },
  {
    "objectID": "notes/3.4-A3C.html",
    "href": "notes/3.4-A3C.html",
    "title": "Advantage actor-critic (A2C, A3C)",
    "section": "",
    "text": "Slides: html pdf"
  },
  {
    "objectID": "notes/3.4-A3C.html#distributed-actor-critic",
    "href": "notes/3.4-A3C.html#distributed-actor-critic",
    "title": "Advantage actor-critic (A2C, A3C)",
    "section": "Distributed actor-critic",
    "text": "Distributed actor-critic\n\nLet’s consider an n-step advantage actor-critic architecture where the Q-value of the action (s_t, a_t) is approximated by the n-step return:\nQ^{\\pi_\\theta}(s_t, a_t) \\approx R_t^n =  \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V_\\varphi(s_{t+n})\n\n\n\nn-step returns. Source: (Sutton and Barto, 1998).\n\n\nThe actor \\pi_\\theta(s, a) uses PG with baseline to learn the policy:\n\n    \\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) \\, (R_t^n - V_\\varphi(s_t)) ]\n\nThe critic V_\\varphi(s) approximates the value of each state by minimizing the mse:\n\n    \\mathcal{L}(\\varphi) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[(R^n_t - V_\\varphi(s_t))^2]\n\n\n\n\nAdvantage actor-critic.\n\n\nThe advantage actor-critic is strictly on-policy:\n\nThe critic must evaluate actions selected the current version of the actor \\pi_\\theta, not an old version or another policy.\nThe actor must learn from the current value function V^{\\pi_\\theta} \\approx V_\\varphi.\n\n\\begin{cases}\n    \\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) \\, (R^n_t - V_\\varphi(s_t)) ] \\\\\n    \\\\\n    \\mathcal{L}(\\varphi) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[(R^n_t - V_\\varphi(s_t))^2] \\\\\n\\end{cases}\n\nWe cannot use an experience replay memory to deal with the correlated inputs, as it is only for off-policy methods. We cannot either get an uncorrelated batch of transitions by acting sequentially with a single agent.\nA simple solution is to have multiple actors with the same weights \\theta interacting in parallel with different copies of the environment: distributed learning.\n\n\n\nDistributed learning. Source: https://ray.readthedocs.io/en/latest/rllib.html\n\n\nEach rollout worker (actor) starts an episode in a different state: at any point of time, the workers will be in uncorrelated states. From time to time, the workers all send their experienced transitions to the learner which updates the policy using a batch of uncorrelated transitions. After the update, the workers use the new policy.\n\n\n\n\n\n\nDistributed RL\n\n\n\n\nInitialize global policy or value network \\theta.\nInitialize N copies of the environment in different states.\nwhile True:\n\nfor each worker in parallel:\n\nCopy the global network parameters \\theta to each worker:\n\n\\theta_k \\leftarrow \\theta\n\nInitialize an empty transition buffer \\mathcal{D}_k.\nPerform d steps with the worker on its copy of the environment.\nAppend each transition (s, a, r, s') to the transition buffer.\n\njoin(): wait for each worker to terminate.\nGather the N transition buffers into a single buffer \\mathcal{D}.\nUpdate the global network on \\mathcal{D} to obtain new weights \\theta.\n\n\n\n\nDistributed learning can be used for any deep RL algorithm, including DQN variants. Distributed DQN variants include GORILA, IMPALA, APE-X, R2D2.”All” you need is one (or more) GPU for training the global network and N CPU cores for the workers. The workers fill the ERM much more quickly.\nIn practice, managing the communication between the workers and the global network through processes can be quite painful. There are some frameworks abstracting the dirty work, such as RLlib https://ray.readthedocs.io/en/latest/rllib.html.\n\n\n\nrllib framework. Source: https://ray.readthedocs.io/en/latest/rllib.html\n\n\nHaving multiple workers interacting with different environments is easy in simulation (Atari games). With physical environments, working in real time, it requires lots of money…"
  },
  {
    "objectID": "notes/3.4-A3C.html#a3c-asynchronous-advantage-actor-critic",
    "href": "notes/3.4-A3C.html#a3c-asynchronous-advantage-actor-critic",
    "title": "Advantage actor-critic (A2C, A3C)",
    "section": "A3C: Asynchronous advantage actor-critic",
    "text": "A3C: Asynchronous advantage actor-critic\n\n(Mnih et al., 2016) proposed the A3C algorithm (asynchronous advantage actor-critic). The stochastic policy \\pi_\\theta is produced by the actor with weights \\theta and learned using :\n\\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) \\, (R^n_t - V_\\varphi(s_t)) ]\nThe value of a state V_\\varphi(s) is produced by the critic with weights \\varphi, which minimizes the mse with the n-step return:\n\\mathcal{L}(\\varphi) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[(R^n_t - V_\\varphi(s_t))^2]\nR^n_t =  \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V_\\varphi(s_{t+n})\nBoth the actor and the critic are trained on batches of transitions collected using parallel workers. Two things are different from the general distributed approach: workers compute partial gradients and updates are asynchronous.\n\n\n\nA3C distributed architecture (Mnih et al., 2016).\n\n\n\n\n\n\n\n\nWorker\n\n\n\n\ndef worker(\\theta, \\varphi):\n\nInitialize empty transition buffer \\mathcal{D}. Initialize the environment to the last state visited by this worker.\nfor n steps:\n\nSelect an action using \\pi_\\theta, store the transition in the transition buffer.\n\nfor each transition in \\mathcal{D}:\n\nCompute the n-step return in each state\n\nR^n_t =  \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V_\\varphi(s_{t+n})\nCompute policy gradient for the actor on the transition buffer:\n\nd\\theta = \\nabla_\\theta \\mathcal{J}(\\theta) = \\frac{1}{n} \\sum_{t=1}^n \\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) \\, (R^n_t - V_\\varphi(s_t))\n\nCompute value gradient for the critic on the transition buffer:\n\nd\\varphi = \\nabla_\\varphi \\mathcal{L}(\\varphi) = -\\frac{1}{n} \\sum_{t=1}^n (R^n_t - V_\\varphi(s_t)) \\, \\nabla_\\varphi V_\\varphi(s_t)\n\nreturn d\\theta, d\\varphi\n\n\n\n\n\n\n\n\n\n\nA2C algorithm\n\n\n\n\nInitialize actor \\theta and critic \\varphi.\nInitialize K workers with a copy of the environment.\nfor t \\in [0, T_\\text{total}]:\n\nfor K workers in parallel:\n\nd\\theta_k, d\\varphi_k = worker(\\theta, \\varphi)\n\njoin()\nMerge all gradients:\n\nd\\theta = \\frac{1}{K} \\sum_{i=1}^K d\\theta_k \\; ; \\; d\\varphi = \\frac{1}{K} \\sum_{i=1}^K d\\varphi_k\n\nUpdate the actor and critic using gradient ascent/descent:\n\n\\theta \\leftarrow \\theta + \\eta \\, d\\theta \\; ; \\; \\varphi \\leftarrow \\varphi - \\eta \\, d\\varphi\n\n\n\nThe previous algorithm depicts A2C, the synchronous version of A3C. A2C synchronizes the workers (threads), i.e. it waits for the K workers to finish their job before merging the gradients and updating the global networks.\nA3C is asynchronous:\n\nThe partial gradients are applied to the global networks as soon as they are available.\nNo need to wait for all workers to finish their job.\n\nAs the workers are not synchronized, this means that one worker could be copying the global networks \\theta and \\varphi while another worker is writing them. This is called a Hogwild! update (Niu et al., 2011): no locks, no semaphores. Many workers can read/write the same data. It turns out NN are robust enough for this kind of updates.\n\n\n\n\n\n\nA3C: asynchronous updates\n\n\n\n\nInitialize actor \\theta and critic \\varphi.\nInitialize K workers with a copy of the environment.\nfor K workers in parallel:\n\nfor t \\in [0, T_\\text{total}]:\n\nCopy the global networks \\theta and \\varphi.\nCompute partial gradients:\n\nd\\theta_k, d\\varphi_k = \\text{worker}(\\theta, \\varphi)\n\nUpdate the global actor and critic using the partial gradients:\n\n\\theta \\leftarrow \\theta + \\eta \\, d\\theta_k\n\\varphi \\leftarrow \\varphi - \\eta \\, d\\varphi_k\n\n\n\n\nA3C does not use an experience replay memory as DQN. Instead, it uses multiple parallel workers to distribute learning. Each worker has a copy of the actor and critic networks, as well as an instance of the environment. Weight updates are synchronized regularly though a master network using Hogwild!-style updates (every n=5 steps!). Because the workers learn different parts of the state-action space, the weight updates are not very correlated.\nA3C still needs target networks to ensure stability. It works best on shared-memory systems (multi-core) as communication costs between GPUs are huge.\nA3C set a new record for Atari games in 2016. The main advantage is that the workers gather experience in parallel: training is much faster than with DQN. LSTMs can be used to improve the performance.\n \nLearning is only marginally better with more threads:\n\n\n\nA3C results (Mnih et al., 2016).\n\n\nbut much faster!\n\n\n\nA3C results (Mnih et al., 2016).\n\n\n\n\n\nA3C came up in 2016. A lot of things happened since then…\n\n\n\nThe Rainbow network combines all DQN improvements and outperforms each of them. Source: (Hessel et al., 2017).\n\n\n\n\n\n\nHessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., et al. (2017). Rainbow: Combining Improvements in Deep Reinforcement Learning. http://arxiv.org/abs/1710.02298.\n\n\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., et al. (2016). Asynchronous Methods for Deep Reinforcement Learning. in Proc. ICML http://arxiv.org/abs/1602.01783.\n\n\nNiu, F., Recht, B., Re, C., and Wright, S. J. (2011). HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent. in Proc. Advances in Neural Information Processing Systems, 21–21. http://arxiv.org/abs/1106.5730.\n\n\nSutton, R. S., and Barto, A. G. (1998). Reinforcement Learning: An introduction. Cambridge, MA: MIT press."
  },
  {
    "objectID": "notes/3.5-DDPG.html",
    "href": "notes/3.5-DDPG.html",
    "title": "Deep Deterministic Policy Gradient (DDPG)",
    "section": "",
    "text": "Slides: html pdf"
  },
  {
    "objectID": "notes/3.5-DDPG.html#deterministic-policy-gradient-theorem",
    "href": "notes/3.5-DDPG.html#deterministic-policy-gradient-theorem",
    "title": "Deep Deterministic Policy Gradient (DDPG)",
    "section": "Deterministic policy gradient theorem",
    "text": "Deterministic policy gradient theorem\n\n\nProblem with stochastic policies\nActor-critic methods are strictly on-policy: the transitions used to train the critic must be generated by the current version of the actor.\n\n    \\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) \\, (R_t - V_\\varphi(s_t)) ]\n\n\n    \\mathcal{L}(\\varphi) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[(R_t - V_\\varphi(s_t))^2]\n\nPast transitions cannot be reused to train the actor (no replay memory). Domain knowledge cannot be used to guide the exploration.\nThe learned policy \\pi_\\theta(s, a) is stochastic. This generates a lot of variance in the obtained returns, therefore in the gradients. This can greatly impair learning (bad convergence) and slow it down (sample complexity). We would not have this problem if the policy was deterministic as in off-policy methods.\nThe objective function that we tried to maximize until now is :\n\n    \\mathcal{J}(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[R(\\tau)]\n\ni.e. we want the returns of all trajectories generated by the stochastic policy \\pi_\\theta to be maximal.\nIt is equivalent to say that we want the value of all states visited by the policy \\pi_\\theta to be maximal: a policy \\pi is better than another policy \\pi' if its expected return is greater or equal than that of \\pi' for all states s.\n\\pi > \\pi' \\Leftrightarrow V^{\\pi}(s) > V^{\\pi'}(s) \\quad \\forall s \\in \\mathcal{S}\nThe objective function can be rewritten as:\n\n    \\mathcal{J}'(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta}[V^{\\pi_\\theta}(s)]\n\nwhere \\rho_\\theta now represents the state visitation distribution, i.e. how often a state s will be visited by the policy \\pi_\\theta.\nThe two objective functions:\n\n    \\mathcal{J}(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[R(\\tau)]\n\nand:\n\n    \\mathcal{J}'(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta}[V^{\\pi_\\theta}(s)]\n\nare not the same: \\mathcal{J} has different values than \\mathcal{J}'.\nHowever, they have a maximum for the same optimal policy \\pi^* and their gradient is the same:\n\n    \\nabla_\\theta \\, \\mathcal{J}(\\theta) =  \\nabla_\\theta \\, \\mathcal{J}'(\\theta)\n\nIf a change in the policy \\pi_\\theta increases the return of all trajectories, it also increases the value of the visited states. Take-home message: their policy gradient is the same, we have the right to re-define the problem like this.\n\n   g = \\nabla_\\theta \\, \\mathcal{J}(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta}[\\nabla_\\theta \\, V^{\\pi_\\theta}(s)]\n\n\n\nDeterministic policy gradient\nWhen introducing Q-values, we obtain the following policy gradient:\n\n   g = \\nabla_\\theta \\, \\mathcal{J}(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta}[\\nabla_\\theta \\, V^{\\pi_\\theta}(s)] =  \\mathbb{E}_{s \\sim \\rho_\\theta}[\\sum_a \\nabla_\\theta \\, \\pi_\\theta(s, a) \\, Q^{\\pi_\\theta}(s, a)]\n\nThis formulation necessitates to integrate overall possible actions.\n\nNot possible with continuous action spaces.\nThe stochastic policy adds a lot of variance.\n\nBut let’s suppose that the policy is deterministic, i.e. it takes a single action in state s. We can note this deterministic policy \\mu_\\theta(s), with:\n\n\\begin{aligned}\n    \\mu_\\theta :  \\; \\mathcal{S} & \\rightarrow \\mathcal{A} \\\\\n    s & \\; \\rightarrow \\mu_\\theta(s) \\\\\n\\end{aligned}\n\nThe deterministic policy gradient becomes:\n\n   g = \\nabla_\\theta \\, \\mathcal{J}(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta}[\\nabla_\\theta \\, Q^{\\mu_\\theta}(s, \\mu_\\theta(s))]\n\nWe can now use the chain rule to decompose the gradient of Q^{\\mu_\\theta}(s, \\mu_\\theta(s)):\n\\nabla_\\theta \\, Q^{\\mu_\\theta}(s, \\mu_\\theta(s)) = \\nabla_a \\, Q^{\\mu_\\theta}(s, a)|_{a = \\mu_\\theta(s)} \\times \\nabla_\\theta \\mu_\\theta(s)\n\n\n\nChain rule applied to the deterministic policy gradient.\n\n\n\\nabla_a \\, Q^{\\mu_\\theta}(s, a)|_{a = \\mu_\\theta(s)} means that we differentiate Q^{\\mu_\\theta} w.r.t. a, and evaluate it in \\mu_\\theta(s). a is a variable, but \\mu_\\theta(s) is a deterministic value (constant).\n\\nabla_\\theta \\mu_\\theta(s) tells how the output of the policy network varies with the parameters of NN: automatic differentiation frameworks such as tensorflow can tell you that.\n\n\n\n\n\n\nDeterministic policy gradient theorem (Silver et al., 2014)\n\n\n\nFor any MDP, the deterministic policy gradient is:\n\\nabla_\\theta \\, \\mathcal{J}(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta}[\\nabla_a \\, Q^{\\mu_\\theta}(s, a)|_{a = \\mu_\\theta(s)} \\times \\nabla_\\theta \\mu_\\theta(s)]\n\n\n\n\nOff-policy actor-critic\nAs always, you do not know the true Q-value Q^{\\mu_\\theta}(s, a), because you search for the policy \\mu_\\theta. (Silver et al., 2014) showed that you can safely (without introducing any bias) replace the true Q-value with an estimate Q_\\varphi(s, a), as long as the estimate minimizes the mse with the TD target:\nQ_\\varphi(s, a) \\approx Q^{\\mu_\\theta}(s, a)\n\\mathcal{L}(\\varphi) = \\mathbb{E}_{s \\sim \\rho_\\theta}[(r(s, \\mu_\\theta(s)) + \\gamma \\, Q_\\varphi(s', \\mu_\\theta(s')) - Q_\\varphi(s, \\mu_\\theta(s)))^2]\nWe come back to an actor-critic architecture:\n\nThe deterministic actor \\mu_\\theta(s) selects a single action in state s.\nThe critic Q_\\varphi(s, a) estimates the value of that action.\n\n\n\n\nActor-critic architecture of the deterministic policy gradient with function approximation.\n\n\nTraining the actor:\n\n    \\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E}_{s \\sim \\rho_\\theta}[\\nabla_\\theta \\mu_\\theta(s) \\times \\nabla_a Q_\\varphi(s, a) |_{a = \\mu_\\theta(s)}]\n\nTraining the critic:\n\n    \\mathcal{L}(\\varphi) = \\mathbb{E}_{s \\sim \\rho_\\theta}[(r(s, \\mu_\\theta(s)) + \\gamma \\, Q_\\varphi(s', \\mu_\\theta(s')) - Q_\\varphi(s, \\mu_\\theta(s)))^2]\n\nIf you act off-policy, i.e. you visit the states s using a behavior policy b, you would theoretically need to correct the policy gradient with importance sampling:\n\n    \\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E}_{s \\sim \\rho_b}[\\sum_a \\, \\frac{\\pi_\\theta(s, a)}{b(s, a)} \\, \\nabla_\\theta \\mu_\\theta(s) \\times \\nabla_a Q_\\varphi(s, a) |_{a = \\mu_\\theta(s)}]\n\nBut your policy is now deterministic: the actor only takes the action a=\\mu_\\theta(s) with probability 1, not \\pi(s, a). The importance weight is 1 for that action, 0 for the other. You can safely sample states from a behavior policy, it won’t affect the deterministic policy gradient:\n\n    \\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E}_{s \\sim \\rho_b}[\\nabla_\\theta \\mu_\\theta(s) \\times \\nabla_a Q_\\varphi(s, a) |_{a = \\mu_\\theta(s)}]\n\nThe critic uses Q-learning, so it is also off-policy. DPG is an off-policy actor-critic architecture!"
  },
  {
    "objectID": "notes/3.5-DDPG.html#ddpg-deep-deterministic-policy-gradient",
    "href": "notes/3.5-DDPG.html#ddpg-deep-deterministic-policy-gradient",
    "title": "Deep Deterministic Policy Gradient (DDPG)",
    "section": "DDPG: Deep Deterministic Policy Gradient",
    "text": "DDPG: Deep Deterministic Policy Gradient\n\nAs the name indicates, DDPG (Lillicrap et al., 2015) is the deep variant of DPG for continuous control. It uses the DQN tricks to stabilize learning with deep networks:\n\nAs DPG is off-policy, an experience replay memory can be used to sample experiences.\nThe actor \\mu_\\theta learns using sampled transitions with DPG.\nThe critic Q_\\varphi uses Q-learning on sampled transitions: target networks can be used to cope with the non-stationarity of the Bellman targets.\n\nContrary to DQN, the target networks are not updated every once in a while, but slowly integrate the trained networks after each update (moving average of the weights):\n\\theta' \\leftarrow \\tau \\theta + (1-\\tau) \\, \\theta'\n\\varphi' \\leftarrow \\tau \\varphi + (1-\\tau) \\, \\varphi'\nA deterministic actor is good for learning (less variance), but not for exploring. We cannot use \\epsilon-greedy or softmax, as the actor outputs directly the policy, not Q-values. For continuous actions, an exploratory noise can be added to the deterministic action:\na_t = \\mu_\\theta(s_t) + \\xi_t\nEx: if the actor wants to move the joint of a robot by 2^o, it will actually be moved from 2.1^o or 1.9^o.\n\n\n\nDeep deterministic policy gradient architecture with exploratory noise.\n\n\nIn DDPG, an Ornstein-Uhlenbeck stochastic process (Uhlenbeck and Ornstein, 1930) is used to add noise to the continuous actions. It is defined by a stochastic differential equation, classically used to describe Brownian motion:\n dx_t = \\theta (\\mu - x_t) dt + \\sigma dW_t \\qquad \\text{with} \\qquad dW_t = \\mathcal{N}(0, dt)\nThe temporal mean of x_t is \\mu= 0, its amplitude is \\theta (exploration level), its speed is \\sigma.\n\n\n\nOrnstein-Uhlenbeck stochastic process.\n\n\nAnother approach to ensure exploration is to add noise to the parameters \\theta of the actor at inference time. For the same input s_t, the output \\mu_\\theta(s_t) will be different every time. The NoisyNet (Fortunato et al., 2017) approach can be applied to any deep RL algorithm to enable a smart state-dependent exploration (e.g. Noisy DQN).\n\n\n\n\n\n\nDDPG: deep deterministic policy gradient\n\n\n\n\nInitialize actor network \\mu_{\\theta} and critic Q_\\varphi, target networks \\mu_{\\theta'} and Q_{\\varphi'}, ERM \\mathcal{D} of maximal size N, random process \\xi.\nfor t \\in [0, T_\\text{max}]:\n\nSelect the action a_t = \\mu_\\theta(s_t) + \\xi and store (s_t, a_t, r_{t+1}, s_{t+1}) in the ERM.\nFor each transition (s_k, a_k, r_k, s'_k) in a minibatch of K transitions randomly sampled from \\mathcal{D}:\n\nCompute the target value using target networks\n\nt_k = r_k + \\gamma \\, Q_{\\varphi'}(s'_k, \\mu_{\\theta'}(s'_k))\nUpdate the critic by minimizing:\n\n\\mathcal{L}(\\varphi) = \\frac{1}{K} \\sum_k (t_k - Q_\\varphi(s_k, a_k))^2\n\nUpdate the actor by applying the deterministic policy gradient:\n\n\\nabla_\\theta \\mathcal{J}(\\theta) = \\frac{1}{K} \\sum_k \\nabla_\\theta \\mu_\\theta(s_k) \\times \\nabla_a Q_\\varphi(s_k, a) |_{a = \\mu_\\theta(s_k)}\n\nUpdate the target networks:\n\n\\theta' \\leftarrow \\tau \\theta + (1-\\tau) \\, \\theta' \\; ; \\; \\varphi' \\leftarrow \\tau \\varphi + (1-\\tau) \\, \\varphi'\n\n\n\nDDPG allows to learn continuous policies: there can be one tanh output neuron per joint in a robot. The learned policy is deterministic: this simplifies learning as we do not need to integrate over the action space after sampling. Exploratory noise (e.g. Ohrstein-Uhlenbeck) has to be added to the selected action during learning in order to ensure exploration. DDPG allows to use an experience replay memory, reusing past samples (better sample complexity than A3C).\n\n\n\n\n\n\n\nExample: learning to drive in a day\n\n\n\n\n\n\n\n\n\nThe algorithm of (Kendall et al., 2018) is based on DDPG with prioritized experience replay. Training is live, with an on-board NVIDIA Drive PX2 GPU. A simulated environment is first used to find the hyperparameters. A variational autoencoder (VAE) is optionally use to pretrain the convolutional layers on random episodes.\nMore info: https://wayve.ai/blog/learning-to-drive-in-a-day-with-reinforcement-learning"
  },
  {
    "objectID": "notes/3.5-DDPG.html#td3---twin-delayed-deep-deterministic-policy-gradient",
    "href": "notes/3.5-DDPG.html#td3---twin-delayed-deep-deterministic-policy-gradient",
    "title": "Deep Deterministic Policy Gradient (DDPG)",
    "section": "TD3 - Twin Delayed Deep Deterministic policy gradient",
    "text": "TD3 - Twin Delayed Deep Deterministic policy gradient\n\nDDPG suffers from several problems:\n\nUnstable (catastrophic forgetting, policy collapse).\nBrittleness (sensitivity to hyperparameters such as learning rates).\nOverestimation of Q-values.\n\nPolicy collapse happens when the bias of the critic is too high for the actor. Example with A2C:\n\n\n\nPolicy collapse happens regularly without warning: the performance is back to random. Source: Oliver Lange (2019). Investigation of Model-Based Augmentation of Model-Free Reinforcement Learning Algorithms. MSc thesis, TU Chemnitz.\n\n\nTD3 (Fujimoto et al., 2018) has been introduced to fix the problems of DDPG.\n\nTwin critics against overestimation\nAs any Q-learning-based method, DDPG overestimates Q-values. The Bellman target t = r + \\gamma \\, \\max_{a'} Q(s', a') uses a maximum over other values, so it is increasingly overestimated during learning. After a while, the overestimated Q-values disrupt training in the actor.\n\n\n\nOverestimation of Q-values by DDPG and TD3 (labelled CDQ here - clipped double Q-learning). Source: (Fujimoto et al., 2018)\n\n\nDouble Q-learning solves the problem by using the target network \\theta' to estimate Q-values, but the value network \\theta to select the greedy action in the next state:\n\n    \\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [(r + \\gamma \\, Q_{\\theta'}(s´, \\text{argmax}_{a'} Q_{\\theta}(s', a')) - Q_\\theta(s, a))^2]\n\nThe idea is to use two different independent networks to reduce overestimation. This does not work well with DDPG, as the Bellman target t = r + \\gamma \\, Q_{\\varphi'}(s', \\mu_{\\theta'}(s')) uses a target actor network that is not very different from the trained deterministic actor.\nTD3 uses two critics \\varphi_1 and \\varphi_2 (and target critics): the Q-value used to train the actor will be the lesser of two evils, i.e. the minimum Q-value:\nt = r + \\gamma \\, \\min(Q_{\\varphi'_1}(s', \\mu_{\\theta'}(s')), Q_{\\varphi'_2}(s', \\mu_{\\theta'}(s')))\nOne of the critic will always be less over-estimating than the other. Better than nothing… Using twin critics is called clipped double learning.\nBoth critics learn in parallel using the same target:\n\\mathcal{L}(\\varphi_1) = \\mathbb{E}[(t - Q_{\\varphi_1}(s, a))^2] \\qquad ; \\qquad \\mathcal{L}(\\varphi_2) = \\mathbb{E}[ (t - Q_{\\varphi_2}(s, a))^2]\nThe actor is trained using the first critic only:\n\\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E}[ \\nabla_\\theta \\mu_\\theta(s) \\times \\nabla_a Q_{\\varphi_1}(s, a) |_{a = \\mu_\\theta(s)} ]\n\n\nDelayed learning for stability\nAnother issue with actor-critic architecture in general is that the critic is always biased during training, what can impact the actor and ultimately collapse the policy:\n\\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E}[ \\nabla_\\theta \\mu_\\theta(s) \\times \\nabla_a Q_{\\varphi_1}(s, a) |_{a = \\mu_\\theta(s)} ]\nQ_{\\varphi_1}(s, a) \\approx Q^{\\mu_\\theta}(s, a)\nThe critic should learn much faster than the actor in order to provide unbiased gradients. Increasing the learning rate in the critic creates instability, reducing the learning rate in the actor slows down learning. The solution proposed by TD3 is to delay the update of the actor, i.e. update it only every d minibatches:\n\nTrain the critics \\varphi_1 and \\varphi_2 on the minibatch.\nevery d steps:\n\nTrain the actor \\theta on the minibatch.\n\n\nThis leaves enough time to the critics to improve their prediction and provides less biased gradients to the actor.\n\n\nTarget exploration\nA last problem with deterministic policies is that they tend to always select the same actions \\mu_\\theta(s) (overfitting). For exploration, some additive noise is added to the selected action:\na = \\mu_\\theta(s) + \\xi\nBut this is not true for the Bellman targets, which use the deterministic action:\nt = r + \\gamma \\, Q_{\\varphi}(s', \\mu_{\\theta}(s'))\nTD3 proposes to also use additive noise in the Bellman targets:\nt = r + \\gamma \\, Q_{\\varphi}(s', \\mu_{\\theta}(s') + \\xi)\nIf the additive noise is zero on average, the Bellman targets will be correct on average (unbiased) but will prevent overfitting of particular actions. The additive noise does not have to be an Ornstein-Uhlenbeck stochastic process, but could simply be a random variable:\n\\xi \\sim \\mathcal{N}(0, 1)\n\n\nAlgorithm\n\n\n\n\n\n\nTD3 - Twin Delayed Deep Deterministic policy gradient\n\n\n\n\nInitialize actor \\mu_{\\theta}, critics Q_{\\varphi_1}, Q_{\\varphi_2}, target networks \\mu_{\\theta'}, Q_{\\varphi_1'},Q_{\\varphi_2'}, ERM \\mathcal{D}, random processes \\xi_1, \\xi_2.\nfor t \\in [0, T_\\text{max}]:\n\nSelect the action a_t = \\mu_\\theta(s_t) + \\xi_1 and store (s_t, a_t, r_{t+1}, s_{t+1}) in the ERM.\nFor each transition (s_k, a_k, r_k, s'_k) in a minibatch sampled from \\mathcal{D}:\n\nCompute the target\n\nt_k = r_k + \\gamma \\, \\min(Q_{\\varphi_1'}(s'_k, \\mu_{\\theta'}(s'_k) + \\xi_2), Q_{\\varphi_2'}(s'_k, \\mu_{\\theta'}(s'_k) + \\xi_2))\nUpdate the critics by minimizing:\n\n\\mathcal{L}(\\varphi_1) = \\frac{1}{K} \\sum_k (t_k - Q_{\\varphi_1}(s_k, a_k))^2 \\qquad ; \\qquad \\mathcal{L}(\\varphi_2) = \\frac{1}{K} \\sum_k (t_k - Q_{\\varphi_2}(s_k, a_k))^2\n\nevery d steps:\n\nUpdate the actor by applying the DPG using Q_{\\varphi_1}:\n\n\\nabla_\\theta \\mathcal{J}(\\theta) = \\frac{1}{K} \\sum_k \\nabla_\\theta \\mu_\\theta(s_k) \\times \\nabla_a Q_{\\varphi_1}(s_k, a) |_{a = \\mu_\\theta(s_k)}\n\nUpdate the target networks:\n\n\\theta' \\leftarrow \\tau \\theta + (1-\\tau) \\, \\theta' \\; ; \\; \\varphi_1' \\leftarrow \\tau \\varphi_1 + (1-\\tau) \\, \\varphi_1' \\; ; \\; \\varphi_2' \\leftarrow \\tau \\varphi_2 + (1-\\tau) \\, \\varphi_2'\n\n\n\n\nTD3 (Fujimoto et al., 2018) introduces three major changes to DDPG:\n\ntwin critics.\ndelayed actor updates.\nnoisy Bellman targets.\n\nTD3 outperforms DDPG (but also PPO and SAC) on continuous control tasks.\n\n\n\nPerformance of TD3 on continuous control tasks compared to the state-of-the-art. Source: (Fujimoto et al., 2018)"
  },
  {
    "objectID": "notes/3.5-DDPG.html#d4pg-distributed-distributional-ddpg",
    "href": "notes/3.5-DDPG.html#d4pg-distributed-distributional-ddpg",
    "title": "Deep Deterministic Policy Gradient (DDPG)",
    "section": "D4PG: Distributed Distributional DDPG",
    "text": "D4PG: Distributed Distributional DDPG\nD4PG (Distributed Distributional DDPG, (Barth-Maron et al., 2018)) combines:\n\nDeterministic policy gradient as in DDPG:\n\n\\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E}_{s \\sim \\rho_b}[\\nabla_\\theta \\mu_\\theta(s) \\times \\nabla_a \\mathbb{E} [\\mathcal{Z}_\\varphi(s, a)] |_{a = \\mu_\\theta(s)}]\n\nDistributional critic: The critic does not predict single Q-values Q_\\varphi(s, a), but the distribution of returns \\mathcal{Z}_\\varphi(s, a) (as in Categorical DQN):\n\n\\mathcal{L}(\\varphi) = \\mathbb{E}_{s \\in \\rho_b} [ \\text{KL}(\\mathcal{T} \\, \\mathcal{Z}_\\varphi(s, a) || \\mathcal{Z}_\\varphi(s, a))]\n\nn-step returns (as in A3C):\n\n\\mathcal{T} \\, \\mathcal{Z}_\\varphi(s_t, a_t)= \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, \\mathcal{Z}_\\varphi(s_{t+n}, \\mu_\\theta(s_{t+n}))\n\nDistributed workers: D4PG uses K=32 or 64 copies of the actor to fill the ERM in parallel.\nPrioritized Experience Replay (PER):\n\nP(k) = \\frac{(|\\delta_k| + \\epsilon)^\\alpha}{\\sum_k (|\\delta_k| + \\epsilon)^\\alpha}\nIt could be called the Rainbow DDPG.\n\n\n\nAll components of D4PG are necessary to beat DDPG. Source: (Barth-Maron et al., 2018)\n\n\n\n\n\n\n\n\n\nParkour networks\n\n\n\nFor Parkour tasks, the states cover two different informations: the terrain (distance to obstacles, etc.) and the proprioception (joint positions of the agent). They enter the actor and critic networks at different locations.\n\n\n\nSource: (Barth-Maron et al., 2018)\n\n\n\n\n\n\n\n\nBarth-Maron, G., Hoffman, M. W., Budden, D., Dabney, W., Horgan, D., TB, D., et al. (2018). Distributed Distributional Deterministic Policy Gradients. http://arxiv.org/abs/1804.08617.\n\n\nFortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., et al. (2017). Noisy Networks for Exploration. http://arxiv.org/abs/1706.10295.\n\n\nFujimoto, S., van Hoof, H., and Meger, D. (2018). Addressing Function Approximation Error in Actor-Critic Methods. http://arxiv.org/abs/1802.09477.\n\n\nKendall, A., Hawke, J., Janz, D., Mazur, P., Reda, D., Allen, J.-M., et al. (2018). Learning to Drive in a Day. http://arxiv.org/abs/1807.00412.\n\n\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., et al. (2015). Continuous control with deep reinforcement learning. CoRR. http://arxiv.org/abs/1509.02971.\n\n\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. (2014). Deterministic Policy Gradient Algorithms. in Proc. ICML Proceedings of Machine Learning Research., eds. E. P. Xing and T. Jebara (PMLR), 387–395. http://proceedings.mlr.press/v32/silver14.html.\n\n\nUhlenbeck, G. E., and Ornstein, L. S. (1930). On the Theory of the Brownian Motion. Physical Review 36. doi:10.1103/PhysRev.36.823."
  },
  {
    "objectID": "notes/3.6-PPO.html",
    "href": "notes/3.6-PPO.html",
    "title": "Natural gradients (TRPO, PPO)",
    "section": "",
    "text": "Slides: html pdf"
  },
  {
    "objectID": "notes/3.6-PPO.html#rationale",
    "href": "notes/3.6-PPO.html#rationale",
    "title": "Natural gradients (TRPO, PPO)",
    "section": "Rationale",
    "text": "Rationale\n\nDQN and DDPG are off-policy methods, so we can use a replay memory.\n\nThey need less samples to converge as they re-use past experiences (sample efficient).\nThe critic is biased (overestimation), so learning is unstable and suboptimal.\n\nA3C is on-policy, we have to use distributed learning.\n\nThe critic is less biased, so it learns better policies (optimality).\nIt however need a lot of samples (sample complexity) as it must collect transitions with the current learned policy.\n\nAll suffer from parameter brittleness: choosing the right hyperparameters for a task is extremely difficult. For example a learning rate of 10^{-5} might work, but not 1.1 * 10^{-5}. Other hyperparameters: size of the ERM, update frequency of the target networks, training frequency. Can’t we do better?\nWhere is the problem with on-policy methods? The policy gradient is unbiased only when the critic Q_\\varphi(s, a) accurately approximates the true Q-values of the current policy.\n\n\\begin{aligned}\n    \\nabla_\\theta J(\\theta) & =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q^{\\pi_\\theta}(s, a)] \\\\\n    & \\approx  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q_\\varphi(s, a)]\n\\end{aligned}\n\nIf transitions are generated by a different (older) policy b, the policy gradient will be wrong. We could correct the policy gradient with importance sampling:\n\n    \\nabla_\\theta J(\\theta) \\approx  \\mathbb{E}_{s \\sim \\rho_b, a \\sim b}[ \\frac{\\pi_\\theta(s, a)}{b(s, a)} \\, \\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q_\\varphi(s, a))]\n\nThis is the off-policy actor-critic (Off-PAC) algorithm of (Degris et al., 2012). It is however limited to linear approximation, as the critic Q_\\varphi(s, a) needs to very quickly adapt to changes in the policy (deep NN are very slow learners) and the importance weight \\frac{\\pi_\\theta(s, a)}{b(s, a)} can have a huge variance.\nOnce we have an estimate of the policy gradient:\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q_\\varphi(s, a)]\n\nwe can update the weights \\theta in the direction of that gradient:\n\n    \\theta \\leftarrow \\theta + \\eta \\, \\nabla_\\theta J(\\theta)\n\n(or some variant of it, such as RMSprop or Adam). We search for the smallest parameter change (controlled by the learning rate \\eta) that produces the biggest positive change in the returns. Choosing the learning rate \\eta is extremely difficult in deep RL:\n\nIf the learning rate is too small, the network converges very slowly, requiring a lot of samples to converge (sample complexity).\nIf the learning rate is too high, parameter updates can totally destroy the policy (instability).\n\nThe learning rate should adapt to the current parameter values in order to stay in a trust region.\n\n\n\nToo big updates can lead to policy collapse. Source: https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-explained-a6ee04eeeee9\n\n\nThe policy gradient tells you in which direction of the parameter space \\theta the return is increasing the most. If you take too big a step in that direction, the new policy might become completely bad (policy collapse). Once the policy has collapsed, the new samples will all have a small return: the previous progress is lost. This is especially true when the parameter space has a high curvature, which is the case with deep NN.\nPolicy collapse is a huge problem in deep RL: the network starts learning correctly but suddenly collapses to a random agent. For on-policy methods, all progress is lost: the network has to relearn from scratch, as the new samples will be generated by a bad policy.\nTrust region optimization searches in the neighborhood of the current parameters \\theta which new value would maximize the return the most. This is a constrained optimization problem: we still want to maximize the return of the policy, but by keeping the policy as close as possible from its previous value.\n\n\n\nThe parameter update should be made inside the trust region. Source: https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-explained-a6ee04eeeee9\n\n\nThe size of the neighborhood determines the safety of the parameter change. In safe regions, we can take big steps. In dangerous regions, we have to take small steps. Problem: how can we estimate the safety of a parameter change?\n\n\n\nThe size of the trust region depends on the curvature of the objective function. Source: https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-explained-a6ee04eeeee9"
  },
  {
    "objectID": "notes/3.6-PPO.html#trpo-trust-region-policy-optimization",
    "href": "notes/3.6-PPO.html#trpo-trust-region-policy-optimization",
    "title": "Natural gradients (TRPO, PPO)",
    "section": "TRPO: Trust Region Policy Optimization",
    "text": "TRPO: Trust Region Policy Optimization\n\nWe want to maximize the expected return of a policy \\pi_\\theta, which is equivalent to the Q-value of every state-action pair visited by the policy:\n\\mathcal{J}(\\theta) = \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta} [Q^{\\pi_\\theta}(s, a)]\nLet’s note \\theta_\\text{old} the current value of the parameters of the policy \\pi_{\\theta_\\text{old}}.\n(Kakade and Langford, 2002) have shown that the expected return of a policy \\pi_\\theta is linked to the expected return of the current policy \\pi_{\\theta_\\text{old}} with:\n\\mathcal{J}(\\theta) = \\mathcal{J}(\\theta_\\text{old}) + \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta} [A^{\\pi_{\\theta_\\text{old}}}(s, a)]\nwhere:\nA^{\\pi_{\\theta_\\text{old}}}(s, a) = Q_\\theta(s, a) - Q_{\\theta_\\text{old}}(s, a)\nis the advantage of taking the action (s, a) and thereafter following \\pi_\\theta, compared to following the current policy \\pi_{\\theta_\\text{old}}.\nThe return under any policy \\theta is equal to the return under \\theta_\\text{old}, plus how the newly chosen actions in the rest of the trajectory improves (or worsens) the returns.\nIf we can estimate the advantages and maximize them, we can find a new policy \\pi_\\theta with a higher return than the current one.\n\\mathcal{L}(\\theta) = \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta} [A^{\\pi_{\\theta_\\text{old}}}(s, a)]\nBy definition, \\mathcal{L}(\\theta_\\text{old}) = 0, so the policy maximizing \\mathcal{L}(\\theta) has positive advantages and is better than \\pi_{\\theta_\\text{old}}.\n\\theta_\\text{new} = \\text{argmax}_\\theta \\; \\mathcal{L}(\\theta) \\; \\Rightarrow \\; \\mathcal{J}(\\theta_\\text{new}) \\geq \\mathcal{J}(\\theta_\\text{old})\nMaximizing the advantages ensures monotonic improvement: the new policy is always better than the previous one. Policy collapse is not possible!\nThe problem is that we have to take samples (s, a) from \\pi_\\theta: we do not know it yet, as it is what we search. The only policy at our disposal to estimate the advantages is the current policy \\pi_{\\theta_\\text{old}}. We could use importance sampling to sample from \\pi_{\\theta_\\text{old}}, but it would introduce a lot of variance (but see PPO later):\n\\mathcal{L}(\\theta) = \\mathbb{E}_{s \\sim \\rho_{\\theta_\\text{old}}, a \\sim \\pi_{\\theta_\\text{old}}} [\\frac{\\pi_{\\theta}(s, a)}{\\pi_{\\theta_\\text{old}}(s, a)} \\, A^{\\pi_{\\theta_\\text{old}}}(s, a)]\nIn TRPO (Schulman et al., 2015b), we are adding a constraint instead:\n\nthe new policy \\pi_{\\theta_\\text{new}} should not be (very) different from \\pi_{\\theta_\\text{old}}.\nthe importance sampling weight \\frac{\\pi_{\\theta_\\text{new}}(s, a)}{\\pi_{\\theta_\\text{old}}(s, a)} will not be very different from 1, so we can omit it.\n\nLet’s define a new objective function \\mathcal{J}_{\\theta_\\text{old}}(\\theta):\n\\mathcal{J}_{\\theta_\\text{old}}(\\theta) = \\mathcal{J}(\\theta_\\text{old}) + \\mathbb{E}_{s \\sim \\rho_{\\theta_\\text{old}}, a \\sim \\pi_{\\theta}} [A^{\\pi_{\\theta_\\text{old}}}(s, a)]\nThe only difference with \\mathcal{J}(\\theta) is that the visited states s are now sampled by the current policy \\pi_{\\theta_\\text{old}}. This makes the expectation tractable: we know how to visit the states, but we compute the advantage of actions taken by the new policy in those states.\nPrevious objective function:\n\\mathcal{J}(\\theta) = \\mathcal{J}(\\theta_\\text{old}) + \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta} [A^{\\pi_{\\theta_\\text{old}}}(s, a)]\nNew objective function:\n\\mathcal{J}_{\\theta_\\text{old}}(\\theta) = \\mathcal{J}(\\theta_\\text{old}) + \\mathbb{E}_{s \\sim \\rho_{\\theta_\\text{old}}, a \\sim \\pi_{\\theta}} [A^{\\pi_{\\theta_\\text{old}}}(s, a)]\nIt is “easy” to observe that the new objective function has the same value in \\theta_\\text{old}:\n\\mathcal{J}_{\\theta_\\text{old}}(\\theta_\\text{old}) = \\mathcal{J}(\\theta_\\text{old})\nand that its gradient w.r.t. \\theta is the same in \\theta_\\text{old}:\n\\nabla_\\theta \\mathcal{J}_{\\theta_\\text{old}}(\\theta)|_{\\theta = \\theta_\\text{old}} = \\nabla_\\theta \\, \\mathcal{J}(\\theta)|_{\\theta = \\theta_\\text{old}}\nAt least locally, maximizing \\mathcal{J}_{\\theta_\\text{old}}(\\theta) is exactly the same as maximizing \\mathcal{J}(\\theta). \\mathcal{J}_{\\theta_\\text{old}}(\\theta) is called a surrogate objective function: it is not what we want to maximize, but it leads to the same result locally.\n\n\n\nThe surrogate objective is locally the same as the true objective, so we can follow its gradient.\n\n\nHow big a step can we take when maximizing \\mathcal{J}_{\\theta_\\text{old}}(\\theta)? \\pi_\\theta and \\pi_{\\theta_\\text{old}} must be close from each other for the approximation to stand.\nThe first variant explored in the TRPO paper is a constrained optimization approach (Lagrange optimization):\n\n    \\max_\\theta \\mathcal{J}_{\\theta_\\text{old}}(\\theta) = \\mathcal{J}(\\theta_\\text{old}) + \\mathbb{E}_{s \\sim \\rho_{\\theta_\\text{old}}, a \\sim \\pi_{\\theta}} [A^{\\pi_{\\theta_\\text{old}}}(s, a)]\n\n\n    \\text{such that:} \\; D_\\text{KL} (\\pi_{\\theta_\\text{old}}||\\pi_\\theta) \\leq \\delta\n\nThe KL divergence between the distributions \\pi_{\\theta_\\text{old}} and \\pi_\\theta must be below a threshold \\delta. This version of TRPO uses a hard constraint: We search for a policy \\pi_\\theta that maximizes the expected return while staying within the trust region around \\pi_{\\theta_\\text{old}}.\nThe second approach regularizes the objective function with the KL divergence:\n\n    \\max_\\theta \\mathcal{L}(\\theta) = \\mathcal{J}_{\\theta_\\text{old}}(\\theta) - C \\, D_\\text{KL} (\\pi_{\\theta_\\text{old}}||\\pi_\\theta)\n\nwhere C is a regularization parameter controlling the importance of the constraint. This surrogate objective function is a lower bound of the initial objective \\mathcal{J}(\\theta):\n\nThe two objectives have the same value in \\theta_\\text{old}:\n\n\\mathcal{L}(\\theta_\\text{old}) = \\mathcal{J}_{\\theta_\\text{old}}(\\theta_\\text{old}) - C \\,  D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_{\\theta_\\text{old}}) = \\mathcal{J}(\\theta_\\text{old})\n\nTheir gradient w.r.t \\theta are the same in \\theta_\\text{old}:\n\n\\nabla_\\theta \\mathcal{L}(\\theta)|_{\\theta = \\theta_\\text{old}} = \\nabla_\\theta \\mathcal{J}(\\theta)|_{\\theta = \\theta_\\text{old}}\n\nThe surrogate objective is always smaller than the real objective, as the KL divergence is positive:\n\n\\mathcal{J}(\\theta) \\geq \\mathcal{J}_{\\theta_\\text{old}}(\\theta) - C \\,  D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta)\n\n\n\nThe surrogate objective adds the constraint that the update should stay in the trust region.\n\n\nThe policy \\pi_\\theta maximizing the surrogate objective \\mathcal{L}(\\theta) = \\mathcal{J}_{\\theta_\\text{old}}(\\theta) - C \\, D_\\text{KL} (\\pi_{\\theta_\\text{old}}||\\pi_\\theta) has a higher expected return than \\pi_{\\theta_\\text{old}}:\n\\mathcal{J}(\\theta) > \\mathcal{J}(\\theta_\\text{old})\nis very close to \\pi_{\\theta_\\text{old}}:\nD_\\text{KL} (\\pi_{\\theta_\\text{old}}||\\pi_\\theta) \\approx 0\nbut the parameters \\theta are much closer to the optimal parameters \\theta^*.\nThe version with a soft constraint necessitates a prohibitively small learning rate in practice. The implementation of TRPO uses the hard constraint with Lagrange optimization, what necessitates using conjugate gradients optimization, the Fisher Information matrix and natural gradients: very complex to implement… However, there is a monotonic improvement guarantee: the successive policies can only get better over time, no policy collapse! This is the major advantage of TRPO compared to the other methods: it always works, although very slowly."
  },
  {
    "objectID": "notes/3.6-PPO.html#ppo-proximal-policy-optimization",
    "href": "notes/3.6-PPO.html#ppo-proximal-policy-optimization",
    "title": "Natural gradients (TRPO, PPO)",
    "section": "PPO: Proximal Policy Optimization",
    "text": "PPO: Proximal Policy Optimization\n\nLet’s take the unconstrained objective function of TRPO:\n\\mathcal{J}_{\\theta_\\text{old}}(\\theta) = \\mathcal{J}(\\theta_\\text{old}) + \\mathbb{E}_{s \\sim \\rho_{\\theta_\\text{old}}, a \\sim \\pi_{\\theta}} [A^{\\pi_{\\theta_\\text{old}}}(s, a)]\n\\mathcal{J}(\\theta_\\text{old}) does not depend on \\theta, so we only need to maximize the advantages:\n\\mathcal{L}(\\theta) = \\mathbb{E}_{s \\sim \\rho_{\\theta_\\text{old}}, a \\sim \\pi_{\\theta}} [A^{\\pi_{\\theta_\\text{old}}}(s, a)]\nIn order to avoid sampling action from the unknown policy \\pi_\\theta, we can use importance sampling with the current policy:\n\\mathcal{L}(\\theta) = \\mathbb{E}_{s \\sim \\rho_{\\theta_\\text{old}}, a \\sim \\pi_{\\theta_\\text{old}}} [\\rho(s, a) \\, A^{\\pi_{\\theta_\\text{old}}}(s, a)]\nwith \\rho(s, a) = \\frac{\\pi_{\\theta}(s, a)}{\\pi_{\\theta_\\text{old}}(s, a)} being the importance sampling weight. But the importance sampling weight \\rho(s, a) introduces a lot of variance, worsening the sample complexity. Is there another way to make sure that \\pi_\\theta is not very different from \\pi_{\\theta_\\text{old}}, therefore reducing the variance of the importance sampling weight?\nThe solution introduced by PPO (Schulman et al., 2017) is simply to clip the importance sampling weight when it is too different from 1:\n\\mathcal{L}(\\theta) = \\mathbb{E}_{s \\sim \\rho_{\\theta_\\text{old}}, a \\sim \\pi_{\\theta_\\text{old}}} [\\min(\\rho(s, a) \\, A^{\\pi_{\\theta_\\text{old}}}(s, a), \\text{clip}(\\rho(s, a), 1-\\epsilon, 1+\\epsilon) \\, A^{\\pi_{\\theta_\\text{old}}}(s, a))]\nFor each sampled action (s, a), we use the minimum between:\n\nthe TRPO unconstrained objective with IS \\rho(s, a) \\, A^{\\pi_{\\theta_\\text{old}}}(s, a).\nthe same, but with the IS weight clipped between 1-\\epsilon and 1+\\epsilon.\n\n\n\n\nClipped importance sampling weight.\n\n\nIf the advantage A^{\\pi_{\\theta_\\text{old}}}(s, a) is positive (better action than usual) and:\n\nthe IS is higher than 1+\\epsilon, we use (1+\\epsilon) \\, A^{\\pi_{\\theta_\\text{old}}}(s, a).\notherwise, we use \\rho(s, a) \\, A^{\\pi_{\\theta_\\text{old}}}(s, a).\n\n\n\n\nClipped objective for positive advantages.\n\n\nIf the advantage A^{\\pi_{\\theta_\\text{old}}}(s, a) is negative (worse action than usual) and:\n\nthe IS is lower than 1-\\epsilon, we use (1-\\epsilon) \\, A^{\\pi_{\\theta_\\text{old}}}(s, a).\notherwise, we use \\rho(s, a) \\, A^{\\pi_{\\theta_\\text{old}}}(s, a).\n\n\n\n\nClipped objective for negative advantages.\n\n\nThis avoids changing too much the policy between two updates:\n\nGood actions (A^{\\pi_{\\theta_\\text{old}}}(s, a) > 0) do not become much more likely than before.\nBad actions (A^{\\pi_{\\theta_\\text{old}}}(s, a) < 0) do not become much less likely than before.\n\nThe PPO clipped objective ensures than the importance sampling weight stays around one, so the new policy is not very different from the old one. It can learn from single transitions.\n\\mathcal{L}(\\theta) = \\mathbb{E}_{s \\sim \\rho_{\\theta_\\text{old}}, a \\sim \\pi_{\\theta_\\text{old}}} [\\min(\\rho(s, a) \\, A^{\\pi_{\\theta_\\text{old}}}(s, a), \\text{clip}(\\rho(s, a), 1-\\epsilon, 1+\\epsilon) \\, A^{\\pi_{\\theta_\\text{old}}}(s, a))]\nThe advantage of an action can be learned using any advantage estimator, for example the n-step advantage:\nA^{\\pi_{\\theta_\\text{old}}}(s_t, a_t) =  \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V_\\varphi(s_{t+n}) - V_\\varphi(s_{t})\nMost implementations use Generalized Advantage Estimation (GAE, (Schulman et al., 2015a)). PPO is therefore an actor-critic method (as TRPO). PPO is on-policy: it collects samples using distributed learning (as A3C) and then applies several updates to the actor and critic.\n\n\n\n\n\n\nPPO: Proximal Policy Optimization\n\n\n\n\nInitialize an actor \\pi_\\theta and a critic V_\\varphi with random weights.\nwhile not converged :\n\nfor N workers in parallel:\n\nCollect T transitions using \\pi_{\\theta}.\nCompute the advantage A_\\varphi(s, a) of each transition using the critic V_\\varphi.\n\nfor K epochs:\n\nSample M transitions \\mathcal{D} from the ones previously collected.\nTrain the actor to maximize the clipped surrogate objective.\n\n\\mathcal{L}(\\theta) = \\mathbb{E}_{s, a \\sim \\mathcal{D}} [\\min(\\rho(s, a) \\, A_\\varphi(s, a), \\text{clip}(\\rho(s, a), 1-\\epsilon, 1+\\epsilon) \\, A_\\varphi(s, a))]\n\nTrain the critic to minimize the advantage.\n\n\\mathcal{L}(\\varphi) = \\mathbb{E}_{s, a \\sim \\mathcal{D}} [(A_\\varphi(s, a))^2]\n\n\n\n\nPPO is an on-policy actor-critic PG algorithm, using distributed learning. Clipping the importance sampling weight allows to avoid policy collapse, by staying in the trust region (the policy does not change much between two updates). The monotonic improvement guarantee is very important: the network will always find a (local) maximum of the returns. PPO is much less sensible to hyperparameters than DDPG (brittleness): works often out of the box with default settings. It does not necessitate complex optimization procedures like TRPO: first-order methods such as SGD work (easy to implement). The actor and the critic can share weights (unlike TRPO), allowing to work with pixel-based inputs, convolutional or recurrent layers. It can use discrete or continuous action spaces, although it is most efficient in the continuous case. Go-to method for robotics. Drawback: not very sample efficient.\nImplementing PPO necessitates quite a lot of tricks (early stopping, MPI). OpenAI Baselines or SpinningUp provide efficient implementations:\nhttps://spinningup.openai.com/en/latest/algorithms/ppo.html\nhttps://github.com/openai/baselines/tree/master/baselines/ppo2\n\n\n\nPerformance of PPO on Mujoco continuous control tasks (Schulman et al., 2017).\n\n\nSee https://openai.com/blog/openai-baselines-ppo/ for more videos."
  },
  {
    "objectID": "notes/3.6-PPO.html#openai-five-dota-2",
    "href": "notes/3.6-PPO.html#openai-five-dota-2",
    "title": "Natural gradients (TRPO, PPO)",
    "section": "OpenAI Five: Dota 2",
    "text": "OpenAI Five: Dota 2\n\nPPO is used by OpenAI to play Dota 2. Their website is very well made:\nhttps://openai.com/projects/five/\n\nWhy is Dota 2 hard?\n\n\n\nDota 2 is much harder for AI than chess or Go. Source: https://openai.com/projects/five/\n\n\n\n\n\n\n\n\n\n\n\nFeature\nChess\nGo\nDota 2\n\n\n\n\nTotal number of moves Number of possible actions Number of inputs\n40 35 70\n150 250 400\n20000 1000 20000\n\n\n\nOpenAI Five is composed of 5 PPO networks (one per player), using 128,000 CPUs and 256 V100 GPUs.\n\n\n\nArchitecture of OpenAI five. Source: https://openai.com/projects/five/\n\n\n\n\n\nHardware requirements. Source: https://openai.com/projects/five/\n\n\n\n\n\nState representation. Source: https://openai.com/projects/five/\n\n\n\n\n\nDeep NN used as the actor. Source: https://openai.com/projects/five/\n\n\nCheck the complete NN architecture at: https://d4mucfpksywv.cloudfront.net/research-covers/openai-five/network-architecture.pdf\nThe agents are trained by self-play. Each worker plays against:\n\nthe current version of the network 80% of the time.\nan older version of the network 20% of the time.\n\nThe reward function is hand-designed using human heuristics: net worth, kills, deaths, assists, last hits…\n\n\n\nOpenAI five. Source: https://openai.com/projects/five/\n\n\nThe discount factor \\gamma is annealed from 0.998 (valuing future rewards with a half-life of 46 seconds) to 0.9997 (valuing future rewards with a half-life of five minutes). Coordinating all the resources (CPU, GPU) is actually the main difficulty: Kubernetes, Azure, and GCP backends for Rapid, TensorBoard, Sentry and Grafana for monitoring…"
  },
  {
    "objectID": "notes/3.6-PPO.html#acer-actor-critic-with-experience-replay",
    "href": "notes/3.6-PPO.html#acer-actor-critic-with-experience-replay",
    "title": "Natural gradients (TRPO, PPO)",
    "section": "ACER: Actor-Critic with Experience Replay",
    "text": "ACER: Actor-Critic with Experience Replay\nACER (Wang et al., 2017) is the off-policy version of PPO:\n\nOff-policy actor-critic architecture (using experience replay),\nRetrace estimation of values (Munos et al. 2016),\nImportance sampling weight truncation with bias correction,\nEfficient trust region optimization (TRPO),\nStochastic Dueling Network (SDN) in order to estimate both Q_\\varphi(s, a) and V_\\varphi(s).\n\nThe performance is comparable to PPO. It works sometimes better than PPO on some environments, sometimes not.\n\n\n\n\nDegris, T., White, M., and Sutton, R. S. (2012). Linear Off-Policy Actor-Critic. in Proceedings of the 2012 International Conference on Machine Learning http://arxiv.org/abs/1205.4839.\n\n\nKakade, S., and Langford, J. (2002). Approximately Optimal Approximate Reinforcement Learning. Proc. 19th International Conference on Machine Learning, 267–274. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.7601.\n\n\nSchulman, J., Chen, X., and Abbeel, P. (2017). Equivalence Between Policy Gradients and Soft Q-Learning. http://arxiv.org/abs/1704.06440.\n\n\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015a). Trust Region Policy Optimization. in Proceedings of the 31 st International Conference on Machine Learning, 1889–1897. http://proceedings.mlr.press/v37/schulman15.html.\n\n\nSchulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. (2015b). High-Dimensional Continuous Control Using Generalized Advantage Estimation. http://arxiv.org/abs/1506.02438.\n\n\nWang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z., Munos, R., et al. (2017). Learning to reinforcement learn. http://arxiv.org/abs/1611.05763."
  },
  {
    "objectID": "notes/3.7-SAC.html",
    "href": "notes/3.7-SAC.html",
    "title": "Maximum Entropy RL (SAC)",
    "section": "",
    "text": "Slides: html pdf"
  },
  {
    "objectID": "notes/3.7-SAC.html#soft-rl",
    "href": "notes/3.7-SAC.html#soft-rl",
    "title": "Maximum Entropy RL (SAC)",
    "section": "Soft RL",
    "text": "Soft RL\n\nAll methods seen so far search the optimal policy that maximizes the return:\n\\pi^* = \\text{arg} \\max_\\pi \\, \\mathbb{E}_{\\pi} [ \\sum_t \\gamma^t \\, r(s_t, a_t, s_{t+1}) ]\nThe optimal policy is deterministic and greedy by definition.\n\\pi^*(s) = \\text{arg} \\max_a Q^*(s, a)\nExploration is ensured externally by :\n\napplying \\epsilon-greedy or softmax on the Q-values (DQN),\nadding exploratory noise (DDPG),\nlearning stochastic policies that become deterministic over time (A3C, PPO).\n\nIs “hard” RL, caring only about exploitation, always the best option?\nThe optimal policy is only greedy for a MDP, not obligatorily for a POMDP. Games like chess are POMDPs: you do not know what your opponent is going to play (missing information). If you always play the same moves (e.g. opening moves), your opponent will adapt and you will end up losing systematically. Variety in playing is beneficial in POMDPs: it can counteract the uncertainty about the environment (Todorov, 2008), (Toussaint, 2009).\nThere are sometimes more than one way to collect rewards, especially with sparse rewards. If exploration decreases too soon, the RL agent will “overfit” one of the paths. If one of the paths is suddenly blocked, the agent would have to completely re-learn its policy. It would be more efficient if the agent had learned all possibles paths, even if some of them are less optimal.\nSoftmax policies allow to learn multimodal policies, but only for discrete action spaces.\n\n    \\pi(s, a) = \\frac{\\exp Q(s, a) / \\tau}{ \\sum_b \\exp Q(s, b) / \\tau}\n\nIn continuous action spaces, we would have to integrate over the whole action space, what is not tractable. Exploratory noise as in DDPG only leads to unimodal policies: greedy action plus some noise.\n\n\n\nUnimodal policies explore around the greedy action. Source: https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/\n\n\n\n\n\nMultimodal policies explore in the whole action space. Source: https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/"
  },
  {
    "objectID": "notes/3.7-SAC.html#continuous-stochastic-policies",
    "href": "notes/3.7-SAC.html#continuous-stochastic-policies",
    "title": "Maximum Entropy RL (SAC)",
    "section": "Continuous stochastic policies",
    "text": "Continuous stochastic policies\n\nThe easiest to implement a stochastic policy with a neural network is a Gaussian policy. Suppose that we want to control a robotic arm with n degrees of freedom. An action \\mathbf{a} is a vector of joint displacements:\n\\mathbf{a} = \\begin{bmatrix} \\Delta \\theta_1 & \\Delta \\theta_2 & \\ldots \\, \\Delta \\theta_n\\end{bmatrix}^T\nA Gaussian policy considers the vector \\mathbf{a} to be sampled from the normal distribution \\mathcal{N}(\\mu_\\theta(s), \\sigma_\\theta(s)). The mean \\mu_\\theta(s) and standard deviation \\sigma_\\theta(s) are vectors that can be the output of the actor neural network with parameters \\theta. Sampling an action from the normal distribution is done through the reparameterization trick:\n\\mathbf{a} = \\mu_\\theta(s) + \\sigma_\\theta(s) \\, \\xi\nwhere \\xi \\sim \\mathcal{N}(0, 1) comes from the standard normal distribution.\nThe good thing with the normal distribution is that we know its pdf:\n\n    \\pi_\\theta(s, a) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_\\theta(s)}} \\, \\exp -\\frac{(a - \\mu_\\theta(s))^2}{2\\sigma^2_\\theta(s)}\n\nWhen estimating the policy gradient (REINFORCE, A3C, PPO, etc):\n\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s \\sim \\rho^\\pi, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta (s, a) \\, \\psi ]\n\nthe log-likelihood \\log \\pi_\\theta (s, a) is a simple function of \\mu_\\theta(s) and \\sigma_\\theta(s):\n\\log \\pi_\\theta (s, a) = -\\frac{(a - \\mu_\\theta(s))^2}{2\\sigma^2_\\theta(s)} - \\frac{1}{2} \\, \\log 2\\pi\\sigma^2_\\theta(s)\nso we can easily compute its gradient w.r.t \\theta and apply backpropagation:\n\n    \\nabla_{\\mu_\\theta(s)} \\log \\pi_\\theta (s, a) = \\frac{a - \\mu_\\theta(s)}{\\sigma_\\theta(s)^2} \\qquad \\nabla_{\\sigma_\\theta(s)} \\log \\pi_\\theta (s, a) = \\frac{(a - \\mu_\\theta(s))^2}{\\sigma_\\theta(s)^3} - \\frac{1}{\\sigma_\\theta(s)}\n\nA Gaussian policy samples actions from the normal distribution \\mathcal{N}(\\mu_\\theta(s), \\sigma_\\theta(s)), with \\mu_\\theta(s) and \\sigma_\\theta(s) being the output of the actor.\n\\mathbf{a} = \\mu_\\theta(s) + \\sigma_\\theta(s) \\, \\xi\nThe score \\nabla_\\theta \\log \\pi_\\theta (s, a) can be obtained easily using the output of the actor:\n\n    \\nabla_{\\mu_\\theta(s)} \\log \\pi_\\theta (s, a) = \\frac{a - \\mu_\\theta(s)}{\\sigma_\\theta(s)^2}\n\n\n\\nabla_{\\sigma_\\theta(s)} \\log \\pi_\\theta (s, a) = \\frac{(a - \\mu_\\theta(s))^2}{\\sigma_\\theta(s)^3} - \\frac{1}{\\sigma_\\theta(s)}\n\nThe rest of the score (\\nabla_\\theta \\mu_\\theta(s) and \\nabla_\\theta \\sigma_\\theta(s)) is the problem of tensorflow/pytorch. This is the same reparametrization trick used in variational autoencoders to allow backpropagation to work through a sampling operation. Beta distributions are an even better choice to parameterize stochastic policies (Chou et al., 2017)."
  },
  {
    "objectID": "notes/3.7-SAC.html#maximum-entropy-rl",
    "href": "notes/3.7-SAC.html#maximum-entropy-rl",
    "title": "Maximum Entropy RL (SAC)",
    "section": "Maximum Entropy RL",
    "text": "Maximum Entropy RL\n\nAlthough stochastic, Gaussian policies are still unimodal policies: they mostly sample actions around the mean \\mu_\\theta(s) and the variance \\sigma_\\theta(s) decreases to 0 with learning. If we want a multimodal policy that learns different solutions, we need to learn a Softmax distribution (Gibbs / Boltzmann) over the action space. How can we do that when the action space is continuous?\nA solution to force the policy to be multimodal is to force it to be as stochastic as possible by maximizing its entropy. Instead of searching for the policy that “only” maximizes the returns:\n\n    \\pi^* = \\text{arg} \\max_\\pi \\, \\mathbb{E}_{\\pi} [ \\sum_t \\gamma^t \\, r(s_t, a_t, s_{t+1}) ]\n\nwe search for the policy that maximizes the returns while being as stochastic as possible:\n\n    \\pi^* = \\text{arg} \\max_\\pi \\, \\mathbb{E}_{\\pi} [ \\sum_t \\gamma^t \\, r(s_t, a_t, s_{t+1}) + \\alpha \\, H(\\pi(s_t))]\n\nThis new objective function defines the maximum entropy RL framework (Williams and Peng, 1991). The entropy of the policy regularizes the objective function: the policy should still maximize the returns, but stay as stochastic as possible depending on the parameter \\alpha. Entropy regularization can always be added to PG methods such as A3C. It is always possible to fall back to hard RL by setting \\alpha to 0.\nThe entropy of a policy in a state s_t is defined by the expected negative log-likelihood of the policy:\nH(\\pi_\\theta(s_t)) = \\mathbb{E}_{a \\sim \\pi_\\theta(s_t)} [- \\log \\pi_\\theta(s_t, a)]\nFor a discrete action space:\n\n    H(\\pi_\\theta(s_t)) = - \\sum_a \\pi_\\theta(s_t, a) \\, \\log \\pi_\\theta(s_t, a)\n\nFor a continuous action space:\n\n    H(\\pi_\\theta(s_t)) = - \\int_a \\pi_\\theta(s_t, a) \\, \\log \\pi_\\theta(s_t, a) \\, da\n\nThe entropy necessitates to sum or integrate the self-information of each possible action in a given state. A deterministic (greedy) policy has zero entropy, the same action is always taken: exploitation. A random policy has a high entropy, you cannot predict which action will be taken: exploration. Maximum entropy RL embeds the exploration-exploitation trade-off inside the objective function instead of relying on external mechanisms such as the softmax temperature.\nIn soft Q-learning (Haarnoja et al., 2017), the objective function is defined over complete trajectories:\n\n    \\mathcal{J}(\\theta) = \\sum_t \\gamma^t \\, \\mathbb{E}_{\\pi} [ r(s_t, a_t, s_{t+1}) + \\alpha \\, H(\\pi(s_t))]\n\nThe goal of the agent is to generate trajectories associated with a lot of rewards (high return) but only visiting states with a high entropy, i.e. where the policy is random (exploration).\nThe agent can decide how the trade-off is solved via regularization:\n\nIf a single action leads to high rewards, the policy may become deterministic.\nIf several actions lead to equivalent rewards, the policy must stay stochastic.\n\nIn soft Q-learning, the policy is implemented as a softmax over soft Q-values:\n\n    \\pi_\\theta(s, a) = \\dfrac{\\exp  \\dfrac{Q^\\text{soft}_\\theta (s, a)}{\\alpha}}{\\sum_b \\exp \\dfrac{Q^\\text{soft}_\\theta (s, b)}{\\alpha}} \\propto \\exp \\dfrac{Q^\\text{soft}_\\theta (s, a)}{\\alpha}\n\n\\alpha plays the role of the softmax temperature parameter \\tau.\nSoft Q-learning belongs to energy-based models, as -\\dfrac{Q^\\text{soft}_\\theta (s, a)}{\\alpha} represents the energy of the Boltzmann distribution (see restricted Boltzmann machines). The partition function \\sum_b \\exp \\dfrac{Q^\\text{soft}_\\theta (s, b)}{\\alpha} is untractable for continuous action spaces, as one would need to integrate over the whole action space, but it will disappear from the equations anyway.\nSoft V and Q values are the equivalent of the hard value functions, but for the new objective:\n\n    \\mathcal{J}(\\theta) = \\sum_t \\gamma^t \\, \\mathbb{E}_{\\pi} [ r(s_t, a_t, s_{t+1}) + \\alpha \\, H(\\pi(s_t))]\n\nThe soft value of an action depends on the immediate reward and the soft value of the next state (soft Bellman equation):\n\n    Q^\\text{soft}_\\theta(s_t, a_t) = \\mathbb{E}_{s_{t+1} \\in \\rho_\\theta} [r(s_t, a_t, s_{t+1}) + \\gamma \\, V^\\text{soft}_\\theta(s_{t+1})]\n\nThe soft value of a state is the expected value over the available actions plus the entropy of the policy.\n\n    V^\\text{soft}_\\theta(s_t) = \\mathbb{E}_{a_{t} \\in \\pi} [Q^\\text{soft}_\\theta(s_{t}, a_{t})] + H(\\pi_\\theta(s_t)) = \\mathbb{E}_{a_{t} \\in \\pi} [Q^\\text{soft}_\\theta(s_{t}, a_{t}) -  \\log \\, \\pi_\\theta(s_t, a_t)]\n\n(Haarnoja et al., 2017) showed that these soft value functions are the solution of the entropy-regularized objective function. All we need is to be able to estimate them… Soft Q-learning uses complex optimization methods (variational inference) to do it, but SAC is more practical."
  },
  {
    "objectID": "notes/3.7-SAC.html#soft-actor-critic-sac",
    "href": "notes/3.7-SAC.html#soft-actor-critic-sac",
    "title": "Maximum Entropy RL (SAC)",
    "section": "Soft Actor-Critic (SAC)",
    "text": "Soft Actor-Critic (SAC)\n\nPutting:\n\n    Q^\\text{soft}_\\theta(s_t, a_t) = \\mathbb{E}_{s_{t+1} \\in \\rho_\\theta} [r(s_t, a_t, s_{t+1}) + \\gamma \\, V^\\text{soft}_\\theta(s_{t+1})]\n\nand:\n\n    V^\\text{soft}_\\theta(s_t) = \\mathbb{E}_{a_{t} \\in \\pi} [Q^\\text{soft}_\\theta(s_{t}, a_{t}) -  \\log \\, \\pi_\\theta(s_t, a_t)]\n\ntogether, we obtain:\n\n    Q^\\text{soft}_\\theta(s_t, a_t) = \\mathbb{E}_{s_{t+1} \\in \\rho_\\theta} [r(s_t, a_t, s_{t+1}) + \\gamma \\, \\mathbb{E}_{a_{t+1} \\in \\pi} [Q^\\text{soft}_\\theta(s_{t+1}, a_{t+1}) -  \\log \\, \\pi_\\theta(s_{t+1}, a_{t+1})]]\n\nIf we want to train a critic Q_\\varphi(s, a) to estimate the true soft Q-value of an action Q^\\text{soft}_\\theta(s, a), we just need to sample (s_t, a_t, r_{t+1}, a_{t+1}) transitions and minimize:\n\n    \\mathcal{L}(\\varphi) = \\mathbb{E}_{s_t, a_t, s_{t+1} \\sim \\rho_\\theta} [(r_{t+1} + \\gamma \\, Q_\\varphi(s_{t+1}, a_{t+1}) - \\log \\pi_\\theta(s_{t+1}, a_{t+1}) - Q_\\varphi(s_{t}, a_{t}) )^2]\n\nThe only difference with a SARSA critic is that the negative log-likelihood of the next action is added to the target. In practice, s_t, a_t and r_{t+1} can come from a replay buffer, but a_{t+1} has to be sampled from the current policy \\pi_\\theta (but not taken!). SAC (Haarnoja et al., 2018) is therefore an off-policy actor-critic algorithm, but with stochastic policies!\nBut how do we train the actor? The policy is defined by a softmax over the soft Q-values, but the log-partition Z is untractable for continuous spaces:\n\n    \\pi_\\theta(s, a) = \\dfrac{\\exp  \\dfrac{Q_\\varphi (s, a)}{\\alpha}}{\\sum_b \\exp \\dfrac{Q_\\varphi (s, b)}{\\alpha}} = \\dfrac{1}{Z} \\, \\exp \\dfrac{Q_\\varphi (s, a)}{\\alpha}\n\nThe trick is to make the parameterized actor \\pi_\\theta learn to be close from this softmax, by minimizing the KL divergence:\n\n    \\mathcal{L}(\\theta) = D_\\text{KL} (\\pi_\\theta(s, a) || \\dfrac{1}{Z} \\, \\exp \\dfrac{Q_\\varphi (s, a)}{\\alpha}) = \\mathbb{E}_{s, a \\sim \\pi_\\theta(s, a)} [- \\log \\dfrac{\\dfrac{1}{Z} \\, \\exp \\dfrac{Q_\\varphi (s, a)}{\\alpha}}{\\pi_\\theta(s, a)}]\n\nAs Z does not depend on \\theta, it will automagically disappear when taking the gradient!\n\n   \\nabla_\\theta \\, \\mathcal{L}(\\theta) = \\mathbb{E}_{s, a} [\\alpha \\, \\nabla_\\theta \\log \\pi_\\theta(s, a) - Q_\\varphi (s, a)]\n\nSo the actor just has to implement a Gaussian policy and we can train it using soft-Q-value.\nSoft Actor-Critic (SAC) is an off-policy actor-critic architecture for maximum entropy RL:\n\n    \\mathcal{J}(\\theta) = \\sum_t \\gamma^t \\, \\mathbb{E}_{\\pi} [ r(s_t, a_t, s_{t+1}) + \\alpha \\, H(\\pi(s_t))]\n\nMaximizing the entropy of the policy ensures an efficient exploration. It is even possible to learn the value of the parameter \\alpha. The critic learns to estimate soft Q-values that take the entropy of the policy into account:\n\n    \\mathcal{L}(\\varphi) = \\mathbb{E}_{s_t, a_t, s_{t+1} \\sim \\rho_\\theta} [(r_{t+1} + \\gamma \\, Q_\\varphi(s_{t+1}, a_{t+1}) - \\log \\pi_\\theta(s_{t+1}, a_{t+1}) - Q_\\varphi(s_{t}, a_{t}) )^2]\n\nThe actor learns a Gaussian policy that becomes close to a softmax over the soft Q-values:\n\n    \\pi_\\theta(s, a) \\propto \\exp \\dfrac{Q_\\varphi (s, a)}{\\alpha}\n\n\n   \\nabla_\\theta \\, \\mathcal{L}(\\theta) = \\mathbb{E}_{s, a} [\\alpha \\, \\nabla_\\theta \\log \\pi_\\theta(s, a) - Q_\\varphi (s, a)]\n\nIn practice, SAC uses clipped double learning like TD3: it takes the lesser of two evils between two critics Q_{\\varphi_1} and Q_{\\varphi_2}. The next action a_{t+1} comes from the current policy, no need for target networks. Unlike TD3, the learned policy is stochastic: no need for target noise as the targets are already stochastic. See https://spinningup.openai.com/en/latest/algorithms/sac.html for a detailed comparison of SAC and TD3. The initial version of SAV additionally learned a soft V-value critic, but this turns out not to be needed.\n\n\n\nSAC compared to DDPG, TD3 and PPO. Source (Haarnoja et al., 2018).\n\n\nThe enhanced exploration strategy through maximum entropy RL allows to learn robust and varied strategies that can cope with changes in the environment.\n\n\n\nSAC on the Walker environment. Source: https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/\n\n\n\n\n\nSAC on the Ant environment. Source: https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/\n\n\nThe low sample complexity of SAC allows to train a real-world robot in less than 2 hours!\n\nAlthough trained on a flat surface, the rich learned stochastic policy can generalize to complex terrains.\n\nWhen trained to stack lego bricks, the robotic arm learns to explore the whole state-action space.\n\n\n\nSource: https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/.\n\n\nThis makes it more robust to external perturbations after training:\n\n\n\nSource: https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/.\n\n\n\n\n\n\nChou, P.-W., Maturana, D., and Scherer, S. (2017). Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution. in International Conference on Machine Learning http://proceedings.mlr.press/v70/chou17a/chou17a.pdf.\n\n\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement Learning with Deep Energy-Based Policies. http://arxiv.org/abs/1702.08165.\n\n\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018). Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. http://arxiv.org/abs/1801.01290.\n\n\nTodorov, E. (2008). General duality between optimal control and estimation. in 2008 47th IEEE Conference on Decision and Control, 4286–4292. doi:10.1109/CDC.2008.4739438.\n\n\nToussaint, M. (2009). Robot Trajectory Optimization Using Approximate Inference. in Proceedings of the 26th Annual International Conference on Machine Learning ICML ’09. (New York, NY, USA: ACM), 1049–1056. doi:10.1145/1553374.1553508.\n\n\nWilliams, R. J., and Peng, J. (1991). Function optimization using connectionist reinforcement learning algorithms. Connection Science 3, 241–268."
  },
  {
    "objectID": "notes/4.1-MB.html",
    "href": "notes/4.1-MB.html",
    "title": "Model-based RL",
    "section": "",
    "text": "Slides: html pdf"
  },
  {
    "objectID": "notes/4.1-MB.html#model-free-vs.-model-based-rl",
    "href": "notes/4.1-MB.html#model-free-vs.-model-based-rl",
    "title": "Model-based RL",
    "section": "Model-free vs. model-based RL",
    "text": "Model-free vs. model-based RL\n\nIn model-free RL (MF) methods, we do not need to know anything about the dynamics of the environment to start learning a policy:\np(s_{t+1} | s_t, a_t) \\; \\; r(s_t, a_t, s_{t+1})\nWe just sample transitions (s, a, r, s') and update Q-values or a policy network. The main advantage is that the agent does not need to “think” when acting: just select the action with highest Q-value (reflexive behavior). The other advantage is that you can use MF methods on any MDP: you do not need to know anything about them. But MF methods are very slow (sample complexity): as they make no assumption, they have to learn everything by trial-and-error from scratch.\n\n\n\nModel-free vs. model-based RL. Source: (Dayan and Niv, 2008)\n\n\nIf you had a model of the environment, you could plan ahead (what would happen if I did that?) and speed up learning (do not explore stupid ideas): model-based RL (MB). In chess, players plan ahead the possible moves up to a certain horizon and evaluate moves based on their emulated consequences. In real-time strategy games, learning the environment (world model) is part of the strategy: you do not attack right away."
  },
  {
    "objectID": "notes/4.1-MB.html#model-predictive-control-mpc",
    "href": "notes/4.1-MB.html#model-predictive-control-mpc",
    "title": "Model-based RL",
    "section": "Model Predictive Control (MPC)",
    "text": "Model Predictive Control (MPC)\n\nLearning the world model is not complicated in theory. We just need to collect enough transitions s_t, a_t, s_{t+1}, r_{t+1} using a random agent (or during learning) and train a model to predict the next state and reward.\n\n\n\nLearning the world model.\n\n\nSuch a model is called the dynamics model, the transition model or the forward model: What happens if I do that? The model can be deterministic (use neural networks) or stochastic (use Gaussian Processes). Given an initial state s_0 and a policy \\pi, you can unroll the future using the local model.\nOnce you have a good transition model, you can generate rollouts, i.e. imaginary trajectories / episodes using the model.\n\\tau = (s_o, a_o, r_ 1, s_1, a_1, \\ldots, s_T)\nYou can then feed these trajectories to any model-free algorithm (value-based, policy-gradient) that will learn to maximize the returns.\n\\mathcal{J}(\\theta) = \\mathbb{E}_{\\tau}[R(\\tau)]\n\n\n\nImaginary rollouts can be used to improve the policy. Source: (Kurutach et al., 2018).\n\n\nThe only sample complexity is the one needed to train the model: the rest is emulated. Drawback: This can only work when the model is close to perfect, especially for long trajectories or probabilistic MDPs.\nFor long horizons, the slightest imperfection in the model can accumulate (drift) and lead to completely wrong trajectories. The emulated trajectory cannot be generated by the current policy \\pi_\\theta, the policy gradient is biased (especially if you are on-policy), the algorithm does not converge. If you have a perfect model, you should not be using RL anyway as classical control methods would be much faster (but see AlphaGo).\n\n\n\nImaginary rollouts generated by an imperfect model can drigt for long horizons. Source: https://medium.com/@jonathan_hui/rl-model-based-reinforcement-learning-3c2b6f0aa323\n\n\nThe solution is to replan at each time step and execute the first planned action in the real environment.\n\n\n\nConstantly replanning allows to correct the trajectories. Source: https://medium.com/@jonathan_hui/rl-model-based-reinforcement-learning-3c2b6f0aa323\n\n\nModel Predictive Control iteratively plans complete trajectories, but only selects the first action.\n\n\n\n\n\n\nModel Predictive Control\n\n\n\n\n\n\nSource: http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_9_model_based_rl.pdf\n\n\n\n\n\n\n\nA neural network can be used to learn the model in an MPC architecture. Source: (Nagabandi et al., 2017).\n\n\nThe planner can actually be anything, it does not have to be a RL algorithm. For example, it can be iLQR (Iterative Linear Quadratic Regulator), a non-linear optimization method (see https://jonathan-hui.medium.com/rl-lqr-ilqr-linear-quadratic-regulator-a5de5104c750).\nAlternatively, one can use random-sampling shooting:\n\nin the current state, select a set of possible actions.\ngenerate rollouts with these action and compute their returns using the model.\nselect the action whose rollout has the highest return.\n\n\n\n\nRandom-sampling shooting. Source: (Nagabandi et al., 2017), https://bair.berkeley.edu/blog/2017/11/30/model-based-rl/.\n\n\nThe main advantage of MPC is that you can change the reward function (the goal) on the fly: what you learn is the model, but planning is just an optimization procedure. You can set intermediary goals to the agent very flexibly: no need for a well-defined reward function. Model imperfection is not a problem as you replan all the time. The model can adapt to changes in the environment (slippery terrain, simulation to real-world).\n\n\n\nApplication of MPC. Source: (Nagabandi et al., 2017), https://bair.berkeley.edu/blog/2017/11/30/model-based-rl/.\n\n\n\n\n\nApplication of MPC. Source: (Nagabandi et al., 2017), https://bair.berkeley.edu/blog/2017/11/30/model-based-rl/."
  },
  {
    "objectID": "notes/4.1-MB.html#dyna-q",
    "href": "notes/4.1-MB.html#dyna-q",
    "title": "Model-based RL",
    "section": "Dyna-Q",
    "text": "Dyna-Q\n\nAnother approach to MB RL is to augment MF methods with MB rollouts. The MF algorithm (e.g. Q-learning) learns from transitions (s, a, r, s') sampled either with:\n\nreal experience: interaction with the environment.\nsimulated experience: simulation by the model.\n\n\n\n\nDyna-Q: RL with planning. Source: https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-5e99cae0abb8\n\n\nIf the simulated transitions are good enough, the MF algorithm can converge using much less real transitions, thereby reducing its sample complexity. The Dyna-Q algorithm (Sutton, 1990) is an extension of Q-learning to integrate a model M(s, a) = (s', r'). The model can be tabular or approximated with a NN.\n\n\n\n\n\n\nDyna-Q\n\n\n\n\nInitialize values Q(s, a) and model M(s, a).\nfor t \\in [0, T_\\text{total}]:\n\nSelect a_t using Q, take it on the real environment and observe s_{t+1} and r_{t+1}.\nUpdate the Q-value of the real action:\n\n\\Delta Q(s_t, a_t) = \\alpha \\, (r_{t+1} + \\gamma \\, \\max_a Q(s_{t+1}, a) - Q(s_t, a_t))\n\nUpdate the model:\n\nM(s, a) \\leftarrow (s_{t+1}, r_{t+1})\n\nfor K steps:\n\nSample a state s_k from a list of visited states.\nSelect a_k using Q, predict s_{k+1} and r_{k+1} using the model M(s_k, a_k).\nUpdate the Q-value of the imagined action:\n\n\\Delta Q(s_k, a_k) = \\alpha \\, (r_{k+1} + \\gamma \\, \\max_a Q(s_{k+1}, a) - Q(s_k, a_k))\n\n\n\n\n\n\n\nDyna-Q: RL with planning. Source: (Sutton and Barto, 1998).\n\n\nIt is interesting to notice that Dyna-Q is very similar to DQN and its experience replay memory. In DQN, the ERM stores real transitions generated in the past. In Dyna-Q, the model generates imagined transitions based on past real transitions.\n\n\n\n\nDayan, P., and Niv, Y. (2008). Reinforcement learning: The Good, The Bad and The Ugly. Current Opinion in Neurobiology 18, 185–196. doi:10.1016/j.conb.2008.08.003.\n\n\nKurutach, T., Clavera, I., Duan, Y., Tamar, A., and Abbeel, P. (2018). Model-Ensemble Trust-Region Policy Optimization. http://arxiv.org/abs/1802.10592.\n\n\nNagabandi, A., Kahn, G., Fearing, R. S., and Levine, S. (2017). Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning. http://arxiv.org/abs/1708.02596.\n\n\nSutton, R. S. (1990). Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming. Machine Learning Proceedings 1990, 216–224. doi:10.1016/B978-1-55860-141-3.50030-4.\n\n\nSutton, R. S., and Barto, A. G. (1998). Reinforcement Learning: An introduction. Cambridge, MA: MIT press."
  },
  {
    "objectID": "notes/4.2-LearnedModels.html",
    "href": "notes/4.2-LearnedModels.html",
    "title": "Learned world models",
    "section": "",
    "text": "Slides: html pdf\nThere are two families of model-based algorithms using a learned transition model:"
  },
  {
    "objectID": "notes/4.2-LearnedModels.html#i2a---imagination-augmented-agents",
    "href": "notes/4.2-LearnedModels.html#i2a---imagination-augmented-agents",
    "title": "Learned world models",
    "section": "I2A - Imagination-augmented agents",
    "text": "I2A - Imagination-augmented agents\n\nI2A (Weber et al., 2017) is a model-based augmented model-free method: it trains a MF algorithm (A3C) with the help of rollouts generated by a MB model.\n\n\n\nSokoban. Source (Weber et al., 2017).\n\n\nThey showcase their algorithm on the puzzle environment Sokoban, where you need to move boxes to specified locations. Sokoban is a quite hard game, as actions are irreversible (you can get stuck) and the solution requires many actions (sparse rewards). MF methods are bad at this game as they learn through trials-and-(many)-errors.\n\nThe model learns to predict the next frame and the next reward based on the four last frames and the chosen action.\n\n\n\nI2A model. Source (Weber et al., 2017).\n\n\nIt is a convolutional autoencoder, taking additionally an action a as input and predicting the next reward. It can be pretrained using a random policy, and later fine-tuned during training.\n\n\n\nI2A architecture. Source (Weber et al., 2017).\n\n\nThe imagination core is composed of the environment model M(s, a) and a rollout policy \\hat{\\pi}. As Sokoban is a POMDP (partially observable), the notation uses observation o_t instead of states s_t, but it does not really matter here. The rollout policy \\hat{\\pi} is a simple and fast policy. It does not have to be the trained policy \\pi. It could even be a random policy, or a pretrained policy using for example A3C directly. In I2A, it is a distilled policy from the trained policy \\pi (see later). Take home message: given the current observation o_t and a policy \\hat{\\pi}, we can predict the next observation \\hat{o}_{t+1} and the next reward \\hat{r}_{t+1}.\nThe imagination rollout module uses the imagination core to predict iteratively the next \\tau frames and rewards using the current frame o_t and the rollout policy:\no_t \\rightarrow \\hat{o}_{t+1} \\rightarrow \\hat{o}_{t+2} \\rightarrow \\ldots \\rightarrow \\hat{o}_{t+\\tau}\nThe \\tau frames and rewards are passed backwards to a convolutional LSTM (from t+\\tau to t) which produces an embedding / encoding of the rollout. The output of the imagination rollout module is a vector e_i (the final state of the LSTM) representing the whole rollout, including the (virtually) obtained rewards. Note that because of the stochasticity of the rollout policy \\hat{\\pi}, different rollouts can lead to different encoding vectors.\nFor the current observation o_t, we then generate one rollout per possible action (5 in Sokoban):\n\nWhat would happen if I do action 1?\nWhat would happen if I do action 2?\netc.\n\nThe resulting vectors are concatenated to the output of model-free path (a convolutional neural network taking the current observation as input). Altogether, we have a huge NN with weights \\theta (model, encoder, MF path) producing an input s_t to the A3C module.\nWe can then learn the policy \\pi and value function V based on this input to maximize the returns:\n\\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) \\, (\\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V_\\varphi(s_{t+n}) - V_\\varphi(s_t)) ]\n\\mathcal{L}(\\varphi) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[(\\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V_\\varphi(s_{t+n}) - V_\\varphi(s_t))^2]\nThe complete architecture may seem complex, but everything is differentiable so we can apply backpropagation and train the network end-to-end using multiple workers. It is the A3C algorithm (MF), but augmented by MB rollouts, i.e. with explicit information about the future.\nThe rollout policy \\hat{\\pi} is trained using policy distillation of the trained policy \\pi (Rusu et al., 2016). The small rollout policy network with weights \\hat{\\theta} tries to copy the outputs \\pi(s, a) of the bigger policy network (A3C). This is a supervised learning task: just minimize the KL divergence between the two policies:\n\\mathcal{L}(\\hat{\\theta}) = \\mathbb{E}_{s, a} [D_\\text{KL}(\\hat{\\pi}(s, a) || \\pi(s, a))]\nAs the network is smaller, it won’t be as good as \\pi, but its learning objective is easier.\n\n\n\nPolicy distillation. Source (Rusu et al., 2016).\n\n\n\n\n\n\n\n\nDistral : distill and transfer learning\n\n\n\nDistillation can be used to ensure generalization over different environments (Teh et al., 2017). Each learning algorithms learns its own task, but tries not to diverge too much from a shared policy, which turns out to be good at all tasks.\n\n\n\nDistral architecture. Source (Teh et al., 2017).\n\n\n\n\nUnsurprisingly, I2A performs better than A3C on Sokoban. The deeper the rollout, the better.\n\n\n\nI2A results on Sokoban. Source (Weber et al., 2017).\n\n\nThe model does not even have to be perfect: the MF path can compensate for imperfections.\n\n\n\nI2A results on Sokoban. Source (Weber et al., 2017)."
  },
  {
    "objectID": "notes/4.2-LearnedModels.html#temporal-difference-models---tdm",
    "href": "notes/4.2-LearnedModels.html#temporal-difference-models---tdm",
    "title": "Learned world models",
    "section": "Temporal difference models - TDM",
    "text": "Temporal difference models - TDM\n\nOne problem with model-based planning is the discretization time step (difference between t and t+1). It is determined by the action rate: how often a different action a_t has to be taken. In robotics, it could be below the millisecond, leading to very long trajectories in terms of steps.\n\n\n\nPlanning a path from Berkeley to the Golden Gate bridge has a very long horizon. Source: https://bairblog.github.io/2018/04/26/tdm/.\n\n\nIf you want to go from Berkeley to the Golden State bridge with your bike, planning over leg movements will be very expensive (long horizon). A solution is multiple steps ahead planning. Instead of learning a one-step model:\ns_{t+1} = f_\\theta(s_t, a_t)\none learns to predict the state achieved in T steps using the current policy:\ns_{t+ T} = f_\\theta(s_t, a_t, \\pi)\nPlanning and acting occur at different time scales.\nA problem with RL in general is how to define the reward function. If you goal is to travel from Berkeley to the Golden State bridge, which reward function should you use? * +1 at the bridge, 0 otherwise (sparse). * +100 at the bridge, -1 otherwise (sparse). * minus the distance to the bridge (dense).\nGoal-conditioned RL defines the reward function using the distance between the achieved state s_{t+1} and a goal state s_g:\nr(s_t, a_t, s_{t+1}) = - || s_{t+1} - s_g ||\nAn action is good if it brings the agent closer to its goal. The Euclidean distance works well for the biking example (e.g. using a GPS), but the metric can be adapted to the task.\nOne advantage is that you can learn multiple “tasks” at the same time with a single policy, not the only one hard-coded in the reward function. Another advantage is that it makes a better use of exploration by learning from mistakes: hindsight experience replay (HER, (Andrychowicz et al., 2017)).\nIf your goal is to reach s_g but the agent generates a trajectory landing in s_{g'}, you can learn that this trajectory is good way to reach s_{g'}! In football, if you try to score a goal but end up doing a pass to a teammate, you can learn that this was a bad shot and a good pass. HER is a model-based method: you implicitly learn a model of the environment by knowing how to reach any position.\n\n\n\nHindsight experience replay allows to learn even from mistakes. Source: https://openai.com/blog/ingredients-for-robotics-research/\n\n\nExploration never fails: you always learn to do something, even if this was not your original goal. The principle of HER can be used in all model-free methods: DQN, DDPG, etc.\nUsing the goal-conditioned reward function r(s_t, a_t, s_{t+1}) = - || s_{t+1} - s_g ||, how can we learn? TDM introduces goal-conditioned Q-value with a horizon T: Q(s, a, s_g, T). The Q-value of an action should denote how close we will be from the goal s_g in T steps. If we can estimate these Q-values, we can use a planning algorithm such as MPC to find the action that will bring us closer to the goal easily:\na^* = \\text{arg}\\max_{a_t} \\, r(s_{t+T}, a_{t+T}, s_{t+T + 1})\nThis corresponds to planning T steps ahead; which action should I do now in order to be close to the goal in T steps?\n\n\n\n\n\n\n\n\nSource: https://bairblog.github.io/2018/04/26/tdm/\n\n\nIf the horizon T is well chosen, we only need to plan over a small number of intermediary positions, not over each possible action. TDM is model-free on each subgoal, but model-based on the whole trajectory.\nHow can we learn the goal-conditioned Q-values Q(s, a, s_g, T) with a model? TDM introduces a recursive relationship for the Q-values:\n\\begin{aligned}\n    Q(s, a, s_g, T) &= \\begin{cases}\n        \\mathbb{E}_{s'} [r(s, a, s')] \\; \\text{if} \\; T=0\\\\\n        &\\\\\n        \\mathbb{E}_{s'} [\\max_a \\, Q(s', a, s_g, T-1)] \\; \\text{otherwise.}\\\\\n        \\end{cases} \\\\\n        &\\\\\n        &= \\mathbb{E}_{s'} [r(s, a, s') \\, \\mathbb{1}(T=0) + \\max_a \\, Q(s', a, s_g, T-1) \\, \\mathbb{1}(T\\neq 0)]\\\\\n\\end{aligned}\n\nIf we plan over T=0 steps, i.e. immediately after the action (s, a), the Q-value is the remaining distance to the goal from the next state s'. Otherwise, it is the Q-value of the greedy action in the next state s' with an horizon T-1 (one step shorter). This allows to learn the Q-values from single transitions (s_t, a_t, s_{t+1}): * with T=0, the target is the remaining distance to the goal. * with T>0, the target is the Q-value of the next action at a shorter horizon.\nThe critic learns to minimize the prediction error off-policy:\n\\mathcal{L}(\\theta) = \\mathbb{E}_{s_t, a_t, s_{t+1} \\in \\mathcal{D}} [(r(s_t, a_t, s_{t+1}) \\, \\mathbb{1}(T=0) + \\max_a \\, Q(s_{t+1}, a, s_g, T-1) \\, \\mathbb{1}(T\\neq 0) - Q(s_t, a_t, s_g, T))^2]\nThis is a model-free Q-learning-like update rule, that can be learned by any off-policy value-based algorithm (DQN, DDPG) and an experience replay memory. The cool trick is that, with a single transition (s_t, a_t, s_{t+1}), you can train the critic with: * different horizons T, e.g. between 0 and T_\\text{max}. * different goals s_g. You can sample any achievable state as a goal, including the “true” s_{t+T} (hindsight).\nYou do not only learn to reach s_g, but any state! TDM learns a lot of information from a single transition, so it has a very good sample complexity.\nTDM learns to break long trajectories into finite horizons (model-based planning) by learning model-free (Q-learning updates). The critic learns how good an action (s, a) is order to reach a state s_g in T steps.\nQ(s, a, s_g, T) = \\mathbb{E}_{s'} [r(s, a, s') \\, \\mathbb{1}(T=0) + \\max_a \\, Q(s', a, s_g, T-1) \\, \\mathbb{1}(T\\neq 0)]\nThe actor uses MPC planning to iteratively select actions that bring us closer to the goal in T steps:\na_t = \\text{arg}\\max_{a} \\, Q(s_{t}, a, s_{g}, T)\nThe argmax can be estimated via sampling. TDM is a model-based method in disguise: it does predict the next state directly, but how much closer it will be to the goal via Q-learning.\nFor problems where the model is easy to learn, the performance of TDM is on par with model-based methods (MPC).\n\n\n\nTDM learns control problems which are easy for MB algorithms. Source: https://bairblog.github.io/2018/04/26/tdm/.\n\n\nModel-free methods have a much higher sample complexity. TDM learns much more from single transitions.\n\n\n\nTDM learns control problems which are easy for MB algorithms. Source: https://bairblog.github.io/2018/04/26/tdm/.\n\n\nFor problems where the model is complex to learn, the performance of TDM is on par with model-free methods (DDPG).\n\n\n\nTDM learns control problems which are easy for MF algorithms. Source: https://bairblog.github.io/2018/04/26/tdm/.\n\n\nModel-based methods suffer from model imprecision on long horizons. TDM plans over shorter horizons T.\n\n\n\nTDM learns control problems which are easy for MF algorithms. Source: https://bairblog.github.io/2018/04/26/tdm/."
  },
  {
    "objectID": "notes/4.2-LearnedModels.html#world-models",
    "href": "notes/4.2-LearnedModels.html#world-models",
    "title": "Learned world models",
    "section": "World models",
    "text": "World models\n\nThe core idea of world models (Ha and Schmidhuber, 2018) is to explicitly separate the world model (what will happen next) from the controller (how to act). Deep RL NN are usually small, as rewards do not contain enough information to train huge networks.\n\n\n\nArchitecture of world models. Source: https://worldmodels.github.io/.\n\n\nA huge world model can be efficiently trained by supervised or unsupervised methods. A small controller should not need too many trials if its input representations are good.\nThe vision module V is trained as a variational autoencoder (VAE) on single frames of the game. The latent vector \\mathbf{z}_t contains a compressed representation of the frame \\mathbf{o}_t.\n\n\n\nVision module. Source: https://worldmodels.github.io/.\n\n\nThe sequence of latent representations \\mathbf{z}_0, \\ldots \\mathbf{z}_t in a game is fed to a LSTM layer together with the actions a_t to compress what happens over time. A Mixture Density Network (MDN) is used to predict the distribution of the next latent representations P(\\mathbf{z}_{t+1} | a_t, \\mathbf{h}_t, \\ldots \\mathbf{z}_t).\nThe RNN-MDN architecture (Ha and Eck, 2017) has been used successfully in the past for sequence generation problems such as generating handwriting and sketches (Sketch-RNN, see https://magenta.tensorflow.org/sketch-rnn-demo for demos).\n\n\n\nMemory module. Source: https://worldmodels.github.io/.\n\n\nThe last step is the controller. It takes a latent representation \\mathbf{z}_t and the current hidden state of the LSTM \\mathbf{h}_t as inputs and selects an action linearly:\na_t = \\text{tanh}(W \\, [\\mathbf{z}_t, \\mathbf{h}_t ] + b)\nA RL actor cannot get simpler as that…\n\n\n\nController. Source: https://worldmodels.github.io/.\n\n\nThe controller is not even trained with RL: it uses a genetic algorithm, the Covariance-Matrix Adaptation Evolution Strategy (CMA-ES), to find the output weights that maximize the returns. The world model is trained by classical supervised learning using a random agent before learning.\nRefer https://worldmodels.github.io/ to see the model in action.\nAlgorithm:\n\nCollect 10,000 rollouts from a random policy.\nTrain VAE (V) to encode each frame into a latent vector \\mathbf{z} \\in \\mathcal{R}^{32}.\nTrain MDN-RNN (M) to model P(\\mathbf{z}_{t+1} | a_t, \\mathbf{h}_t, \\ldots \\mathbf{z}_t).\nEvolve Controller (C) to maximize the expected cumulative reward of a rollout.\n\nThe world model V+M is learned offline with a random agent, using unsupervised learning. The controller C has few weights (1000) and can be trained by evolutionary algorithms, not even RL. The network can even learn by playing entirely in its own imagination as the world model can be applied on itself and predict all future frames. It just need to additionally predict the reward. The learned policy can then be transferred to the real environment."
  },
  {
    "objectID": "notes/4.2-LearnedModels.html#deep-planning-network---planet",
    "href": "notes/4.2-LearnedModels.html#deep-planning-network---planet",
    "title": "Learned world models",
    "section": "Deep Planning Network - PlaNet",
    "text": "Deep Planning Network - PlaNet\n\nPlaNet (Hafner et al., 2019) extends the idea of World models by learning the model together with the policy (end-to-end). It learns a latent dynamics model that takes the past observations o_t into account (needed for POMDPs):\ns_{t}, r_{t+1}, \\hat{o}_t = f(o_t, a_t, s_{t-1})\nand plans in the latent space using multiple rollouts:\na_t = \\text{arg}\\max_a \\mathbb{E}[R(s_t, a, s_{t+1}, \\ldots)]\nThe latent dynamics model is a sequential variational autoencoder learning concurrently:\n\nAn encoder from the observation o_t to the latent space s_t: q(s_t | o_t).\nA decoder from the latent space to the reconstructed observation \\hat{o}_t: p(\\hat{o}_t | s_t).\nA transition model to predict the next latent representation given an action: p(s_{t+1} | s_t, a_t).\nA reward model predicting the immediate reward: p(r_t | s_t).\n\n\n\n\nLatent dynamics model of PlaNet (Hafner et al., 2019). Source: https://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html.\n\n\nThe loss function to train this recurrent state-space model (RSSM), with a deterministic component in the transition model (RNN) and stochastic components is not shown here.\nTraining sequences (o_1, a_1, o_2, \\ldots, o_T) can be generated off-policy (e.g. from demonstrations) or on-policy.\n\n\n\nLatent dynamics model of PlaNet (Hafner et al., 2019). Source: https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html.\n\n\nFrom a single observation o_t encoded into s_t, 10000 rollouts are generated using random sampling. A belief over action sequences is updated using the cross-entropy method (CEM) in order to restrict the search. The first action of the sequence with the highest estimated return (reward model) is executed. At the next time step, planning starts from scratch: Model Predictive Control. There is no actor in PlaNet, only a transition model used for planning.\n\n\n\nPlanning module of PlaNet (Hafner et al., 2019). Source: https://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html.\n\n\nPlanet learns continuous image-based control problems in 2000 episodes, where D4PG needs 50 times more.\n\nThe latent dynamics model can learn 6 control tasks at the same time. As there is no actor, but only a planner, the same network can control all agents!"
  },
  {
    "objectID": "notes/4.2-LearnedModels.html#dreamer",
    "href": "notes/4.2-LearnedModels.html#dreamer",
    "title": "Learned world models",
    "section": "Dreamer",
    "text": "Dreamer\nDreamer (Hafner et al., 2020) extends the idea of PlaNet by additionally training an actor instead of using a MPC planner. The latent dynamics model is the same RSSM architecture. Training a “model-free” actor on imaginary rollouts instead of MPC planning should reduce the computational time.\n\n\n\nArchitecture of Dreamer. Source: https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html.\n\n\nThe behavior module learns to predict the value of a state V_\\varphi(s) and the policy \\pi_\\theta(s) (actor-critic). It is trained in imagination in the latent space using the reward model for the immediate rewards (to compute returns) and the transition model for the next states.\n\n\n\nTraining of the actor-critic behaviour module is end-to-end using a single rollout. Source: https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html.\n\n\nThe current observation o_t is encoded into a state s_t, the actor selects an action a_t, the transition model predicts s_{t+1}, the reward model predicts r_{t+1}, the critic predicts V_\\varphi(s_t). At the end of the sequence, we apply backpropagation-through-time to train the actor and the critic.\nThe critic V_\\varphi(s_t) is trained on the imaginary sequence (s_t, a_t, r_{t+1}, s_{t+1}, \\ldots, s_T) to minimize the prediction error with the \\lambda-return:\nR^\\lambda_t = (1  - \\lambda) \\, \\sum_{n=1}^{T-t-1} \\lambda^{n-1} \\, R^n_t + \\lambda^{T-t-1} \\, R_t\nThe actor \\pi_\\theta(s_t, a_t) is trained on the sequence to maximize the sum of the value of the future states:\n\\mathcal{J}(\\theta) = \\mathbb{E}_{s_t, a_t \\sim \\pi_\\theta} [\\sum_{t'=t}^T V_\\varphi(s_{t'})]\nThe main advantage of training an actor is that we need only one rollout when training it: backpropagation maximizes the expected returns. When acting, we just need to encode the history of the episode in the latent space, and the actor becomes model-free!\n\n\n\nOverview of Dreamer. Source: https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html.\n\n\nDreamer beats model-free and model-based methods on 20 continuous control tasks.\n\n\n\nDreamer on several continuous control tasks. Source: https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html.\n\n\n\n\n\nPerformance of Dreamer. Source: https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html.\n\n\nIt also learns Atari and Deepmind lab video games, sometimes on par with Rainbow or IMPALA!\n\n\n\nDreamer on Atari and DML games. Source: https://dreamrl.github.io/.\n\n\n\n\n\nPerformance of Dreamer. Source: https://dreamrl.github.io/l.\n\n\n\n\n\n\nAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., et al. (2017). Hindsight Experience Replay. http://arxiv.org/abs/1707.01495.\n\n\nFeinberg, V., Wan, A., Stoica, I., Jordan, M. I., Gonzalez, J. E., and Levine, S. (2018). Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning. http://arxiv.org/abs/1803.00101.\n\n\nGu, S., Lillicrap, T., Sutskever, I., and Levine, S. (2016). Continuous Deep Q-Learning with Model-based Acceleration. http://arxiv.org/abs/1603.00748.\n\n\nHa, D., and Eck, D. (2017). A Neural Representation of Sketch Drawings. http://arxiv.org/abs/1704.03477.\n\n\nHa, D., and Schmidhuber, J. (2018). World Models. doi:10.5281/zenodo.1207631.\n\n\nHafner, D., Lillicrap, T., Ba, J., and Norouzi, M. (2020). Dream to Control: Learning Behaviors by Latent Imagination. http://arxiv.org/abs/1912.01603.\n\n\nHafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., et al. (2019). Learning Latent Dynamics for Planning from Pixels. http://arxiv.org/abs/1811.04551.\n\n\nPong, V., Gu, S., Dalal, M., and Levine, S. (2018). Temporal Difference Models: Model-Free Deep RL for Model-Based Control. http://arxiv.org/abs/1802.09081.\n\n\nRusu, A. A., Colmenarejo, S. G., Gulcehre, C., Desjardins, G., Kirkpatrick, J., Pascanu, R., et al. (2016). Policy Distillation. http://arxiv.org/abs/1511.06295.\n\n\nTeh, Y. W., Bapst, V., Czarnecki, W. M., Quan, J., Kirkpatrick, J., Hadsell, R., et al. (2017). Distral: Robust Multitask Reinforcement Learning. http://arxiv.org/abs/1707.04175.\n\n\nWeber, T., Racanière, S., Reichert, D. P., Buesing, L., Guez, A., Rezende, D. J., et al. (2017). Imagination-Augmented Agents for Deep Reinforcement Learning. http://arxiv.org/abs/1707.06203."
  },
  {
    "objectID": "notes/4.3-AlphaGo.html",
    "href": "notes/4.3-AlphaGo.html",
    "title": "AlphaGo",
    "section": "",
    "text": "Slides: html pdf"
  },
  {
    "objectID": "notes/4.3-AlphaGo.html#the-game-of-go",
    "href": "notes/4.3-AlphaGo.html#the-game-of-go",
    "title": "AlphaGo",
    "section": "The game of Go",
    "text": "The game of Go\n\n\n\n\n\n\n\nPlay Go in Chemnitz:\n\n\n\nhttps://www.facebook.com/GoClubChemnitz/\n\n\n\n\n\nThe Goban.\n\n\nGo is an ancient two-opponents board game, where each player successively places stones on a 19x19 grid. When a stone is surrounded by four opponents, it dies. The goal is to ensure strategical position in order to cover the biggest territory. There are around 10^{170} possible states and 250 actions available at each turn (10^{761} possible games), making it a much harder game than chess for a computer (35 possible actions, 10^{120} possible games). A game lasts 150 moves on average (80 in chess). Up until 2015 and AlphaGo, Go AIs could not compete with world-class experts, and people usually considered AI would need at least another 20 years to solve it.\n\nMinimax and Alpha-Beta\n\n\n\nExample of a game tree.\n\n\nMinimax algorithm expand the whole game tree, simulating the moves of the MAX (you) and MIN (your opponent) players. The final outcome (win or lose) is assigned to the leaves. It allows to solve zero sum games: what MAX wins is lost by MIN, and vice-versa. We suppose MIN plays optimally (i.e. in his own interest).\n\n\n\nMinimax iteratively backpropagates the value of the leaves according to who plays.\n\n\nThe value of the leaves is propagated backwards to the starting position: MAX chooses the action leading to the state with the highest value, MIN does the opposite. For most games, the tree becomes too huge for such a systematic search: * The value of all states further than a couple of moves away are approximated by a heuristic function: the value V(s) of these states. * Obviously useless parts of the tree are pruned: Alpha-Beta algorithm.\nAlpha-Beta methods work well for simple problems where the complete game tree can be manipulated: Tic-Tac-Toe has only a couple of possible states and actions (3^9 = 19000 states).\n\n\n\nGame tree of tic-tac-toe.\n\n\nIt also works when precise heuristics can be derived in a reasonable time. This is the principle of IBM DeepBlue which was the first Chess AI to beat a world champion (Garry Kasparov) in 1995. Carefully engineered heuristics (with the help of chess masters) allowed DeepBlue to search 6 moves away what is the best situation it can arrive in.\nBut it does not work in Go because its branching factor (250 actions possible from each state) is to huge: the tree explodes very soon. 250^{6} \\approx 10^{15}, so even if your processor evaluates 1 billion nodes per second, it would need 11 days to evaluate a single position 6 moves away…\n\n\n\nGame (sub-)tree of Go. Source: https://www.quora.com/What-does-it-mean-that-AlphaGo-relied-on-Monte-Carlo-tree-search/answer/Kostis-Gourgoulias"
  },
  {
    "objectID": "notes/4.3-AlphaGo.html#alphago",
    "href": "notes/4.3-AlphaGo.html#alphago",
    "title": "AlphaGo",
    "section": "AlphaGo",
    "text": "AlphaGo\n\n\nArchitecture\nAlphaGo (Silver et al., 2016) uses four different neural networks:\n\nThe rollout policy and the SL policy network use supervised learning to predict expert human moves in any state.\nThe RL policy network uses self-play and reinforcement learning to learn new strategies.\nThe value network learns to predict the outcome of a game (win/lose) from the current state.\n\nThe rollout policy and the value network are used to guide stochastic tree exploration in Monte-Carlo Tree Search (MCTS) (MPC-like planning algorithm).\n\n\n\nArchitecture of AlphaGo (Silver et al., 2016).\n\n\nSupervised learning is used for bootstrapping the policy network.\nA policy network \\rho_\\sigma is trained to predict human expert moves:\n\n30M expert games have been gathered: input is board configuration, output is the move played by the expert.\nThe CNN has 13 convolutional layers (5x5) and no max-pooling.\nThe accuracy at the end of learning is 57% (not bad, but not sufficient to beat experts).\n\nA faster rollout policy network \\rho_\\pi is also trained:\n\nOnly one layer, views only part of the state (around the last opponent’s move).\nPrediction accuracy of 24%.\nInference time is only 2 \\mus, instead of 3 ms for the policy network \\rho_\\sigma.\n\nThe SL policy network \\rho_\\sigma is used to initialize the weights of the RL policy network \\rho_\\rho, so it can start exploring from a decent policy.\nThe RL policy network then plays against an older version of itself (\\approx target network) to improve its policy, updating the weights using Policy Gradient (REINFORCE):\n\n    \\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, R ]\n\nwhere R = +1 when the game is won, -1 otherwise.\nThe idea of playing against an older version of the same network (self-play) allows to learn offline. The RL policy network already wins 85% of the time against the strongest AI at the time (Pachi), but not against expert humans. A value network \\nu_\\theta finally learns to predict the outcome of a game (+1 when winning, -1 when losing) based on the self-play positions generated by the RL policy network.\n\n\n\nThe policy network learns the probability of selecting different moves. Source: (Silver et al., 2016).\n\n\n\n\n\nThe value network learns to predict the value of any possible state under the learned policy. Source: (Silver et al., 2016).\n\n\n\n\nMonte-Carlo Tree Search\n\n\n\nMonte-Carlo Tree Search (MCTS). Source: (Silver et al., 2016).\n\n\nThe final AlphaGo player uses Monte-Carlo Tree Search (MCTS), which is an incremental tree search (depth-limited), biased by the Q-value of known transitions. The game tree is traversed depth-first from the current state, but the order of the visits depends on the value of the transition. MCTS was previously the standard approach for Go AIs, but based on expert moves only, not deep networks. One step of MCTS consists of four phases:\n\nSelection phase\nIn the selection phase, a path is found in the tree of possible actions using Upper Confidence Bound (UCB). The probability of selecting an action when sampling the tree depends on:\n\nIts Q-value Q(s, a) (as learned by MCTS): how likely this action leads to winning.\nIts prior probability: how often human players would play it, given by the SL policy network \\rho_\\sigma.\nIts number of visits N(s, a): this ensures exploration during the sampling.\n\na_t = \\text{argmax}_a \\, Q(s, a) + K \\cdot \\frac{P(s, a)}{1 + N(s, a)}\n\n\nExpansion phase\nIn the expansion phase, a leaf state s_L of the game tree is reached. The leaf is expanded, and the possible successors of that state are added to the tree. One requires a model to know which states are possible successors, but this is very easy in Go.\ns_{t+1} = f(s_t, a_t)\nThe tree therefore grows every time a Monte-Carlo sampling (“episode”) is done.\n\n\nEvaluation phase\nIn the evaluation phase, the leaf s_L is evaluated both by:\n\nthe RL value network \\nu_\\theta (how likely can we win from that state),\na random rollout until the end of the game using the fast rollout policy \\rho_\\pi.\n\nThe random rollout consists in “emulating” the end of the game using the fast rollout policy network. The rollout is of course imperfect, but complements the value network: they are more accurate together than alone!\nV(s_L) = (1 - \\lambda)  \\, \\nu_\\theta(s_L) + \\lambda \\, R_\\text{rollout} \nThis solves the bias/variance trade-off.\n\n\nBackup phase\nIn the backup phase, the Q-values of all actions taken when descending the tree are updated with the value of the leaf node:\nQ(s, a) = \\frac{1}{N(s, a)} \\sum_{i=1}^{n} V(s_L^i) \nThis is a Monte Carlo method: perform one episode and update the Q-value of all taken actions. However, it never uses real rewards, only value estimates. The Q-values are learned by using both the learned value of future states (value network) and internal simulations (rollout).\nThe four phases are then repeated as long as possible (time is limited in Go), to expand the game tree as efficiently as possible. The game tree is repeatedly sampled and grows after each sample. When the time is up, the greedy action (highest Q-value) in the initial state is chosen and played. For the next move, the tree is reset and expanded again (MPC replanning).\nIn the end, during MCTS, only the value network \\nu_\\theta, the SL policy network \\rho_\\sigma and the fast rollout policy \\rho_\\pi are used. The RL policy network \\rho_\\rho is only used to train the value network \\nu_\\theta. i.e. to predict which positions are interesting or not. However, the RL policy network can discover new strategies by playing many times against itself, without relying on averaging expert moves like the previous approaches.\nAlphaGo was able to beat Lee Sedol in 2016, 19 times World champion. It relies on human knowledge to bootstrap a RL agent (supervised learning). The RL agent discovers new strategies by using self-play: during the games against Lee Sedol, it was able to use novel moves which were never played before and surprised its opponent. The neural networks are only used to guide random search using MCTS: the policy network alone is not able to beat grandmasters. Training took several weeks on 1202 CPUs and 176 GPUs.\n\n\n\n\n\n\nBut is Go that hard compared to robotics?\n\n\n\n\nFully deterministic. There is no noise in the rules of the game; if the two players take the same sequence of actions, the states along the way will always be the same.\nFully observed. Each player has complete information and there are no hidden variables. For example, Texas hold’em does not satisfy this property because you cannot see the cards of the other player.\nThe action space is discrete. A number of unique moves are available. In contrast, in robotics you might want to instead emit continuous-valued torques at each joint.\nWe have access to a perfect simulator (the game itself), so the effects of any action are known exactly. This is a strong assumption that AlphaGo relies on quite strongly, but is also quite rare in other real-world problems.\nEach episode/game is relatively short, of approximately 200 actions. This is a relatively short time horizon compared to other RL settings which may involve thousands (or more) of actions per episode.\nThe evaluation is clear, fast and allows a lot of trial-and-error experience. In other words, the agent can experience winning/losing millions of times, which allows is to learn, slowly but surely, as is common with deep neural network optimization.\nThere are huge datasets of human play game data available to bootstrap the learning, so AlphaGo doesn’t have to start from scratch.\n\nSource: https://medium.com/@karpathy/alphago-in-context-c47718cb95a5"
  },
  {
    "objectID": "notes/4.3-AlphaGo.html#alphazero",
    "href": "notes/4.3-AlphaGo.html#alphazero",
    "title": "AlphaGo",
    "section": "AlphaZero",
    "text": "AlphaZero\n\n\n\n\nPerformance of AlphaZero (Silver et al., 2018). Source: https://deepmind.com/blog/alphago-zero-learning-scratch/.\n\n\nAlphaZero (Silver et al., 2018) totally skips the supervised learning part: the RL policy network starts self-play from scratch!\n\n\n\nArchitecture of AlphaZero (Silver et al., 2018).\n\n\nThe RL policy network uses MCTS to select moves, not a softmax-like selection as in AlphaGo. The policy and value networks are merged into a two-headed monster: the convolutional residual layers are shared to predict both:\n\nThe policy \\pi_\\theta(s), which is only used to guide MCTS (prior of UCB).\n\na_t = \\text{argmax}_a \\, Q(s, a) + K \\cdot \\frac{\\pi_\\theta(s, a)}{1 + N(s, a)}\n\nThe state value V_\\varphi(s) for the value of the leaves (no fast rollout).\n\nThe loss function used to train the network is a compound loss:\n\n    \\mathcal{L}(\\theta) = (R − V_\\varphi(s))^2 - \\pi_\\text{MCTS}(s) \\, \\log \\pi_\\theta(s) + c ||\\theta||^2\n\nThe policy head \\pi_\\theta(s) learns to mimic the actions selected by MCTS by minimizing the cross-entropy (or KL). The value network V_\\varphi(s) learns to predict the return by minimizing the mse.\n\n\n\n\n\n\nAlphaZero\n\n\n\n\nInitialize neural network.\nPlay self-play games, using 1,600 MCTS simulations per move (which takes about 0.4 seconds).\nSample 2,048 positions from the most recent 500,000 games, along with whether the game was won or lost.\nTrain the neural network, using both A) the move evaluations produced by the MCTS lookahead search and B) whether the current player won or lost.\nFinally, every 1,000 iterations of steps 3-4, evaluate the current neural network against the previous best version; if it wins at least 55% of the games, begin using it to generate self-play games instead of the prior version.\n\nRepeat steps 3-4 700,000 times, while the self-play games are continuously being played .\nSource:https://hackernoon.com/the-3-tricks-that-made-alphago-zero-work-f3d47b6686ef\n\n\nBy using a single network instead of four and learning faster, AlphaZero also greatly reduces the energy consumption.\n\n\n\nPower consumption of AlphaZero (Silver et al., 2018). Source: https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go.\n\n\nThe same algorithm can also play Chess and Shogi! The network weights are reset for each game, but it uses the same architecture and hyperparameters. After only 8 hours of training, AlphaZero beats Stockfish with 28-72-00, the best Chess AI at the time, which itself beats any human. This proves the algorithm is generic and can be applied to any board game.\n\n\n\nAlphaZero can reach sota performance on Go, Cjess and Shogi. Source: https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go"
  },
  {
    "objectID": "notes/4.3-AlphaGo.html#muzero",
    "href": "notes/4.3-AlphaGo.html#muzero",
    "title": "AlphaGo",
    "section": "MuZero",
    "text": "MuZero\nMuZero (Schrittwieser et al., 2019) is the latest extension of AlphaZero. Instead of relying on a perfect simulator for the MCTS, it learns the dynamics model instead.\ns_{t+1}, r_{t+1} = f(s_t, a_t)\n\n\n\nArchitecture of MuZero (Schrittwieser et al., 2019).\n\n\nMuZero is composed of three neural networks:\n\nThe representation network s= h(o_1, \\ldots, o_t) (encoder) transforming the history of observations into a state representation (latent space).\nThe dynamics model s', r = g(s, a) used to generate rollouts for MCTS.\nThe policy and value network \\pi, V = f(s) learning the policy with PG.\n\nThe dynamics model s', r = g(s, a) replaces the perfect simulator in MCTS. It is used in the expansion phase of MCTS to add new nodes. Importantly, nodes are latent representations of the observations, not observations directly. This is a similar idea to World Models and PlaNet/Dreamer, which plan in the latent space of a VAE. Selection in MCTS still follows an upper confidence bound using the learned policy \\pi:\nThe actions taking during self-play are taken from the MCTS search as in AlphaZero. Note that the network plays each turn: there is additional information about whether the network is playing white or black. Self-played games are stored in a huge experience replay memory.\nFinally, complete games sampled from the ERM are used to learn simultaneously the three networks f, g and h:\n\n\n\n\n\nMuZero beats AlphaZero on Chess, Go and Shogi, but also R2D2 on Atari games. The representation network h allows to encode the Atari frames in a compressed manner that allows planning over raw images.\n\n\n\nPerformance of MuZero (Schrittwieser et al., 2019).\n\n\n\n\n\n\nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., et al. (2019). Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model. http://arxiv.org/abs/1911.08265.\n\n\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., et al. (2016). Mastering the game of Go with deep neural networks and tree search. Nature 529, 484–489. doi:10.1038/nature16961.\n\n\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., et al. (2018). A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science 362, 1140–1144. doi:10.1126/science.aar6404."
  },
  {
    "objectID": "notes/4.4-SR.html",
    "href": "notes/4.4-SR.html",
    "title": "Successor representations",
    "section": "",
    "text": "Slides: html pdf"
  },
  {
    "objectID": "notes/4.4-SR.html#model-based-vs.-model-free",
    "href": "notes/4.4-SR.html#model-based-vs.-model-free",
    "title": "Successor representations",
    "section": "Model-based vs. Model-free",
    "text": "Model-based vs. Model-free\n\n\nComparison\nModel-free methods use the reward prediction error (RPE) to update values:\n\n    \\delta_t = r_{t+1} + \\gamma \\, V^\\pi(s_{t+1}) - V^\\pi(s_t)\n\n\n    \\Delta V^\\pi(s_t) = \\alpha \\, \\delta_t\n\nEncountered rewards propagate very slowly to all states and actions. If the environment changes (transition probabilities, rewards), they have to relearn everything. After training, selecting an action is very fast.\nModel-based RL can learn very fast changes in the transition or reward distributions:\n\n    \\Delta r(s_t, a_t, s_{t+1}) = \\alpha \\, (r_{t+1} - r(s_t, a_t, s_{t+1}))\n\n\n    \\Delta p(s' | s_t, a_t) = \\alpha \\, (\\mathbb{I}(s_{t+1} = s') - p(s' | s_t, a_t))\n\nBut selecting an action requires planning in the tree of possibilities (slow).\nRelative advantages of MF and MB methods:\n\n\n\n\n\n\n\n\n\n\n\nInference speed\nSample complexity\nOptimality\nFlexibility\n\n\n\n\nModel-free\nfast\nhigh\nyes\nno\n\n\nModel-based\nslow\nlow\nas good as the model\nyes\n\n\n\nA trade-off would be nice… Most MB models in the deep RL literature are hybrid MB/MF models anyway.\n\n\nGoal-directed learning vs. habit formation\nTwo forms of behavior are observed in the animal psychology literature:\n\nGoal-directed behavior learns Stimulus \\rightarrow Response \\rightarrow Outcome associations.\nHabits are developed by overtraining Stimulus \\rightarrow Response associations.\n\nThe main difference is that habits are not influenced by outcome devaluation, i.e. when reard slose their value.\n\n\n\nOutcome devaluation. Credit: Bernard W. Balleine\n\n\nThe classical theory assigns MF to habits and MB to goal-directed, mostly because their sensitivity to outcome devaluation. The open question is the arbitration mechanism between these two segregated process: who takes control? Recent work suggests both systems are largely overlapping."
  },
  {
    "objectID": "notes/4.4-SR.html#successor-representations",
    "href": "notes/4.4-SR.html#successor-representations",
    "title": "Successor representations",
    "section": "Successor representations",
    "text": "Successor representations\n\n\nPrinciple of successor representations\nSuccessor representations (SR, (Dayan, 1993)) have been introduced to combine MF and MB properties. Let’s split the definition of the value of a state:\n\n\\begin{align}\n    V^\\pi(s) &= \\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, r_{t+k+1} | s_t =s] \\\\\n            &\\\\\n               &= \\mathbb{E}_{\\pi} [\\begin{bmatrix} 1 \\\\ \\gamma \\\\ \\gamma^2 \\\\ \\ldots \\\\ \\gamma^\\infty \\end{bmatrix} \\times\n                  \\begin{bmatrix} \\mathbb{I}(s_{t}) \\\\ \\mathbb{I}(s_{t+1}) \\\\ \\mathbb{I}(s_{t+2}) \\\\ \\ldots \\\\ \\mathbb{I}(s_{\\infty}) \\end{bmatrix}  \\times\n                  \\begin{bmatrix} r_{t+1} \\\\ r_{t+2} \\\\ r_{t+3} \\\\ \\ldots \\\\ r_{t+\\infty} \\end{bmatrix}\n                | s_t =s]\\\\\n\\end{align}\n\nwhere \\mathbb{I}(s_{t}) is 1 when the agent is in s_t at time t, 0 otherwise.\nThe left part corresponds to the transition dynamics: which states will be visited by the policy, discounted by \\gamma. The right part corresponds to the immediate reward in each visited state. Couldn’t we learn the transition dynamics and the reward distribution separately in a model-free manner?\nSR rewrites the value of a state into an expected discounted future state occupancy M^\\pi(s, s') and an expected immediate reward r(s') by summing over all possible states s' of the MDP:\n\n\\begin{align}\n    V^\\pi(s) &= \\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, r_{t+k+1} | s_t =s] \\\\\n            &\\\\\n               &= \\sum_{s' \\in \\mathcal{S}} \\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, \\mathbb{I}(s_{t+k}=s') \\times r_{t+k+1}  | s_t =s]\\\\\n            &\\\\\n               &\\approx \\sum_{s' \\in \\mathcal{S}} \\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, \\mathbb{I}(s_{t+k}=s')  | s_t =s] \\times \\mathbb{E} [r_{t+1}  | s_{t}=s']\\\\\n            &\\\\\n               &\\approx \\sum_{s' \\in \\mathcal{S}} M^\\pi(s, s') \\times r(s')\\\\\n\\end{align}\n\nThe underlying assumption is that the world dynamics are independent from the reward function (which does not depend on the policy). This allows to re-use knowledge about world dynamics in other contexts (e.g. a new reward function in the same environment): transfer learning.\n\n\n\nTransfer learning in Gridworld. Source: https://awjuliani.medium.com/the-present-in-terms-of-the-future-successor-representations-in-reinforcement-learning-316b78c5fa3\n\n\nWhat matters is the states that you will visit and how interesting they are, not the order in which you visit them. Knowing that being in the mensa will eventually get you some food is enough to know that being in the mensa is a good state: you do not need to remember which exact sequence of transitions will put food in your mouth.\nSR algorithms must estimate two quantities:\n\nThe expected immediate reward received after each state:\n\nr(s) = \\mathbb{E} [r_{t+1} | s_t = s]\n\nThe expected discounted future state occupancy (the SR itself):\n\nM^\\pi(s, s') = \\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, \\mathbb{I}(s_{t+k} = s') | s_t = s]\nThe value of a state s is then computed with:\n\n    V^\\pi(s) = \\sum_{s' \\in \\mathcal{S}} M(s, s') \\times r(s')\n\nwhat allows to infer the policy (e.g. using an actor-critic architecture). The immediate reward for a state can be estimated very quickly and flexibly after receiving each reward:\n\n    \\Delta \\, r(s_t) = \\alpha \\, (r_{t+1} - r(s_t))\n\nImagine a very simple MDP with 4 states and a single deterministic action:\n\n\n\n\n\nThe transition matrix \\mathcal{P}^\\pi depicts the possible (s, s') transitions:\n\\mathcal{P}^\\pi = \\begin{bmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 1 \\\\\n\\end{bmatrix}\n\nThe SR matrix M also represents the future transitions discounted by \\gamma:\nM = \\begin{bmatrix}\n1 & \\gamma & \\gamma^2 & \\gamma^3 \\\\\n0 & 1 & \\gamma & \\gamma^2 \\\\\n0 & 0 & 1  & \\gamma\\\\\n0 & 0 & 0 & 1 \\\\\n\\end{bmatrix}\n\n\n\n\n\n\n\nSR matrix in a Tolman’s maze\n\n\n\n Source: (Russek et al., 2017)\nThe SR represents whether a state can be reached soon from the current state (b) using the current policy. The SR depends on the policy: * A random agent will map the local neighborhood (c). * A goal-directed agent will have SR representations that follow the optimal path (d).\nIt is therefore different from the transition matrix, as it depends on behavior and rewards. The exact dynamics are lost compared to MB: it only represents whether a state is reachable, not how.\n\n\nThe SR matrix reflects the proximity between states depending on the transitions and the policy. it does not have to be a spatial relationship.\n\n\n\n\n\n\n\n\nProbabilistic MDP and the corresponding SR matrix. Source: (Stachenfeld et al., 2017).\n\n\n\n\nLearning successor representations\nHow can we learn the SR matrix for all pairs of states?\n\n    M^\\pi(s, s') = \\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, \\mathbb{I}(s_{t+k} = s') | s_t = s]\n\nWe first notice that the SR obeys a recursive Bellman-like equation:\n\\begin{aligned}\n    M^\\pi(s, s') &= \\mathbb{I}(s_{t} = s') + \\mathbb{E}_{\\pi} [\\sum_{k=1}^\\infty \\gamma^k \\, \\mathbb{I}(s_{t+k} = s') | s_t = s] \\\\\n            &= \\mathbb{I}(s_{t} = s') + \\gamma \\, \\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, \\mathbb{I}(s_{t+k+1} = s') | s_t = s] \\\\\n            &= \\mathbb{I}(s_{t} = s') + \\gamma \\, \\mathbb{E}_{s_{t+1} \\sim \\mathcal{P}^\\pi(s' | s)} [\\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, \\mathbb{I}(s_{t+k} = s') | s_{t+1} = s] ]\\\\\n            &= \\mathbb{I}(s_{t} = s') + \\gamma \\, \\mathbb{E}_{s_{t+1} \\sim \\mathcal{P}^\\pi(s' | s)} [M^\\pi(s_{t+1}, s')]\\\\\n\\end{aligned}\n\nThis is reminiscent of TDM: the remaining distance to the goal is 0 if I am already at the goal, or gamma the distance from the next state to the goal.\nIf we know the transition matrix for a fixed policy \\pi:\n\\mathcal{P}^\\pi(s, s') = \\sum_a \\pi(s, a) \\, p(s' | s, a)\nwe can obtain the SR directly with matrix inversion as we did in dynamic programming:\n\n    M^\\pi = I + \\gamma \\, \\mathcal{P}^\\pi \\times M^\\pi\n\nso that:\n\n    M^\\pi = (I - \\gamma \\, \\mathcal{P}^\\pi)^{-1}\n\nThis DP approach is called model-based SR (MB-SR, (Momennejad et al., 2017)) as it necessitates to know the environment dynamics.\nIf we do not know the transition probabilities, we simply sample a single s_t, s_{t+1} transition:\n\n    M^\\pi(s_t, s') \\approx \\mathbb{I}(s_{t} = s') + \\gamma \\, M^\\pi(s_{t+1}, s')\n\nWe can define a sensory prediction error (SPE):\n\n    \\delta^\\text{SR}_t = \\mathbb{I}(s_{t} = s') + \\gamma \\, M^\\pi(s_{t+1}, s') - M(s_t, s')\n\nthat is used to update an estimate of the SR:\n\n    \\Delta M^\\pi(s_t, s') = \\alpha \\, \\delta^\\text{SR}_t\n\nThis is SR-TD, using a SPE instead of RPE, which learns only from transitions but ignores rewards.\nThe SPE has to be applied on ALL successor states s' after a transition (s_t, s_{t+1}):\n\n    M^\\pi(s_t, \\mathbf{s'}) = M^\\pi(s_t, \\mathbf{s'}) + \\alpha \\, (\\mathbb{I}(s_{t}=\\mathbf{s'}) + \\gamma \\, M^\\pi(s_{t+1}, \\mathbf{s'}) - M(s_t, \\mathbf{s'}))\n\nContrary to the RPE, the SPE is a vector of prediction errors, used to update one row of the SR matrix. The SPE tells how surprising a transition s_t \\rightarrow s_{t+1} is for the SR.\n\n\n\n\n\n\nSummary\n\n\n\nThe SR matrix represents the expected discounted future state occupancy:\nM^\\pi(s, s') = \\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, \\mathbb{I}(s_{t+k} = s') | s_t = s]\nIt can be learned using a TD-like SPE from single transitions:\n\n    M^\\pi(s_t, \\mathbf{s'}) = M^\\pi(s_t, \\mathbf{s'}) + \\alpha \\, (\\mathbb{I}(s_{t}=\\mathbf{s'}) + \\gamma \\, M^\\pi(s_{t+1}, \\mathbf{s'}) - M(s_t, \\mathbf{s'}))\n\nThe immediate reward in each state can be learned independently from the policy:\n\n    \\Delta \\, r(s_t) = \\alpha \\, (r_{t+1} - r(s_t))\n\nThe value V^\\pi(s) of a state is obtained by summing of all successor states:\n\n    V^\\pi(s) = \\sum_{s' \\in \\mathcal{S}} M(s, s') \\times r(s')\n\nThis critic can be used to train an actor \\pi_\\theta using regular TD learning (e.g. A3C).\n\n\nNote that it is straightforward to extend the idea of SR to state-action pairs:\nM^\\pi(s, a, s') = \\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, \\mathbb{I}(s_{t+k} = s') | s_t = s, a_t = a]\nallowing to estimate Q-values:\n\n    Q^\\pi(s, a) = \\sum_{s' \\in \\mathcal{S}} M(s, a, s') \\times r(s')\n\nusing SARSA or Q-learning-like SPEs:\n\n    \\delta^\\text{SR}_t = \\mathbb{I}(s_{t} = s') + \\gamma \\, M^\\pi(s_{t+1}, a_{t+1}, s') - M(s_t, a_{t}, s')\n\ndepending on the choice of the next action a_{t+1} (on- or off-policy)."
  },
  {
    "objectID": "notes/4.4-SR.html#successor-features",
    "href": "notes/4.4-SR.html#successor-features",
    "title": "Successor representations",
    "section": "Successor features",
    "text": "Successor features\n\nThe SR matrix associates each state to all others (N\\times N matrix):\n\ncurse of dimensionality.\nonly possible for discrete state spaces.\n\nA better idea is to describe each state s by a feature vector \\phi(s) = [\\phi_i(s)]_{i=1}^d with less dimensions than the number of states. This feature vector can be constructed (see the lecture on function approximation) or learned by an autoencoder (latent representation).\nThe successor feature representation (SFR) represents the discounted probability of observing a feature \\phi_j after being in s. Instead of predicting when the agent will see a cat after being in the current state s, the SFR predicts when it will see eyes, ears or whiskers independently:\n\n    M^\\pi_j(s) = M^\\pi(s, \\phi_j) = \\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, \\mathbb{I}(\\phi_j(s_{t+k})) | s_t = s, a_t = a]\n\nLinear SFR (Gehring, 2015) supposes that it can be linearly approximated from the features of the current state:\n\n    M^\\pi_j(s) = M^\\pi(s, \\phi_j) = \\sum_{i=1}^d m_{i, j} \\, \\phi_i(s)\n\nThe value of a state is now defined as the sum over successor features of their immediate reward discounted by the SFR:\n\n    V^\\pi(s) = \\sum_{j=1}^d M^\\pi_j(s) \\, r(\\phi_j) = \\sum_{j=1}^d r(\\phi_j) \\, \\sum_{i=1}^d m_{i, j} \\, \\phi_i(s)\n\nThe SFR matrix M^\\pi = [m_{i, j}]_{i, j} associates each feature \\phi_i of the current state to all successor features \\phi_j: Knowing that I see a kitchen door in the current state, how likely will I see a food outcome in the near future?\nEach successor feature \\phi_j is associated to an expected immediate reward r(\\phi_j): A good state is a state where food features (high r(\\phi_j)) are likely to happen soon (high m_{i, j}).\nIn matrix-vector form:\n\n    V^\\pi(s) = \\mathbf{r}^T \\times M^\\pi \\times \\phi(s)\n\nThe reward vector \\mathbf{r} only depends on the features and can be learned independently from the policy, but can be made context-dependent: Food features can be made more important when the agent is hungry, less when thirsty.\nTransfer learning becomes possible in the same environment: * Different goals (searching for food or water, going to place A or B) only require different reward vectors. * The dynamics of the environment are stored in the SFR.\nHow can we learn the SFR matrix M^\\pi?\n\n    V^\\pi(s) = \\mathbf{r}^T \\times M^\\pi \\times \\phi(s)\n\nWe only need to use the sensory prediction error for a transition between the feature vectors \\phi(s_t) and \\phi(s_{t+1}):\n\\delta_t^\\text{SFR} = \\phi(s_t) + \\gamma \\, M^\\pi \\times \\phi(s_{t+1}) - M^\\pi \\times \\phi(s_t)\nand use it to update the whole matrix:\n\\Delta M^\\pi = \\delta_t^\\text{SFR} \\times \\phi(s_t)^T\nHowever, this linear approximation scheme only works for fixed feature representation \\phi(s). We need to go deeper…"
  },
  {
    "objectID": "notes/4.4-SR.html#deep-successor-reinforcement-learning",
    "href": "notes/4.4-SR.html#deep-successor-reinforcement-learning",
    "title": "Successor representations",
    "section": "Deep Successor Reinforcement Learning",
    "text": "Deep Successor Reinforcement Learning\n\n\n\nDeep Successor Reinforcement Learning architecture. (Kulkarni et al., 2016).\n\n\nEach state s_t is represented by a D-dimensional (D=512) vector \\phi(s_t) = f_\\theta(s_t) which is the output of an encoder. A decoder g_{\\hat{\\theta}} is used to provide a reconstruction loss, so \\phi(s_t) is a latent representation of an autoencoder:\n\\mathcal{L}_\\text{reconstruction}(\\theta, \\hat{\\theta}) = \\mathbb{E}[(g_{\\hat{\\theta}}(\\phi(s_t)) - s_t)^2]\nThe immediate reward R(s_t) is linearly predicted from the feature vector \\phi(s_t) using a reward vector \\mathbf{w}.\nR(s_t) = \\phi(s_t)^T \\times \\mathbf{w}\n\\mathcal{L}_\\text{reward}(\\mathbf{w}, \\theta) = \\mathbb{E}[(r_{t+1} - \\phi(s_t)^T \\times \\mathbf{w})^2]\nThe reconstruction loss is important, otherwise the latent representation \\phi(s_t) would be too reward-oriented and would not generalize. The reward function is learned on a single task, but it can fine-tuned on another task, with all other weights frozen.\nFor each available action a, a DNN u_\\alpha predicts the future feature occupancy M(s, s', a) for the current state:\nm_{s_t a} = u_\\alpha(s_t, a)\nThe Q-value of an action is simply the dot product between the SR of an action and the reward vector \\mathbf{w}:\nQ(s_t, a) = \\mathbf{w}^T \\times m_{s_t a} \nThe selected action is \\epsilon-greedily selected around the greedy action:\na_t = \\text{arg}\\max_a Q(s_t, a)\nThe SR of each action is learned using the Q-learning-like SPE (with fixed \\theta and a target network u_{\\alpha'}):\n\\mathcal{L}^\\text{SPE}(\\alpha) = \\mathbb{E}[\\sum_a (\\phi(s_t) + \\gamma \\, \\max_{a'} u_{\\alpha'}(s_{t+1}, a') - u_\\alpha(s_t, a))^2]\nThe compound loss is used to train the complete network end-to-end off-policy using a replay buffer (DQN-like).\n\\mathcal{L}(\\theta, \\hat{\\theta}, \\mathbf{w}, \\alpha) = \\mathcal{L}_\\text{reconstruction}(\\theta, \\hat{\\theta}) + \\mathcal{L}_\\text{reward}(\\mathbf{w}, \\theta) + \\mathcal{L}^\\text{SPE}(\\alpha)\n\n\n\nDeep Successor Reinforcement Learning algorithm. (Kulkarni et al., 2016).\n\n\n\n\n\nDeep Successor Reinforcement Learning results. (Kulkarni et al., 2016).\n\n\nThe interesting property is that you do not need rewards to learn: * A random agent can be used to learn the encoder and the SR, but \\mathbf{w} can be left untouched. * When rewards are introduced (or changed), only \\mathbf{w} has to be adapted, while DQN would have to re-learn all Q-values.\n\n\n\nChange in the value of distal rewards. (Kulkarni et al., 2016).\n\n\nThis is the principle of latent learning in animal psychology: fooling around in an environment without a goal allows to learn the structure of the world, what can speed up learning when a task is introduced. The SR is a cognitive map of the environment: learning task-unspecific relationships.\n\n\n\n\n\n\nNote\n\n\n\nThe same idea was published by three different groups at the same time (preprint in 2016, conference in 2017): (Barreto et al., 2016), (Kulkarni et al., 2016), (Zhang et al., 2016). The (Barreto et al., 2016) is from Deepmind, so it tends to be cited more…\n\n\n\n\n\n\n\n\nVisual Semantic Planning using Deep Successor Representations\n\n\n\nSee the paper: (Zhu et al., 2017)\n\n\n\n\n\n\n\nBarreto, A., Dabney, W., Munos, R., Hunt, J. J., Schaul, T., van Hasselt, H., et al. (2016). Successor Features for Transfer in Reinforcement Learning. http://arxiv.org/abs/1606.05312.\n\n\nDayan, P. (1993). Improving Generalization for Temporal Difference Learning: The Successor Representation. Neural Computation 5, 613–624. doi:10.1162/neco.1993.5.4.613.\n\n\nGehring, C. A. (2015). Approximate Linear Successor Representation. in, 5. http://people.csail.mit.edu/gehring/publications/clement-gehring-rldm-2015.pdf.\n\n\nKulkarni, T. D., Saeedi, A., Gautam, S., and Gershman, S. J. (2016). Deep Successor Reinforcement Learning. http://arxiv.org/abs/1606.02396.\n\n\nMomennejad, I., Russek, E. M., Cheong, J. H., Botvinick, M. M., Daw, N. D., and Gershman, S. J. (2017). The successor representation in human reinforcement learning. Nature Human Behaviour 1, 680–692. doi:10.1038/s41562-017-0180-8.\n\n\nRussek, E. M., Momennejad, I., Botvinick, M. M., Gershman, S. J., and Daw, N. D. (2017). Predictive representations can link model-based reinforcement learning to model-free mechanisms. PLOS Computational Biology 13, e1005768. doi:10.1371/journal.pcbi.1005768.\n\n\nStachenfeld, K. L., Botvinick, M. M., and Gershman, S. J. (2017). The hippocampus as a predictive map. Nature Neuroscience 20, 1643–1653. doi:10.1038/nn.4650.\n\n\nZhang, J., Springenberg, J. T., Boedecker, J., and Burgard, W. (2016). Deep Reinforcement Learning with Successor Features for Navigation across Similar Environments. http://arxiv.org/abs/1612.05533.\n\n\nZhu, Y., Gordon, D., Kolve, E., Fox, D., Fei-Fei, L., Gupta, A., et al. (2017). Visual Semantic Planning using Deep Successor Representations. http://arxiv.org/abs/1705.08080."
  },
  {
    "objectID": "notes/5.1-Outlook.html",
    "href": "notes/5.1-Outlook.html",
    "title": "Outlook",
    "section": "",
    "text": "Slides: html pdf"
  },
  {
    "objectID": "notes/5.1-Outlook.html#limits-of-deep-reinforcement-learning",
    "href": "notes/5.1-Outlook.html#limits-of-deep-reinforcement-learning",
    "title": "Outlook",
    "section": "Limits of deep reinforcement learning",
    "text": "Limits of deep reinforcement learning\n\n\nOverview\nModel-free methods (DQN, A3C, DDPG, PPO, SAC) are able to find optimal policies in complex MDPs by just sampling transitions. They suffer however from a high sample complexity, i.e. they need ridiculous amounts of samples to converge.\nModel-based methods (I2A, Dreamer, MuZero) use learned dynamics to predict the future and plan the consequences of an action. The sample complexity is lower, but learning a good model can be challenging. Inference times can be prohibitive.\n\n\n\nOverview of deep RL methods. Source: https://github.com/avillemin/RL-Personnal-Notebook\n\n\nDeep RL is still very unstable. Depending on initialization, deep RL networks may or may not converge (30% of runs converge to a worse policy than a random agent). Careful optimization such as TRPO / PPO help, but not completely. You never know if failure is your fault (wrong network, bad hyperparameters, bug), or just bad luck.\n\n\nDeep RL is popular because it's the only area in ML where it's socially acceptable to train on the test set.\n\n— Jacob Andreas ((jacobandreas?)) October 28, 2017\n\n\nAs it uses neural networks, deep RL overfits its training data, i.e. the environment it is trained on. If you change anything to the environment dynamics, you need to retrain from scratch. OpenAI Five collects 900 years of game experience per day on Dota 2: it overfits the game, it does not learn how to play. Modify the map a little bit and everything is gone (but see Meta RL - RL^2 later).\nClassical methods sometimes still work better. Model Predictive Control (MPC) is able to control Mujoco robots much better than RL through classical optimization techniques (e.g. iterative LQR) while needing much less computations. If you have a good physics model, do not use DRL. Reserve it for unknown systems, or when using noisy sensors (images). Genetic algorithms (CMA-ES) sometimes give better results than RL to train policy networks.\n\nYou cannot do that with deep RL (yet):\n\n\n\nRL libraries\n\nkeras-rl: many deep RL algorithms implemented directly in keras: DQN, DDQN, DDPG, Continuous DQN (CDQN or NAF), Cross-Entropy Method (CEM)…\n\nhttps://github.com/matthiasplappert/keras-rl\n\nOpenAI Baselines from OpenAI: A2C, ACER, ACKTR, DDPG, DQN, PPO, TRPO… Not maintained anymore.\n\nhttps://github.com/openai/baselines\n\nStable baselines from Inria Flowers, a clean rewrite of OpenAI baselines also including SAC and TD3.\n\nhttps://github.com/hill-a/stable-baselines\n\nrlkit from Vitchyr Pong (PhD student at Berkeley) with in particular model-based algorithms (TDM).\n\nhttps://github.com/vitchyr/rlkit\n\nchainer-rl implemented in Chainer: A3C, ACER, DQN, DDPG, PGT, PCL, PPO, TRPO.\n\nhttps://github.com/chainer/chainerrl\n\nRL Mushroom is a very modular library based on Pytorch allowing to implement DQN and variants, DDPG, SAC, TD3, TRPO, PPO.\n\nhttps://github.com/MushroomRL/mushroom-rl\n\nTensorforce implement in tensorflow: DQN and variants, A3C, DDPG, TRPO, PPO.\n\nhttps://github.com/tensorforce/tensorforce\n\nTensorflow Agents is officially supported by tensorflow: DQN, A3C, DDPG, TD3, PPO, SAC.\n\nhttps://github.com/tensorflow/agents\n\nCoach from Intel Nervana also provides many state-of-the-art algorithms.\n\nhttps://github.com/NervanaSystems/coach\n\n\n\nDeep RL algorithms available in Coach. Source: https://github.com/NervanaSystems/coach\n\n\n\nrllib is part of the more global ML framework Ray, which also includes Tune for hyperparameter optimization.\n\nIt has implementations in both tensorflow and Pytorch.\nAll major model-free algorithms are implemented (DQN, Rainbow, A3C, DDPG, PPO, SAC), including their distributed variants (Ape-X, IMPALA, TD3) but also model-based algorithms (Dreamer!)\nhttps://docs.ray.io/en/master/rllib.html\n\n\n\nArchitecture of rllib. Source: https://docs.ray.io/en/master/rllib.html"
  },
  {
    "objectID": "notes/5.1-Outlook.html#inverse-rl---learning-the-reward-function",
    "href": "notes/5.1-Outlook.html#inverse-rl---learning-the-reward-function",
    "title": "Outlook",
    "section": "Inverse RL - learning the reward function",
    "text": "Inverse RL - learning the reward function\n\nRL is an optimization method: it maximizes the reward function that you provide it. If you do not design the reward function correctly, the agent may not do what you expect. In the Coast runners game, turbos provide small rewards but respawn very fast: it is more optimal to collect them repeatedly than to try to finish the race.\n\nDefining the reward function that does what you want becomes an art. RL algorithms work better with dense rewards than sparse ones. It is tempting to introduce intermediary rewards. You end up covering so many special cases that it becomes unusable: Go as fast as you can but not in a curve, except if you are on a closed circuit but not if it rains…\n\nIn the OpenAI Lego stacking paper (Popov et al., 2017), it was perhaps harder to define the reward function than to implement DDPG.\n\n\n\nLego stacking handmade reward function (Popov et al., 2017).\n\n\nThe goal of inverse RL (see (Arora and Doshi, 2019) for a review) is to learn from demonstrations (e.g. from humans) which reward function is maximized. This is not imitation learning, where you try to learn and reproduce actions. The goal if to find a parametrized representation of the reward function:\n\\hat{r}(s) = \\sum_{i=1}^K w_i \\, \\varphi_i(s)\nWhen the reward function has been learned, you can train a RL algorithm to find the optimal policy.\n\n\n\nInverse RL allows to learn from demonstrations. Source: http://www.miubiq.cs.titech.ac.jp/modeling-risk-anticipation-and-defensive-driving-on-residential-roads-using-inverse-reinforcement-learning/"
  },
  {
    "objectID": "notes/5.1-Outlook.html#intrinsic-motivation-and-curiosity",
    "href": "notes/5.1-Outlook.html#intrinsic-motivation-and-curiosity",
    "title": "Outlook",
    "section": "Intrinsic motivation and curiosity",
    "text": "Intrinsic motivation and curiosity\nOne fundamental problem of RL is its dependence on the reward function. When rewards are sparse, the agent does not learn much (but see successor representations) unless its random exploration policy makes it discover rewards. The reward function is handmade, what is difficult in realistic complex problems.\nHuman learning does not (only) rely on maximizing rewards or achieving goals. Especially infants discover the world by playing, i.e. interacting with the environment out of curiosity.\n\nWhat happens if I do that? Oh, that’s fun.\n\nThis called intrinsic motivation: we are motivated by understanding the world, not only by getting rewards. Rewards are internally generated.\n\n\n\nIn intrinsic motivation, rewards are generated internally depending on the achieved states. Source: (Barto, 2013).\n\n\nWhat is intrinsically rewarding / motivating / fun? Mostly what has unexpected consequences.\n\nIf you can predict what is going to happen, it becomes boring.\nIf you cannot predict, you can become curious and try to explore that action.\n\n\n\n\nIntrinsic rewards are defined by the ability to predict states. Source: https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa\n\n\nThe intrinsic reward (IR) of an action is defined as the sensory prediction error:\n\n    \\text{IR}(s_t, a_t, s_{t+1}) = || f(s_t, a_t) - s_{t+1}||\n\nwhere f(s_t, a_t) is a forward model predicting the sensory consequences of an action. An agent maximizing the IR will tend to visit unknown / poorly predicted states (exploration).\nIs it a good idea to predict frames directly? Frames are highly dimensional and there will always be a remaining error.\n\n\n\nIntrinsic rewards are defined by the ability to predict states. Source: https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa\n\n\nMoreover, they can be noisy and unpredictable, without being particularly interesting.\n\n\n\nFalling leaves are hard to predict, but hardly interesting. Source: Giphy.\n\n\nWhat can we do? As usual, predict in a latent space!\nThe intrinsic curiosity module (ICM, (Pathak et al., 2017)) learns to provide an intrinsic reward for a transition (s_t, a_t, s_{t+1}) by comparing the predicted latent representation \\hat{\\phi}(s_{t+1}) (using a forward model) to its “true” latent representation \\phi(s_{t+1}). The feature representation \\phi(s_t) is trained using an inverse model predicting the action leading from s_t to s_{t+1}.\n\n\n\nintrinsic curiosity module. (Pathak et al., 2017)\n\n\n\n\n\n\n\n\n\nCuriosity-driven RL on Atari games (Burda et al., 2018):"
  },
  {
    "objectID": "notes/5.1-Outlook.html#hierarchical-rl---learning-different-action-levels",
    "href": "notes/5.1-Outlook.html#hierarchical-rl---learning-different-action-levels",
    "title": "Outlook",
    "section": "Hierarchical RL - learning different action levels",
    "text": "Hierarchical RL - learning different action levels\n\nIn all previous RL methods, the action space is fixed. When you read a recipe, the actions are “Cut carrots”, “Boil water”, etc. But how do you perform these high-level actions? Break them into subtasks iteratively until you arrive to muscle activations. But it is not possible to learn to cook a boeuf bourguignon using muscle activations as actions.\n\n\n\nHierarchical structure of preparing a boeuf bourguignon. Source: https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/\n\n\nSub-policies (options) can be trained to solve simple tasks (going left, right, etc). A meta-learner or controller then learns to call each sub-policy when needed, at a much lower frequency (Frans et al., 2017).\n\n\n\nMeta Learning Shared Hierarchies (Frans et al., 2017). Source: https://openai.com/blog/learning-a-hierarchy/\n\n\n\nSome additional references on Hierarchical Reinforcement Learning\n\nMLSH: Frans, K., Ho, J., Chen, X., Abbeel, P., and Schulman, J. (2017). Meta Learning Shared Hierarchies. arXiv:1710.09767.\nFUN: Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver, D., et al. (2017). FeUdal Networks for Hierarchical Reinforcement Learning. arXiv:1703.01161\nOption-Critic architecture: Bacon, P.-L., Harb, J., and Precup, D. (2016). The Option-Critic Architecture. arXiv:1609.05140.\nHIRO: Nachum, O., Gu, S., Lee, H., and Levine, S. (2018). Data-Efficient Hierarchical Reinforcement Learning. arXiv:1805.08296.\nHAC: Levy, A., Konidaris, G., Platt, R., and Saenko, K. (2019). Learning Multi-Level Hierarchies with Hindsight. arXiv:1712.00948.\nSpinal-cortical: Heess, N., Wayne, G., Tassa, Y., Lillicrap, T., Riedmiller, M., and Silver, D. (2016). Learning and Transfer of Modulated Locomotor Controllers. arXiv:1610.05182."
  },
  {
    "objectID": "notes/5.1-Outlook.html#meta-reinforcement-learning---rl2",
    "href": "notes/5.1-Outlook.html#meta-reinforcement-learning---rl2",
    "title": "Outlook",
    "section": "Meta Reinforcement learning - RL^2",
    "text": "Meta Reinforcement learning - RL^2\nMeta learning is the ability to reuse skills acquired on a set of tasks to quickly acquire new (similar) ones (generalization).\n \nMeta RL is based on the idea of fast and slow learning: * Slow learning is the adaptation of weights in the NN. * Fast learning is the adaptation to changes in the environment.\nA simple strategy developed concurrently by (Wang et al., 2017) and (Duan et al., 2016)is to have a model-free algorithm (e.g. A3C) integrate with a LSTM layer not only the current state s_t, but also the previous action a_{t-1} and reward r_t.\n\n\n\nMeta RL uses a LSTM layer to encode past actions and rewards in the state representation. Source: (Wang et al., 2017)\n\n\nThe policy of the agent becomes memory-guided: it selects an action depending on what it did before, not only the state.\n\n\n\nMeta RL algorithms are trained on a set of similar MDPs. Source: (Duan et al., 2016)\n\n\nThe algorithm is trained on a set of similar MDPs:\n\nSelect a MDP \\mathcal{M}.\nReset the internal state of the LSTM.\nSample trajectories and adapt the weights.\nRepeat 1, 2 and 3.\n\nThe meta RL can be be trained an a multitude of 2-armed bandits, each giving a reward of 1 with probability p and 1-p. Left is a classical bandit algorithm, right is the meta bandit:\n\n\n\nClassical bandit (left) and meta-bandit (right) learning a new two-armed bandit problem. Source: https://hackernoon.com/learning-policies-for-learning-policies-meta-reinforcement-learning-rl%C2%B2-in-tensorflow-b15b592a2ddf\n\n\nThe meta bandit has learned that the best strategy for any 2-armed bandit is to sample both actions randomly at the beginning and then stick to the best one. The meta bandit does not learn to solve each problem, it learns how to solve them.\n\n\n\n\n\n\nModel-Based Meta-Reinforcement Learning for Flight with Suspended Payloads\n\n\n\n(Belkhale et al., 2021) https://sites.google.com/view/meta-rl-for-flight\n\n\n\nAdditional references on meta RL:\n\nMeta RL: Wang JX, Kurth-Nelson Z, Tirumala D, Soyer H, Leibo JZ, Munos R, Blundell C, Kumaran D, Botvinick M. (2016). Learning to reinforcement learn. arXiv:161105763.\nRL^2 Duan Y, Schulman J, Chen X, Bartlett PL, Sutskever I, Abbeel P. 2016. RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning. arXiv:161102779.\nMAML: Finn C, Abbeel P, Levine S. (2017). Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. arXiv:170303400.\nPEARL: Rakelly K, Zhou A, Quillen D, Finn C, Levine S. (2019). Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables. arXiv:190308254.\nPOET: Wang R, Lehman J, Clune J, Stanley KO. (2019). Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions. arXiv:190101753.\nMetaGenRL: Kirsch L, van Steenkiste S, Schmidhuber J. (2020). Improving Generalization in Meta Reinforcement Learning using Learned Objectives. arXiv:191004098.\nBotvinick M, Ritter S, Wang JX, Kurth-Nelson Z, Blundell C, Hassabis D. (2019). Reinforcement Learning, Fast and Slow. Trends in Cognitive Sciences 23:408–422. doi:10.1016/j.tics.2019.02.006\nhttps://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html\nhttps://hackernoon.com/learning-policies-for-learning-policies-meta-reinforcement-learning-rl%C2%B2-in-tensorflow-b15b592a2ddf\nhttps://towardsdatascience.com/learning-to-learn-more-meta-reinforcement-learning-f0cc92c178c1\nhttps://eng.uber.com/poet-open-ended-deep-learning/\n\n\n\n\n\nArora, S., and Doshi, P. (2019). A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress. http://arxiv.org/abs/1806.06877.\n\n\nBarto, A. G. (2013). “Intrinsic Motivation and Reinforcement Learning,” in Intrinsically Motivated Learning in Natural and Artificial Systems, eds. G. Baldassarre and M. Mirolli (Berlin, Heidelberg: Springer), 17–47. doi:10.1007/978-3-642-32375-1_2.\n\n\nBelkhale, S., Li, R., Kahn, G., McAllister, R., Calandra, R., and Levine, S. (2021). Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads. IEEE Robot. Autom. Lett., 1–1. doi:10.1109/LRA.2021.3057046.\n\n\nBurda, Y., Edwards, H., Pathak, D., Storkey, A., Darrell, T., and Efros, A. A. (2018). Large-Scale Study of Curiosity-Driven Learning. http://arxiv.org/abs/1808.04355.\n\n\nDuan, Y., Schulman, J., Chen, X., Bartlett, P. L., Sutskever, I., and Abbeel, P. (2016). RL$2̂$: Fast Reinforcement Learning via Slow Reinforcement Learning. http://arxiv.org/abs/1611.02779.\n\n\nFrans, K., Ho, J., Chen, X., Abbeel, P., and Schulman, J. (2017). Meta Learning Shared Hierarchies. http://arxiv.org/abs/1710.09767.\n\n\nPathak, D., Agrawal, P., Efros, A. A., and Darrell, T. (2017). Curiosity-driven Exploration by Self-supervised Prediction. http://arxiv.org/abs/1705.05363.\n\n\nPopov, I., Heess, N., Lillicrap, T., Hafner, R., Barth-Maron, G., Vecerik, M., et al. (2017). Data-efficient Deep Reinforcement Learning for Dexterous Manipulation. http://arxiv.org/abs/1704.03073.\n\n\nWang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R., Kavukcuoglu, K., et al. (2017). Sample Efficient Actor-Critic with Experience Replay. http://arxiv.org/abs/1611.01224."
  },
  {
    "objectID": "exercises/Content.html",
    "href": "exercises/Content.html",
    "title": "List of exercises",
    "section": "",
    "text": "You will find below links to download the notebooks for the exercises (which you have to fill) and their solution (which you can look at after you finished the exercise). It is recommended not to look at the solution while doing the exercise unless you are lost.\nAlternatively, you can run the notebooks directly on Colab (https://colab.research.google.com/) if you have a Google account.\nFor instructions on how to install a Python distribution on your computer, check this page.\nYou will also find videos presenting the exercises and commenting their solution.\nThe solution of each exercise is rendered in the following pages."
  },
  {
    "objectID": "exercises/Content.html#introduction-to-python",
    "href": "exercises/Content.html#introduction-to-python",
    "title": "List of exercises",
    "section": "Introduction to Python",
    "text": "Introduction to Python\nThis exercise is an introduction to Python for absolute beginners. If you already know Python, you can safely skip it.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#numpy-and-matplotlib",
    "href": "exercises/Content.html#numpy-and-matplotlib",
    "title": "List of exercises",
    "section": "Numpy and Matplotlib",
    "text": "Numpy and Matplotlib\nThe goal of this exercise is to present the basics of the numerical library numpy as well as the visualization library matplotlib.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#sampling",
    "href": "exercises/Content.html#sampling",
    "title": "List of exercises",
    "section": "Sampling",
    "text": "Sampling\nThe goal of this exercise is to understand how to sample rewards from a n-armed bandits and to understand the central limit theorem.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#bandits---part-1",
    "href": "exercises/Content.html#bandits---part-1",
    "title": "List of exercises",
    "section": "Bandits - part 1",
    "text": "Bandits - part 1\nThe goal of this exercise is to implement simple action selection mechanisms for the n-armed bandit:\n\nGreedy action selection\n\\epsilon-greedy action selection\nSoftmax action selection\n\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#bandits---part-2",
    "href": "exercises/Content.html#bandits---part-2",
    "title": "List of exercises",
    "section": "Bandits - part 2",
    "text": "Bandits - part 2\nThe goal of this exercise is to further investigate the properties of the action selection algorithms for the n-armed bandit.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#dynamic-programming",
    "href": "exercises/Content.html#dynamic-programming",
    "title": "List of exercises",
    "section": "Dynamic programming",
    "text": "Dynamic programming\nThe goal of this exercise is to apply policy iteration and value iteration on the recycling robot MDP.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#gym",
    "href": "exercises/Content.html#gym",
    "title": "List of exercises",
    "section": "Gym",
    "text": "Gym\nThe goal of this exercise is to install gym and learn how to use its interface.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#monte-carlo-control",
    "href": "exercises/Content.html#monte-carlo-control",
    "title": "List of exercises",
    "section": "Monte Carlo control",
    "text": "Monte Carlo control\nThe goal of this exercise is to implement on-policy Monte-Carlo control on the Taxi gym environment.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#temporal-difference",
    "href": "exercises/Content.html#temporal-difference",
    "title": "List of exercises",
    "section": "Temporal difference",
    "text": "Temporal difference\nThe goal of this exercise is to implement Q-learning on the Taxi gym environment.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#eligibility-traces",
    "href": "exercises/Content.html#eligibility-traces",
    "title": "List of exercises",
    "section": "Eligibility traces",
    "text": "Eligibility traces\nThe goal of this exercise is to implement Q-learning with eligibility traces on the Gridworld environment.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#keras",
    "href": "exercises/Content.html#keras",
    "title": "List of exercises",
    "section": "Keras",
    "text": "Keras\nThe goal of this exercise is to quickly discover keras and to understand why neural networks (and SGD) need i.i.d. samples.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Content.html#dqn",
    "href": "exercises/Content.html#dqn",
    "title": "List of exercises",
    "section": "DQN",
    "text": "DQN\nThe goal of this exercise is to implement DQN on the Cartpole balancing problem.\nNotebook: download .ipynb or run on colab.\nSolution: download .ipynb or run on colab.\nPresentation\n\nCommented solution"
  },
  {
    "objectID": "exercises/Installation.html",
    "href": "exercises/Installation.html",
    "title": "Python installation",
    "section": "",
    "text": "Note\n\n\n\nIn the B202 (FRIZ), conda is already installed. You first need to initialize it by typing this command in a terminal:\nconda init tcsh\nClose the terminal, open a new one, and type:\nconda create --name deeprl python=3.9\nconda activate deeprl\nconda install -c conda-forge numpy matplotlib jupyterlab\nThis can take a while, be patient.\nBefore every session, or when you open a new terminal, you will need to type:\nconda activate deeprl\nHere are the main Python dependencies necessary for the exercises:\nIf you are using Linux, you can probably install all the dependencies (except gym) from your package manager. For the others, use either Anaconda or Colab."
  },
  {
    "objectID": "exercises/Installation.html#anaconda",
    "href": "exercises/Installation.html#anaconda",
    "title": "Python installation",
    "section": "Anaconda",
    "text": "Anaconda\n\nInstalling Anaconda\nPython should be already installed if you use Linux, a very old version if you use MacOS, and probably nothing under Windows. Moreover, Python 2.7 became obsolete in December 2019 but is still the default on some distributions.\nFor these reasons, we strongly recommend installing Python 3 using the Anaconda distribution, or even better the community-driven fork Miniforge:\nhttps://github.com/conda-forge/miniforge\nAnaconda offers all the major Python packages in one place, with a focus on data science and machine learning. To install it, simply download the installer / script for your OS and follow the instructions. Beware, the installation takes quite a lot of space on the disk (around 1 GB), so choose the installation path wisely.\n\n\nInstalling packages\nTo install packages (for example numpy), you just have to type in a terminal:\nconda install numpy\nRefer to the docs (https://docs.anaconda.com/anaconda/) to know more.\nIf you prefer your local Python installation, or if a package is not available or outdated with Anaconda, the pip utility allows to also install virtually any Python package:\npip install numpy\n\n\nVirtual environments\nIt is a good idea to isolate the required packages from the rest of your Python installation, otherwise conflicts between package versions may arise.\nVirtual environments allow to create an isolated Python distribution for a project. The Python ecosystem offers many tools for that:\n\nvenv, the default Python 3 module.\nvirtualenv\npyenv\npipenv\n\nAs we advise to use Anaconda, we focus here on conda environments, but the logic is always the same.\nTo create a conda environment with the name deeprl using Python 3.9, type in a terminal:\nconda create --name deeprl python=3.9\nYou should see that it installs a bunch of basic packages along python:\n(base) ~/ conda create --name deeprl python=3.9\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /Users/vitay/Applications/miniforge3/envs/deeprl\n\n  added / updated specs:\n    - python=3.9\n\n\nThe following NEW packages will be INSTALLED:\n\n  bzip2              conda-forge/osx-arm64::bzip2-1.0.8-h3422bc3_4 None\n  ca-certificates    conda-forge/osx-arm64::ca-certificates-2022.9.24-h4653dfc_0 None\n  libffi             conda-forge/osx-arm64::libffi-3.4.2-h3422bc3_5 None\n  libsqlite          conda-forge/osx-arm64::libsqlite-3.39.4-h76d750c_0 None\n  libzlib            conda-forge/osx-arm64::libzlib-1.2.12-h03a7124_4 None\n  ncurses            conda-forge/osx-arm64::ncurses-6.3-h07bb92c_1 None\n  openssl            conda-forge/osx-arm64::openssl-3.0.5-h03a7124_2 None\n  pip                conda-forge/noarch::pip-22.2.2-pyhd8ed1ab_0 None\n  python             conda-forge/osx-arm64::python-3.9.13-h96fcbfb_0_cpython None\n  readline           conda-forge/osx-arm64::readline-8.1.2-h46ed386_0 None\n  setuptools         conda-forge/noarch::setuptools-65.4.1-pyhd8ed1ab_0 None\n  sqlite             conda-forge/osx-arm64::sqlite-3.39.4-h2229b38_0 None\n  tk                 conda-forge/osx-arm64::tk-8.6.12-he1e0b03_0 None\n  tzdata             conda-forge/noarch::tzdata-2022d-h191b570_0 None\n  wheel              conda-forge/noarch::wheel-0.37.1-pyhd8ed1ab_0 None\n  xz                 conda-forge/osx-arm64::xz-5.2.6-h57fd34a_0 None\n\n\nProceed ([y]/n)?\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n#\n# To activate this environment, use\n#\n#     $ conda activate deeprl\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\nRetrieving notices: ...working... done\nAs indicated at the end of the message, you need to activate the environment to use its packages:\nconda activate deeprl\nWhen you are done, you can deactivate it, or simply close the terminal.\n\n\n\n\n\n\nNote\n\n\n\nYou need to activate the environment every time you start an exercise or open a new terminal!\n\n\nYou can then install all the required packages to their latest versions, alternating between conda and pip:\nconda install numpy matplotlib jupyterlab\npip install tensorflow\npip install gym[all]\n\n\n\n\n\n\nHint\n\n\n\nIf you installed the regular Anaconda and not miniforge, we strongly advise to force using the conda forge channel:\nconda install -c conda-forge numpy matplotlib jupyterlab\n\n\nAlternatively, you can use one of the following files and install everything in one shot:\n\nconda-linux.yml for Linux and (possibly) Windows.\nconda-macos.yml for MacOS arm64 (M1). Untested on Intel-based macs.\n\nconda env create -f conda-linux.yml\nconda env create -f conda-macos.yml\n\n\n\n\n\n\nNote\n\n\n\nIf you have a CUDA-capable NVIDIA graphical card, follow these instructions to install tensorflow:\nhttps://www.tensorflow.org/install/pip\n\n\n\n\nUsing notebooks\nWhen the installation is complete, you just need to download the Jupyter notebooks (.ipynb) on this page, activate your environment, and type:\njupyter lab name_of_the_notebook.ipynb\nto open a browser tab with the notebook."
  },
  {
    "objectID": "exercises/Installation.html#colab",
    "href": "exercises/Installation.html#colab",
    "title": "Python installation",
    "section": "Colab",
    "text": "Colab\nAnother option is to run the notebooks in the cloud, for example on Google Colab:\nhttps://colab.research.google.com/\nColab has all major ML packages already installed, so you do not have to care about anything. Under conditions, you can also use a GPU for free (but for maximally 24 hours in a row).\nA link to run the notebooks on colab is provided in the list of exercises. Note that you will need a Google account (a dedicated one is fine if you are concerned about privacy).\nIf you want to save your progress, make a copy of the notebook in your Google drive when you open the link, or download the notebook at the end."
  },
  {
    "objectID": "exercises/1-Python-solution.html",
    "href": "exercises/1-Python-solution.html",
    "title": "Introduction To Python",
    "section": "",
    "text": "Python is a powerful, flexible programming language widely used for scientific computing, in web/Internet development, to write desktop graphical user interfaces (GUIs), create games, and much more. It became the de facto standard for machine learning, with a huge variety of specialized libraries such as:\nPython is an high-level, interpreted, object-oriented language written in C, which means it is compiled on-the-fly, at run-time execution. Its syntax is close to C, but without prototyping (whether a variable is an integer or a string will be automatically determined by the context). It can be executed either directly in an interpreter (à la Matlab), in a script or in a notebook (as here).\nThe documentation on Python can be found at http://docs.python.org.\nMany resources to learn Python exist on the Web:\nThis notebook only introduces you to the basics, so feel free to study additional resources if you want to master Python programming."
  },
  {
    "objectID": "exercises/1-Python-solution.html#working-with-python",
    "href": "exercises/1-Python-solution.html#working-with-python",
    "title": "Introduction To Python",
    "section": "Working With Python",
    "text": "Working With Python\nThere are basically three ways to program in Python: the interpreter for small commands, scripts for longer programs and notebooks (as here) for interactive programming.\n\nPython Interpreter\nTo start the Python interpreter, simply type its name in a terminal under Linux:\nuser@machine ~ $ python\nPython 3.7.4 (default, Jul 16 2019, 07:12:58) \n[GCC 9.1.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \nYou can then type anything at the prompt, for example a print statement:\n>>> print(\"Hello World!\")\nHello World!\nTo exit Python call the exit() function (or Ctrl+d):\n>>> exit()\n\n\nScripts\nInstead of using the interpreter, you can run scripts which can be executed sequentially. Simply edit a text file called MyScript.py containing for example:\n# MyScript.py\n# Implements the Hello World example.\n\ntext = 'Hello World!' # define a string variable\n\nprint(text)\nThe # character is used for comments. Execute this script by typing in a Terminal:\npython MyScript.py\nAs it is a scripted language, each instruction in the script is executed from the beginning to the end, except for the declared functions or classes which can be used later.\n\n\nJupyter Notebooks\nA third recent (but very useful) option is to use Jupyter notebooks (formerly IPython notebooks).\nJupyter notebooks allow you to edit Python code in your browser (but also Julia, R, Scala…) and run it locally.\nTo launch a Jupyter notebook, type in a terminal:\njupyter notebook\nand create a new notebook (Python 3)\nWhen a Jupyter notebook already exists (here 1-Python.ipynb), you can also start it directly:\njupyter notebook 1-Python.ipynb\nAlternatively, Jupyter lab has a more modern UI, but is still in beta.\nThe main particularity of notebooks is that code is not executed sequentially from the beginning to the end, but only when a cell is explicitly run with Ctrl + Enter (the active cell stays the same) or Shift + Enter (the next cell becomes active).\nTo edit a cell, select it and press Enter (or double-click).\nQ: In the next cell, run the Hello World! example:\n\ntext = 'Hello World!'\nprint(text)\n\nHello World!\n\n\nThere are three types of cells:\n\nPython cells allow to execute Python code (the default)\nMarkdown cells which allow to document the code nicely (code, equations), like the current one.\nRaw cell are passed to nbconvert directly, it allows you to generate html or pdf versions of your notebook (not used here).\n\nBeware that the order of execution of the cells matters!\nQ: In the next three cells, put the following commands:\n\ntext = \"Text A\"\ntext = \"Text B\"\nprint(text)\n\nand run them in different orders (e.g. 1, 2, 3, 1, 3)\n\ntext = \"Text A\"\n\n\ntext = \"Text B\"\n\n\nprint(text)\n\nText A\n\n\nExecuting a cell can therefore influence cells before and after it. If you want to run the notebook sequentially, select Kernel/Restart & Run all.\nTake a moment to explore the options in the menu (Insert cells, Run cells, Download as Python, etc)."
  },
  {
    "objectID": "exercises/1-Python-solution.html#basics-in-python",
    "href": "exercises/1-Python-solution.html#basics-in-python",
    "title": "Introduction To Python",
    "section": "Basics In Python",
    "text": "Basics In Python\n\nPrint Statement\nIn Python 3, the print() function is a regular function:\nprint(value1, value2, ...)\nYou can give it as many arguments as you want (of whatever type), they will be printed one after another separated by spaces.\nQ: Try to print “Hello World!” using two different strings “Hello” and “World!”:\n\ntext1 = 'Hello'\ntext2 = \"World!\"\n\nprint(text1, text2)\n\nHello World!\n\n\n\n\nData Types\nAs Python is an interpreted language, variables can be assigned without specifying their type: it will be inferred at execution time.\nThe only thing that counts is how you initialize them and which operations you perform on them.\na = 42          # Integer\nb = 3.14159     # Double precision float\nc = 'My string' # String\nd = False       # Boolean\ne = a > b       # Boolean\nQ: Print these variables as well as their type:\nprint(type(a))\n\na = 42          # Integer\nb = 3.14159     # Double precision float\nc = 'My string' # String\nd = False       # Boolean\ne = a > b       # Boolean\n\nprint('Value of a is', a,', Type of a is:', type(a))\nprint('Value of b is', b,', Type of b is:', type(b))\nprint('Value of c is', c,', Type of c is:', type(c))\nprint('Value of d is', d,', Type of d is:', type(d))\nprint('Value of e is', e,', Type of e is:', type(e))\n\nValue of a is 42 , Type of a is: <class 'int'>\nValue of b is 3.14159 , Type of b is: <class 'float'>\nValue of c is My string , Type of c is: <class 'str'>\nValue of d is False , Type of d is: <class 'bool'>\nValue of e is True , Type of e is: <class 'bool'>\n\n\n\n\nAssignment Statement And Operators\n\nAssignment Statement\nThe assignment can be done for a single variable, or for a tuple of variables separated by commas:\nm = 5 + 7\n\nx, y = 10, 20\n\na, b, c, d = 5, 'Text', None, x==y\nQ: Try these assignments and print the values of the variables.\n\nm = 5 + 7\nx, y = 10, 20\na, b, c, d = 5, 'Text', None, x==y\n\nprint(m, x, y, a, b, c, d,)\n\n12 10 20 5 Text None False\n\n\n\n\nOperators\nMost usual operators are available:\n+ , - , * , ** , / , // , %\n== , != , <> , > , >= , < , <=\nand , or , not\nQ: Try them and comment on their behaviour. Observe in particular what happens when you add an integer and a float.\n\nx = 3 + 5.\nprint(x, type(x))\n\n8.0 <class 'float'>\n\n\nQ: Notice how integer division is handled by python 3 by dividing an integer by either an integer or a float:\n\nprint(5/2)\nprint(5/2.)\n\n2.5\n2.5\n\n\n\n\n\nStrings\nA string in Python can be surrounded by either single or double quotes (no difference as long as they match). Three double quotes allow to print new lines directly (equivalent of \\n in C).\nQ: Use the function print() to see the results of the following statements:\na = 'abc'\n\nb = \"def\"\n\nc = \"\"\"aaa\nbbb\nccc\"\"\"\n\nd = \"xxx'yyy\"\n\ne = 'mmm\"nnn'\n\nf = \"aaa\\nbbb\\nccc\"\n\na = 'abc'\nb = \"def\"\nc = \"\"\"aaa\nbbb\nccc\"\"\"\nd = \"xxx'yyy\"\ne = 'mmm\"nnn'\nf = \"aaa\\nbbb\\nccc\"\n\nprint(a)\nprint(b)\nprint(c)\nprint(d)\nprint(e)\nprint(f)\n\nabc\ndef\naaa\nbbb\nccc\nxxx'yyy\nmmm\"nnn\naaa\nbbb\nccc\n\n\n\n\nLists\nPython knows a number of compound data types, used to group together other values. The most versatile is the list, which can be written as a list of comma-separated values (items) between square brackets []. List items need not all to have the same type.\na = ['spam', 'eggs', 100, 1234]\nQ: Define a list of various variables and print it:\n\na = ['spam', 'eggs', 100, 1234]\n\nprint(a)\n\n['spam', 'eggs', 100, 1234]\n\n\nThe number of items in a list is available through the len() function applied to the list:\nlen(a)\nQ: Apply len() on the list, as well as on a string:\n\nprint(\"Length of the list:\", len(a))\nprint(\"Length of the word spam:\", len('spam'))\n\nLength of the list: 4\nLength of the word spam: 4\n\n\nTo access the elements of the list, indexing and slicing can be used.\n\nAs in C, indices start at 0, so a[0] is the first element of the list, a[3] is its fourth element.\nNegative indices start from the end of the list, so a[-1] is the last element, a[-2] the last but one, etc.\nSlices return a list containing a subset of elements, with the form a[start:stop], stop being excluded. a[1:3] returns the second and third elements. WHen omitted, start is 0 (a[:2] returns the two first elements) and stop is the length of the list (a[1:] has all elements of a except the first one).\n\nQ: Experiment with indexing and slicing on your list.\n\nprint(a)\nprint(\"a[0]\", a[0])\nprint(\"a[3]\", a[3])\nprint(\"a[-1]\", a[-1])\nprint(\"a[1:3]\", a[1:3])\n\n['spam', 'eggs', 100, 1234]\na[0] spam\na[3] 1234\na[-1] 1234\na[1:3] ['eggs', 100]\n\n\nCopying lists can cause some problems:\na = [1,2,3] # Initial list\n\nb = a # \"Copy\" the list by reference \n\na[0] = 9 # Change one item of the initial list\nQ: Now print a and b. What happens?\n\na = [1,2,3] # Initial list\n\nb = a # \"Copy\" the list by reference \n\na[0] = 9 # Change one item of the initial list\n\nprint('a :', a)\nprint('b :', b)\n\na : [9, 2, 3]\nb : [9, 2, 3]\n\n\nA: B = A does not make a copy of the content of A, but of its reference (pointer). So a and b both points at the same object.\nThe solution is to use the built-in copy() method of lists:\nb = a.copy()\nQ: Try it and observe the difference.\n\na = [1, 2, 3]\nb = a.copy()\na[0] = 9\n\nprint(a)\nprint(b)\n\n[9, 2, 3]\n[1, 2, 3]\n\n\nLists are objects, with a lot of different built-in methods (type help(list) in the interpreter or in a cell):\n\na.append(x): Add an item to the end of the list.\na.extend(L): Extend the list by appending all the items in the given list.\na.insert(i, x): Insert an item at a given position.\na.remove(x): Remove the first item from the list whose value is x.\na.pop(i): Remove the item at the given position in the list, and return it.\na.index(x): Return the index in the list of the first item whose value is x.\na.count(x): Return the number of times x appears in the list.\na.sort(): Sort the items of the list, in place.\na.reverse(): Reverse the elements of the list, in place.\n\nQ: Try out quickly these methods, in particular append() which we will use quite often.\n\na = [1, 2, 3]\n\na.append(4)\n\nprint(a)\n\n[1, 2, 3, 4]\n\n\n\n\nDictionaries\nAnother useful data type built into Python is the dictionary. Unlike lists, which are indexed by a range of numbers from 0 to length -1, dictionaries are indexed by keys, which can be any immutable type; strings and numbers can always be keys.\nDictionaries can be defined by curly braces {} instead of square brackets. The content is defined by key:item pairs, the item can be of any type:\ntel = {\n    'jack': 4098, \n    'sape': 4139\n}\nTo retrieve an item, simply use the key:\ntel_jack = tel['jack']\nTo add an entry to the dictionary, just use the key and assign a value to the item. It automatically extends the dictionary (warning, it can be dangerous!).\ntel['guido'] = 4127\nQ: Create a dictionary and elements to it.\n\ntel = {'jack': 4098, 'sape': 4139}\ntel_jack = tel['jack']\ntel['guido'] = 4127\n\nprint(tel)\nprint(tel_jack)\n\n{'jack': 4098, 'sape': 4139, 'guido': 4127}\n4098\n\n\nThe keys() method of a dictionary object returns a list of all the keys used in the dictionary, in the order in which you added the keys (if you want it sorted, just apply the sorted() function on it).\na = tel.keys()\nb = sorted(tel.keys())\nvalues() does the same for the value of the items:\nc = tel.values()\nQ: Do it on your dictionary.\n\na = tel.keys()\nb = sorted(a)\nc = tel.values()\n\nprint(a)\nprint(b)\nprint(c)\n\ndict_keys(['jack', 'sape', 'guido'])\n['guido', 'jack', 'sape']\ndict_values([4098, 4139, 4127])\n\n\n\n\nIf Statement\nPerhaps the most well-known conditional statement type is the if statement. For example:\nif x < 0:\n    print('x =', x, 'is negative')\nelif x == 0:\n    print('x =', x, 'is zero')\nelse:\n    print('x =', x, 'is positive')\nQ: Give a value to the variable x and see what this statement does.\n\nx = 5\n\nif x < 0 :\n    print('x =', x, 'is negative')\nelif x == 0:\n    print('x =', x, 'is zero')\nelse:\n    print('x =', x, 'is positive')\n\nx = 5 is positive\n\n\nImportant! The main particularity of the Python syntax is that the scope of the different structures (functions, for, if, while, etc…) is defined by the indentation, not by curly braces {}. As long as the code stays at the same level, it is in the same scope:\nif x < 0 :\n    print('x =', x, 'is negative')\n    x = -x\n    print('x =', x, 'is now positive')\nelif x == 0:\n    print('x =', x, 'is zero')\nelse:\n    print('x =', x, 'is positive')\nA reasonable choice is to use four spaces for the indentation instead of tabs (configure your text editor if you are not using Jupyter).\nWhen the scope is terminated, you need to come back at exactly the same level of indentation. Try this misaligned structure and observe what happens:\nif x < 0 :\n    print('x =', x, 'is negative')\n elif x == 0:\n    print('x =', x, 'is zero')\n else:\n    print('x =', x, 'is positive')\nJupyter is nice enough to highlight it for you, but not all text editors do that…\n\nif x < 0 :\n    print('x =', x, 'is negative')\n elif x == 0:\n    print('x =', x, 'is zero')\n else:\n    print('x =', x, 'is positive')\n\nIndentationError: unindent does not match any outer indentation level (<tokenize>, line 3)\n\n\nIn a if statement, there can be zero or more elif parts. What to do when the condition is true should be indented. The keyword \"elif\" is a shortened form of \"else if\", and is useful to avoid excessive indentation. An if ... elif ... elif ... sequence is a substitute for the switch or case statements found in other languages.\nThe elif and else statements are optional. You can also only use the if statement alone:\na = [1, 2, 0]\nhas_zero = False\nif 0 in a:\n    has_zero = True\nNote the use of the in keyword to know if an element exists in a list.\n\n\nFor loops\nThe for statement in Python differs a bit from what you may be used to in C, Java or Pascal.\nRather than always iterating over an arithmetic progression of numbers (like in Pascal), or giving the user the ability to define both the iteration step and halting condition (as C), Python’s for statement iterates over the items of any sequence (a list or a string), in the order they appear in the sequence.\nlist_words = ['cat', 'window', 'defenestrate']\n\nfor word in list_words:\n    print(word, len(word))\nQ: Iterate over the list you created previously and print each element.\n\na = ['spam', 'eggs', 100, 1234]\n\nfor el in a:\n    print(el)\n\nspam\neggs\n100\n1234\n\n\nIf you do need to iterate over a sequence of numbers, the built-in function range() comes in handy. It generates lists containing arithmetic progressions:\nfor i in range(5):\n    print(i)\nQ: Try it.\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\nrange(N) generates a list of N number starting from 0 until N-1.\nIt is possible to specify a start value (0 by default), an end value (excluded) and even a step:\nrange(5, 10)\nrange(5, 10, 2)\nQ: Print the second and fourth elements of your list (['spam', 'eggs', 100, 1234]) using range().\n\nfor i in range(1, 4, 2):\n    print(a[i])\n\neggs\n1234\n\n\nTo iterate over all the indices of a list (0, 1, 2, etc), you can combine range() and len() as follows:\nfor idx in range(len(a)):\nThe enumerate() function allows to get at the same time the index and the content:\nfor i, val in enumerate(a):\n    print(i, val)\n\nfor i, val in enumerate(a):\n    print(i, val)\n\n0 spam\n1 eggs\n2 100\n3 1234\n\n\nTo get iteratively the keys and items of a dictionary, use the items() method of dictionary:\nfor key, val in tel.items():\nQ: Print one by one all keys and values of your dictionary.\n\ntel = {'jack': 4098, 'sape': 4139, 'guido': 4127}\n\nfor name, number in tel.items():\n    print(name, number)\n\njack 4098\nsape 4139\nguido 4127\n\n\n\n\nFunctions\nAs in most procedural languages, you can define functions. Functions are defined by the keyword def. Only the parameters of the function are specified (without type), not the return values.\nThe content of the function has to be incremented as with for loops.\nReturn values can be specified with the return keywork. It is possible to return several values at the same time, separated by commas.\ndef say_hello_to(first, second):\n    question = 'Hello, I am '+ first + '!'\n    answer = 'Hello '+ first + '! I am ' + second + '!'\n    return question, answer\nTo call that function, pass the arguments that you need and retrieve the retruned values separated by commas.\nquestion, answer = say_hello_to('Jack', 'Gill')\nQ: Test it with different names as arguments.\n\ndef say_hello_to(first, second):\n    question = 'Hello, I am '+ first + '!'\n    answer = 'Hello '+ first + '! I am ' + second + '!'\n    return question, answer\n\nquestion, answer = say_hello_to('Jack', 'Gill')\n\nprint(question)\nprint(answer)\n\nHello, I am Jack!\nHello Jack! I am Gill!\n\n\nQ: Redefine the tel dictionary {'jack': 4098, 'sape': 4139, 'guido': 4127} if needed, and create a function that returns a list of names whose number is higher than 4100.\n\ndef filter_dict(tel):\n    answer = []\n    for name, number in tel.items():\n        if number >= 4100:\n            answer.append(name)\n    return answer\n\ntel = {'jack': 4098, 'sape': 4139, 'guido': 4127}\nnames = filter_dict(tel)\nprint(names)\n\n['sape', 'guido']\n\n\nFunctions can take several arguments (with default values or not). The name of the argument can be specified during the call, so their order won’t matter.\nQ: Try these different calls to the say_hello_to() function:\nquestion, answer = say_hello_to('Jack', 'Gill')\nquestion, answer = say_hello_to(first='Jack', second='Gill')\nquestion, answer = say_hello_to(second='Jack', first='Gill')\n\nquestion, answer = say_hello_to('Jack', 'Gill')\nprint(question)\nprint(answer)\nquestion, answer = say_hello_to(first='Jack', second='Gill')\nprint(question)\nprint(answer)\nquestion, answer = say_hello_to(second='Jack', first='Gill')\nprint(question)\nprint(answer)\n\nHello, I am Jack!\nHello Jack! I am Gill!\nHello, I am Jack!\nHello Jack! I am Gill!\nHello, I am Gill!\nHello Gill! I am Jack!\n\n\nDefault values can be specified for the last arguments, for example:\ndef add (a, b=1):\n    return a + b\n\nx = add(2, 3) # returns 5\ny = add(2) # returns 3\nz = add(a=4) # returns 5\nQ: Modify say_hello_to() so that the second argument is your own name by default.\n\ndef say_hello_to(first, second=\"Julien\"):\n    question = 'Hello, I am '+ first + '!'\n    answer = 'Hello '+ first + '! I am ' + second + '!'\n    return question, answer\n\nquestion, answer = say_hello_to('Jack', 'Gill')\nprint(question)\nprint(answer)\n\nquestion, answer = say_hello_to('Jack')\nprint(question)\nprint(answer)\n\nHello, I am Jack!\nHello Jack! I am Gill!\nHello, I am Jack!\nHello Jack! I am Julien!\n\n\n\n\nClasses\nClasses are structures allowing to:\n\nStore information in an object.\nApply functions on this information.\n\nIn a nutshell:\nclass Foo:\n    \n    def __init__(self, val):\n        self.val = val\n        \n    def print(self):\n        print(self.val)\n   \n    def set_val(self, val):\n        self.val = val\n        self.print()\n        \na = Foo(42)\na.print()\nThis defines the class Foo. The first (obligatory) method of the class is the constructor __init__. This determines how the instance a will be instantiated after the call to a = Foo(42). The first argument is self, which represents the current instance of the class. We can specify other arguments to the constructor (here val), which can be processed or stored.\nHere we store val as an attribute of the class Foo with self.val. It is data that will be specific to each created object: if you create b = Foo(\"reinforcement_learning\"), the attribute self.val will have different values between the two instances. As always in Python, the type does not matter, it can be a float, a string, a numpy array, another object…\nAttributes are accessible from each object as:\nx = a.val\nYou can set its value by:\na.val = 12\nClasses can define methods that can manipulate class attributes as any regular method. The first argument must always be self. With the self object, you can access all attributes (or other methods) of the instance.\nWith our toy class, a.set_val(34) does exactly the same as a.val = 34, or a.print() is the same as print(a.val).\nFor C++/Java experts: attributes and methods are always public in Python. If you want to make an attribute private, preprend its name with an underscore, e.g. self._val. It will then not be part of the API of the class (but can be read or written publicly anyway…).\nQ: Play around with this basic class, create different objects with different attributes, print them, change them, etc.\n\nclass Foo:\n    \n    def __init__(self, val):\n        self.val = val\n    \n    def print(self):\n        print(self.val)\n    \n    def set_val(self, val):\n        self.val = val\n        self.print()\n\na = Foo(42)\na.print()\nprint(a.val)\na.set_val(32)\na.print()\n\n42\n42\n32\n32\n\n\nA major concept in object-oriented programming (OOP) is class inheritance. We will not use it much in these exercises, but let’s talk shortly about it.\nInheriting a class is creating a new class that inherits from the attributes and methods of another class (a kind of “copy” of the definition of the class). You can then add new attributes or methods, or overload existing ones.\nExample:\nclass Bar(Foo):\n    def add(self, val):\n        self.val += val\n    def print(self):\n        print(\"val =\", self.val)\nBar is a child class of Foo. It inherits all attributes and methods, including __init__, print and set_val. It creates a new method add and overloads print: the old definition of print in Foo does not exist anymore for instances of the Bar class (but does for instances of the Foo class). The constructor can also be overloaded, for example to add new arguments:\nclass Bar(Foo):\n    def __init__(self, val, val2):\n        self.val2 = val2\n        super().__init__(val)\n    def add(self, val):\n        self.val += val\n    def print(self):\n        print(\"val =\", self.val)\nsuper().__init__(val) calls the constructor of the Foo class (the “super” class of bar), so it sets the value of self.val.\nQ: Play around with inheritance to understand the concept.\n\nclass Bar(Foo):\n    def __init__(self, val, val2):\n        self.val2 = val2\n        super().__init__(val)\n    def add(self, val):\n        self.val += val\n    def print(self):\n        print(\"val =\", self.val)\n        \nb = Bar(12, 23)\nb.add(30)\nb.print()\n\nval = 42"
  },
  {
    "objectID": "exercises/1-Python-solution.html#exercise",
    "href": "exercises/1-Python-solution.html#exercise",
    "title": "Introduction To Python",
    "section": "Exercise",
    "text": "Exercise\nIn cryptography, a Caesar cipher is a very simple encryption technique in which each letter in the plain text is replaced by a letter some fixed number of positions down the alphabet. For example, with a shift of 3, A would be replaced by D, B would become E, and so on. The method is named after Julius Caesar, who used it to communicate with his generals. ROT-13 (“rotate by 13 places”) is a widely used example of a Caesar cipher where the shift is 13. In Python, the key for ROT-13 may be represented by means of the following dictionary:\n\ncode = {'a':'n', 'b':'o', 'c':'p', 'd':'q', 'e':'r', 'f':'s',\n        'g':'t', 'h':'u', 'i':'v', 'j':'w', 'k':'x', 'l':'y',\n        'm':'z', 'n':'a', 'o':'b', 'p':'c', 'q':'d', 'r':'e',\n        's':'f', 't':'g', 'u':'h', 'v':'i', 'w':'j', 'x':'k',\n        'y':'l', 'z':'m', 'A':'N', 'B':'O', 'C':'P', 'D':'Q',\n        'E':'R', 'F':'S', 'G':'T', 'H':'U', 'I':'V', 'J':'W',\n        'K':'X', 'L':'Y', 'M':'Z', 'N':'A', 'O':'B', 'P':'C',\n        'Q':'D', 'R':'E', 'S':'F', 'T':'G', 'U':'H', 'V':'I', \n        'W':'J', 'X':'K', 'Y':'L', 'Z':'M'}\n\nQ: Your task in this final exercise is to implement an encoder/decoder of ROT-13. Once you’re done, you will be able to read the following secret message:\nJnvg, jung qbrf vg unir gb qb jvgu qrrc yrneavat??\nThe idea is to write a decode() function taking the message and the code dictionary as inputs, and returning the decoded message. It should iterate over all letters of the message and replace them with the decoded letter. If the letter is not in the dictionary (e.g. punctuation), keep it as it is.\n\n# Method to decode a message\ndef decode(msg, code):\n    result = \"\"\n    for letter in msg:\n        if letter in code.keys():\n            result += code[letter]\n        else:\n            result += letter\n    return result\n\n# Message to decode\nmsg = \"Jnvg, jung qbrf vg unir gb qb jvgu qrrc yrneavat??\"\n\n# Decode the message\ndecoded = decode(msg, code)\nprint(decoded)"
  },
  {
    "objectID": "exercises/2-Numpy-solution.html",
    "href": "exercises/2-Numpy-solution.html",
    "title": "Numpy and Matplotlib",
    "section": "",
    "text": "NumPy is a linear algebra library in Python, with computationally expensive methods written in FORTRAN for speed.\n\nThe reference manual is at https://numpy.org/doc/stable/.\nA nice tutorial can be found at https://numpy.org/doc/stable/user/quickstart.html\nor: https://cs231n.github.io/python-numpy-tutorial/\nIf you already know Matlab, a comparison is at https://numpy.org/doc/stable/user/numpy-for-matlab-users.html\n\n\n\nTo import a library in Python, you only need to use the keyword import at the beginning of your script / notebook (or more exactly, before you use it).\nimport numpy\nThink of it as the equivalent of #include <numpy.h> in C/C++ (if you know Java, you will not be shocked). You can then use the functions and objects provided by the library using the namespace of the library:\nx = numpy.array([1, 2, 3])\nIf you do not want to type numpy. everytime, and if you are not afraid that numpy redefines any important function, you can also simply import every definition declared by the library in your current namespace with:\nfrom numpy import *\nand use the objects directly:\nx = array([1, 2, 3])\nHowever, it is good practice to give an alias to the library when its name is too long (numpy is still okay, but think of matplotlib…):\nimport numpy as np \nYou can then use the objects like this:\nx = np.array([1, 2, 3])\nRemember that you can get help on any NumPy function:\nhelp(np.array)\nhelp(np.ndarray.transpose)\n\nimport numpy as np\n\n\n\n\nThe basic object in NumPy is an array with d-dimensions (1D = vector, 2D = matrix, 3D or more = tensor). They can store either integers or floats, using various precisions.\nIn order to create a vector of three floats, you simply have to build an array() object by providing a list of floats as input:\nA = np.array( [ 1., 2., 3.] )\nMatrices should be initialized with a list of lists. For a 3x4 matrix of 8 bits unsigned integers, it is:\nB = np.array( [ \n    [ 1, 2, 3, 4],\n    [ 5, 6, 7, 8],\n    [ 4, 3, 2, 1] \n  ] , dtype=np.uint8)\nMost of the time, you won’t care about the type (the default floating-point precision is what you want for machine learning), but if you need it, you can always specify it with the parameter dtype={int32, uint16, float64, ...}. Note that even if you pass integers to the array (np.array( [ 1, 2, 3] )), they will be converted to floats by default.\nThe following attributes of an array can be accessed:\n\nA.shape : returns the shape of the vector (n,) or matrix (m, n).\nA.size : returns the total number of elements in the array.\nA.ndim : returns the number of dimensions of the array (vector: 1, matrix:2).\nA.dtype.name : returns the type of data stored in the array (int32, uint16, float64…).\n\nQ: Define the two arrays A and B from above and print those attributes. Modify the arrays (number of elements, type) and observe how they change.\nHint: you can print an array just like any other Python object.\n\nA = np.array( [ 1., 2., 3.] )\n\nprint(A)\nprint('Shape of A is :', A.shape)\nprint('Size of A is :', A.size)\nprint('Number of dimensions of A is :', A.ndim)\nprint('Type of elements in A is :', A.dtype.name)\n\nB = np.array( [ \n    [ 1, 2, 3, 4],\n    [ 5, 6, 7, 8],\n    [ 4, 3, 2, 1] \n] , dtype=np.uint8)\n\nprint(B)\nprint('Shape of B is :', B.shape)\nprint('Size of B is :', B.size)\nprint('Number of dimensions of B is :', B.ndim)\nprint('Type of elements in B is :', B.dtype.name)\n\n[1. 2. 3.]\nShape of A is : (3,)\nSize of A is : 3\nNumber of dimensions of A is : 1\nType of elements in A is : float64\n[[1 2 3 4]\n [5 6 7 8]\n [4 3 2 1]]\nShape of B is : (3, 4)\nSize of B is : 12\nNumber of dimensions of B is : 2\nType of elements in B is : uint8\n\n\nInternally, the values are stored sequentially as a vector, even if your array has more than one dimension. The apparent shape is just used for mathematical operations. You can reshape a matrix very easily with the reshape() method:\nB = np.array( [ \n    [ 1, 2, 3, 4],\n    [ 5, 6, 7, 8],\n    [ 4, 3, 2, 1] \n]) # B has 3 rows, 4 columns\n\nC = B.reshape((6, 2)) # C has 6 rows, 2 columns\nThe only thing to respect is that the total number of elements must be the same. Beware also of the order in which the elements will be put.\nQ: Create a vector with 8 elements and reshape it into a 2x4 matrix.\n\nB = np.array( [1, 2, 3, 4, 5, 6, 7, 8])\n\nC = B.reshape((2, 4))\nprint(C)\n\n[[1 2 3 4]\n [5 6 7 8]]\n\n\n\n\n\nProviding a list of values to array() would be tedious for large arrays. Numpy offers constructors that allow to construct simply most vectors or matrices.\nnp.zeros(shape) creates an array of shape shape filled with zeros. Note: if you give a single integer for the shape, it will be interpreted as a vector of shape (d,).\nnp.ones(shape) creates an array of shape shape filled with ones.\nnp.full(shape, val) creates an array of shape shape filled with val.\nnp.eye(n) creates a diagonal matrix of shape (n, n).\nnp.arange(a, b) creates a vector of integers whose value linearly increase from a to b (excluded).\nnp.linspace(a, b, n) creates a vector of n values evenly distributed between a and b (included).\nQ: Create and print:\n\na 2x3 matrix filled with zeros.\na vector of 12 elements initialized to 3.14.\na vector of 11 elements whose value linearly increases from 0.0 to 10.0.\na vector of 11 elements whose value linearly increases from 10 to 20.\n\n\nA = np.zeros((2,3))  \nB = np.full(12, 3.14)  # 3.14 * np.ones(12) would also work\nC = np.linspace(0.0, 10.0, 11) \nD = np.arange(10, 21)\n\n\n\n\nIn many cases, it is useful to initialize a vector or matrix with random values. Random number generators (rng) allows to draw numbers from any probability distribution (uniform, normal, etc.) using pseudo-random methods.\nIn numpy versions before 1.16, the numpy.random module had direct methods allowing to initialize arrays:\nA = np.random.uniform(-1.0, 1.0, (10, 10)) # a 10x10 matrix with values uniformly taken between -1 and 1\nSince numpy 1.16, this method has been deprecated in favor of a more explicit initialization of the underlying rng:\nrng = np.random.default_rng()\nA = rng.uniform(-1.0, 1.0, (10, 10))\nThe advantages of this new method (reproducibility, parallel seeds) will not matter for these exercises, but let’s take good habits already.\nThe generator has many built-in methods, covering virtually any useful probability distribution. Read the documentation of the random generator:\nhttps://numpy.org/doc/stable/reference/random/generator.html\nQ: Create:\n\nA vector of 20 elements following a normal distribution with mean 2.0 and standard devation 3.0.\nA 10x10 matrix whose elements come from the exponential distribution with \\beta = 2.\nA vector of 10 integers randomly chosen between 1 and 100 (hint: involves arange and rng.choice).\n\n\nrng = np.random.default_rng()\nA = rng.normal(2.0, 3.0, 20)\nB = rng.exponential(2.0, (10, 10))\nC = rng.choice(np.arange(1, 101), 10)\n\n\n\n\nTo access a particular element of a matrix, you can use the usual Python list style (the first element has a rank of 0), once per dimension:\nA = np.array(\n    [ \n        [ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]\n    ]\n)\n\nx = A[0, 2] # The element on the first row and third column\nFor matrices, the first index represents the rows, the second the columns. [0, 2] represents the element at the first row, third column.\nQ: Define this matrix and replace the element 12 by a zero using indices:\n\nA = np.array(\n    [ \n        [ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]\n    ]\n)\n\nA[2, 3] = 0.\nprint(A)\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11  0]]\n\n\nIt is possible to access complete row or columns of a matrix using slices. The : symbol is a shortcut for “everything”:\nb = A[:, 2] # third column\nc = A[0, :] # first row\nQ: Set the fourth column of A to 1.\n\nA[:, 3] = 1.\n\nprint(A)\n\n[[ 1  2  3  1]\n [ 5  6  7  1]\n [ 9 10 11  1]]\n\n\nAs for python lists, you can specify a range start:stop to get only a subset of a row/column (beware, stop is excluded):\nd = A[0, 1:3] # second and third elements of the first row\ne = A[1, :2] # first and second elements of the second row\nYou can use boolean arrays to retrieve indices:\nA = np.array( \n    [ [ -2,  2,  1, -4],\n      [  3, -1, -5, -3] ])\n\nnegatives = A < 0 # Boolean array where each element is True when the condition is met.\nA[negatives] = 0 # All negative elements of A (where the boolean matrix is True) will be set to 0\nA simpler way to write it is:\nA[A < 0] = 0\nQ: print A, negatives and A again after the assignment:\n\nA = np.array( \n    [ [ -2,  2,  1, -4],\n      [  3, -1, -5, -3] ])\nnegatives = A < 0\nA[negatives] = 0\n\nprint(negatives)\nprint(A)\n\n[[ True False False  True]\n [False  True  True  True]]\n[[0 2 1 0]\n [3 0 0 0]]\n\n\n\n\n\nLet’s first define some matrices:\n\nA = np.array( [ [ 1, 2, 3, 4],\n                [ 5, 6, 7, 8] ])\n\nB = np.array( [ [ 1, 2],\n                [ 3, 4],\n                [ 5, 6],\n                [ 7, 8] ])\n\nC = np.array( [ [  1,  2,  3,  4],\n                [  5,  6,  7,  8],\n                [  9,  0,  1,  1],\n                [ 13,  7,  2,  6] ])\n\n\n\nA matrix can be transposed with the transpose() method or the .T shortcut:\nD = A.transpose() \nE = A.T # equivalent\nQ: Try it:\n\nD = A.transpose() \nE = A.T\n\nprint(A)\nprint(D)\n\n[[1 2 3 4]\n [5 6 7 8]]\n[[1 5]\n [2 6]\n [3 7]\n [4 8]]\n\n\ntranspose() does not change A, it only returns a transposed copy. To transpose A definitely, you have to use the assigment A = A.T\n\n\n\nThere are two manners to multiply matrices:\n\nelement-wise: Two arrays of exactly the same shape can be multiplied element-wise by using the * operator:\n\nD = A * B\n\nalgebrically: To perform a matrix multiplication, you have to use the dot() method. Beware: the dimensions must match! (m, n) * (n, p) = (m, p)\n\nE = np.dot(A,  B)\nQ: Use the matrices A and B previously defined and multiply them element-wise and algebrically. You may have to transpose one of them.\n\nD = A * B.T\nE = A.T * B\n\nprint(D)\nprint(E)\n\nF = np.dot(A, B)\nG = np.dot(B, A)\n\nprint(F)\nprint(G)\n\n[[ 1  6 15 28]\n [10 24 42 64]]\n[[ 1 10]\n [ 6 24]\n [15 42]\n [28 64]]\n[[ 50  60]\n [114 140]]\n[[11 14 17 20]\n [23 30 37 44]\n [35 46 57 68]\n [47 62 77 92]]\n\n\n\n\n\n* and np.dot also apply on matrix-vector multiplications \\mathbf{y} = A \\times \\mathbf{x} or vector-vector multiplications.\nQ: Define a vector \\mathbf{x} with four elements and multiply it with the matrix A using * and np.dot. What do you obtain? Try the same by multiplying the vector \\mathbf{x} and itself.\n\nx = np.array([1, 2, 3, 4])\n\ny = np.dot(A, x)\nz = A*x\n\np = x*x\nq = np.dot(x, x)\n\nprint(y)\nprint(z)\nprint(p)\nprint(q)\n\n[30 70]\n[[ 1  4  9 16]\n [ 5 12 21 32]]\n[ 1  4  9 16]\n30\n\n\nA: the element-wise multiplies each column of the matrix by the corresponding element of the vector. np.dot works as expected. The same happens for vector-vector multiplications: element-wise for *, dot-product for np.dot (hence the name of the method).\n\n\n\nInverting a Matrix (when possible) can be done using the inv() method whitch is defined in the linalg submodule of NumPy.\ninv_C = np.linalg.inv(C)\nQ:\n\nInvert C and print the result.\nMultiply C with its inverse and print the result. What do observe? Why is Numpy called a numerical computation library?\n\n\ninv_C = np.linalg.inv(C)\n\nprint(inv_C)\nprint(np.dot(C,inv_C))\n\n[[-0.0467033   0.00274725  0.0989011   0.01098901]\n [-0.62362637  0.27197802 -0.20879121  0.08791209]\n [-0.61263736  0.4478022   0.12087912 -0.20879121]\n [ 1.03296703 -0.47252747 -0.01098901  0.10989011]]\n[[ 1.00000000e+00  0.00000000e+00 -8.32667268e-17  5.55111512e-17]\n [ 0.00000000e+00  1.00000000e+00 -2.77555756e-17 -1.11022302e-16]\n [ 2.22044605e-16 -1.11022302e-16  1.00000000e+00 -8.32667268e-17]\n [ 0.00000000e+00 -2.22044605e-16  1.11022302e-16  1.00000000e+00]]\n\n\nA: Some elements which should be 0 have a very small value. This is due to numerical precision issues. Numpy does not make symbolic computations like Mathematica or sympy, it deals with numbers up to a certain precision.\n\n\n\n\nOne can sum the elements of a matrix globally, row-wise or column-wise:\n# Globally\nS1 = np.sum(A)\n\n# Per column\nS2 = np.sum(A, axis=0) \n\n# Per row\nS3 = np.sum(A, axis=1) \nQ: Try them:\n\n# Globally\nS1 = np.sum(A)\n\n# Per column\nS2 = np.sum(A, axis=0) \n\n# Per row\nS3 = np.sum(A, axis=1) \n\nprint(A)\nprint(S1)\nprint(S2)\nprint(S3)\n\n[[1 2 3 4]\n [5 6 7 8]]\n36\n[ 6  8 10 12]\n[10 26]\n\n\nYou also have access to the minimum (np.min()), maximum (np.max()), mean (np.mean()) of an array, also per row/column.\nQ: Try them out:\n\nprint(np.min(A))\nprint(np.max(A))\nprint(np.mean(A))\n\nprint(np.min(A, axis=0))\nprint(np.max(A, axis=0))\nprint(np.mean(A, axis=0))\n\nprint(np.min(A, axis=1))\nprint(np.max(A, axis=1))\nprint(np.mean(A, axis=1))\n\n1\n8\n4.5\n[1 2 3 4]\n[5 6 7 8]\n[3. 4. 5. 6.]\n[1 5]\n[4 8]\n[2.5 6.5]\n\n\n\n\n\nYou can apply any usual mathematical operations (cos, sin, exp, etc…) on each element of a matrix (element-wise):\nD = np.exp(A)\nE = np.cos(A)\nF = np.log(A)\nG = (A+3) * np.cos(A-2)\nQ: Try it.\n\nD = np.exp(A)\nE = np.cos(A)\nF = np.log(A)\nG = (A+3) * np.cos(A-2)\n\nprint(D)\nprint(E)\nprint(F)\nprint(G)\n\n[[2.71828183e+00 7.38905610e+00 2.00855369e+01 5.45981500e+01]\n [1.48413159e+02 4.03428793e+02 1.09663316e+03 2.98095799e+03]]\n[[ 0.54030231 -0.41614684 -0.9899925  -0.65364362]\n [ 0.28366219  0.96017029  0.75390225 -0.14550003]]\n[[0.         0.69314718 1.09861229 1.38629436]\n [1.60943791 1.79175947 1.94591015 2.07944154]]\n[[ 2.16120922  5.          3.24181384 -2.91302786]\n [-7.91993997 -5.88279259  2.83662185 10.56187315]]"
  },
  {
    "objectID": "exercises/2-Numpy-solution.html#matplotlib",
    "href": "exercises/2-Numpy-solution.html#matplotlib",
    "title": "Numpy and Matplotlib",
    "section": "Matplotlib",
    "text": "Matplotlib\nMatplotlib is a python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.\n\nReference: http://matplotlib.org\nTutorial by N. Rougier: http://www.labri.fr/perso/nrougier/teaching/matplotlib\n\nThis is the default historical visualization library in Python, which anybody should know, but not the nicest. If you are interested in having better visualizations, have a look at:\n\nseaborn https://seaborn.pydata.org/\nggplot2 https://ggplot2.tidyverse.org/\nbokeh https://docs.bokeh.org/\nplotly https://plotly.com/python/\n\nWe will nevertheless stick to matplotlib in these exercises.\nThe pyplot module is the most famous, as it has a similar interface to Matlab. It is customary to use the plt namescape for it:\n\nimport matplotlib.pyplot as plt\n\n\nplt.plot()\nThe plt.plot() command allows to make simple line drawings:\nx = np.linspace(0., 10., 100)\ny = x**2 + 1.\n\nplt.figure()\nplt.plot(x, y)\nplt.show()\n\nx = np.linspace(0., 10., 100)\ny = x**2 + 1.\n\nplt.figure()\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n\n\n\nplot() takes two vectors x and y as inputs (they must have the same size) and plots them against each other. It is standard to define the x-axis with np.linspace() if you just want to plot a function. 100 points is usually a good choice, but you can experiments with less points.\nThe call to plt.show() is obligatory at the end to display the window when using a script (very common mistake to forget it!). It is not needed in Jupyter notebooks as it is implicitly called, but let’s take the habit anyway.\nThe call to plt.figure() is also optional, as a new figure is created when you call plt.plot() for the first time.\nQ: Create a third vector z (e.g. z = -x**2 + 2) and plot it against x right after y (i.e. between plt.plot(x, y) and plt.show()). What happens?\n\nx = np.linspace(0., 10., 100)\ny = x**2 + 1.\nz = -x**2 + 2.\n\nplt.figure()\nplt.plot(x, y)\nplt.plot(x, z)\nplt.show()\n\n\n\n\n\n\n\n\nQ: Now call plt.figure() again between the two plots. What happens?\n\nx = np.linspace(0., 10., 100)\ny = x**2 + 1.\nz = -x**2 + 2.\n\nplt.figure()\nplt.plot(x, y)\nplt.figure()\nplt.plot(x, z)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBy default, the plot is quite empty. This is fine when experimenting in a notebook, but not when incorporating the figures in your thesis. You can make a plot look better by adding a title, labels on the axes, etc.\nplt.title('My title')\nplt.xlabel('x-axis')\nplt.ylabel('y-axis')\nQ: Make the previous plots nicer by adding legends and axes.\nHint: if you know LateX equations, you can insert simple formulas in the title or axes by using two dollar signs $$.\n\nx = np.linspace(0., 10., 100)\ny = x**2 + 1.\nz = -x**2 + 2.\n\nplt.figure()\nplt.plot(x, y)\nplt.title(\"Function $x^2 + 1$\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\nplt.figure()\nplt.plot(x, z)\nplt.title(\"Function $-x^2 + 2$\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you make multiple plots on the same figure by calling plt.plot() multiple times, you can add a label to each plot to create a legend with plt.legend():\nplt.plot(x, y, label='y')\nplt.plot(x, z, label='z')\nplt.legend()\n\nx = np.linspace(0., 10., 100)\ny = x**2 + 1.\nz = -x**2 + 2.\n\nplt.figure()\nplt.plot(x, y, label=\"$x^2 + 1$\")\nplt.plot(x, z, label=\"$-x^2 + 2$\")\nplt.title(\"Functions $x^2 + 1$ and $-x^2 + 2$\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAnother advantage of declaring a figure is that you can modify its size (which is very small in a notebook by default) with the figsize argument in inches:\nplt.figure(figsize=(16, 10))\nQ: Experiment with figure sizes.\n\nx = np.linspace(0., 10., 100)\ny = x**2 + 1.\nz = -x**2 + 2.\n\nplt.figure(figsize=(12, 10))\nplt.plot(x, y, label=\"$x^2 + 1$\")\nplt.plot(x, z, label=\"$-x^2 + 2$\")\nplt.title(\"Functions $x^2 + 1$ and $-x^2 + 2$\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSide-by-side plots\nTo make separate plots in the same figure, you can use plt.subplot(abc).\nThe function takes three digits a, b, c as input (e.g. 221 or 122) where:\n\na is the number of rows.\nb is the number of columns.\nc is the index (starting at 1) of the current subplot.\n\nHere is a dummy example of a 2x2 grid of plots:\nplt.subplot(221)\nplt.plot(x, y)\n\nplt.subplot(222)\nplt.plot(x, z)\n\nplt.subplot(223)\nplt.plot(y, x)\n\nplt.subplot(224)\nplt.plot(z, x)\nQ: Try it.\n\nx = np.linspace(0., 10., 100)\ny = x**2 + 1.\nz = -x**2 + 2.\n\nplt.figure(figsize=(12, 10))\nplt.subplot(221)\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\n\nplt.subplot(222)\nplt.plot(x, z)\nplt.xlabel('x')\nplt.ylabel('z')\n\nplt.subplot(223)\nplt.plot(y, x)\nplt.xlabel('y')\nplt.ylabel('x')\n\nplt.subplot(224)\nplt.plot(z, x)\nplt.xlabel('z')\nplt.ylabel('x')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nplt.imshow\nMatrices can be displayed using plt.imshow(). You can choose the color code with the cmap argument (e.g. gray or hot).\nplt.imshow(A, cmap=plt.cm.hot, interpolation='nearest')\nplt.colorbar()\nplt.colorbar() allows to show a vertical bar indicating the color code.\nThe interpolation method can also be selected for small matrices ('nearest by default, but you can choose interpolation=\"bicubic\" for a smoother display).\n(0, 0) is at the top-left of the image, the first axis is vertical. Change it with the origin parameter.\nQ: Create a 10x10 matrix (e.g. randomly) and plot it. Try different color maps (https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html and interpolation methods.\n\nA = rng.uniform(0., 1., (10, 10))\n\nplt.figure()\nplt.imshow(A, cmap=plt.cm.hot, interpolation='nearest')\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nplt.scatter\nIf you want to display dots instead of of lines or pixels, plt.scatter takes two vectors of same size and plots them against each other:\nplt.scatter(x, y)\nQ: Create two vectors with 100 elements and make a scatter plot.\n\nx = rng.uniform(0., 1., 100)\ny = rng.uniform(0., 1., 100)\n\nplt.figure()\nplt.scatter(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nplt.hist()\nHistograms can be useful to visualize the distribution of some data. If z is a vector of values, the histogram is simply:\nplt.hist(z, bins=20)\nThe number of bins is 10 by default, but you can of course change it.\nQ: Draw 1000 values from a normal distribution of your choice and make an histogram.\n\nz = rng.normal(10., 2.0, 1000)\n\nplt.figure()\nplt.hist(z, bins=20)\nplt.show()"
  },
  {
    "objectID": "exercises/3-Sampling-solution.html",
    "href": "exercises/3-Sampling-solution.html",
    "title": "Sampling",
    "section": "",
    "text": "In this first exercise, we will investigate how to evaluate the Q-value of each action available in a 5-armed bandit. It is mostly to give you intuition about the limits of sampling and the central limit theorem.\nLet’s start with importing numpy and matplotlib:"
  },
  {
    "objectID": "exercises/3-Sampling-solution.html#sampling-a-n-armed-bandit",
    "href": "exercises/3-Sampling-solution.html#sampling-a-n-armed-bandit",
    "title": "Sampling",
    "section": "Sampling a n-armed bandit",
    "text": "Sampling a n-armed bandit\nLet’s now create the n-armed bandit. The only thing we need to do is to randomly choose 5 true Q-values Q^*(a).\n\n\n\n\n\nTo be generic, let’s define nb_actions=5 and create an array corresponding to the index of each action (0, 1, 2, 3, 4) for plotting purpose.\n\nnb_actions = 5\nactions = np.arange(nb_actions)\n\nQ: Create a numpy array Q_star with nb_actions values, normally distributed with a mean of 0 and standard deviation of 1 (as in the lecture).\n\nrng = np.random.default_rng()\nQ_star = rng.normal(0, 1, nb_actions)\n\nQ: Plot the Q-values. Identify the optimal action a^*.\nTip: you could plot the array Q_star with plt.plot, but that would be ugly. Check the documentation of the plt.bar method.\n\nprint(\"Optimal action:\", Q_star.argmax())\n\nplt.figure(figsize=(10, 6))\nplt.bar(actions, Q_star)\nplt.xlabel('Actions')\nplt.ylabel('$Q^*(a)$')\nplt.show()\n\nOptimal action: 0\n\n\n\n\n\n\n\n\n\nGreat, now let’s start evaluating these Q-values with random sampling.\nQ: Define an action sampling method get_reward taking as arguments: * The array Q_star. * The index a of the action you want to sample (between 0 and 4). * An optional variance argument var, which should have the value 1.0 by default.\nIt should return a single value, sampled from the normal distribution with mean Q_star[a] and variance var.\n\ndef get_reward(Q_star, a, var=1.0):\n    return float(rng.normal(Q_star[a], var, 1))\n\nQ: For each possible action a, take nb_samples=10 out of the reward distribution and store them in a numpy array. Compute the mean of the samples for each action separately in a new array Q_t. Make a bar plot of these estimated Q-values.\n\nnb_samples = 10\nrewards = np.zeros((nb_actions, nb_samples))\n\nfor a in actions:\n    for play in range(nb_samples):\n        rewards[a, play] = get_reward(Q_star, a, var=1.0)\n\nQ_t = np.mean(rewards, axis=1)\n    \nplt.figure(figsize=(10, 6))\nplt.bar(actions, Q_t)\nplt.xlabel('Actions')\nplt.ylabel('$Q_t(a)$')\nplt.show()\n\n\n\n\n\n\n\n\nQ: Make a bar plot of the difference between the true values Q_star and the estimates Q_t. Conclude. Re-run the sampling cell with different numbers of samples.\n\nplt.figure(figsize=(10, 6))\nplt.bar(actions, Q_t - Q_star)\nplt.xlabel('Actions')\nplt.ylabel('$Q^*(a) - Q_t(a)$')\nplt.show()\n\n\n\n\n\n\n\n\nQ: To better understand the influence of the number of samples on the accuracy of the sample average, create a for loop over the preceding code, with a number of samples increasing from 1 to 100. For each value, compute the mean square error (mse) between the estimates Q_t and the true values Q^*.\nThe mean square error is simply defined over the N = nb_actions actions as:\n\\epsilon = \\frac{1}{N} \\, \\sum_{a=0}^{N-1} (Q_t(a) - Q^*(a))^2\nAt the end of the loop, plot the evolution of the mean square error with the number of samples. You can append each value of the mse in an empty list and then plot it with plt.plot, for example.\n\nerrors = []\nfor nb_sample in range(1, 100):\n    \n    rewards = np.zeros((nb_actions, nb_sample))\n\n    for a in actions:\n        for play in range(nb_sample):\n            rewards[a, play] = get_reward(Q_star, a, var=1.0)\n\n    Q_t = np.mean(rewards, axis=1)\n    error = np.mean((Q_star - Q_t)**2)\n    errors.append(error)\n    \nplt.figure(figsize=(10, 6))\nplt.plot(errors)\nplt.show()\n\n\n\n\n\n\n\n\nThe plot should give you an indication of how many samples you at least need to correctly estimate each action (30 or so). But according to the central limit theorem (CLT), the variance of the sample average also varies with the variance of the distribution itself.\n\nThe distribution of sample averages is normally distributed with mean \\mu and variance \\frac{\\sigma^2}{N}.\n\nS_N \\sim \\mathcal{N}(\\mu, \\frac{\\sigma}{\\sqrt{N}})\nQ: Vary the variance of the reward distribution (as an argument to get_reward) and re-run the previous experiment. Do not hesitate to take more samples. Conclude.\n\nerrors = []\nfor nb_sample in range(1, 1000):\n    \n    rewards = np.zeros((nb_actions, nb_sample))\n\n    for a in actions:\n        for play in range(nb_sample):\n            rewards[a, play] = get_reward(Q_star, a, var=10.0)\n\n    Q_t = np.mean(rewards, axis=1)\n    error = np.mean((Q_star - Q_t)**2)\n    errors.append(error)\n    \nprint(error)\nplt.figure(figsize=(10, 6))\nplt.plot(errors)\nplt.show()\n\n0.18921383192673544\n\n\n\n\n\n\n\n\n\nA: the higher the variance of the distribution, the more samples we need to get correct estimates."
  },
  {
    "objectID": "exercises/3-Sampling-solution.html#bandit-environment",
    "href": "exercises/3-Sampling-solution.html#bandit-environment",
    "title": "Sampling",
    "section": "Bandit environment",
    "text": "Bandit environment\nIn order to prepare the next exercise, let’s now implement the n-armed bandit in a Python class. As reminded in the tutorial on Python, a class is defined using this structure:\nclass MyClass:\n    \"\"\"\n    Documentation of the class.\n    \"\"\"\n    def __init__(self, param1, param2):\n        \"\"\"\n        Constructor of the class.\n        \n        :param param1: first parameter.\n        :param param2: second parameter.\n        \"\"\"\n        self.param1 = param1\n        self.param2 = param2\n        \n    def method(self, another_param):\n        \"\"\"\n        Method to do something.\n        \n        :param another_param: another parameter.\n        \"\"\"\n        return (another_param + self.param1)/self.param2\nYou can then create an object of the type MyClass:\nmy_object = MyClass(param1= 1.0, param2=2.0)\nand call any method of the class on the object:\nresult = my_object.method(3.0)\nQ: Create a Bandit class taking as arguments:\n\nnb_actions: number of arms.\nmean: mean of the normal distribution for Q^*.\nstd_Q: standard deviation of the normal distribution for Q^*.\nstd_r: standard deviation of the normal distribution for the sampled rewards.\n\nThe constructor should initialize a Q_star array accordingly and store it as an attribute. It should also store the optimal action.\nAdd a method step(action) that samples a reward for a particular action and returns it.\n\nclass Bandit:\n    \"\"\"\n    n-armed bandit.\n    \"\"\"\n    def __init__(self, nb_actions, mean=0.0, std_Q=1.0, std_r=1.0):\n        \"\"\"\n        :param nb_actions: number of arms.\n        :param mean: mean of the normal distribution for $Q^*$.\n        :param std_Q: standard deviation of the normal distribution for $Q^*$.\n        :param std_r: standard deviation of the normal distribution for the sampled rewards.\n        \"\"\"\n        # Store parameters\n        self.nb_actions = nb_actions\n        self.mean = mean\n        self.std_Q = std_Q\n        self.std_r = std_r\n        \n        # Initialize the true Q-values\n        self.Q_star = rng.normal(self.mean, self.std_Q, self.nb_actions)\n        \n        # Optimal action\n        self.a_star = self.Q_star.argmax()\n        \n    def step(self, action):\n        \"\"\"\n        Sampled a single reward from the bandit.\n        \n        :param action: the selected action.\n        :return: a reward.\n        \"\"\"\n        return float(rng.normal(self.Q_star[action], self.std_r, 1))\n\nQ: Create a 5-armed bandits and sample each action multiple times. Compare the mean reward to the ground truth as before.\n\nnb_actions = 5\nbandit = Bandit(nb_actions)\n\nall_rewards = []\nfor t in range(1000):\n    rewards = []\n    for a in range(nb_actions):\n        rewards.append(bandit.step(a))\n    all_rewards.append(rewards)\n    \nmean_reward = np.mean(all_rewards, axis=0)\n\nplt.figure(figsize=(20, 6))\nplt.subplot(131)\nplt.bar(range(nb_actions), bandit.Q_star)\nplt.xlabel(\"Actions\")\nplt.ylabel(\"$Q^*(a)$\")\nplt.subplot(132)\nplt.bar(range(nb_actions), mean_reward)\nplt.xlabel(\"Actions\")\nplt.ylabel(\"$Q_t(a)$\")\nplt.subplot(133)\nplt.bar(range(nb_actions), np.abs(bandit.Q_star - mean_reward))\nplt.xlabel(\"Actions\")\nplt.ylabel(\"Absolute error\")\nplt.show()"
  },
  {
    "objectID": "exercises/4-Bandits-solution.html",
    "href": "exercises/4-Bandits-solution.html",
    "title": "Bandits",
    "section": "",
    "text": "In this part, we will investigate the properties of the action selection schemes seen in the lecture and compare their properties:\nLet’s re-use the definitions of the last exercise:"
  },
  {
    "objectID": "exercises/4-Bandits-solution.html#greedy-action-selection",
    "href": "exercises/4-Bandits-solution.html#greedy-action-selection",
    "title": "Bandits",
    "section": "Greedy action selection",
    "text": "Greedy action selection\nIn greedy action selection, we systematically chose the action with the highest estimated Q-value at each play (or randomly when there are ties):\na_t = \\text{argmax}_a Q_t(a)\nWe maintain estimates Q_t of the action values (initialized to 0) using the online formula:\nQ_{t+1}(a_t) = Q_t(a_t) + \\alpha \\, (r_{t} - Q_t(a_t))\nwhen receiving the sampled reward r_t after taking the action a_t. The learning rate \\alpha can be set to 0.1 at first.\nThe algorithm simply alternates between these two steps for 1000 plays (or steps): take an action, update its Q-value.\nQ: Implement the greedy algorithm on the 5-armed bandit.\nYour algorithm will look like this:\n\nCreate a 5-armed bandit (mean of zero, variance of 1).\nInitialize the estimated Q-values to 0 with an array of the same size as the bandit.\nfor 1000 plays:\n\nSelect the greedy action a_t^* using the current estimates.\nSample a reward from \\mathcal{N}(Q^*(a_t^*), 1).\nUpdate the estimated Q-value of the action taken.\n\n\nAdditionally, you will store the received rewards at each step in an initially empty list or a numpy array of the correct size and plot it in the end. You will also plot the true Q-values and the estimated Q-values at the end of the 1000 plays.\nTip: to implement the argmax, do not rely on np.argmax(). If there are ties in the array, for example at the beginning:\nx = np.array([0, 0, 0, 0, 0])\nx.argmax() will return you the first occurrence of the maximum 0.0 of the array. In this case it will be the index 0, so you will always select the action 0 first.\nIt is much more efficient to retrieve the indices of all maxima and randomly select one of them:\na = rng.choice(np.where(x == x.max())[0])\nnp.where(x == x.max()) returns a list of indices where x is maximum. rng.choice() randomly selects one of them.\n\n# Learning rate\nalpha = 0.1\n\n# Bandit\nbandit = Bandit(nb_actions)\n\n# Estimates\nQ_t = np.zeros(nb_actions)\n\n# Store the rewards after each step\nrewards = []\n\n# For 1000 plays\nfor step in range(1000):\n    \n    # Select the action greedily w.r.t Q_t\n    action = rng.choice(np.where(Q_t == Q_t.max())[0])\n    \n    # Sample the reward\n    reward = bandit.step(action)\n    \n    # Store the received reward\n    rewards.append(reward)\n    \n    # Update the Q-value estimate of the action\n    Q_t[action] += alpha * (reward - Q_t[action])\n    \n# Plot the Q-values and the evolution of rewards\nplt.figure(figsize=(16, 5))\nplt.subplot(131)\nplt.bar(range(nb_actions), bandit.Q_star)\nplt.xlabel(\"Actions\")\nplt.ylabel(\"$Q^*(a)$\")\nplt.subplot(132)\nplt.bar(range(nb_actions), Q_t)\nplt.xlabel(\"Actions\")\nplt.ylabel(\"$Q_t(a)$\")\nplt.subplot(133)\nplt.plot(rewards)\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Reward\")\nplt.show()\n\n\n\n\n\n\n\n\nQ: Re-run your algorithm multiple times with different values of Q^* (simply recreate the Bandit) and observe:\n\nHow much reward you get.\nHow your estimated Q-values in the end differ from the true Q-values.\nWhether greedy action action selection finds the optimal action or not.\n\nA: The plot with rewards is very noisy, you do not really see whether you have learned something because of the randomness of the rewards. More often than not, greedy action selection finds the optimal action, or least a not-that-bad action. The estimates Q_t have however nothing to see with the true Q-values, as you quickly select the same action and never update the other ones.\nBefore going further, let’s turn the agent into a class for better reusability.\nQ: Create a GreedyAgent class taking the bandit as an argument as well as the learning rate alpha=0.1:\nbandit = Bandit(nb_actions)\n\nagent = GreedyAgent(bandit, alpha=0.1)\nThe constructor should initialize the array of estimated Q-values Q_t and store it as an attribute.\nDefine a method act(self) that returns the index of the greedy action based on the current estimates, as well as a method update(self, action, reward) that allows to update the estimated Q-value of the action given the obtained reward. Define also a train(self, nb_steps) method that implements the complete training process for nb_steps=1000 plays and returns the list of obtained rewards.\nclass GreedyAgent:\n    def __init__(self, bandit, alpha):\n        # TODO\n        \n    def act(self):      \n        action = # TODO\n        return action\n        \n    def update(self, action, reward):\n        # TODO\n        \n    def train(self, nb_steps):\n        # TODO\nRe-run the experiment using this Greedy agent.\n\nclass GreedyAgent:\n    \n    def __init__(self, bandit, alpha):\n        \n        self.bandit = bandit\n        self.alpha = alpha\n        \n        # Estimated Q-values\n        self.Q_t = np.zeros(self.bandit.nb_actions)\n        \n    def act(self):\n        \n        action = rng.choice(np.where(self.Q_t == self.Q_t.max())[0])\n        return action\n        \n    def update(self, action, reward):\n        \n        self.Q_t[action] += self.alpha * (reward - self.Q_t[action])\n    \n        \n    def train(self, nb_steps):\n        \n        rewards = []\n\n        for step in range(nb_steps):\n\n            # Select the action \n            action = self.act()\n\n            # Sample the reward\n            reward = self.bandit.step(action)\n\n            # Store the received reward\n            rewards.append(reward)\n\n            # Update the Q-value estimate of the action\n            self.update(action, reward)\n            \n        return np.array(rewards)\n\n\n# Learning rate\nalpha = 0.1\n\n# Bandit\nbandit = Bandit(nb_actions)\n\n# Estimates\nagent = GreedyAgent(bandit, alpha)\n\n# Train for 1000 plays\nrewards = agent.train(1000)\n    \n# Plot the Q-values and the evolution of rewards\nplt.figure(figsize=(16, 5))\nplt.subplot(131)\nplt.bar(range(nb_actions), bandit.Q_star)\nplt.xlabel(\"Actions\")\nplt.ylabel(\"$Q^*(a)$\")\nplt.subplot(132)\nplt.bar(range(nb_actions), agent.Q_t)\nplt.xlabel(\"Actions\")\nplt.ylabel(\"$Q_t(a)$\")\nplt.subplot(133)\nplt.plot(rewards)\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Reward\")\nplt.show()\n\n\n\n\n\n\n\n\nQ: Modify the train() method so that it also returns a list of binary values (0 and 1) indicating for each play whether the agent chose the optimal action. Plot this list and observe the lack of exploration.\nHint: the index of the optimal action is already stored in the bandit: bandit.a_star.\n\nclass GreedyAgent:\n    def __init__(self, bandit, alpha):\n        \n        self.bandit = bandit\n        self.alpha = alpha\n        \n        # Estimated Q-values\n        self.Q_t = np.zeros(self.bandit.nb_actions)\n        \n    def act(self):\n        \n        action = rng.choice(np.where(self.Q_t == self.Q_t.max())[0])\n        return action\n        \n    def update(self, action, reward):\n        \n        self.Q_t[action] += self.alpha * (reward - self.Q_t[action])\n        \n    def train(self, nb_steps):\n        \n        rewards = []\n        optimal = []\n\n        for step in range(1000):\n\n            # Select the action \n            action = self.act()\n\n            # Sample the reward\n            reward = self.bandit.step(action)\n\n            # Store the received reward\n            rewards.append(reward)\n            \n            # Optimal action\n            if action == self.bandit.a_star:\n                optimal.append(1.0)\n            else:\n                optimal.append(0.0)\n\n            # Update the Q-value estimate of the action\n            self.update(action, reward)\n            \n        return np.array(rewards), np.array(optimal)\n\n\n# Learning rate\nalpha = 0.1\n\n# Bandit\nbandit = Bandit(nb_actions)\n\n# Estimates\nagent = GreedyAgent(bandit, alpha)\n\n# Store the rewards after each step\nrewards, optimal = agent.train(1000)\n    \n# Plot the Q-values and the evolution of rewards\nplt.figure(figsize=(15, 10))\nplt.subplot(221)\nplt.bar(range(nb_actions), bandit.Q_star)\nplt.xlabel(\"Actions\")\nplt.ylabel(\"$Q^*(a)$\")\nplt.subplot(222)\nplt.bar(range(nb_actions), agent.Q_t)\nplt.xlabel(\"Actions\")\nplt.ylabel(\"$Q_t(a)$\")\nplt.subplot(223)\nplt.plot(rewards)\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Reward\")\nplt.subplot(224)\nplt.plot(optimal)\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Optimal\")\nplt.show()\n\n\n\n\n\n\n\n\nThe evolution of the received rewards and optimal actions does not give a clear indication of the successful learning, as it is strongly dependent on the true Q-values. To truly estimate the performance of the algorithm, we have to average these results over many runs, e.g. 200.\nQ: Run the learning procedure 200 times (new bandit and agent every time) and average the results. Give a unique name to these arrays (e.g. rewards_greedy and optimal_greedy) as we will do comparisons later. Compare the results with the lecture, where a 10-armed bandit was used.\n\n# Number of arms\nnb_actions = 5\n\n# Learning rate\nalpha = 0.1\n\nrewards_greedy = []\noptimal_greedy = []\n\nfor trial in range(200):\n\n    # Bandit\n    bandit = Bandit(nb_actions)\n\n    # Estimates\n    agent = GreedyAgent(bandit, alpha)\n\n    # Store the rewards after each step\n    rewards, optimal = agent.train(1000)\n    \n    rewards_greedy.append(rewards)\n    optimal_greedy.append(optimal)\n    \nrewards_greedy = np.mean(rewards_greedy, axis=0)\noptimal_greedy = np.mean(optimal_greedy, axis=0)\n    \n# Plot the Q-values and the evolution of rewards\nplt.figure(figsize=(12, 5))\nplt.subplot(121)\nplt.plot(rewards_greedy)\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Reward\")\nplt.subplot(122)\nplt.plot(optimal_greedy)\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Optimal\")\nplt.show()\n\n\n\n\n\n\n\n\nA: the greedy agent selects the optimal action around 80% of the time, vs. 50% for the 10-armed bandits. It is really not bad knowing that it starts at chance level (20% for 5 actions)."
  },
  {
    "objectID": "exercises/4-Bandits-solution.html#epsilon-greedy-action-selection",
    "href": "exercises/4-Bandits-solution.html#epsilon-greedy-action-selection",
    "title": "Bandits",
    "section": "\\epsilon-greedy action selection",
    "text": "\\epsilon-greedy action selection\nThe main drawback of greedy action selection is that it does not explore: as soon as it finds an action better than the others (with a sufficiently positive true Q-value, i.e. where the sampled rewards are mostly positive), it will keep selecting that action and avoid exploring the other options.\nThe estimated Q-value of the selected action will end up being quite correct, but those of the other actions will stay at 0.\nIn \\epsilon-greedy action selection, the greedy action a_t^* (with the highest estimated Q-value) will be selected with a probability 1-\\epsilon, the others with a probability of \\epsilon altogether.\n\n    \\pi(a) = \\begin{cases} 1 - \\epsilon \\; \\text{if} \\; a = a_t^* \\\\ \\frac{\\epsilon}{|\\mathcal{A}| - 1} \\; \\text{otherwise.} \\end{cases}\n\nIf you have |\\mathcal{A}| = 5 actions, the four non-greedy actions will be selected with a probability of \\frac{\\epsilon}{4}.\nQ: Create a EpsilonGreedyAgent (possibly inheriting from GreedyAgent to reuse code) to implement \\epsilon-greedy action selection (with \\epsilon=0.1 at first). Do not overwrite the arrays previously calculated (mean reward and optimal actions), as you will want to compare the two methods in a single plot.\nTo implement \\epsilon-greedy, you need to:\n\nSelect the greedy action a = a^*_t.\nDraw a random number between 0 and 1 (rng.random()).\nIf this number is smaller than \\epsilon, you need to select another action randomly in the remaining ones (rng.choice()).\nOtherwise, keep the greedy action.\n\n\nclass EpsilonGreedyAgent(GreedyAgent):\n    \n    def __init__(self, bandit, alpha, epsilon):\n        \n        self.epsilon = epsilon\n        \n        # List of actions\n        self.actions = np.arange(bandit.nb_actions)\n        \n        # Call the constructor of GreedyAgent\n        super().__init__(bandit, alpha)\n        \n    def act(self):\n        \n        action = rng.choice(np.where(self.Q_t == self.Q_t.max())[0])\n        \n        if rng.random() < self.epsilon:\n            action = rng.choice(self.actions[self.actions != action])\n            \n        return action\n\n\n# Number of arms\nnb_actions = 5\n\n# Learning rate\nalpha = 0.1\n\n# Epsilon for exploration\nepsilon = 0.1\n\nrewards_egreedy = []\noptimal_egreedy = []\n\nfor trial in range(200):\n\n    # Bandit\n    bandit = Bandit(nb_actions)\n\n    # Estimates\n    agent = EpsilonGreedyAgent(bandit, alpha, epsilon)\n\n    # Store the rewards after each step\n    rewards, optimal = agent.train(1000)\n    \n    rewards_egreedy.append(rewards)\n    optimal_egreedy.append(optimal)\n    \nrewards_egreedy = np.mean(rewards_egreedy, axis=0)\noptimal_egreedy = np.mean(optimal_egreedy, axis=0)\n    \n# Plot the Q-values and the evolution of rewards\nplt.figure(figsize=(12, 5))\nplt.subplot(121)\nplt.plot(rewards_greedy, label=\"Greedy\")\nplt.plot(rewards_egreedy, label=\"$\\epsilon$-Greedy\")\nplt.legend()\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Reward\")\nplt.subplot(122)\nplt.plot(optimal_greedy, label=\"Greedy\")\nplt.plot(optimal_egreedy, label=\"$\\epsilon$-Greedy\")\nplt.legend()\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Optimal\")\nplt.show()\n\n\n\n\n\n\n\n\nQ: Compare the properties of greedy and \\epsilon-greedy (speed, optimality, etc). Vary the value of the parameter \\epsilon (0.0001 until 0.5) and conclude.\nA: Depending on the value of \\epsilon, \\epsilon-greedy can perform better that greedy in the end, but will necessitate more time at the beginning. If there is too much exploration, \\epsilon-greedy can be even worse than greedy."
  },
  {
    "objectID": "exercises/4-Bandits-solution.html#softmax-action-selection",
    "href": "exercises/4-Bandits-solution.html#softmax-action-selection",
    "title": "Bandits",
    "section": "Softmax action selection",
    "text": "Softmax action selection\nTo avoid exploring actions which are clearly not optimal, another useful algorithm is softmax action selection. In this scheme, the estimated Q-values are ransformed into a probability distribution using the softmax opertion:\n\n    \\pi(a) = \\frac{\\exp \\frac{Q_t(a)}{\\tau}}{ \\sum_b \\exp \\frac{Q_t(b)}{\\tau}}\n\nFor each action, the term \\exp \\frac{Q_t(a)}{\\tau} is proportional to Q_t(a) but made positive. These terms are then normalized by the denominator in order to obtain a sum of 1, i.e. they are the parameters of a discrete probability distribution. The temperature \\tau controls the level of exploration just as \\epsilon for \\epsilon-greedy.\nIn practice, \\exp \\frac{Q_t(a)}{\\tau} can be very huge if the Q-values are high or the temperature is small, creating numerical instability (NaN). It is much more stable to substract the maximal Q-value from all Q-values before applying the softmax:\n\n    \\pi(a) = \\frac{\\exp \\displaystyle\\frac{Q_t(a) - \\max_a Q_t(a)}{\\tau}}{ \\sum_b \\exp \\displaystyle\\frac{Q_t(b) - \\max_b Q_t(b)}{\\tau}}\n\nThis way, Q_t(a) - \\max_a Q_t(a) is always negative, so its exponential is between 0 and 1.\nQ: Implement the softmax action selection (with \\tau=0.5 at first) and compare its performance to greedy and \\epsilon-greedy. Vary the temperature \\tau and find the best possible value. Conclude.\nHint: To select actions with different probabilities, check the doc of rng.choice().\n\nclass SoftmaxAgent(GreedyAgent):\n    \n    def __init__(self, bandit, alpha, tau):\n        self.tau = tau\n        \n        # List of actions\n        self.actions = np.arange(bandit.nb_actions)\n        \n        # Call the constructor of GreedyAgent\n        super().__init__(bandit, alpha)\n        \n    def act(self):\n        \n        logit = np.exp((self.Q_t - self.Q_t.max())/self.tau)\n        \n        proba_softmax = logit / np.sum(logit)\n        \n        action = rng.choice(self.actions, p=proba_softmax) \n            \n        return action\n\n\n# Number of arms\nnb_actions = 5\n\n# Learning rate\nalpha = 0.1\n\n# Tau for exploration\ntau = 0.1\n\nrewards_softmax = []\noptimal_softmax = []\n\nfor trial in range(200):\n\n    # Bandit\n    bandit = Bandit(nb_actions)\n\n    # Estimates\n    agent = SoftmaxAgent(bandit, alpha, tau)\n\n    # Store the rewards after each step\n    rewards, optimal = agent.train(1000)\n    \n    rewards_softmax.append(rewards)\n    optimal_softmax.append(optimal)\n    \nrewards_softmax = np.mean(rewards_softmax, axis=0)\noptimal_softmax = np.mean(optimal_softmax, axis=0)\n    \n# Plot the Q-values and the evolution of rewards\nplt.figure(figsize=(12, 5))\nplt.subplot(121)\nplt.plot(rewards_greedy, label=\"Greedy\")\nplt.plot(rewards_egreedy, label=\"$\\epsilon$-Greedy\")\nplt.plot(rewards_softmax, label=\"Softmax\")\nplt.legend()\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Reward\")\nplt.subplot(122)\nplt.plot(optimal_greedy, label=\"Greedy\")\nplt.plot(optimal_egreedy, label=\"$\\epsilon$-Greedy\")\nplt.plot(optimal_softmax, label=\"Softmax\")\nplt.legend()\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Optimal\")\nplt.show()\n\n\n\n\n\n\n\n\nA: softmax loses less time than \\epsilon-greedy exploring the really bad solutions, so it is optimal earlier. It can be more efficient and optimal than the other methods, but finding the right value for \\tau (0.1 works well) is difficult: its optimum value depends on the scaling of Q, you cannot know it in advance…"
  },
  {
    "objectID": "exercises/4-Bandits-solution.html#exploration-scheduling",
    "href": "exercises/4-Bandits-solution.html#exploration-scheduling",
    "title": "Bandits",
    "section": "Exploration scheduling",
    "text": "Exploration scheduling\nThe problem with this version of softmax (with a constant temperature) is that even after it has found the optimal action, it will still explore the other ones (although more rarely than at the beginning). The solution is to schedule the exploration parameter so that it explores a lot at the beginning (high temperature) and gradually switches to more exploitation (low temperature).\nMany schemes are possible for that, the simplest one (exponential decay) being to multiply the value of \\tau by a number very close to 1 after each play:\n\\tau = \\tau \\times (1 - \\tau_\\text{decay})\nQ: Implement in a class SoftmaxScheduledAgent temperature scheduling for the softmax algorithm (\\epsilon-greedy would be similar) with \\tau=1 initially and \\tau_\\text{decay} = 0.01 (feel free to change these values). Plot the evolution of tau and of the standard deviation of the choices of the optimal action. Conclude.\n\nclass SoftmaxScheduledAgent(SoftmaxAgent):\n    \n    def __init__(self, bandit, alpha, tau, tau_decay):\n        self.tau_decay = tau_decay\n        \n        self.tau_history = []\n        \n        # List of actions\n        self.actions = np.arange(bandit.nb_actions)\n        \n        # Call the constructor of GreedyAgent\n        super().__init__(bandit, alpha, tau)\n    \n        \n    def act(self):\n        \n        # Action selection\n        logit = np.exp((self.Q_t - self.Q_t.max())/self.tau)\n        proba_softmax = logit / np.sum(logit)        \n        action = rng.choice(self.actions, p=proba_softmax) \n        \n        # Decay tau\n        self.tau = self.tau * (1 - self.tau_decay)\n        self.tau_history.append(self.tau)\n            \n        return action\n\n\n# Learning rate\nalpha = 0.1\n\n# Tau for exploration\ntau = 1.0\ntau_decay = 0.01\n\nrewards_softmaxscheduled = []\noptimal_softmaxscheduled = []\n\nfor trial in range(200):\n\n    # Bandit\n    bandit = Bandit(nb_actions)\n\n    # Estimates\n    agent = SoftmaxScheduledAgent(bandit, alpha, tau, tau_decay)\n\n    # Store the rewards after each step\n    rewards, optimal = agent.train(1000)\n    \n    rewards_softmaxscheduled.append(rewards)\n    optimal_softmaxscheduled.append(optimal)\n    \n    \nrewards_softmaxscheduled = np.mean(rewards_softmaxscheduled, axis=0)\noptimal_softmaxscheduled_std = np.std(optimal_softmaxscheduled, axis=0)\noptimal_softmaxscheduled = np.mean(optimal_softmaxscheduled, axis=0)\n    \n# Plot the Q-values and the evolution of rewards\nplt.figure(figsize=(16, 16))\nplt.subplot(221)\nplt.plot(rewards_greedy, label=\"Greedy\")\nplt.plot(rewards_egreedy, label=\"$\\epsilon$-Greedy\")\nplt.plot(rewards_softmax, label=\"Softmax\")\nplt.plot(rewards_softmaxscheduled, label=\"Softmax (scheduled)\")\nplt.legend()\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Reward\")\nplt.subplot(222)\nplt.plot(optimal_greedy, label=\"Greedy\")\nplt.plot(optimal_egreedy, label=\"$\\epsilon$-Greedy\")\nplt.plot(optimal_softmax, label=\"Softmax\")\nplt.plot(optimal_softmaxscheduled, label=\"Softmax (scheduled)\")\nplt.legend()\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Optimal\")\nplt.subplot(223)\nplt.plot(agent.tau_history)\nplt.xlabel(\"Plays\")\nplt.ylabel(\"$\\tau$\")\nplt.subplot(224)\nplt.plot(optimal_softmaxscheduled_std)\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Variance\")\nplt.show()\n\n\n\n\n\n\n\n\nA: Scheduling drastically improves how often the optimal action is selected. In terms of mean reward, the difference is not that big, as there is often a “second best” action whose expected reward is close. We can see that the variance of the optimal action selection follows the parameter \\tau.\nQ: Experiment with different schedules (initial values, decay rate) and try to find the best setting.\n\n# Learning rate\nalpha = 0.1\n\n# Tau for exploration\ntau = 10.0\ntau_decay = 0.05\n\nrewards_softmaxscheduled = []\noptimal_softmaxscheduled = []\n\nfor trial in range(200):\n\n    # Bandit\n    bandit = Bandit(nb_actions)\n\n    # Estimates\n    agent = SoftmaxScheduledAgent(bandit, alpha, tau, tau_decay)\n\n    # Store the rewards after each step\n    rewards, optimal = agent.train(1000)\n    \n    rewards_softmaxscheduled.append(rewards)\n    optimal_softmaxscheduled.append(optimal)\n    \n    \nrewards_softmaxscheduled = np.mean(rewards_softmaxscheduled, axis=0)\noptimal_softmaxscheduled_std = np.std(optimal_softmaxscheduled, axis=0)\noptimal_softmaxscheduled = np.mean(optimal_softmaxscheduled, axis=0)\n    \n# Plot the Q-values and the evolution of rewards\nplt.figure(figsize=(16, 16))\nplt.subplot(221)\nplt.plot(rewards_greedy, label=\"Greedy\")\nplt.plot(rewards_egreedy, label=\"$\\epsilon$-Greedy\")\nplt.plot(rewards_softmax, label=\"Softmax\")\nplt.plot(rewards_softmaxscheduled, label=\"Softmax (scheduled)\")\nplt.legend()\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Reward\")\nplt.subplot(222)\nplt.plot(optimal_greedy, label=\"Greedy\")\nplt.plot(optimal_egreedy, label=\"$\\epsilon$-Greedy\")\nplt.plot(optimal_softmax, label=\"Softmax\")\nplt.plot(optimal_softmaxscheduled, label=\"Softmax (scheduled)\")\nplt.legend()\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Optimal\")\nplt.subplot(223)\nplt.plot(agent.tau_history)\nplt.xlabel(\"Plays\")\nplt.ylabel(\"$\\tau$\")\nplt.subplot(224)\nplt.plot(optimal_softmaxscheduled_std)\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Variance\")\nplt.show()\n\n\n\n\n\n\n\n\nA: no unique answer here, but a very high exploration parameter initially which decreases quite fast leads to very performant solutions. Take-home message: scheduling is very important, but it is quite difficult to find the optimal schedule."
  },
  {
    "objectID": "exercises/5-Bandits2-solution.html",
    "href": "exercises/5-Bandits2-solution.html",
    "title": "Bandits - part 2",
    "section": "",
    "text": "In the exercise, we will investigate in more details the properties of the bandit algorithms implemented last time and investigate reinforcement comparison.\nQ: Start by copying all class definitions of the last exercise (Bandit, Greedy, \\epsilon-Greedy, softmax) and re-run the experiments with correct values for the parameters in a single cell. We will ignore exploration scheduling (although we should not)."
  },
  {
    "objectID": "exercises/5-Bandits2-solution.html#reward-distribution",
    "href": "exercises/5-Bandits2-solution.html#reward-distribution",
    "title": "Bandits - part 2",
    "section": "Reward distribution",
    "text": "Reward distribution\nWe are now going to vary the reward distributions and investigate whether the experimental results we had previously when the true Q-values are in \\mathcal{N}(0, 1) and the rewards have a variance of 1 still hold.\nQ: Let’s now change the distribution of true Q-values from \\mathcal{N}(0, 1) to \\mathcal{N}(10, 10) when creating the bandits and re-run the algorithms. What happens and why? Modify the values of epsilon and tau to try to get a better behavior.\n\n# Number of arms\nnb_actions = 5\n\n# Learning rate\nalpha = 0.1\n\n# epsilon for exploration\nepsilon = 0.1\n\n# Tau for exploration\ntau = 3.0\n\nrewards_greedy = []\noptimal_greedy = []\n\nrewards_egreedy = []\noptimal_egreedy = []\n\nrewards_softmax = []\noptimal_softmax = []\n\nfor trial in range(200):\n\n    # Bandit\n    bandit = Bandit(nb_actions, mean=10.0, std_Q=10.0, std_r=1.0)\n\n    # Greedy\n    agent = GreedyAgent(bandit, alpha)\n    rewards, optimal = agent.train(1000)    \n    rewards_greedy.append(rewards)\n    optimal_greedy.append(optimal)\n\n    # Greedy\n    agent = EpsilonGreedyAgent(bandit, alpha, epsilon)\n    rewards, optimal = agent.train(1000)    \n    rewards_egreedy.append(rewards)\n    optimal_egreedy.append(optimal)\n\n    # Softmax\n    agent = SoftmaxAgent(bandit, alpha, tau)\n    rewards, optimal = agent.train(1000)    \n    rewards_softmax.append(rewards)\n    optimal_softmax.append(optimal)\n    \nrewards_greedy = np.mean(rewards_greedy, axis=0)\noptimal_greedy = np.mean(optimal_greedy, axis=0)\nrewards_egreedy = np.mean(rewards_egreedy, axis=0)\noptimal_egreedy = np.mean(optimal_egreedy, axis=0)\nrewards_softmax = np.mean(rewards_softmax, axis=0)\noptimal_softmax = np.mean(optimal_softmax, axis=0)\n    \n# Plot the Q-values and the evolution of rewards\nplt.figure(figsize=(12, 5))\nplt.subplot(121)\nplt.plot(rewards_greedy, label=\"Greedy\")\nplt.plot(rewards_egreedy, label=\"$\\epsilon$-Greedy\")\nplt.plot(rewards_softmax, label=\"Softmax\")\nplt.legend()\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Reward\")\nplt.subplot(122)\nplt.plot(optimal_greedy, label=\"Greedy\")\nplt.plot(optimal_egreedy, label=\"$\\epsilon$-Greedy\")\nplt.plot(optimal_softmax, label=\"Softmax\")\nplt.legend()\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Optimal\")\nplt.show()\n\n\n\n\n\n\n\n\nA: Greedy does not work anymore and stays at chance level. The first action it samples will provide a non-zero reward, so its estimated Q-value becomes positive (initial estimate of 0) and it will keep selecting this non-optimal greedy action all along.\n\\epsilon-greedy still works quite well (although a bit slower as the estimates must go from 0 to 10), even with its default value of 0.1.\nSoftmax does not work unless you increase the temperature to 3 or so. The correct value of tau depends on the scaling of the Q-values, so it has to be adapted to every new problem, contrary to \\epsilon-greedy. But with the correct value of tau, you get a good solution much earlier."
  },
  {
    "objectID": "exercises/5-Bandits2-solution.html#optimistic-initialization",
    "href": "exercises/5-Bandits2-solution.html#optimistic-initialization",
    "title": "Bandits - part 2",
    "section": "Optimistic initialization",
    "text": "Optimistic initialization\nThe initial estimates of 0 are now very pessimistic compared to the average reward you can get (between 10 and 20). This was not the case in the original setup.\nQ: Modify the classes so that they accept a parameter Q_init allowing to intialize the estimates Q_t to something different from 0. Change the initial value of the estimates to 10 for each algorithm. What happens? Conclude on the importance of reward scaling.\n\n# Number of arms\nnb_actions = 5\n\n# Learning rate\nalpha = 0.1\n\n# epsilon for exploration\nepsilon = 0.1\n\n# Tau for exploration\ntau = 3.0\n\n# Optimistic initialization\nQ_init = 10.0\n\nrewards_greedy = []\noptimal_greedy = []\n\nrewards_egreedy = []\noptimal_egreedy = []\n\nrewards_softmax = []\noptimal_softmax = []\n\nfor trial in range(200):\n\n    # Bandit\n    bandit = Bandit(nb_actions, mean=10.0, std_Q=10.0, std_r=1.0)\n\n    # Greedy\n    agent = GreedyAgent(bandit, alpha, Q_init)\n    rewards, optimal = agent.train(1000)    \n    rewards_greedy.append(rewards)\n    optimal_greedy.append(optimal)\n\n    # Greedy\n    agent = EpsilonGreedyAgent(bandit, alpha, epsilon, Q_init)\n    rewards, optimal = agent.train(1000)    \n    rewards_egreedy.append(rewards)\n    optimal_egreedy.append(optimal)\n\n    # Softmax\n    agent = SoftmaxAgent(bandit, alpha, tau, Q_init)\n    rewards, optimal = agent.train(1000)    \n    rewards_softmax.append(rewards)\n    optimal_softmax.append(optimal)\n    \nrewards_greedy = np.mean(rewards_greedy, axis=0)\noptimal_greedy = np.mean(optimal_greedy, axis=0)\nrewards_egreedy = np.mean(rewards_egreedy, axis=0)\noptimal_egreedy = np.mean(optimal_egreedy, axis=0)\nrewards_softmax = np.mean(rewards_softmax, axis=0)\noptimal_softmax = np.mean(optimal_softmax, axis=0)\n    \n# Plot the Q-values and the evolution of rewards\nplt.figure(figsize=(12, 5))\nplt.subplot(121)\nplt.plot(rewards_greedy, label=\"Greedy\")\nplt.plot(rewards_egreedy, label=\"$\\epsilon$-Greedy\")\nplt.plot(rewards_softmax, label=\"Softmax\")\nplt.legend()\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Reward\")\nplt.subplot(122)\nplt.plot(optimal_greedy, label=\"Greedy\")\nplt.plot(optimal_egreedy, label=\"$\\epsilon$-Greedy\")\nplt.plot(optimal_softmax, label=\"Softmax\")\nplt.legend()\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Optimal\")\nplt.show()\n\n\n\n\n\n\n\n\nA: Now we are back to quite the same results as before (greedy might still be worse, but not at chance level anymore). This shows the importance of reward scaling: the amplitude of the rewards influences a lot the success of the different methods (\\epsilon-greedy is more robust). This mean you need to know the mean expected reward in advance, but you are not supposed to know that as you have not sampled anything at the beginning…\nLet’s now use optimistic initialization, i.e. initialize the estimates to a much higher value than what is realistic.\nQ: Implement optimistic initialization by initializing the estimates of all three algorithms to 25. What happens?\n\n# Number of arms\nnb_actions = 5\n\n# Learning rate\nalpha = 0.1\n\n# epsilon for exploration\nepsilon = 0.1\n\n# Tau for exploration\ntau = 3.0\n\n# Optimistic initialization\nQ_init = 25.0\n\nrewards_greedy = []\noptimal_greedy = []\n\nrewards_egreedy = []\noptimal_egreedy = []\n\nrewards_softmax = []\noptimal_softmax = []\n\nfor trial in range(200):\n\n    # Bandit\n    bandit = Bandit(nb_actions, mean=10.0, std_Q=10.0, std_r=1.0)\n\n    # Greedy\n    agent = GreedyAgent(bandit, alpha, Q_init)\n    rewards, optimal = agent.train(1000)    \n    rewards_greedy.append(rewards)\n    optimal_greedy.append(optimal)\n\n    # Greedy\n    agent = EpsilonGreedyAgent(bandit, alpha, epsilon, Q_init)\n    rewards, optimal = agent.train(1000)    \n    rewards_egreedy.append(rewards)\n    optimal_egreedy.append(optimal)\n\n    # Softmax\n    agent = SoftmaxAgent(bandit, alpha, tau, Q_init)\n    rewards, optimal = agent.train(1000)    \n    rewards_softmax.append(rewards)\n    optimal_softmax.append(optimal)\n    \nrewards_greedy = np.mean(rewards_greedy, axis=0)\noptimal_greedy = np.mean(optimal_greedy, axis=0)\nrewards_egreedy = np.mean(rewards_egreedy, axis=0)\noptimal_egreedy = np.mean(optimal_egreedy, axis=0)\nrewards_softmax = np.mean(rewards_softmax, axis=0)\noptimal_softmax = np.mean(optimal_softmax, axis=0)\n    \n# Plot the Q-values and the evolution of rewards\nplt.figure(figsize=(12, 5))\nplt.subplot(121)\nplt.plot(rewards_greedy, label=\"Greedy\")\nplt.plot(rewards_egreedy, label=\"$\\epsilon$-Greedy\")\nplt.plot(rewards_softmax, label=\"Softmax\")\nplt.legend()\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Reward\")\nplt.subplot(122)\nplt.plot(optimal_greedy, label=\"Greedy\")\nplt.plot(optimal_egreedy, label=\"$\\epsilon$-Greedy\")\nplt.plot(optimal_softmax, label=\"Softmax\")\nplt.legend()\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Optimal\")\nplt.show()\n\n\n\n\n\n\n\n\nA: With optimistic initialization, greedy action selection becomes the most efficient method: exploration is ensured by the fact that all actions be be executed at some point, as they can only be disappointing. The received rewards are always lower than the expectation at the beginning, so there is no need for additional exploration mechanisms. But it necessitates to know in advance what the maximal reward is… Note that reducing the exploration parameters help \\epsilon-greedy and softmax behave like greedy."
  },
  {
    "objectID": "exercises/5-Bandits2-solution.html#reinforcement-comparison",
    "href": "exercises/5-Bandits2-solution.html#reinforcement-comparison",
    "title": "Bandits - part 2",
    "section": "Reinforcement comparison",
    "text": "Reinforcement comparison\nThe problem with the previous value-based methods is that the Q-value estimates depend on the absolute magnitude of the rewards (by definition). The hyperparameters of the learning algorithms (learning rate, exploration, initial values) will therefore be very different depending on the scaling of the rewards (between 0 and 1, between -100 and 100, etc).\nA way to get rid of this dependency is to introduce preferences p_t(a) for each action instead of the estimated Q-values. Preferences should follow the Q-values: an action with a high Q-value should have a high Q-value and vice versa, but we do not care about its exact scaling.\nIn reinforcement comparison, we introduce a baseline \\tilde{r}_t which is the average received reward regardless the action, i.e. there is a single value for the whole problem. This average reward is simply updated after each action with a moving average of the received rewards:\n\\tilde{r}_{t+1} = \\tilde{r}_{t} + \\alpha \\, (r_t - \\tilde{r}_{t})\nThe average reward is used to update the preference for the action that was just executed:\np_{t+1}(a_t) = p_{t}(a_t) + \\beta \\, (r_t - \\tilde{r}_{t})\nIf the action lead to more reward than usual, its preference should be increased (good surprise). If the action lead to less reward than usual, its preference should be decreased (bad surprise).\nAction selection is simply a softmax over the preferences, without the temperature parameter (as we do not care about the scaling):\n\n    \\pi (a) = \\frac{\\exp p_t(a)}{ \\sum_b \\exp p_t(b)}\n\nQ: Implement reinforcement comparison (with \\alpha=\\beta=0.1) and compare it to the other methods on the default settings.\n\nclass ReinforcementComparisonAgent (GreedyAgent):\n    \n    def __init__(self, bandit, alpha, beta, r_init=0.0):\n        \n        self.bandit = bandit\n        self.alpha = alpha\n        self.beta = beta\n        \n        # List of actions\n        self.actions = np.arange(bandit.nb_actions)\n        \n        # Preferences\n        self.p_t = np.zeros(self.bandit.nb_actions)\n        \n        # Mean reward\n        self.r_mean = r_init\n        \n        \n    def act(self):\n        \n        logit = np.exp((self.p_t - self.p_t.max()))\n        \n        proba_softmax = logit / np.sum(logit)\n        \n        action = rng.choice(self.actions, p=proba_softmax) \n            \n        return action\n        \n    def update(self, action, reward):\n        \n        self.p_t[action] += self.beta * (reward - self.r_mean)\n        \n        self.r_mean += self.alpha * (reward - self.r_mean)\n        \n\n\n# Number of arms\nnb_actions = 5\n\n# Learning rate\nalpha = 0.1\nbeta = 0.1\n\n# epsilon for exploration\nepsilon = 0.1\n\n# Tau for exploration\ntau = 0.1\n\nrewards_greedy = []\noptimal_greedy = []\n\nrewards_egreedy = []\noptimal_egreedy = []\n\nrewards_softmax = []\noptimal_softmax = []\n\nrewards_rc = []\noptimal_rc = []\n\nfor trial in range(200):\n\n    # Bandit\n    bandit = Bandit(nb_actions)\n\n    # Greedy\n    agent = GreedyAgent(bandit, alpha)\n    rewards, optimal = agent.train(1000)    \n    rewards_greedy.append(rewards)\n    optimal_greedy.append(optimal)\n\n    # Greedy\n    agent = EpsilonGreedyAgent(bandit, alpha, epsilon)\n    rewards, optimal = agent.train(1000)    \n    rewards_egreedy.append(rewards)\n    optimal_egreedy.append(optimal)\n\n    # Softmax\n    agent = SoftmaxAgent(bandit, alpha, tau)\n    rewards, optimal = agent.train(1000)    \n    rewards_softmax.append(rewards)\n    optimal_softmax.append(optimal)\n\n    # Reinforcement comparison\n    agent = ReinforcementComparisonAgent(bandit, alpha, beta)\n    rewards, optimal = agent.train(1000)    \n    rewards_rc.append(rewards)\n    optimal_rc.append(optimal)\n    \nrewards_greedy = np.mean(rewards_greedy, axis=0)\noptimal_greedy = np.mean(optimal_greedy, axis=0)\nrewards_egreedy = np.mean(rewards_egreedy, axis=0)\noptimal_egreedy = np.mean(optimal_egreedy, axis=0)\nrewards_softmax = np.mean(rewards_softmax, axis=0)\noptimal_softmax = np.mean(optimal_softmax, axis=0)\nrewards_rc = np.mean(rewards_rc, axis=0)\noptimal_rc = np.mean(optimal_rc, axis=0)\n    \n# Plot the Q-values and the evolution of rewards\nplt.figure(figsize=(12, 5))\nplt.subplot(121)\nplt.plot(rewards_greedy, label=\"Greedy\")\nplt.plot(rewards_egreedy, label=\"$\\epsilon$-Greedy\")\nplt.plot(rewards_softmax, label=\"Softmax\")\nplt.plot(rewards_rc, label=\"Reinforcement comparison\")\nplt.legend()\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Reward\")\nplt.subplot(122)\nplt.plot(optimal_greedy, label=\"Greedy\")\nplt.plot(optimal_egreedy, label=\"$\\epsilon$-Greedy\")\nplt.plot(optimal_softmax, label=\"Softmax\")\nplt.plot(optimal_rc, label=\"Reinforcement comparison\")\nplt.legend()\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Optimal\")\nplt.show()\n\n\n\n\n\n\n\n\nA: RC is slower at the beginning, but ends up being more optimal. We never estimate the Q-values, but we do not care about them, we only want to perform the correct actions. We also get rid of the temperature parameter.\nQ: Compare all methods with optimistic initialization. The true Q-values come from \\mathcal{N}(10, 10), the estimated Q-values are initialized to 20 for greedy, \\epsilon-greedy and softmax, and the average reward is initialized to 20 for RC (the preferences are initialized at 0).\n\n# Number of arms\nnb_actions = 5\n\n# Learning rate\nalpha = 0.1\nbeta = 0.1\n\n# epsilon for exploration\nepsilon = 0.1\n\n# Tau for exploration\ntau = 1.0\n\n# Optimistic initialization\nQ_init = 25.0\n\nrewards_greedy = []\noptimal_greedy = []\n\nrewards_egreedy = []\noptimal_egreedy = []\n\nrewards_softmax = []\noptimal_softmax = []\n\nrewards_rc = []\noptimal_rc = []\n\nfor trial in range(200):\n\n    # Bandit\n    bandit = Bandit(nb_actions, mean=10.0, std_Q=10.0, std_r=1.0)\n\n    # Greedy\n    agent = GreedyAgent(bandit, alpha, Q_init)\n    rewards, optimal = agent.train(1000)    \n    rewards_greedy.append(rewards)\n    optimal_greedy.append(optimal)\n\n    # Greedy\n    agent = EpsilonGreedyAgent(bandit, alpha, epsilon, Q_init)\n    rewards, optimal = agent.train(1000)    \n    rewards_egreedy.append(rewards)\n    optimal_egreedy.append(optimal)\n\n    # Softmax\n    agent = SoftmaxAgent(bandit, alpha, tau, Q_init)\n    rewards, optimal = agent.train(1000)    \n    rewards_softmax.append(rewards)\n    optimal_softmax.append(optimal)\n\n    # Reinforcement comparison\n    agent = ReinforcementComparisonAgent(bandit, alpha, beta, Q_init)\n    rewards, optimal = agent.train(1000)    \n    rewards_rc.append(rewards)\n    optimal_rc.append(optimal)\n    \nrewards_greedy = np.mean(rewards_greedy, axis=0)\noptimal_greedy = np.mean(optimal_greedy, axis=0)\nrewards_egreedy = np.mean(rewards_egreedy, axis=0)\noptimal_egreedy = np.mean(optimal_egreedy, axis=0)\nrewards_softmax = np.mean(rewards_softmax, axis=0)\noptimal_softmax = np.mean(optimal_softmax, axis=0)\nrewards_rc = np.mean(rewards_rc, axis=0)\noptimal_rc = np.mean(optimal_rc, axis=0)\n    \n# Plot the Q-values and the evolution of rewards\nplt.figure(figsize=(12, 5))\nplt.subplot(121)\nplt.plot(rewards_greedy, label=\"Greedy\")\nplt.plot(rewards_egreedy, label=\"$\\epsilon$-Greedy\")\nplt.plot(rewards_softmax, label=\"Softmax\")\nplt.plot(rewards_rc, label=\"Reinforcement comparison\")\nplt.legend()\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Reward\")\nplt.subplot(122)\nplt.plot(optimal_greedy, label=\"Greedy\")\nplt.plot(optimal_egreedy, label=\"$\\epsilon$-Greedy\")\nplt.plot(optimal_softmax, label=\"Softmax\")\nplt.plot(optimal_rc, label=\"Reinforcement comparison\")\nplt.legend()\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Optimal\")\nplt.show()"
  },
  {
    "objectID": "exercises/5-Bandits2-solution.html#bonus-questions",
    "href": "exercises/5-Bandits2-solution.html#bonus-questions",
    "title": "Bandits - part 2",
    "section": "Bonus questions",
    "text": "Bonus questions\nAt this point, you should have understood the main concepts of bandit algorithm. If you have time and interest, you can also implement Gradient Bandit (reinforcement comparison where all actions are updated, not just the one which was executed) and UCB (upper-bound confidence).\n\nclass GradientBanditAgent (ReinforcementComparisonAgent):\n    \n    def __init__(self, bandit, alpha, beta, r_init=0.0):\n        \n        # Call the constructor of ReinforcementComparisonAgent\n        super().__init__(bandit, alpha, beta, r_init)\n        \n        \n    def act(self):\n        \n        logit = np.exp((self.p_t - self.p_t.max()))\n        \n        self.pi = logit / np.sum(logit)\n        \n        action = rng.choice(self.actions, p=self.pi) \n            \n        return action\n    \n    def update(self, action, reward):\n        \n        for a  in self.actions:\n            if a == action:\n                self.p_t[a] += self.beta * (reward - self.r_mean) * (1 - self.pi[a])\n            else:\n                self.p_t[a] -= self.beta * (reward - self.r_mean) * self.pi[a]\n        \n        self.r_mean += self.alpha * (reward - self.r_mean)\n        \n\n\nclass UCBAgent(GreedyAgent):\n    \n    def __init__(self, bandit, alpha, c, Q_init=0.0):\n        \n        self.c = c\n        \n        # List of actions\n        self.actions = np.arange(bandit.nb_actions)\n        \n        # Visitation count initialized to 1 to avoid the division by 0\n        self.N = np.ones(bandit.nb_actions)\n        \n        # Time\n        self.t = 1\n        \n        # Call the constructor of GreedyAgent\n        super().__init__(bandit, alpha, Q_init)\n        \n    def act(self):\n        # Exploration bonus\n        Q = self.Q_t + self.c * np.sqrt(np.log(self.t)/self.N)\n        \n        # Greedy action selection\n        action = rng.choice(np.where(Q == Q.max())[0])\n        \n        # Time passes\n        self.N[action] += 1\n        self.t += 1\n            \n        return action\n\n\n# Number of arms\nnb_actions = 5\n\n# Learning rate\nalpha = 0.1\nbeta = 0.1\n\n# epsilon for exploration\nepsilon = 0.1\n\n# Tau for exploration\ntau = 0.1\n\nrewards_greedy = []\noptimal_greedy = []\n\nrewards_egreedy = []\noptimal_egreedy = []\n\nrewards_softmax = []\noptimal_softmax = []\n\nrewards_rc = []\noptimal_rc = []\n\nrewards_gb = []\noptimal_gb = []\n\nrewards_ucb = []\noptimal_ucb = []\n\nfor trial in range(200):\n\n    # Bandit\n    bandit = Bandit(nb_actions)\n\n    # Greedy\n    agent = GreedyAgent(bandit, alpha)\n    rewards, optimal = agent.train(1000)    \n    rewards_greedy.append(rewards)\n    optimal_greedy.append(optimal)\n\n    # Greedy\n    agent = EpsilonGreedyAgent(bandit, alpha, epsilon)\n    rewards, optimal = agent.train(1000)    \n    rewards_egreedy.append(rewards)\n    optimal_egreedy.append(optimal)\n\n    # Softmax\n    agent = SoftmaxAgent(bandit, alpha, tau)\n    rewards, optimal = agent.train(1000)    \n    rewards_softmax.append(rewards)\n    optimal_softmax.append(optimal)\n\n    # Reinforcement comparison\n    agent = ReinforcementComparisonAgent(bandit, alpha, beta)\n    rewards, optimal = agent.train(1000)    \n    rewards_rc.append(rewards)\n    optimal_rc.append(optimal)\n\n    # Gradient Bandit\n    agent = GradientBanditAgent(bandit, alpha, beta)\n    rewards, optimal = agent.train(1000)    \n    rewards_gb.append(rewards)\n    optimal_gb.append(optimal)\n\n    # UCB\n    agent = UCBAgent(bandit, alpha, c=0.5)\n    rewards, optimal = agent.train(1000)    \n    rewards_ucb.append(rewards)\n    optimal_ucb.append(optimal)\n    \nrewards_greedy = np.mean(rewards_greedy, axis=0)\noptimal_greedy = np.mean(optimal_greedy, axis=0)\nrewards_egreedy = np.mean(rewards_egreedy, axis=0)\noptimal_egreedy = np.mean(optimal_egreedy, axis=0)\nrewards_softmax = np.mean(rewards_softmax, axis=0)\noptimal_softmax = np.mean(optimal_softmax, axis=0)\nrewards_rc = np.mean(rewards_rc, axis=0)\noptimal_rc = np.mean(optimal_rc, axis=0)\nrewards_gb = np.mean(rewards_gb, axis=0)\noptimal_gb = np.mean(optimal_gb, axis=0)\nrewards_ucb = np.mean(rewards_ucb, axis=0)\noptimal_ucb = np.mean(optimal_ucb, axis=0)\n    \n# Plot the Q-values and the evolution of rewards\nplt.figure(figsize=(12, 5))\nplt.subplot(121)\nplt.plot(rewards_greedy, label=\"Greedy\")\nplt.plot(rewards_egreedy, label=\"$\\epsilon$-Greedy\")\nplt.plot(rewards_softmax, label=\"Softmax\")\nplt.plot(rewards_rc, label=\"Reinforcement comparison\")\nplt.plot(rewards_gb, label=\"Gradient bandit\")\nplt.plot(rewards_ucb, label=\"UCB\")\nplt.legend()\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Reward\")\nplt.subplot(122)\nplt.plot(optimal_greedy, label=\"Greedy\")\nplt.plot(optimal_egreedy, label=\"$\\epsilon$-Greedy\")\nplt.plot(optimal_softmax, label=\"Softmax\")\nplt.plot(optimal_rc, label=\"Reinforcement comparison\")\nplt.plot(optimal_gb, label=\"Gradient bandit\")\nplt.plot(optimal_ucb, label=\"UCB\")\nplt.legend()\nplt.xlabel(\"Plays\")\nplt.ylabel(\"Optimal\")\nplt.show()"
  },
  {
    "objectID": "exercises/6-DP-solution.html",
    "href": "exercises/6-DP-solution.html",
    "title": "Dynamic programming",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nThe goal of this exercise is to find the optimal policy for the recycling robot.\nIn this problem, a recycling robot has to search for empty cans to collect (each can defines a “reward” given to the robot). It can also decide to stay where it is to save its battery and wait that somebody brings it a can (which gives less cans in average than actively searching for them).\nThe robot has two battery levels, high and low.\nState-action transitions are probabilistic, i.e. they bring the robot in different states based on different probabilities \\alpha and \\beta.\nThis problem defines a finite MDP, with two states high and low corresponding to the battery level. The actions search and wait are possible in the high and low states, while the action recharge is only possible in the low state.\n\\begin{aligned}\n    \\mathcal{S} &=& \\{ \\text{high}, \\text{low} \\} \\\\\n    \\mathcal{A}(\\text{high} ) &=& \\{ \\text{search}, \\text{wait} \\} \\\\\n    \\mathcal{A}(\\text{low} ) &=& \\{ \\text{search}, \\text{wait}, \\text{recharge} \\}\n\\end{aligned}\nThe action search brings on average a reward of \\mathcal{R}^\\text{search}, the action wait a reward of \\mathcal{R}^\\text{wait}, the action recharge brings no reward, but allows to get in the high state.\nNote that if the robot decides to search in the low state, there is a probability 1 - \\beta that it totally empties its battery, requiring human intervention. This is punished with a negative reward of -3.\nThe transition and reward probabilities of each transition is defined in the following table, completely defining a MDP.\nThe goal of this exercise is to find the optimal policy \\pi^* of the robot, i.e to find for each state the action that should be performed systematically in order to gather the maximum of reward on the long term.\nWe will apply here two dynamic programming methods, policy iteration and value iteration, to solve the Bellman equations.\nThe Bellman equation for the state function is:\nV^{\\pi} (s)  = \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{\\pi} (s') ]\nQ: On paper, adapt the Bellman equation to the problem. First, for every state s and possible action a, find the optimal value of the action with the form:\nQ^{\\pi} (s, a) = f( V^\\pi (\\text{high}), V^\\pi (\\text{low}), \\alpha, \\beta, \\gamma, \\mathcal{R}^{\\text{search}}, \\mathcal{R}^{\\text{wait}} )\nDeduce the Bellman equation for the two states V^\\pi (\\text{high}) and V^\\pi (\\text{low}).\nA:\nQ^\\pi(\\text{high}, \\text{search}) = \\alpha \\, (\\mathcal{R }^\\text{search} + \\gamma \\, V^\\pi(\\text{high} )) + (1- \\alpha)\\, (\\mathcal{R}^\\text{search} + \\gamma \\, V^\\pi(\\text{low}))\nQ^\\pi(\\text{high}, \\text{wait}) = \\mathcal{R}^\\text{wait} + \\gamma \\, V^\\pi(\\text{high})\nQ^\\pi(\\text{low}, \\text{search}) = \\beta * (\\mathcal{R }^\\text{search} + \\gamma \\,  V^\\pi(\\text{low})) + (1- \\beta) \\, (-3 + \\gamma \\, V^\\pi(\\text{high}))\nQ^\\pi(\\text{low}, \\text{wait}) = \\mathcal{R}^\\text{wait} + \\gamma \\, V^\\pi(\\text{low})\nQ^\\pi(\\text{low}, \\text{recharge}) = \\gamma \\, V^\\pi(\\text{high})\nV^\\pi(\\text{high}) = \\pi(\\text{high}, \\text{search}) \\, Q^\\pi(\\text{high}, \\text{search}) +  \\pi(\\text{high}, \\text{wait}) \\, Q^\\pi(\\text{high}, \\text{wait})\nV^\\pi(\\text{low}) = \\pi(\\text{low}, \\text{search}) \\, Q^\\pi(\\text{low}, \\text{search}) +  \\pi(\\text{low}, \\text{wait}) \\, Q^\\pi(\\text{low}, \\text{wait}) + \\pi(\\text{low}, \\text{recharge}) \\,  Q^\\pi(\\text{low}, \\text{recharge})"
  },
  {
    "objectID": "exercises/6-DP-solution.html#policy-iteration",
    "href": "exercises/6-DP-solution.html#policy-iteration",
    "title": "Dynamic programming",
    "section": "Policy Iteration",
    "text": "Policy Iteration\nNow that we have the Bellman equations for the two states high and low, we can solve them using iterative policy evaluation for a fixed policy \\pi.\n\nIterative policy evaluation\nLet’s start by setting the parameters of the MDP. In the rest of the exercise, you will modify these parameters to investigate how it changes the optimal policy.\n\n# Transition probabilities\nalpha = 0.3\nbeta = 0.2\n\n# Discount parameter\ngamma = 0.7\n\n# Expected rewards\nr_search = 6.0\nr_wait = 2.0\n\nThere are many ways to represent states and actions in a MDP. The suggestion for this exercise is to use dictionaries here the keys are the actions’ name and the vaues are indices:\n\nnb_states = 2\nnb_actions = 3\n\ns = {'high': 0, 'low': 1}\na = {'search': 0, 'wait': 1, 'recharge': 2}\n\nUsing dictionaries, you can access numpy arrays with s['high'] or a['recharge'] instead of 0 and 2, what will make the code readable.\nThe next step is to initialize numpy arrays where we will store the V and Q values. V will have only two elements for high and low, while Q will be a 2x3 matrix with one element for each state-action pair. Notice that (high, recharge) is not a possible action, so this element will not be be updated.\n\nV = np.zeros(nb_states)\nQ = np.zeros((nb_states, nb_actions))\n\nYou can then access the individual values with V[s['high']] or Q[s['low'], a['wait']].\nWe can now evaluate a policy \\pi. In dynamic programming, the policies are deterministic, as we want to estimate the optimal policy.\nTo implement the policy, we just need to assign the index of an action to each state, i.e. \\pi(s). The following cell creates an initial policy \\pi where the agent searches in both states high and low. We here make sure that the array contains integers (0, 1 or 2), but that is not even necessary.\n\npi = np.array([a['search'], a['search']], dtype=int)\n\nQ: Evaluate this policy using iterative policy evaluation.\nWe would normally only need to update the V-value of the two states using:\n\n     V (s) \\leftarrow \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V (s') ] \\quad \\forall s \\in \\mathcal{S}\n\nThe code will be more readable if you first update the Q-values of the 5 state-action pairs:\n\n     Q (s, a) \\leftarrow  \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V (s') ] \\quad \\forall s \\in \\mathcal{S}\n\nand only then update the two V-values:\n\n     V (s) \\leftarrow \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, Q(s, a)\n\nThese updates should normally be applied until the V-values converge. For simplicity, we could decide to simply apply 50 updates or so, and hope that it is enough.\nRecord the V-value of the two states after each update and plot them.\n\nV = np.zeros(nb_states)\nQ = np.zeros((nb_states, nb_actions))\n\nV_high_history = []\nV_low_history = []\n\nfor k in range(50):\n    \n    Q[s['high'], a['search']] = alpha * (r_search + gamma * V[s['high']]) + (1 - alpha) * (r_search + gamma*V[s['low']])\n    Q[s['high'], a['wait']] = r_wait + gamma * V[s['high']]\n    \n    Q[s['low'], a['search']] = beta * (r_search + gamma * V[s['low']]) + (1 - beta) * (-3 + gamma*V[s['high']])\n    Q[s['low'], a['wait']] = r_wait + gamma * V[s['low']]\n    Q[s['low'], a['recharge']] = gamma * V[s['high']]\n    \n    V[s['high']] = Q[s['high'], pi[s['high']]]\n    V[s['low']] = Q[s['low'], pi[s['low']]]\n    \n    V_high_history.append(V[s['high']])\n    V_low_history.append(V[s['low']])\n    \nprint(Q)\n    \nplt.figure(figsize=(10, 6))\nplt.plot(V_high_history, label=\"high\")\nplt.plot(V_low_history, label=\"low\")\nplt.legend()\nplt.show()\n\n[[11.28888873  9.90222206  0.        ]\n [ 5.9555554   6.16888873  7.90222206]]\n\n\n\n\n\n\n\n\n\nQ: Do the V-values converge? How fast? What do the final values represent? Change the value of \\gamma and conclude on its importance (do not forget to reset the V and Q arrays to 0!).\nA: The V-values converge quite fast (~15 iterations) to their true value. The high state has a higher value than the low state, as there is no risk in that state to receive the punishment of -3.\nThe final value is the expected return in that state, that is:\nR_t = \\sum_{k=0}^\\infty \\gamma^k \\, r_{t+k+1}\n\\gamma completely changes the scale of the return. Small values of \\gamma lead to small returns (only a couple of rewards count in the sum), while high values lead to high returns (there are a lot of rewards to be summed, especially because the task is continuing).\nQ: Print the Q-values at the end of the policy evaluation. What would the greedy policy with respect to these Q-values?\nQ: Change the initial policy to this policy and evaluate it. What happens? Compare the final value of the states under both policies. Which one is the best?\nA: The greedy policy w.r.t. the Q-values is searching in high, recharging in low, as the Q-values are maximal for these actions.\nIf we evaluate this policy, we observe that:\n\nthe value of both states is higher: this is a better policy, as we collect more return on average.\nthe greedy policy does not change after the evaluation: we have found the optimal policy already!\n\n\n\nPolicy iteration\nImproving the policy is now straightforward. We just to look at the Q-values in each state, and change the policy so that it takes the action with the maximal Q-value. If this does not change the policy (we still take the same actions), we have found the optimal policy, we can stop.\nQ: Implement policy iteration.\nDo not forget to reset the V and Q arrays at the beginning of the cell, as well as the original policy.\nUse an infinite loop that you will quit when the policy has not changed between two iterations. Something like:\nwhile True:\n    # 1 - Policy evaluation\n    for k in range(50):\n        # Update the values\n    \n    # 2 - Policy improvement\n    \n    if pi != pi_old:\n        break\nBeware: if you simply assign the policy to another array and modify the policy:\npi_old = pi\npi[s['high']] = a['search']\npi_old will also change! You need to .copy() the policy.\n\nV = np.zeros(nb_states)\nQ = np.zeros((nb_states, nb_actions))\n\npi = np.array([a['search'], a['search']], dtype=int)\n\nV_high_history = []\nV_low_history = []\n\nt = 1\nwhile True:\n    # Policy evaluation\n    for k in range(50):\n\n        Q[s['high'], a['search']] = alpha * (r_search + gamma * V[s['high']]) + (1 - alpha) * (r_search + gamma*V[s['low']])\n        Q[s['high'], a['wait']] = r_wait + gamma * V[s['high']]\n\n        Q[s['low'], a['search']] = beta * (r_search + gamma * V[s['low']]) + (1 - beta) * (-3 + gamma*V[s['high']])\n        Q[s['low'], a['wait']] = r_wait + gamma * V[s['low']]\n        Q[s['low'], a['recharge']] = gamma * V[s['high']]\n\n        V[s['high']] = Q[s['high'], pi[s['high']]]\n        V[s['low']] = Q[s['low'], pi[s['low']]]\n\n        V_high_history.append(V[s['high']])\n        V_low_history.append(V[s['low']])\n        \n    # Policy improvement\n    pi_old = pi.copy()\n    pi[s['high']] = Q[s['high'], :2].argmax()\n    pi[s['low']] = Q[s['low'], :].argmax()\n    \n    print('Greedy policy after iteration', t)\n    print('pi(high)=', pi[s['high']])\n    print('pi(low)=', pi[s['low']])\n    print('-')\n    \n    # Exit if the policy does not change\n    if pi[s['high']] == pi_old[s['high']] and pi[s['low']] == pi_old[s['low']]:\n        break\n    t += 1\n    \nplt.figure(figsize=(10, 6))   \nplt.plot(V_high_history, label=\"high\")\nplt.plot(V_low_history, label=\"low\")\nplt.legend()\nplt.show()\n\nGreedy policy after iteration 1\npi(high)= 0\npi(low)= 2\n-\nGreedy policy after iteration 2\npi(high)= 0\npi(low)= 2\n-"
  },
  {
    "objectID": "exercises/6-DP-solution.html#value-iteration",
    "href": "exercises/6-DP-solution.html#value-iteration",
    "title": "Dynamic programming",
    "section": "Value iteration",
    "text": "Value iteration\nIn value iteration, we merge the policy evaluation and improvement in a single update rule:\n\n    V (s) \\leftarrow \\max_{a \\in \\mathcal{A}(s)} \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V (s') ]\n\nThe value of state takes the value of its greedy action. The policy is therefore implicitly greedy w.r.t the Q-values.\nThe algorithm becomes:\n\nwhile not converged:\n\nfor all states s:\n\nUpdate the value estimates with:\n\n\n      V (s)  \\leftarrow \\max_{a \\in \\mathcal{A}(s)} \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V (s') ]\n  \n\n\nQ: Modify your previous code to implement value iteration. Use a fixed number of iterations (e.g. 50) as in policy evaluation. Visualize the evolution of the V-values and print the greedy policy after each iteration. Conclude.\n\nV = np.zeros(nb_states)\nQ = np.zeros((nb_states, nb_actions))\n\npi = np.array([a['search'], a['search']], dtype=int)\n\nV_high_history = []\nV_low_history = []\n\nfor k in range(50):\n    \n    # Policy evaluation\n    Q[s['high'], a['search']] = alpha * (r_search + gamma * V[s['high']]) + (1 - alpha) * (r_search + gamma*V[s['low']])\n    Q[s['high'], a['wait']] = r_wait + gamma * V[s['high']]\n\n    Q[s['low'], a['search']] = beta * (r_search + gamma * V[s['low']]) + (1 - beta) * (-3 + gamma*V[s['high']])\n    Q[s['low'], a['wait']] = r_wait + gamma * V[s['low']]\n    Q[s['low'], a['recharge']] = gamma * V[s['high']]\n\n    V[s['high']] = Q[s['high'], :2].max()\n    V[s['low']] = Q[s['low'], :].max()\n\n    V_high_history.append(V[s['high']])\n    V_low_history.append(V[s['low']])\n        \n    # Compute the greedy policy\n    pi_old = pi.copy()\n    pi[s['high']] = Q[s['high'], :2].argmax()\n    pi[s['low']] = Q[s['low'], :].argmax()\n    \nprint('Greedy policy after iteration', k)\nprint('pi(high)=', pi[s['high']])\nprint('pi(low)=', pi[s['low']])\nprint(\"V=\", V)\nprint(\"Q=\", Q)\n    \nplt.figure(figsize=(10, 6))  \nplt.plot(V_high_history, label=\"high\")\nplt.plot(V_low_history, label=\"low\")\nplt.legend()\nplt.show()\n\nGreedy policy after iteration 49\npi(high)= 0\npi(low)= 2\nV= [13.4228186   9.39597296]\nQ= [[13.4228186  11.39597296  0.        ]\n [ 7.63221457  8.57718102  9.39597296]]\n\n\n\n\n\n\n\n\n\nQ: Change the value of the discount factor \\gamma =0.3 so that the agent becomes short-sighted: it only takes into account the immediate rewards, but forgets about the long-term. Does it change the strategy? Explain why.\n\n# Transition probabilities\nalpha = 0.3\nbeta = 0.2\n\n# Discount parameter\ngamma = 0.3\n\n# Expected rewards\nr_search = 6.0\nr_wait = 2.0\n\nV = np.zeros(nb_states)\nQ = np.zeros((nb_states, nb_actions))\n\npi = np.array([a['search'], a['search']], dtype=int)\n\nV_high_history = []\nV_low_history = []\n\nfor k in range(50):\n    \n    # Policy evaluation\n    Q[s['high'], a['search']] = alpha * (r_search + gamma * V[s['high']]) + (1 - alpha) * (r_search + gamma*V[s['low']])\n    Q[s['high'], a['wait']] = r_wait + gamma * V[s['high']]\n\n    Q[s['low'], a['search']] = beta * (r_search + gamma * V[s['low']]) + (1 - beta) * (-3 + gamma*V[s['high']])\n    Q[s['low'], a['wait']] = r_wait + gamma * V[s['low']]\n    Q[s['low'], a['recharge']] = gamma * V[s['high']]\n\n    V[s['high']] = Q[s['high'], :2].max()\n    V[s['low']] = Q[s['low'], :].max()\n\n    V_high_history.append(V[s['high']])\n    V_low_history.append(V[s['low']])\n        \n    # Compute the greedy policy\n    pi_old = pi.copy()\n    pi[s['high']] = Q[s['high'], :2].argmax()\n    pi[s['low']] = Q[s['low'], :].argmax()\n    \nprint('Greedy policy after iteration', k)\nprint('pi(high)=', pi[s['high']])\nprint('pi(low)=', pi[s['low']])\nprint(\"V=\", V)\nprint(\"Q=\", Q)\n    \nplt.figure(figsize=(10, 6))  \nplt.plot(V_high_history, label=\"high\")\nplt.plot(V_low_history, label=\"low\")\nplt.legend()\nplt.show()\n\nGreedy policy after iteration 49\npi(high)= 0\npi(low)= 1\nV= [7.25274725 2.85714286]\nQ= [[7.25274725 4.17582418 0.        ]\n [0.71208791 2.85714286 2.17582418]]\n\n\n\n\n\n\n\n\n\nA: The agent now decides to wait in the low state (r=2) instead of recharging (r=0) and then be in the high state (r=6). The agent is so greedy that it cannot stand not getting reward for one step, although he will collect much more reard later.\nQ: Change \\gamma to 0.99 (far-sighted agent). What does it change and why?\n\n# Transition probabilities\nalpha = 0.3\nbeta = 0.2\n\n# Discount parameter\ngamma = 0.99\n\n# Expected rewards\nr_search = 6.0\nr_wait = 2.0\n\nV = np.zeros(nb_states)\nQ = np.zeros((nb_states, nb_actions))\n\npi = np.array([a['search'], a['search']], dtype=int)\n\nV_high_history = []\nV_low_history = []\n\nfor k in range(50):\n    \n    # Policy evaluation\n    Q[s['high'], a['search']] = alpha * (r_search + gamma * V[s['high']]) + (1 - alpha) * (r_search + gamma*V[s['low']])\n    Q[s['high'], a['wait']] = r_wait + gamma * V[s['high']]\n\n    Q[s['low'], a['search']] = beta * (r_search + gamma * V[s['low']]) + (1 - beta) * (-3 + gamma*V[s['high']])\n    Q[s['low'], a['wait']] = r_wait + gamma * V[s['low']]\n    Q[s['low'], a['recharge']] = gamma * V[s['high']]\n\n    V[s['high']] = Q[s['high'], :2].max()\n    V[s['low']] = Q[s['low'], :].max()\n\n    V_high_history.append(V[s['high']])\n    V_low_history.append(V[s['low']])\n        \n    # Compute the greedy policy\n    pi_old = pi.copy()\n    pi[s['high']] = Q[s['high'], :2].argmax()\n    pi[s['low']] = Q[s['low'], :].argmax()\n    \nprint('Greedy policy after iteration', k)\nprint('pi(high)=', pi[s['high']])\nprint('pi(low)=', pi[s['low']])\nprint(\"V=\", V)\nprint(\"Q=\", Q)\n    \nplt.figure(figsize=(10, 6))  \nplt.plot(V_high_history, label=\"high\")\nplt.plot(V_low_history, label=\"low\")\nplt.legend()\nplt.show()\n\nGreedy policy after iteration 49\npi(high)= 0\npi(low)= 2\nV= [141.37219244 137.82818773]\nQ= [[141.37219244 139.82818773   0.        ]\n [135.92647479 136.31962304 137.82818773]]\n\n\n\n\n\n\n\n\n\nA: The optimal policy stays the same (search in high, recharge in low) but the V values grow very high. The difference between the values of the high and low state is comparatively very small: the high state is always only one action away from the low state, it is nothing with such a high gamma.\nQ: Change the parameters to:\n\\alpha = 0.01 \\quad \\beta = 0.2 \\quad \\gamma = 0.7 \\quad \\mathcal{R}^{\\text{search}} = 6 \\quad  \\mathcal{R}^{\\text{wait}} = 5\nFind the optimal policy. What is the optimal action to be taken in the state high, although the probability to stay in this state is very small? Why?\n\n# Transition probabilities\nalpha = 0.01\nbeta = 0.2\n\n# Discount parameter\ngamma = 0.7\n\n# Expected rewards\nr_search = 6.0\nr_wait = 5.0\n\nV = np.zeros(nb_states)\nQ = np.zeros((nb_states, nb_actions))\n\npi = np.array([a['search'], a['search']], dtype=int)\n\nV_high_history = []\nV_low_history = []\n\nfor k in range(50):\n    \n    # Policy evaluation\n    Q[s['high'], a['search']] = alpha * (r_search + gamma * V[s['high']]) + (1 - alpha) * (r_search + gamma*V[s['low']])\n    Q[s['high'], a['wait']] = r_wait + gamma * V[s['high']]\n\n    Q[s['low'], a['search']] = beta * (r_search + gamma * V[s['low']]) + (1 - beta) * (-3 + gamma*V[s['high']])\n    Q[s['low'], a['wait']] = r_wait + gamma * V[s['low']]\n    Q[s['low'], a['recharge']] = gamma * V[s['high']]\n\n    V[s['high']] = Q[s['high'], :2].max()\n    V[s['low']] = Q[s['low'], :].max()\n\n    V_high_history.append(V[s['high']])\n    V_low_history.append(V[s['low']])\n        \n    # Compute the greedy policy\n    pi_old = pi.copy()\n    pi[s['high']] = Q[s['high'], :2].argmax()\n    pi[s['low']] = Q[s['low'], :].argmax()\n    \nprint('Greedy policy after iteration', k)\nprint('pi(high)=', pi[s['high']])\nprint('pi(low)=', pi[s['low']])\nprint(\"V=\", V)\nprint(\"Q=\", Q)\n    \nplt.figure(figsize=(10, 6))  \nplt.plot(V_high_history, label=\"high\")\nplt.plot(V_low_history, label=\"low\")\nplt.legend()\nplt.show()\n\nGreedy policy after iteration 49\npi(high)= 0\npi(low)= 1\nV= [17.67371571 16.66666637]\nQ= [[17.67371571 17.37160091  0.        ]\n [11.030614   16.66666637 12.37160091]]\n\n\n\n\n\n\n\n\n\nA: The agent now decides to wait in the low state and accumulate quite a lot of rewards (r=5, compared to 6 while searching). But it is still worth searching in the high state: even if we transition immediately into low with 99% probability, one still gets 6 instead of 5, so the return is higher than when waiting in high.\nQ: Find a set of parameters where it would be optimal to search while in the low state.\n\n# Transition probabilities\nalpha = 0.01\nbeta = 0.8\n\n# Discount parameter\ngamma = 0.7\n\n# Expected rewards\nr_search = 10.0\nr_wait = 5.0\n\nV = np.zeros(nb_states)\nQ = np.zeros((nb_states, nb_actions))\n\npi = np.array([a['search'], a['search']], dtype=int)\n\nV_high_history = []\nV_low_history = []\n\nfor k in range(50):\n    \n    # Policy evaluation\n    Q[s['high'], a['search']] = alpha * (r_search + gamma * V[s['high']]) + (1 - alpha) * (r_search + gamma*V[s['low']])\n    Q[s['high'], a['wait']] = r_wait + gamma * V[s['high']]\n\n    Q[s['low'], a['search']] = beta * (r_search + gamma * V[s['low']]) + (1 - beta) * (-3 + gamma*V[s['high']])\n    Q[s['low'], a['wait']] = r_wait + gamma * V[s['low']]\n    Q[s['low'], a['recharge']] = gamma * V[s['high']]\n\n    V[s['high']] = Q[s['high'], :2].max()\n    V[s['low']] = Q[s['low'], :].max()\n\n    V_high_history.append(V[s['high']])\n    V_low_history.append(V[s['low']])\n        \n    # Compute the greedy policy\n    pi_old = pi.copy()\n    pi[s['high']] = Q[s['high'], :2].argmax()\n    pi[s['low']] = Q[s['low'], :].argmax()\n    \nprint('Greedy policy after iteration', k)\nprint('pi(high)=', pi[s['high']])\nprint('pi(low)=', pi[s['low']])\nprint(\"V=\", V)\nprint(\"Q=\", Q)\n    \nplt.figure(figsize=(10, 6))  \nplt.plot(V_high_history, label=\"high\")\nplt.plot(V_low_history, label=\"low\")\nplt.legend()\nplt.show()\n\nGreedy policy after iteration 49\npi(high)= 0\npi(low)= 0\nV= [28.03236199 25.7375694 ]\nQ= [[28.03236199 24.62265325  0.        ]\n [25.7375694  23.01629844 19.62265325]]"
  },
  {
    "objectID": "exercises/7-Gym-solution.html",
    "href": "exercises/7-Gym-solution.html",
    "title": "Gym environments",
    "section": "",
    "text": "In this course, we will mostly address RL environments available in the OpenAI Gym framework:\nhttps://gym.openai.com\nIt provides a multitude of RL problems, from simple text-based problems with a few dozens of states (Gridworld, Taxi) to continuous control problems (Cartpole, Pendulum) to Atari games (Breakout, Space Invaders) to complex robotics simulators (Mujoco):\nhttps://gym.openai.com/envs\nHowever, gym is not maintained by OpenAI anymore since September 2022. We will use instead the gymnasium library maintained by the Farama foundation, which will keep on maintaining and improving the library.\nhttps://gymnasium.farama.org/\nYou can install gymnasium and its dependencies using:\npip install -U gymnasium pygame moviepy\npip install gymnasium[classic_control]\npip install gymnasium[box2d]\nFor this exercise and the following, we will focus on simple environments whose installation is straightforward: toy text, classic control and box2d. More complex environments based on Atari games or the Mujoco physics simulator are described in the last (optional) section of this notebook, as they require additional dependencies.\nOn colab, gym cannot open graphical windows for visualizing the environments, as it is not possible in the browser. We will see a workaround allowing to produce videos. Running that cell in colab should allow you to run the simplest environments:\n\ntry:\n    import google.colab\n    IN_COLAB = True\nexcept:\n    IN_COLAB = False\n\nif IN_COLAB:\n    !pip install -U gymnasium pygame moviepy\n    !pip install gymnasium[box2d]\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\nimport gymnasium as gym\nprint(\"gym version:\", gym.__version__)\n\nfrom moviepy.editor import ImageSequenceClip, ipython_display\n\nclass GymRecorder(object):\n    \"\"\"\n    Simple wrapper over moviepy to generate a .gif with the frames of a gym environment.\n    \n    The environment must have the render_mode `rgb_array_list`.\n    \"\"\"\n    def __init__(self, env):\n        self.env = env\n        self._frames = []\n\n    def record(self, frames):\n        \"To be called at the end of an episode.\"\n        for frame in frames:\n            self._frames.append(np.array(frame))\n\n    def make_video(self, filename):\n        \"Generates the gif video.\"\n        directory = os.path.dirname(os.path.abspath(filename))\n        if not os.path.exists(directory):\n            os.mkdir(directory)\n        self.clip = ImageSequenceClip(list(self._frames), fps=self.env.metadata[\"render_fps\"])\n        self.clip.write_gif(filename, fps=self.env.metadata[\"render_fps\"])\n        del self._frames\n        self._frames = []\n\ngym version: 0.26.3"
  },
  {
    "objectID": "exercises/7-Gym-solution.html#interacting-with-an-environment",
    "href": "exercises/7-Gym-solution.html#interacting-with-an-environment",
    "title": "Gym environments",
    "section": "Interacting with an environment",
    "text": "Interacting with an environment\nA gym environment is created using:\nenv = gym.make('CartPole-v1', render_mode=\"human\")\nwhere ‘CartPole-v1’ should be replaced by the environment you want to interact with. The following cell lists the environments available to you (including the different versions).\n\nfor env in gym.envs.registry.items():\n    print(env[0])\n\nCartPole-v0\nCartPole-v1\nMountainCar-v0\nMountainCarContinuous-v0\nPendulum-v1\nAcrobot-v1\nLunarLander-v2\nLunarLanderContinuous-v2\nBipedalWalker-v3\nBipedalWalkerHardcore-v3\nCarRacing-v2\nBlackjack-v1\nFrozenLake-v1\nFrozenLake8x8-v1\nCliffWalking-v0\nTaxi-v3\nReacher-v2\nReacher-v4\nPusher-v2\nPusher-v4\nInvertedPendulum-v2\nInvertedPendulum-v4\nInvertedDoublePendulum-v2\nInvertedDoublePendulum-v4\nHalfCheetah-v2\nHalfCheetah-v3\nHalfCheetah-v4\nHopper-v2\nHopper-v3\nHopper-v4\nSwimmer-v2\nSwimmer-v3\nSwimmer-v4\nWalker2d-v2\nWalker2d-v3\nWalker2d-v4\nAnt-v2\nAnt-v3\nAnt-v4\nHumanoid-v2\nHumanoid-v3\nHumanoid-v4\nHumanoidStandup-v2\nHumanoidStandup-v4\nGymV26Environment-v0\n\n\nThe render_mode argument defines how you will see the environment:\n\nNone (default): allows to train a DRL algorithm without wasting computational resources rendering it.\nrgb_array_list: allows to get numpy arrays corresponding to each frame. Will be useful when generating videos.\nansi: string representation of each state. Only available for the “Toy text” environments.\nhuman: graphical window displaying the environment live.\n\nThe main interest of gym(nasium) is that all problems have a common interface defined by the class gym.Env. There are only three methods that have to be used when interacting with an environment:\n\nstate, info = env.reset() restarts the environment and returns an initial state s_0.\nstate, reward, terminal, truncated, info = env.step(action) takes an action a_t and returns:\n\nthe new state s_{t+1},\nthe reward r_{t+1},\ntwo boolean flags indicating whether the current state is terminal (won/lost) or truncated (timeout),\na dictionary containing additional info for debugging (you can ignore it most of the time).\n\nenv.render() displays the current state of the MDP. When the render mode is set to rgb_array_list or human, it does not even have to called explicitly (since gym 0.25).\n\nWith this interface, we can interact with the environment in a standardized way:\n\nWe first create the environment.\nFor a fixed number of episodes:\n\nWe pick an initial state with reset().\nUntil the episode is terminated:\n\nWe select an action using our RL algorithm or randomly.\nWe take that action (step()), observe the new state and the reward.\nWe go into the new state.\n\n\n\nThe following cell shows how to interact with the CartPole environment using a random policy. Note that it will only work on your computer, not in colab.\n\nenv = gym.make('CartPole-v1', render_mode=\"human\")\n\nfor episode in range(10):\n    state, info = env.reset()\n    done = False\n    while not done:\n        # Select an action randomly\n        action = env.action_space.sample()\n        \n        # Sample a single transition\n        next_state, reward, terminal, truncated, info = env.step(action)\n        \n        # Go in the next state\n        state = next_state\n\n        # End of the episode\n        done = terminal or truncated\n\nenv.close()\n\nOn colab (or whenever you want to record videos of the episodes instead of watching them live), you need to create the environment with the rendering mode rgb_array_list.\nYou then create a GymRecorder object (defined in the first cell of this notebook).\nrecorder = GymRecorder(env)\nAt the end of each episode, you tell the recorder to record all frames generated during the episode. The frames returned by env.render() are (width, height, 3) numpy arrays which are accumulated by the environment during the episode and flushed when env.reset() is called.\nif done:\n    recorder.record(env.render())\nYou can then generate a gif at the end of the simulation with:\nrecorder.make_video('videos/CartPole-v1.gif')\nFinally, you can render the gif in the notebook by calling at the very last line of the cell:\nipython_display('videos/CartPole-v1.gif')\n\nenv = gym.make('CartPole-v1', render_mode=\"rgb_array_list\")\nrecorder = GymRecorder(env)\n\nfor episode in range(10):\n    state, info = env.reset()\n\n    done = False\n    while not done:\n        # Select an action randomly\n        action = env.action_space.sample()\n        \n        # Sample a single transition\n        next_state, reward, terminal, truncated, info = env.step(action)\n        \n        # Go in the next state\n        state = next_state\n\n        # End of the episode\n        done = terminal or truncated\n\n        if done:\n            recorder.record(env.render())\n\nrecorder.make_video('videos/CartPole-v1.gif')\nipython_display('videos/CartPole-v1.gif', autoplay=1, loop=1)\n\nMoviePy - Building file videos/CartPole-v1.gif with imageio.\n\n\n                                                              \n\n\n\n\n\nEach environment defines its state space (env.observation_space) and action space (env.action_space).\nState and action spaces can either be :\n\ndiscrete (gym.spaces.Discrete(nb_states)), with states being an integer between 0 and nb_states -1.\nfeature-based (gym.spaces.Box(low=0, high=255, shape=(SCREEN_HEIGHT, SCREEN_WIDTH, 3))) for pixel frames.\ncontinuous. Example for two joints of a robotic arm limited between -180 and 180 degrees:\n\ngym.spaces.Box(-180.0, 180.0, (2, ))\nYou can sample a state or action randomly from these spaces:\n\naction_space = gym.spaces.Box(-180.0, 180.0, (2, ))\naction = action_space.sample()\nprint(action)\n\n[-113.33049   83.84796]\n\n\nSampling the action space is particularly useful for exploration. We use it here to perform random (but valid) actions:\naction = env.action_space.sample()\nQ: Create a method random_interaction(env, number_episodes, recorder=None) that takes as arguments:\n\nThe environment.\nThe number of episodes to be performed.\nAn optional GymRecorder object that may record the frames of the environment if it is not None (if renderer is not None:). Otherwise, do not nothing.\n\nThe method should return the list of undiscounted returns (\\gamma=1, i.e. just the sum of rewards obtained during each episode) for all episodes.\n\ndef random_interaction(env, number_episodes, recorder=None):\n\n    returns = []\n\n    # Sample episodes\n    for episode in range(number_episodes):\n\n        # Sample the initial state\n        state, info = env.reset()\n\n        return_episode = 0.0\n        done = False\n        while not done:\n\n            # Select an action randomly\n            action = env.action_space.sample()\n        \n            # Sample a single transition\n            next_state, reward, terminal, truncated, info = env.step(action)\n\n            # Update the return\n            return_episode += reward\n        \n            # Go in the next state\n            state = next_state\n\n            # End of the episode\n            done = terminal or truncated\n\n            # Record\n            if recorder is not None:\n                recorder.record(env.render())\n\n        returns.append(return_episode)\n\n    return returns\n\nQ: Use that method to visualize all the available simple environments for a few episodes:\n\nCartPole-v1\nMountainCar-v0\nPendulum-v1\nAcrobot-v1\nLunarLander-v2\nBipedalWalker-v3\nCarRacing-v2\nBlackjack-v1\nFrozenLake-v1\nCliffWalking-v0\nTaxi-v3\n\nIf you do many episodes (CarRacing or Taxi have very long episodes with a random policy), plot the obtained returns to see how they vary.\nIf you managed to install the mujoco and atari dependencies, feel free to visualize them too.\nTo understand what an environment is about, run help(env).\n\nenvname = 'CartPole-v1'\nenv = gym.make(envname, render_mode=\"rgb_array_list\")\nrecorder = GymRecorder(env)\n\nreturns = random_interaction(env, 10, recorder)\n\nplt.figure(figsize=(10, 6))\nplt.plot(returns)\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"Return\")\nplt.show()\n\nvideo = \"videos/\" + envname + \".gif\"\nrecorder.make_video(video)\nipython_display(video)\n\n\n\n\n\n\n\n\nMoviePy - Building file videos/CartPole-v1.gif with imageio.\n\n\n                                                              \n\n\n\n\n\n\nenvname = 'CarRacing-v2'\nenv = gym.make(envname, render_mode=\"rgb_array_list\")\nrecorder = GymRecorder(env)\n\nreturns = random_interaction(env, 1, recorder)\n\nvideo = \"videos/\" + envname + \".gif\"\nrecorder.make_video(video)\nipython_display(video)\n\n\nenvname = 'Taxi-v3'\nenv = gym.make(envname, render_mode=\"rgb_array_list\")\nrecorder = GymRecorder(env)\n\nreturns = random_interaction(env, 1, recorder)\n\nvideo = \"videos/\" + envname + \".gif\"\nrecorder.make_video(video)\nipython_display(video)"
  },
  {
    "objectID": "exercises/7-Gym-solution.html#creating-your-own-environment",
    "href": "exercises/7-Gym-solution.html#creating-your-own-environment",
    "title": "Gym environments",
    "section": "Creating your own environment",
    "text": "Creating your own environment\n\nRandom environment\nYou can create your own environment using the gym interface:\nhttps://gymnasium.farama.org/tutorials/environment_creation/\nHere is an example of a dummy environment with discrete states and actions, where the transition probabilities and rewards are completely random:\n\nclass RandomEnv(gym.Env):\n    \"Random discrete environment that does nothing.\"\n    \n    metadata = {\"render_modes\": [\"ansi\"], \"render_fps\": 1}\n\n    def __init__(self, nb_states, nb_actions, max_episode_steps=10, render_mode=\"ansi\"):\n\n        self.nb_states = nb_states\n        self.nb_actions = nb_actions\n        self.max_episode_steps = max_episode_steps\n        self.render_mode = render_mode\n\n        # State space, can be discrete or continuous.\n        self.observation_space = gym.spaces.Discrete(nb_states)\n        \n        # Action space, can be discrete or continuous.\n        self.action_space = gym.spaces.Discrete(nb_actions)    \n\n        # Reset\n        self.reset()\n\n\n    def reset(self, seed=None, options=None):\n\n        # Re-initialize time\n        self.current_step = 0\n        \n        # Sample one state randomly \n        self.state = self.observation_space.sample()\n        \n        return self.state, info\n\n    def step(self, action):\n\n        # Random transition to another state\n        self.state = self.observation_space.sample() \n        \n        # Random reward\n        reward = np.random.uniform(0, 1, 1)[0] \n        \n        # Terminate the episode after 10 steps\n        terminal = False \n        truncated = False\n\n        self.current_step +=1\n        if self.current_step % self.max_episode_steps == 0:\n            truncated = True \n\n        info = {} # No info\n\n        return self.state, reward, terminal, truncated, info\n\n\n    def render(self):\n        if self.render_mode == \"ansi\":\n            description = \"Step \" + str(self.current_step) + \": state \" + str(self.state)\n            return description\n        return None\n\nThe different methods should be quite self-explanatory.\nmetadata defines which render modes are available for this environment (here only the text mode “ansi”).\nThe constructor accepts the size of the state and action spaces as arguments, the duration of the episode and the render mode.\nreset() samples an initial state randomly.\nstep() ignores the action, samples a new state and a reward, and truncates an episode after max_episode_steps.\nrender() returns a string with the current state.\nQ: Interact with the random environment for a couple of episodes.\nAs the mode is ansi (text-based), you will need to print the string returned by render() after each step:\nwhile not done:\n\n    action = env.action_space.sample()\n    \n    next_state, reward, terminal, truncated, info = env.step(action)\n\n    print(env.render())\n\n# Create the environment\nenv = RandomEnv(nb_states=10, nb_actions=4)\n\n# Sample episodes\nfor episode in range(2):\n\n    print(\"Episode\", episode)\n\n    # Sample the initial state\n    state, info = env.reset()\n\n    # Render the initial state\n    print(env.render())\n\n    # Episode\n    return_episode = 0.0\n    done = False\n    while not done:\n        # Select an action randomly\n        action = env.action_space.sample()\n        \n        # Sample a single transition\n        next_state, reward, terminal, truncated, info = env.step(action)\n        \n        # Go in the next state\n        state = next_state\n\n        # Update return\n        return_episode += reward\n\n        # Render the current state\n        print(env.render())\n\n        # End of the episode\n        done = terminal or truncated\n    \n    print(\"Return of the episode:\", return_episode)\n    print('-'*10)\n\nEpisode 0\nStep 0: state 1\nStep 1: state 9\nStep 2: state 5\nStep 3: state 8\nStep 4: state 4\nStep 5: state 0\nStep 6: state 0\nStep 7: state 8\nStep 8: state 0\nStep 9: state 4\nStep 10: state 0\nReturn of the episode: 5.1922292854731875\n----------\nEpisode 1\nStep 0: state 9\nStep 1: state 6\nStep 2: state 5\nStep 3: state 3\nStep 4: state 9\nStep 5: state 6\nStep 6: state 0\nStep 7: state 9\nStep 8: state 1\nStep 9: state 6\nStep 10: state 6\nReturn of the episode: 4.080812142958859\n----------\n\n\n\n\nRecycling robot\nQ: Create a RecyclingRobot gym-like environment using last week’s exercise.\nThe parameters alpha, beta, r_wait and r_search should be passed to the constructor of the environment and saved as attributes.\nThe state space is discrete, with two states high and low which will have indices 0 and 1. The three discrete actions search, wait and recharge have indices 0, 1, and 2.\nThe initial state of the MDP (reset()) should be the high state.\nThe step() should generate transitions according to the dynamics of the MDP. Depending on the current state and the chosen action, make a transition to another state. For the actions search and wait, sample the reward from the normal distribution with mean r_search (resp. r_wait) and variance 0.5.\nIf the random agent selects recharge in high, do nothing (next state is high, reward is 0).\nRendering is just printing the current state. There is nothing to close, so you do not even need to redefine the function.\nAlthough the recycling robot is a continuing task, limit the number of steps per episode to 10, as in the the previous random environment.\nInteract randomly with the MDP for several episodes and observe the returns.\n\nclass RecyclingRobot(gym.Env):\n    \"Recycling robot environment.\"\n\n    metadata = {\"render_modes\": [\"ansi\"], \"render_fps\": 1}\n\n    def __init__(self, alpha, beta, r_search, r_wait, max_episode_steps=10, render_mode=\"ansi\"):\n        \n        # Store parameters\n        self.alpha = alpha\n        self.beta = beta\n        self.r_search = r_search\n        self.r_wait = r_wait\n        self.max_episode_steps = max_episode_steps\n        self.render_mode = render_mode\n        \n        # State space, can be discrete or continuous.\n        self.observation_space = gym.spaces.Discrete(2)        \n        self.states =  ['high', 'low']\n        \n        # Action space, can be discrete or continuous.\n        self.action_space = gym.spaces.Discrete(3)  \n        self.actions = ['search', 'wait', 'recharge']  \n\n        # Reset\n        self.reset()\n    \n    def reset(self, seed=None, options=None):\n        \n        # Re-initialize time\n        self.current_step = 0\n\n        # Start in the high state\n        self.state = 0\n        \n        return self.state, {}\n    \n    def step(self, action):\n        \n        if self.state == 0: # high\n            if action == 0: # search\n                p = np.random.rand()\n                if p < self.alpha:\n                    self.state = 0 # high\n                else:\n                    self.state = 1 # low\n                self.reward = float(np.random.normal(self.r_search, 0.5, 1))\n            elif action == 1: # wait\n                self.state = 0 # high\n                self.reward = float(np.random.normal(self.r_wait, 0.5, 1))\n            elif action == 2: # recharge\n                self.state = 0 # high\n                self.reward = 0.0\n                \n        elif self.state == 1: # low\n            if action == 0: # search\n                p = np.random.rand()\n                if p < self.beta:\n                    self.state = 1 # low\n                    self.reward = float(np.random.normal(self.r_search, 0.5, 1))\n                else:\n                    self.state = 0 # high\n                    self.reward = -3.0\n            elif action == 1: # wait\n                self.state = 1 # low\n                self.reward = float(np.random.normal(self.r_wait, 0.5, 1))\n            elif action == 2: # recharge\n                self.state = 0 # high\n                self.reward = 0.0\n        \n        terminal = False\n        truncated = False\n        self.current_step +=1\n        if self.current_step % self.max_episode_steps == 0:\n            truncated = True \n\n        info = {} # No info\n\n        return self.state, self.reward, terminal, truncated, info\n\n    def render(self):\n        \n        if self.render_mode == \"ansi\":\n            description = \"Step \" + str(self.current_step) + \": state \" + self.states[self.state]\n            return description\n            \n        return None\n\n\n# Create the environment\nenv = RecyclingRobot(alpha=0.3, beta=0.2, r_search=6, r_wait=2)\n\n# Sample episodes\nfor episode in range(10):\n\n    print(\"Episode:\", episode)\n\n    # Sample the initial state\n    state, info = env.reset()\n    print(env.render())\n\n    return_episode = 0.0\n    done = False\n    while not done:\n\n        # Select an action randomly\n        action = env.action_space.sample()\n        \n        # Sample a single transition\n        next_state, reward, terminal, truncated, info = env.step(action)\n        \n        print(env.states[state], \"+\", env.actions[action], \"->\", env.states[next_state], \":\", reward)\n        \n        # Go in the next state\n        state = next_state\n\n        # Update return\n        return_episode += reward\n\n        # Render the current state\n        print(env.render())\n\n        # End of the episode\n        done = terminal or truncated\n    \n    print(\"Return of the episode:\", return_episode)\n    print('-'*10)\n\nEpisode: 0\nStep 0: state high\nhigh + wait -> high : 0.9926896172638973\nStep 1: state high\nhigh + search -> high : 6.044639393966334\nStep 2: state high\nhigh + wait -> high : 1.7843696484829583\nStep 3: state high\nhigh + wait -> high : 2.0670005955585715\nStep 4: state high\nhigh + search -> low : 5.957491732182248\nStep 5: state low\nlow + recharge -> high : 0.0\nStep 6: state high\nhigh + search -> low : 5.457678766952196\nStep 7: state low\nlow + wait -> low : 1.8188830295071319\nStep 8: state low\nlow + wait -> low : 1.3876912287573466\nStep 9: state low\nlow + wait -> low : 1.6819199961846023\nStep 10: state low\nReturn of the episode: 27.192364008855286\n----------\nEpisode: 1\nStep 0: state high\nhigh + search -> high : 5.572138533547902\nStep 1: state high\nhigh + wait -> high : 1.1486508555847896\nStep 2: state high\nhigh + wait -> high : 2.279889263285329\nStep 3: state high\nhigh + search -> low : 6.494055697805454\nStep 4: state low\nlow + recharge -> high : 0.0\nStep 5: state high\nhigh + wait -> high : 1.8047539666845087\nStep 6: state high\nhigh + recharge -> high : 0.0\nStep 7: state high\nhigh + recharge -> high : 0.0\nStep 8: state high\nhigh + recharge -> high : 0.0\nStep 9: state high\nhigh + search -> low : 4.971823740828002\nStep 10: state low\nReturn of the episode: 22.271312057735983\n----------\nEpisode: 2\nStep 0: state high\nhigh + wait -> high : 1.4954032519891451\nStep 1: state high\nhigh + wait -> high : 2.615658976896022\nStep 2: state high\nhigh + search -> low : 5.870728209883454\nStep 3: state low\nlow + recharge -> high : 0.0\nStep 4: state high\nhigh + recharge -> high : 0.0\nStep 5: state high\nhigh + recharge -> high : 0.0\nStep 6: state high\nhigh + recharge -> high : 0.0\nStep 7: state high\nhigh + wait -> high : 1.7318712411130477\nStep 8: state high\nhigh + search -> low : 5.869598042680883\nStep 9: state low\nlow + search -> high : -3.0\nStep 10: state high\nReturn of the episode: 14.583259722562552\n----------\nEpisode: 3\nStep 0: state high\nhigh + wait -> high : 1.8704400202294913\nStep 1: state high\nhigh + wait -> high : 2.199151786189652\nStep 2: state high\nhigh + recharge -> high : 0.0\nStep 3: state high\nhigh + search -> high : 6.374117907859067\nStep 4: state high\nhigh + wait -> high : 2.047224854037378\nStep 5: state high\nhigh + recharge -> high : 0.0\nStep 6: state high\nhigh + search -> low : 6.388941445413761\nStep 7: state low\nlow + search -> low : 5.210137698665632\nStep 8: state low\nlow + search -> high : -3.0\nStep 9: state high\nhigh + search -> high : 5.855027701534508\nStep 10: state high\nReturn of the episode: 26.945041413929484\n----------\nEpisode: 4\nStep 0: state high\nhigh + wait -> high : 1.631665329384869\nStep 1: state high\nhigh + search -> low : 5.319715749856758\nStep 2: state low\nlow + search -> high : -3.0\nStep 3: state high\nhigh + wait -> high : 2.378998995127933\nStep 4: state high\nhigh + search -> low : 5.806651697786137\nStep 5: state low\nlow + wait -> low : 2.129664234675324\nStep 6: state low\nlow + recharge -> high : 0.0\nStep 7: state high\nhigh + wait -> high : 2.834240916466443\nStep 8: state high\nhigh + wait -> high : 1.7997698349447324\nStep 9: state high\nhigh + search -> high : 6.202732964448335\nStep 10: state high\nReturn of the episode: 25.10343972269053\n----------\nEpisode: 5\nStep 0: state high\nhigh + search -> low : 5.9965509795066\nStep 1: state low\nlow + wait -> low : 1.2950486166075321\nStep 2: state low\nlow + recharge -> high : 0.0\nStep 3: state high\nhigh + recharge -> high : 0.0\nStep 4: state high\nhigh + search -> low : 5.927555171389823\nStep 5: state low\nlow + recharge -> high : 0.0\nStep 6: state high\nhigh + search -> low : 5.320303212893826\nStep 7: state low\nlow + search -> high : -3.0\nStep 8: state high\nhigh + search -> high : 5.734416339406706\nStep 9: state high\nhigh + wait -> high : 1.8590910729746617\nStep 10: state high\nReturn of the episode: 23.13296539277915\n----------\nEpisode: 6\nStep 0: state high\nhigh + search -> low : 5.240399718468027\nStep 1: state low\nlow + wait -> low : 2.660844737989303\nStep 2: state low\nlow + search -> high : -3.0\nStep 3: state high\nhigh + wait -> high : 2.148465254283021\nStep 4: state high\nhigh + wait -> high : 2.9607453094916503\nStep 5: state high\nhigh + recharge -> high : 0.0\nStep 6: state high\nhigh + wait -> high : 1.5670146240326495\nStep 7: state high\nhigh + search -> low : 6.939658729753589\nStep 8: state low\nlow + search -> low : 5.9179485235605815\nStep 9: state low\nlow + wait -> low : 2.459387109208085\nStep 10: state low\nReturn of the episode: 26.894464006786908\n----------\nEpisode: 7\nStep 0: state high\nhigh + recharge -> high : 0.0\nStep 1: state high\nhigh + wait -> high : 1.8299745995503027\nStep 2: state high\nhigh + search -> low : 6.239327064020724\nStep 3: state low\nlow + search -> high : -3.0\nStep 4: state high\nhigh + search -> low : 6.3967660461499385\nStep 5: state low\nlow + recharge -> high : 0.0\nStep 6: state high\nhigh + search -> low : 6.486763134990279\nStep 7: state low\nlow + recharge -> high : 0.0\nStep 8: state high\nhigh + recharge -> high : 0.0\nStep 9: state high\nhigh + recharge -> high : 0.0\nStep 10: state high\nReturn of the episode: 17.952830844711244\n----------\nEpisode: 8\nStep 0: state high\nhigh + recharge -> high : 0.0\nStep 1: state high\nhigh + recharge -> high : 0.0\nStep 2: state high\nhigh + search -> low : 5.756093041793521\nStep 3: state low\nlow + recharge -> high : 0.0\nStep 4: state high\nhigh + recharge -> high : 0.0\nStep 5: state high\nhigh + recharge -> high : 0.0\nStep 6: state high\nhigh + search -> high : 6.857965211331541\nStep 7: state high\nhigh + search -> low : 4.548730998859437\nStep 8: state low\nlow + search -> high : -3.0\nStep 9: state high\nhigh + search -> low : 5.911527753326792\nStep 10: state low\nReturn of the episode: 20.07431700531129\n----------\nEpisode: 9\nStep 0: state high\nhigh + recharge -> high : 0.0\nStep 1: state high\nhigh + wait -> high : 2.4696783160136264\nStep 2: state high\nhigh + wait -> high : 1.5966320897550017\nStep 3: state high\nhigh + recharge -> high : 0.0\nStep 4: state high\nhigh + search -> high : 6.050054935064024\nStep 5: state high\nhigh + search -> high : 5.895240773540324\nStep 6: state high\nhigh + recharge -> high : 0.0\nStep 7: state high\nhigh + recharge -> high : 0.0\nStep 8: state high\nhigh + wait -> high : 1.7429824248639083\nStep 9: state high\nhigh + recharge -> high : 0.0\nStep 10: state high\nReturn of the episode: 17.75458853923688\n----------\n\n\n\n\nRandom agent\nTo be complete, let’s implement the random agent as a class. The class should look like:\nclass RandomAgent:\n    \"\"\"\n    Random agent exploring uniformly the environment.\n    \"\"\"\n    \n    def __init__(self, env):\n        self.env = env\n    \n    def act(self, state):\n        \"Returns a random action by sampling the action space.\"\n        action = # TODO\n        return action\n    \n    def update(self, state, action, reward, next_state):\n        \"Updates the agent using the transition (s, a, r, s').\"\n        pass\n    \n    def train(self, nb_episodes, render=False):\n        \"Runs the agent on the environment for nb_episodes. Returns the list of obtained returns.\"\n        \n        # List of returns\n        returns = []\n\n        # TODO\n            \n        return returns\nThe environment is passed to the constructor. act(state) should sample a random action. update(state, action, reward, next_state) does nothing for the random agent (pass is a Python command doing nothing), but we will implement it in the next exercises.\ntrain(nb_episodes, render) implements the interaction loop between the agent and the environment for a fixed number of episodes. It should return the list of obtained returns. render defines whether you print the state at each step or not.\nQ: Implement the random agent and have it interact with the environment for a fixed number of episodes.\n\nclass RandomAgent:\n    \"\"\"\n    Random agent exploring uniformly the environment.\n    \"\"\"\n    \n    def __init__(self, env):\n        self.env = env\n    \n    def act(self, state):\n        \"Returns a random action by sampling the action space.\"\n        return self.env.action_space.sample()\n    \n    def update(self, state, action, reward, next_state):\n        \"Updates the agent using the transition (s, a, r, s').\"\n        pass\n    \n    def train(self, nb_episodes, render=False):\n        \"Runs the agent on the environment for nb_episodes. Returns the list of obtained rewards.\"\n        # List of returns\n        returns = []\n\n        for episode in range(nb_episodes):\n            if render:\n                print(\"Episode:\", episode)\n\n            # Sample the initial state\n            state, info = self.env.reset()\n            if render:\n                print(self.env.render())\n\n            return_episode = 0.0\n            done = False\n            while not done:\n\n                # Select an action randomly\n                action = self.env.action_space.sample()\n                \n                # Sample a single transition\n                next_state, reward, terminal, truncated, info = self.env.step(action)\n                \n                # Go in the next state\n                state = next_state\n\n                # Update return\n                return_episode += reward\n\n                # Render the current state\n                if render:\n                    print(env.render())\n\n                # End of the episode\n                done = terminal or truncated\n            \n            returns.append(return_episode)\n\n        return returns\n\n\n# Create the environment\nenv = RecyclingRobot(alpha=0.3, beta=0.2, r_search=6, r_wait=2)\n\n# Creating the random agent\nagent = RandomAgent(env)\n\n# Train the agent for 10 episodes\nreturns = agent.train(10)\n\n\n# Plot the rewards\nplt.figure(figsize=(10, 6))\nplt.plot(returns)\nplt.xlabel(\"Steps\")\nplt.ylabel(\"Return\")\nplt.show()\n\n\n\n\n\n\n\n\nThat’s it! We now “only” need to define classes for all the sampling-based RL algorithms (MC, TD, deep RL) and we can interact with any environment with a single line!"
  },
  {
    "objectID": "exercises/7-Gym-solution.html#mujoco-and-atari-environments",
    "href": "exercises/7-Gym-solution.html#mujoco-and-atari-environments",
    "title": "Gym environments",
    "section": "Mujoco and Atari environments",
    "text": "Mujoco and Atari environments\nNote: both mujoco and atari environments will not work on colab.\nYou may have to install non-Python packages on your computer, such as openGL. A lot of debugging in sight…\nThe environments should work under Linux and MacOS, but I am not sure about windows.\n\nMujoco\nTo install the mujoco environments of gymnasium, this should work:\npip install mujoco\npip install gymnasium[mujoco]\nInteraction should work as usual. See all environments here: https://gymnasium.farama.org/environments/mujoco/\n\nenvname = 'Walker2d-v4'\nenv = gym.make(envname, render_mode=\"rgb_array_list\")\nrecorder = GymRecorder(env)\n\nreturns = random_interaction(env, 10, recorder)\n\nvideo = \"videos/\" + envname + \".gif\"\nrecorder.make_video(video)\nipython_display(video)\n\nMoviePy - Building file videos/Walker2d-v4.gif with imageio."
  },
  {
    "objectID": "exercises/7-Gym-solution.html#atari",
    "href": "exercises/7-Gym-solution.html#atari",
    "title": "Gym environments",
    "section": "Atari",
    "text": "Atari\nAs of November 2022, Atari games do not work yet with gymnasium. We have to install the original gym package temporarily (the version should at least be 0.25):\npip install gym\nThe atari games are available as binary ROM files, which have to be downloaded separately. The AutoROM package can do that for you: https://github.com/Farama-Foundation/AutoROM\npip install autorom\nAutoROM --accept-license\nYou can then install the atari submodules of gym (in particular ale_py):\npip install gym[atari]\nThe trick is now to import gym under another name, as we renamed gymnasium as gym in the earlier cells. The environments should be closed at the end, otherwise the window will stay open.\nChek out the list of Atari games here: https://gymnasium.farama.org/environments/atari/\n\nimport gym as gym_original\n\nenv = gym_original.make('ALE/Breakout-v5', render_mode='human')\n\nreturns = random_interaction(env, 1, None)\n\nenv.close()"
  },
  {
    "objectID": "exercises/8-MonteCarlo-solution.html",
    "href": "exercises/8-MonteCarlo-solution.html",
    "title": "Monte-Carlo control",
    "section": "",
    "text": "We start by importing gym. The environment we will use is text-based, so there is no need for all the boilerplate of last exercise: we simply pip install gym if we are on Colab."
  },
  {
    "objectID": "exercises/8-MonteCarlo-solution.html#the-taxi-environment",
    "href": "exercises/8-MonteCarlo-solution.html#the-taxi-environment",
    "title": "Monte-Carlo control",
    "section": "The taxi environment",
    "text": "The taxi environment\nIn this exercise, we are going to apply on-policy Monte-Carlo control on the Taxi environment available in gym:\nhttps://gym.openai.com/envs/Taxi-v3/\n\nimport gym.envs.toy_text.taxi\nhelp(gym.envs.toy_text.taxi)\n\nHelp on module gym.envs.toy_text.taxi in gym.envs.toy_text:\n\nNAME\n    gym.envs.toy_text.taxi\n\nCLASSES\n    gym.envs.toy_text.discrete.DiscreteEnv(gym.core.Env)\n        TaxiEnv\n    \n    class TaxiEnv(gym.envs.toy_text.discrete.DiscreteEnv)\n     |  The Taxi Problem\n     |  from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\"\n     |  by Tom Dietterich\n     |  \n     |  Description:\n     |  There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n     |  \n     |  Observations:\n     |  There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations.\n     |  Note that there are 400 states that can actually be reached during an episode. The missing states correspond to situations in which the passenger is at the same location as their destination, as this typically signals the end of an episode.\n     |  Four additional states can be observed right after a successful episodes, when both the passenger and the taxi are at the destination.\n     |  This gives a total of 404 reachable discrete states.\n     |  \n     |  Passenger locations:\n     |  - 0: R(ed)\n     |  - 1: G(reen)\n     |  - 2: Y(ellow)\n     |  - 3: B(lue)\n     |  - 4: in taxi\n     |  \n     |  Destinations:\n     |  - 0: R(ed)\n     |  - 1: G(reen)\n     |  - 2: Y(ellow)\n     |  - 3: B(lue)\n     |  \n     |  Actions:\n     |  There are 6 discrete deterministic actions:\n     |  - 0: move south\n     |  - 1: move north\n     |  - 2: move east\n     |  - 3: move west\n     |  - 4: pickup passenger\n     |  - 5: drop off passenger\n     |  \n     |  Rewards:\n     |  There is a default per-step reward of -1,\n     |  except for delivering the passenger, which is +20,\n     |  or executing \"pickup\" and \"drop-off\" actions illegally, which is -10.\n     |  \n     |  Rendering:\n     |  - blue: passenger\n     |  - magenta: destination\n     |  - yellow: empty taxi\n     |  - green: full taxi\n     |  - other letters (R, G, Y and B): locations for passengers and destinations\n     |  \n     |  state space is represented by:\n     |      (taxi_row, taxi_col, passenger_location, destination)\n     |  \n     |  Method resolution order:\n     |      TaxiEnv\n     |      gym.envs.toy_text.discrete.DiscreteEnv\n     |      gym.core.Env\n     |      builtins.object\n     |  \n     |  Methods defined here:\n     |  \n     |  __init__(self)\n     |      Initialize self.  See help(type(self)) for accurate signature.\n     |  \n     |  decode(self, i)\n     |  \n     |  encode(self, taxi_row, taxi_col, pass_loc, dest_idx)\n     |  \n     |  render(self, mode='human')\n     |      Renders the environment.\n     |      \n     |      The set of supported modes varies per environment. (And some\n     |      environments do not support rendering at all.) By convention,\n     |      if mode is:\n     |      \n     |      - human: render to the current display or terminal and\n     |        return nothing. Usually for human consumption.\n     |      - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n     |        representing RGB values for an x-by-y pixel image, suitable\n     |        for turning into a video.\n     |      - ansi: Return a string (str) or StringIO.StringIO containing a\n     |        terminal-style text representation. The text can include newlines\n     |        and ANSI escape sequences (e.g. for colors).\n     |      \n     |      Note:\n     |          Make sure that your class's metadata 'render.modes' key includes\n     |            the list of supported modes. It's recommended to call super()\n     |            in implementations to use the functionality of this method.\n     |      \n     |      Args:\n     |          mode (str): the mode to render with\n     |      \n     |      Example:\n     |      \n     |      class MyEnv(Env):\n     |          metadata = {'render.modes': ['human', 'rgb_array']}\n     |      \n     |          def render(self, mode='human'):\n     |              if mode == 'rgb_array':\n     |                  return np.array(...) # return RGB frame suitable for video\n     |              elif mode == 'human':\n     |                  ... # pop up a window and render\n     |              else:\n     |                  super(MyEnv, self).render(mode=mode) # just raise an exception\n     |  \n     |  ----------------------------------------------------------------------\n     |  Data and other attributes defined here:\n     |  \n     |  metadata = {'render.modes': ['human', 'ansi']}\n     |  \n     |  ----------------------------------------------------------------------\n     |  Methods inherited from gym.envs.toy_text.discrete.DiscreteEnv:\n     |  \n     |  reset(self)\n     |      Resets the environment to an initial state and returns an initial\n     |      observation.\n     |      \n     |      Note that this function should not reset the environment's random\n     |      number generator(s); random variables in the environment's state should\n     |      be sampled independently between multiple calls to `reset()`. In other\n     |      words, each call of `reset()` should yield an environment suitable for\n     |      a new episode, independent of previous episodes.\n     |      \n     |      Returns:\n     |          observation (object): the initial observation.\n     |  \n     |  seed(self, seed=None)\n     |      Sets the seed for this env's random number generator(s).\n     |      \n     |      Note:\n     |          Some environments use multiple pseudorandom number generators.\n     |          We want to capture all such seeds used in order to ensure that\n     |          there aren't accidental correlations between multiple generators.\n     |      \n     |      Returns:\n     |          list<bigint>: Returns the list of seeds used in this env's random\n     |            number generators. The first value in the list should be the\n     |            \"main\" seed, or the value which a reproducer should pass to\n     |            'seed'. Often, the main seed equals the provided 'seed', but\n     |            this won't be true if seed=None, for example.\n     |  \n     |  step(self, a)\n     |      Run one timestep of the environment's dynamics. When end of\n     |      episode is reached, you are responsible for calling `reset()`\n     |      to reset this environment's state.\n     |      \n     |      Accepts an action and returns a tuple (observation, reward, done, info).\n     |      \n     |      Args:\n     |          action (object): an action provided by the agent\n     |      \n     |      Returns:\n     |          observation (object): agent's observation of the current environment\n     |          reward (float) : amount of reward returned after previous action\n     |          done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n     |          info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n     |  \n     |  ----------------------------------------------------------------------\n     |  Methods inherited from gym.core.Env:\n     |  \n     |  __enter__(self)\n     |      Support with-statement for the environment.\n     |  \n     |  __exit__(self, *args)\n     |      Support with-statement for the environment.\n     |  \n     |  __str__(self)\n     |      Return str(self).\n     |  \n     |  close(self)\n     |      Override close in your subclass to perform any necessary cleanup.\n     |      \n     |      Environments will automatically close() themselves when\n     |      garbage collected or when the program exits.\n     |  \n     |  ----------------------------------------------------------------------\n     |  Readonly properties inherited from gym.core.Env:\n     |  \n     |  unwrapped\n     |      Completely unwrap this env.\n     |      \n     |      Returns:\n     |          gym.Env: The base non-wrapped gym.Env instance\n     |  \n     |  ----------------------------------------------------------------------\n     |  Data descriptors inherited from gym.core.Env:\n     |  \n     |  __dict__\n     |      dictionary for instance variables (if defined)\n     |  \n     |  __weakref__\n     |      list of weak references to the object (if defined)\n     |  \n     |  ----------------------------------------------------------------------\n     |  Data and other attributes inherited from gym.core.Env:\n     |  \n     |  action_space = None\n     |  \n     |  observation_space = None\n     |  \n     |  reward_range = (-inf, inf)\n     |  \n     |  spec = None\n\nDATA\n    MAP = ['+---------+', '|R: | : :G|', '| : | : : |', '| : : : : |', '| ...\n\nFILE\n    /Users/vitay/Applications/miniforge3/envs/deeprl/lib/python3.9/site-packages/gym/envs/toy_text/taxi.py\n\n\n\n\nLet’s create the environment, initialize it and render the first state:\n\nenv = gym.make(\"Taxi-v3\")\nstate = env.reset()\nenv.render()\n\n+---------+\n|R: | : :G|\n| : | : : |\n| : : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n\n\n\nThe agent is the yellow square. It can move up, down, left or right if there is no wall (the pipes and dashes). Its goal is to pick clients at the blue location and drop them off at the pink location. These locations are fixed (R, G, B, Y), but which one is the pick-up location and which one is the drop-off destination changes between each episode.\nQ: Re-run the previous cell multiple times to observe the diversity of initial states.\nThe following cell prints the action space of the environment:\n\nprint(\"Action Space\", env.action_space)\nprint(\"Number of actions\", env.action_space.n)\n\nAction Space Discrete(6)\nNumber of actions 6\n\n\nThere are 6 discrete actions: south, north, east, west, pickup, dropoff.\nLet’s now look at the observation space (state space):\n\nprint(\"State Space\", env.observation_space)\nprint(\"Number of states\", env.observation_space.n)\n\nState Space Discrete(500)\nNumber of states 500\n\n\nThere are 500 discrete states. What are they?\n\nThe taxi can be anywhere in the 5x5 grid, giving 25 different locations.\nThe passenger can be at any of the four locations R, G, B, Y or in the taxi: 5 values.\nThe destination can be any of the four locations: 4 values.\n\nThis gives indeed 25x5x4 = 500 different combinations.\nThe internal representation of a state is a number between 0 and 499. You can use the encode and decode methods of the environment to relate it to the state variables.\n\nstate = env.encode(2, 1, 1, 0) # (taxi row, taxi column, passenger index, destination index)\nprint(\"State:\", state)\n\nstate = env.decode(328) \nprint(\"State:\", list(state))\n\nState: 224\nState: [3, 1, 2, 0]\n\n\nThe reward function is simple:\n\nr = 20 when delivering the client at the correct location.\nr = -10 when picking or dropping a client illegally (picking where there is no client, dropping a client somewhere else, etc)\nr = -1 for all other transitions in order to incent the agent to be as fast as possible.\n\nThe actions pickup and dropoff are very dangerous: take them at the wrong time and your return will be very low. The navigation actions are less critical.\nDepending on the initial state, the taxi will need at least 10 steps to deliver the client, so the maximal return you can expect is around 10 (+20 for the success, -1 for all the steps).\nThe task is episodic: if you have not delivered the client within 200 steps, the episode stops (no particular reward)."
  },
  {
    "objectID": "exercises/8-MonteCarlo-solution.html#random-agent",
    "href": "exercises/8-MonteCarlo-solution.html#random-agent",
    "title": "Monte-Carlo control",
    "section": "Random agent",
    "text": "Random agent\nLet’s now define a random agent that just samples the action space.\nQ: Modify the random agent of last time, so that the agent performs a fixed number of episodes, not steps. Make sure to use the done flag to break the for loop and start in a new state. Optionally render the state of the agent at every step. At the end of each episode, compute its return using \\gamma = 1.0 (i.e. simply sum the obtained rewards) and print the list in the end.\nTip: If you render the state at every step, they will be printed one after the other. To have an animation, you can clear the output of the cell using:\nfrom IPython.display import clear_output # already imported\n\nfor t in episode: \n    # ...\n    clear_output(wait=True)\n    env.render()\n    # ...\nTip: The animation may be too fast to visualize anything. To force the framerate to be low enough, you can make Python “sleep” (do nothing) for a few milliseconds after each rendering:\nimport time # already imported\n\nfor t in episode: \n    clear_output(wait=True)\n    env.render()\n    time.sleep(0.1) # sleep for 100 milliseconds\n\nclass RandomAgent:\n    \"\"\"\n    Random agent exploring uniformly the environment.\n    \"\"\"\n    \n    def __init__(self, env):\n        \"\"\"\n        :param env: gym-like environment\n        \"\"\"\n        self.env = env\n    \n    def act(self, state):\n        \"Returns a random action by sampling the action space.\"\n        return self.env.action_space.sample()\n    \n    def update(self, state, action, reward, next_state):\n        \"Updates the agent using the transition (s, a, r, s').\"\n        pass\n    \n    def train(self, nb_episodes, render=False):\n        \"Runs the agent on the environment for nb_episodes. Returns the list of obtained returns.\"\n\n        # List of returns\n        returns = []\n\n        # Sample nb_episodes episodes\n        for episode in range(nb_episodes):\n\n            # Reset\n            state = self.env.reset()\n            done = False\n\n            rewards = 0.0\n\n            # Sample the episode\n            while not done:\n\n                # Render the current state\n                if render:\n                    clear_output(wait=True)\n                    self.env.render()\n                    time.sleep(0.1)\n\n                # Select an action \n                action = agent.act(state)\n\n                # Perform the action\n                next_state, reward, done, info = self.env.step(action)\n\n                # Store the reward\n                rewards += reward\n\n                # Go in the next state\n                state = next_state\n\n            # Store the return of the episode\n            returns.append(rewards)\n            \n        return returns\n\n\n# Create the environment\nenv = gym.make(\"Taxi-v3\")\n\n# Create the agent\nagent = RandomAgent(env)\n\n# Train for 10 episodes\nreturns = agent.train(10, render=True)\n\nenv.close()\n\n# Print the returns\nfor episode, ret in enumerate(returns):\n    print('Episode', episode+1, ': return', ret)\n\n+---------+\n|R: | : :G|\n| : | : : |\n| : : : : |\n| | : | : |\n|Y|_: |B: |\n+---------+\n  (West)\nEpisode 1 : return -767.0\nEpisode 2 : return -776.0\nEpisode 3 : return -668.0\nEpisode 4 : return -767.0\nEpisode 5 : return -920.0\nEpisode 6 : return -794.0\nEpisode 7 : return -686.0\nEpisode 8 : return -767.0\nEpisode 9 : return -812.0\nEpisode 10 : return -731.0\n\n\nQ: What do you think of the returns obtained by the random agent? Conclude on the difficulty of the task.\nA: The optimal returns are around 10, but the obtained returns with a random policy are very negative (-700, mostly due to many illegal pickups or dropoffs). One can expect a very huge variance of the returns, so learning will be slow."
  },
  {
    "objectID": "exercises/8-MonteCarlo-solution.html#on-policy-monte-carlo-control",
    "href": "exercises/8-MonteCarlo-solution.html#on-policy-monte-carlo-control",
    "title": "Monte-Carlo control",
    "section": "On-policy Monte-Carlo control",
    "text": "On-policy Monte-Carlo control\nNow let’s apply on-policy MC control on the Taxi environment. As a reminder, here the meta-algorithm:\n\nwhile True:\n\nGenerate an episode \\tau = (s_0, a_0, r_1, \\ldots, s_T) using the current stochastic policy \\pi.\nFor each state-action pair (s_t, a_t) in the episode, update the estimated Q-value:\n\n\n      Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\, (R_t - Q(s_t, a_t))\n  \n\nFor each state s_t in the episode, improve the policy (e.g. \\epsilon-greedy):\n\n\n      \\pi(s_t, a) = \\begin{cases}\n                      1 - \\epsilon \\; \\text{if} \\; a = a^* \\\\\n                      \\frac{\\epsilon}{|\\mathcal{A(s_t)}-1|} \\; \\text{otherwise.} \\\\\n                    \\end{cases}\n  \n\nIn practice, we will need:\n\na Q-table storing the estimated Q-value of each state-action pair: its size will be (500, 6).\nan \\epsilon-greedy action selection to select actions in the current state.\nan learning mechanism allowing to update the Q-value of all state-action pairs encountered in the episode.\n\nQ: Create a MonteCarloAgent agent implementing on-policy MC for the Taxi environment. Use \\gamma = 0.9, \\epsilon = 0.1 and \\alpha=0.01 (pass these parameters to the constructor of the agent and store them). Train the agent for 20000 episodes (yes, 20000… Start with one episode to debug everything and then launch the simulation. It should take around one minute). Save the return of each episode in a list and plot them in the end.\nImplementing the action selection should not be a problem, it is the same as for bandits. Little trick (not obligatory): you can implement \\epsilon-greedy as:\naction = self.Q[state, :].argmax()\nif rng.random() < epsilon:\n    action = self.env.action_space.sample()\nThis is not exactly \\epsilon-greedy, as env.action_space.sample() may select the greedy action again. In practice it does not matter, it only changes the meaning of \\epsilon, but the action selection stays similar. It is better to rely on env.action_space.sample() for the exploration, as some Gym problem work better with a normal distribution for the exploration than with uniform (e.g. continuous problems).\nDo not select the greedy action with self.Q[state, :].argmax() but rng.random.choice(np.where(self.Q[state, :] == self.Q[state, :].max())[0]): at the beginning of learning, where the Q-values are all 0, you would otherwise always take the first action (south).\nThe update() method should take a complete episode as argument, using a list of (state, action, reward) transitions. It should be called at the end of an episode only, not after every step.\nA bit tricky is the calculation of the returns for each visited state. The naive approach would look like:\nT = len(episode)\nfor t in range(T):\n    state, action, reward = episode[t]\n    return_state = 0.0\n    for k in range(t, T): # rewards coming after t\n        next_state, next_action, next_reward = episode[k]\n        return_state += gamma**k * reward\n    self.Q[state, action] += alpha * (return_state - self.Q[state, action])\nThe double for loop can be computationally expensive for long episodes (complexity T log T). It is much more efficient to iterate backwards on the episode, starting from the last transition and iterating until the first one, and using the fact that:\nR_{t} = r_{t+1} + \\gamma \\, R_{t+1}\nThe terminal state s_T has a return of 0 by definition. The last transition s_{T-1} \\rightarrow s_{T} has therefore a return of R_{T-1} = r_T. The transition before that has a return of R_{T-2} = r_{T-1} + \\gamma \\, R_{T-1}, and so on. You can then compute the returns of each action taken in the episode (and update its Q-value) in linear time.\nTo iterate backwards over the list of transitions, use the reversed() operator:\nl = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\nfor a in reversed(l):\n    print(a)\n\nclass MonteCarloAgent:\n    \"\"\"\n    Online Monte-Carlo agent.\n    \"\"\"\n    \n    def __init__(self, env, gamma, epsilon, alpha):\n        \"\"\"\n        :param env: gym-like environment\n        :param gamma: discount factor\n        :param epsilon: exploration parameter\n        :param alpha: learning rate\n        \"\"\"\n        self.env = env\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.alpha = alpha\n        \n        # Q_table\n        self.Q = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n    \n    def act(self, state):\n        \"Returns an action using epsilon-greedy action selection.\"\n        \n        action = rng.choice(np.where(self.Q[state, :] == self.Q[state, :].max())[0])\n        \n        if rng.random() < self.epsilon:\n            action = self.env.action_space.sample() \n        \n        return action\n    \n    def update(self, episode):\n        \"Updates the agent using a complete episode.\"\n        # Terminal states have a return of 0\n        return_episode = 0.0\n        \n        # Iterate backwards over the episode\n        for state, action, reward in reversed(episode):\n            \n            # Compute the return\n            return_episode = reward + self.gamma * return_episode\n            \n            # Update the Q-value\n            self.Q[state, action] += self.alpha * (return_episode - self.Q[state, action])\n            \n        return return_episode\n    \n    def train(self, nb_episodes, render=False):\n        \"\"\"\n        Runs the agent on the environment for nb_episodes. Returns the list of obtained returns.\n        \"\"\"\n\n        # Returns\n        returns = []\n\n        # Fixed number of episodes\n        for episode in range(nb_episodes):\n\n            # Reset\n            state = self.env.reset()\n            done = False\n            nb_steps = 0\n\n            # Store transitions\n            transitions = []\n\n            # Sample the episode\n            while not done:\n                \n                # Render the current state\n                if render:\n                    clear_output(wait=True)\n                    self.env.render()\n                    time.sleep(0.1)\n\n                # Select an action \n                action = self.act(state)\n\n                # Perform the action\n                next_state, reward, done, info = self.env.step(action)\n\n                # Store the transition\n                transitions.append([state, action, reward])\n\n                # Go in the next state\n                state = next_state\n\n                # Increment time\n                nb_steps += 1\n\n            # Update the Monte Carlo agent after the episode is completed\n            return_episode = self.update(transitions)    \n\n            # Store info\n            returns.append(return_episode)\n            \n            # Print info\n            if episode % 100 == 0:\n                clear_output(wait=True)\n                print('Episode', episode, ':')\n                print(' return:', return_episode)\n                print(' steps:', nb_steps)\n            \n            \n        return returns\n\n\n# Parameters\ngamma = 0.9\nepsilon = 0.1\nalpha = 0.01\nnb_episodes = 20000\n\n# Create the environment\nenv = gym.make(\"Taxi-v3\")\n\n# Create the agent\nagent = MonteCarloAgent(env, gamma, epsilon, alpha)\n\n# Train the agent \nreturns = agent.train(nb_episodes)\n\nenv.close()\n\nplt.figure(figsize=(10, 6))\nplt.plot(returns)\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"Returns\")\nplt.show()\n\nEpisode 19900 :\n return: -1.5271139055699985\n steps: 13\n\n\n\n\n\n\n\n\n\nAs you may observe, the returns have a huge variance due to the exploration, what makes the plot quite ugly and unreadable. The following function allows to smooth the returns using a sliding average over the last N epochs:\n\ndef running_average(x, N):\n    cumsum = np.cumsum(np.insert(np.array(x), 0, 0)) \n    return (cumsum[N:] - cumsum[:-N]) / N\n\nQ: Plot the returns as well as their sliding average. Comment on the influence of exploration.\n\n# Plot the returns\nplt.figure(figsize=(10, 6))\nplt.plot(returns)\nplt.plot(running_average(returns, 1000))\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"Returns\")\nplt.show()\n\n\n\n\n\n\n\n\nQ: Extend the agent with a test(self, render) method that performs a single episode on the environment without exploration, optionally renders each state but does not learn. You will have to re-train the agent, because the definition of its class has changed. Backup the previous value of epsilon in a temporary variable and reset it at the end of the episode. Have the method return the undiscounted sum of rewards in the episode, as well as the number of steps until termination.\nAfter training (you can reduce the number of episodes), first observe a couple of test episodes with rendering on. Is the policy any good?\nPerform 1000 test episodes without rendering and report the mean return over these 1000 episodes as the final performance of your agent.\n\nclass MonteCarloAgentTest (MonteCarloAgent):\n    \"\"\"\n    Online Monte-Carlo agent with a test method.\n    \"\"\"\n\n    def test(self, render=True):\n        \"Performs a test episode without exploration.\"\n        previous_epsilon = self.epsilon\n        self.epsilon = 0.0\n        \n        # Reset\n        state = self.env.reset()\n        done = False\n        nb_steps = 0\n        rewards= 0\n\n        # Sample the episode\n        while not done:\n            if render:\n                clear_output(wait=True)\n                self.env.render()\n                time.sleep(0.1)\n            action = self.act(state)\n            next_state, reward, done, info = self.env.step(action)\n            rewards += reward\n            state = next_state\n            nb_steps += 1\n            \n        self.epsilon = previous_epsilon\n            \n        return rewards, nb_steps\n\n\n# Parameters\ngamma = 0.9\nepsilon = 0.1\nalpha = 0.01\nnb_episodes = 20000\n\n# Create the environment\nenv = gym.make(\"Taxi-v3\")\n\n# Create the agent\nagent = MonteCarloAgentTest(env, gamma, epsilon, alpha)\n\n# Train the agent\nreturns = agent.train(nb_episodes)\n\n# Plot training returns\nplt.figure(figsize=(10, 6))\nplt.plot(returns)\nplt.plot(running_average(returns, 1000))\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"Returns\")\nplt.show()\n\n# Test the agent for 1000 episodes\ntest_returns = []\ntest_steps = []\nfor episode in range(1000):\n    return_episode, nb_steps = agent.test(render=False)\n    test_returns.append(return_episode)\n    test_steps.append(nb_steps)\nprint(\"Test performance\", np.mean(test_returns))\n\nplt.figure(figsize=(15, 6))\nplt.subplot(121)\nplt.hist(test_returns)\nplt.xlabel(\"Returns\")\nplt.subplot(122)\nplt.hist(test_steps)\nplt.xlabel(\"Number of steps\")\nplt.show()\n\nEpisode 19900 :\n return: -4.440939433444476\n steps: 17\n\n\n\n\n\n\n\n\n\nTest performance 5.399\n\n\n\n\n\n\n\n\n\n\n# Visualize some episodes\nreturns = []\nlength_episode = []\n\nfor episode in range(5):\n    \n    # Perform one episode\n    return_episode, nb_steps = agent.test(render=True)\n    \n    # Store info\n    returns.append(return_episode)\n    length_episode.append(nb_steps)\n\n\nprint(\"Returns:\", returns)\nprint(\"Number of steps\", length_episode)\n\n+---------+\n|R: | : :G|\n| : | : : |\n| : : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n  (North)\nReturns: [4, 4, 1, 5, 1]\nNumber of steps [17, 17, 20, 16, 20]\n\n\nA: The agent successfully picks and delivers the client at the correct location. The path is not always the shortest one (especially in the vast area before G), but that is fine."
  },
  {
    "objectID": "exercises/8-MonteCarlo-solution.html#experiments",
    "href": "exercises/8-MonteCarlo-solution.html#experiments",
    "title": "Monte-Carlo control",
    "section": "Experiments",
    "text": "Experiments\n\nEarly stopping\nQ: Train the agent for the smallest number of episodes where the returns seem to have stabilized (e.g. 2000 episodes). Test the agent. Does it work? Why?\n\n# Parameters\ngamma = 0.9\nepsilon = 0.1\nalpha = 0.01\nnb_episodes = 2000\n\n# Create the environment\nenv = gym.make(\"Taxi-v3\")\n\n# Create the agent\nagent = MonteCarloAgentTest(env, gamma, epsilon, alpha)\n\n# Train the agent\nreturns = agent.train(nb_episodes)\n\n# Plot training returns\nplt.figure(figsize=(10, 6))\nplt.plot(returns)\nplt.plot(running_average(returns, 1000))\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"Returns\")\nplt.show()\n\n# Test the agent for 1000 episodes\ntest_returns = []\ntest_steps = []\nfor episode in range(1000):\n    return_episode, nb_steps = agent.test(render=False)\n    test_returns.append(return_episode)\n    test_steps.append(nb_steps)\nprint(\"Test performance\", np.mean(test_returns))\n\nplt.figure(figsize=(15, 6))\nplt.subplot(121)\nplt.hist(test_returns)\nplt.xlabel(\"Returns\")\nplt.subplot(122)\nplt.hist(test_steps)\nplt.xlabel(\"Number of steps\")\nplt.show()\n\nEpisode 1900 :\n return: -13.138105953844926\n steps: 200\n\n\n\n\n\n\n\n\n\nTest performance -153.435\n\n\n\n\n\n\n\n\n\nA: Although the returns are already between -20 and +10, as after 50000 episodes, the greedy policy is not optimal as some states still have a very bad policy (the agent sometimes goes back and forth between two states) or just a bad one (not optimal path). The negative returns at the end of learning are due to the exploration: the agent performs illegal pickups or dropoffs. Early in learning, this is due to bad estimates of the Q-values. The return of an episode is a bad estimate of the performance: it report both the exploration and the exploitation. Unfortunately, it is the only one we have…\n\n\nDiscount rate\nQ: Change the value of the discount factor \\gamma. As the task is episodic (maximum 200 steps), try a discount rate of 1. What happens? Conclude.\n\n# Parameters\ngamma = 1.0\nepsilon = 0.1\nalpha = 0.01\nnb_episodes = 20000\n\n# Create the environment\nenv = gym.make(\"Taxi-v3\")\n\n# Create the agent\nagent = MonteCarloAgentTest(env, gamma, epsilon, alpha)\n\n# Train the agent\nreturns = agent.train(nb_episodes)\n\n# Plot training returns\nplt.figure(figsize=(10, 6))\nplt.plot(returns)\nplt.plot(running_average(returns, 1000))\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"Returns\")\nplt.show()\n\n# Test the agent for 1000 episodes\ntest_returns = []\ntest_steps = []\nfor episode in range(1000):\n    return_episode, nb_steps = agent.test(render=False)\n    test_returns.append(return_episode)\n    test_steps.append(nb_steps)\nprint(\"Test performance\", np.mean(test_returns))\n\nplt.figure(figsize=(15, 6))\nplt.subplot(121)\nplt.hist(test_returns)\nplt.xlabel(\"Returns\")\nplt.subplot(122)\nplt.hist(test_steps)\nplt.xlabel(\"Number of steps\")\nplt.show()\n\nEpisode 19900 :\n return: 1.0\n steps: 20\n\n\n\n\n\n\n\n\n\nTest performance -30.198\n\n\n\n\n\n\n\n\n\nA: Rather tricky question… With a discount factor of 1, the agent does not converge as fast as with gamma = 0.9. This is due to the variance of the returns: imagine your episode is optimal all along, but at the last moment, the agent performs an illegal dropoff action. The undiscounted return of the episode will be negative and all actions taken during the episode will be punished, although only the last one is responsible for the bad return.\nUsing a discount factor < 1 allows the first actions to stop caring about the final rewards, as they are discounted by \\gamma^T, which is very small. Take-home message: even if your task is episodic, use \\gamma < 1.\n\n\nLearning rate\nQ: Vary the learning rate alpha. What happens?\n\n# Parameters\ngamma = 0.9\nepsilon = 0.1\nalpha = 0.5\nnb_episodes = 20000\n\n# Create the environment\nenv = gym.make(\"Taxi-v3\")\n\n# Create the agent\nagent = MonteCarloAgentTest(env, gamma, epsilon, alpha)\n\n# Train the agent\nreturns = agent.train(nb_episodes)\n\n# Plot training returns\nplt.figure(figsize=(10, 6))\nplt.plot(returns)\nplt.plot(running_average(returns, 1000))\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"Returns\")\nplt.show()\n\n# Test the agent for 1000 episodes\ntest_returns = []\ntest_steps = []\nfor episode in range(1000):\n    return_episode, nb_steps = agent.test(render=False)\n    test_returns.append(return_episode)\n    test_steps.append(nb_steps)\nprint(\"Test performance\", np.mean(test_returns))\n\nplt.figure(figsize=(15, 6))\nplt.subplot(121)\nplt.hist(test_returns)\nplt.xlabel(\"Returns\")\nplt.subplot(122)\nplt.hist(test_steps)\nplt.xlabel(\"Number of steps\")\nplt.show()\n\nEpisode 19900 :\n return: -6.352700362282921\n steps: 21\n\n\n\n\n\n\n\n\n\nTest performance -165.175\n\n\n\n\n\n\n\n\n\nA: If the learning rate is too high (0.5), the network does not converge and becomes unstable, as updates “erases” very quickly the previous values, what is bad given the high variance of the returns. If the learning rate is too low, learning takes forever. Classical machine learning problem… 0.01 works actually quite well.\n\n\nExploration parameter\nQ: Vary the exploration parameter epsilon and observe its impact on learning.\n\n# Parameters\ngamma = 0.9\nepsilon = 0.01 # or 0.3, etc.\nalpha = 0.01\nnb_episodes = 20000\n\n# Create the environment\nenv = gym.make(\"Taxi-v3\")\n\n# Create the agent\nagent = MonteCarloAgentTest(env, gamma, epsilon, alpha)\n\n# Train the agent\nreturns = agent.train(nb_episodes)\n\n# Plot training returns\nplt.figure(figsize=(10, 6))\nplt.plot(returns)\nplt.plot(running_average(returns, 1000))\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"Returns\")\nplt.show()\n\n# Test the agent for 1000 episodes\ntest_returns = []\ntest_steps = []\nfor episode in range(1000):\n    return_episode, nb_steps = agent.test(render=False)\n    test_returns.append(return_episode)\n    test_steps.append(nb_steps)\nprint(\"Test performance\", np.mean(test_returns))\n\nplt.figure(figsize=(15, 6))\nplt.subplot(121)\nplt.hist(test_returns)\nplt.xlabel(\"Returns\")\nplt.subplot(122)\nplt.hist(test_steps)\nplt.xlabel(\"Number of steps\")\nplt.show()\n\nEpisode 19900 :\n return: -9.999999992944923\n steps: 200\n\n\n\n\n\n\n\n\n\nTest performance -125.804\n\n\n\n\n\n\n\n\n\nA: The exploration should be at least 0.1 in order to find the optimal policy. 0.2 or 0.3 find a good policy faster, but explore too much at the end of learning.\n\n\nExploration scheduling\nEven with a good learning rate (0.01) and a discount factor of 0.9, the exploration parameter as a huge impact on the performance: too low and the agent does not find the optimal policy, too high and the agent is inefficient at the end of learning.\nQ: Implement scheduling for epsilon. You can use exponential scheduling as in the bandits exercise:\n\\epsilon = \\epsilon \\times (1 - \\epsilon_\\text{decay})\nat the end of each episode, with \\epsilon_\\text{decay} being a small decay parameter (1e-5 or so).\nFind a correct value for \\epsilon_\\text{decay}. Do not hesitate to fine-tune alpha at the same time.\nTip: Prepare and visualize the scheduling in a different cell, and use the initial value of \\epsilon and \\epsilon_\\text{decay} that seem to make sense.\n\nclass DecayMonteCarloAgent (MonteCarloAgentTest):\n    \"\"\"\n    Monte-Carlo agent with decay of the exploration parameter.\n    \"\"\"\n    \n    def __init__(self, env, gamma, epsilon, decay_epsilon, alpha):\n        \"\"\"\n        :param env: gym-like environment\n        :param gamma: discount factor\n        :param epsilon: exploration parameter\n        :param decay_epsilon: exploration decay parameter\n        :param alpha: learning rate\n        \"\"\"\n        self.decay_epsilon = decay_epsilon\n        \n        super().__init__(env, gamma, epsilon, alpha)\n    \n    \n    def train(self, nb_episodes, render=False):\n        \"Runs the agent on the environment for nb_episodes. Returns the list of obtained returns.\"\n\n        # Returns\n        returns = []\n\n        # Fixed number of episodes\n        for episode in range(nb_episodes):\n\n            # Reset\n            state = self.env.reset()\n            done = False\n            nb_steps = 0\n\n            # Store transitions\n            transitions = []\n\n            # Sample the episode\n            while not done:\n                \n                # Render the current state\n                if render:\n                    clear_output(wait=True)\n                    self.env.render()\n                    time.sleep(0.1)\n\n                # Select an action \n                action = self.act(state)\n\n                # Perform the action\n                next_state, reward, done, info = self.env.step(action)\n\n                # Store the transition\n                transitions.append([state, action, reward])\n\n                # Go in the next state\n                state = next_state\n\n                # Increment time\n                nb_steps += 1\n\n            # Update the Monte Carlo agent after the episode is completed\n            return_episode = self.update(transitions)   \n            \n            # Decay epsilon\n            self.epsilon = self.epsilon * (1 - self.decay_epsilon)\n\n            # Store info\n            returns.append(return_episode)\n            \n            # Print info\n            if episode % 100 == 0:\n                clear_output(wait=True)\n                print('Episode', episode, ':')\n                print(' return:', return_episode)\n                print(' steps:', nb_steps)\n                print(' epsilon:', self.epsilon)\n            \n            \n        return returns\n\n\n# Parameters\ngamma = 0.9\nepsilon = 0.4\ndecay_epsilon = 1e-4\nalpha = 0.05\nnb_episodes = 20000\n\n# Create the environment\nenv = gym.make(\"Taxi-v3\")\n\n# Create the agent\nagent = DecayMonteCarloAgent(env, gamma, epsilon, decay_epsilon, alpha)\n\n# Train the agent\nreturns = agent.train(nb_episodes)\n\n# Plot training returns\nplt.figure(figsize=(10, 6))\nplt.plot(returns)\nplt.plot(running_average(returns, 1000))\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"Returns\")\nplt.show()\n\n# Test the agent for 1000 episodes\ntest_returns = []\ntest_steps = []\nfor episode in range(1000):\n    return_episode, nb_steps = agent.test(render=False)\n    test_returns.append(return_episode)\n    test_steps.append(nb_steps)\nprint(\"Test performance\", np.mean(test_returns))\n\nplt.figure(figsize=(15, 6))\nplt.subplot(121)\nplt.hist(test_returns)\nplt.xlabel(\"Returns\")\nplt.subplot(122)\nplt.hist(test_steps)\nplt.xlabel(\"Number of steps\")\nplt.show()\n\nEpisode 19900 :\n return: -2.3744025150129984\n steps: 14\n epsilon: 0.054667262335293364\n\n\n\n\n\n\n\n\n\nTest performance 7.281\n\n\n\n\n\n\n\n\n\n\n# Visualize some episodes\nreturns = []\nlength_episode = []\n\nfor episode in range(5):\n    \n    # Perform one episode\n    return_episode, nb_steps = agent.test(render=True)\n    \n    # Store info\n    returns.append(return_episode)\n    length_episode.append(nb_steps)\n\n\nprint(\"Returns:\", returns)\nprint(\"Number of steps\", length_episode)\n\n+---------+\n|R: | : :G|\n| : | : : |\n| : : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n  (North)\nReturns: [10, 3, 5, 7, 11]\nNumber of steps [11, 18, 16, 14, 10]\n\n\nA: As seen with bandits, decaying the exploration parameter with the right schedule improves very significantly the speed of learning and the optimality."
  },
  {
    "objectID": "exercises/9-TD-solution.html",
    "href": "exercises/9-TD-solution.html",
    "title": "Q-learning",
    "section": "",
    "text": "In this short exercise, we are going to apply Q-learning on the Taxi environment used last time for MC control.\nAs a reminder, Q-learning updates the Q-value of a state-action pair after each transition, using the update rule:\n\\Delta Q(s_t, a_t) = \\alpha \\, (r_{t+1} + \\gamma \\, \\max_{a'} \\, Q(s_{t+1}, a') - Q(s_t, a_t))\nQ: Update the class you designed for online MC in the last exercise so that it implements Q-learning.\nThe main difference is that the update() method has to be called after each step of the episode, not at the end. It simplifies a lot the code too (no need to iterate backwards on the episode).\nYou can use the following parameters at the beginning, but feel free to change them:\n\nDiscount factor \\gamma = 0.9.\nLearning rate \\alpha = 0.1.\nEpsilon-greedy action selection, with an initial exploration parameter of 1.0 and an exponential decay of 10^{-5} after each update (i.e. every step!).\nA total number of episodes of 20000.\n\nKeep the general structure of the class: train() for the main loop, test() to run one episode without exploration, etc. Add a method to compute the discounted return of each episode, as it will not be done automatically by the update() method anymore. Plot the training and test performance in the end and render the learned deterministic policy for 10 episodes.\nNote: if s_{t+1} is terminal (done is true after the transition), the target should not be r_{t+1} + \\gamma \\, \\max_{a'} \\, Q(s_{t+1}, a'), but simply r_{t+1} as there is no next action.\n\nclass QLearningAgent:\n    \"\"\"\n    Q-learning agent.\n    \"\"\"\n    \n    def __init__(self, env, gamma, epsilon, decay_epsilon, alpha):\n        \"\"\"\n        :param env: gym-like environment\n        :param gamma: discount factor\n        :param epsilon: exploration parameter\n        :param decay_epsilon: exploration decay parameter\n        :param alpha: learning rate\n        \"\"\"\n        self.env = env\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.decay_epsilon = decay_epsilon\n        self.alpha = alpha\n        \n        # Q_table\n        self.Q = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n    \n    def act(self, state):\n        \"Returns an action using epsilon-greedy action selection.\"\n        \n        action = rng.choice(np.where(self.Q[state, :] == self.Q[state, :].max())[0])\n        \n        if rng.random() < self.epsilon:\n            action = self.env.action_space.sample() \n        \n        return action\n    \n    def update(self, state, action, reward, next_state, done):\n        \"Updates the agent using a single transition.\"\n        \n        # Bellman target\n        target = reward\n        \n        if not done:\n            target += self.gamma * self.Q[next_state, :].max()\n        \n        # Update the Q-value\n        self.Q[state, action] += self.alpha * (target - self.Q[state, action])\n            \n        # Decay epsilon\n        self.epsilon = self.epsilon * (1 - self.decay_epsilon)\n            \n    \n    def train(self, nb_episodes, render=False):\n        \"Runs the agent on the environment for nb_episodes. Returns the list of obtained returns.\"\n\n        # Returns\n        returns = []\n\n        # Fixed number of episodes\n        for episode in range(nb_episodes):\n\n            # Reset\n            state = self.env.reset()\n            done = False\n            nb_steps = 0\n\n            # Store rewards\n            rewards = []\n\n            # Sample the episode\n            while not done:\n                \n                # Render the current state\n                if render:\n                    clear_output(wait=True)\n                    self.env.render()\n                    time.sleep(0.1)\n\n                # Select an action \n                action = self.act(state)\n\n                # Perform the action\n                next_state, reward, done, info = self.env.step(action)\n                \n                # Append reward\n                rewards.append(reward)\n\n                # Learn from the transition\n                self.update(state, action, reward, next_state, done)\n\n                # Go in the next state\n                state = next_state\n\n                # Increment time\n                nb_steps += 1\n\n            # Compute the discounted return of the episode.\n            return_episode = self.discounted_return(rewards)    \n\n            # Store info\n            returns.append(return_episode)\n            \n            # Print info\n            if episode % 100 == 0:\n                clear_output(wait=True)\n                print('Episode', episode, ':')\n                print(' return:', return_episode)\n                print(' steps:', nb_steps)\n                print(' epsilon:', self.epsilon)\n            \n        return returns\n    \n    def test(self, render=True):\n        \"Performs a test episode without exploration.\"\n        previous_epsilon = self.epsilon\n        self.epsilon = 0.0\n        \n        # Reset\n        state = self.env.reset()\n        done = False\n        nb_steps = 0\n        rewards= 0\n\n        # Sample the episode\n        while not done:\n            if render:\n                clear_output(wait=True)\n                self.env.render()\n                time.sleep(0.1)\n            action = self.act(state)\n            next_state, reward, done, info = self.env.step(action)\n            rewards += reward\n            state = next_state\n            nb_steps += 1\n            \n        self.epsilon = previous_epsilon\n            \n        return rewards, nb_steps\n    \n    def discounted_return(self, rewards):\n        \"Computes the discounted return of an episode based on the list of rewards.\"\n        ret = 0.0\n        for reward in reversed(rewards):\n            ret = reward + self.gamma*ret\n        return ret\n\n\n# Parameters\ngamma = 0.9\nepsilon = 1.0\ndecay_epsilon = 1e-5\nalpha = 0.1\nnb_episodes = 20000\n\n# Create the environment\nenv = gym.make(\"Taxi-v3\")\n\n# Create the agent\nagent = QLearningAgent(env, gamma, epsilon, decay_epsilon, alpha)\n\n# Train the agent \nreturns = agent.train(nb_episodes)\n\n# Plot training returns\nplt.figure(figsize=(10, 6))\nplt.plot(returns)\nplt.plot(running_average(returns, 1000))\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"Returns\")\nplt.show()\n\n# Test the agent for 1000 episodes\ntest_returns = []\ntest_steps = []\nfor episode in range(1000):\n    return_episode, nb_steps = agent.test(render=False)\n    test_returns.append(return_episode)\n    test_steps.append(nb_steps)\nprint(\"Test performance\", np.mean(test_returns))\n\nplt.figure(figsize=(15, 6))\nplt.subplot(121)\nplt.hist(test_returns)\nplt.xlabel(\"Returns\")\nplt.subplot(122)\nplt.hist(test_steps)\nplt.xlabel(\"Number of steps\")\nplt.show()\n\nEpisode 19900 :\n return: -1.5271139055699985\n steps: 13\n epsilon: 0.01856884818045739\n\n\n\n\n\n\n\n\n\nTest performance 7.888\n\n\n\n\n\n\n\n\n\n\n# Visualize some episodes\nreturns = []\nlength_episode = []\n\nfor episode in range(5):\n    \n    # Perform one episode\n    return_episode, nb_steps = agent.test(render=True)\n    \n    # Store info\n    returns.append(return_episode)\n    length_episode.append(nb_steps)\n\n\nprint(\"Returns:\", returns)\nprint(\"Number of steps\", length_episode)\n\n+---------+\n|R: | : :G|\n| : | : : |\n| : : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n  (North)\nReturns: [6, 7, 7, 8, 4]\nNumber of steps [15, 14, 14, 13, 17]\n\n\nQ: Compare the performance of Q-learning to online MC. Experiment with parameters (gamma, epsilon, alpha, etc.).\n\n# Parameters\ngamma = 1.0\nepsilon = 1.0\ndecay_epsilon = 1e-5\nalpha = 0.05\nnb_episodes = 20000\n\n# Create the environment\nenv = gym.make(\"Taxi-v3\")\n\n# Create the agent\nagent = QLearningAgent(env, gamma, epsilon, decay_epsilon, alpha)\n\n# Train the agent \nreturns = agent.train(nb_episodes)\n\n# Plot training returns\nplt.figure(figsize=(10, 6))\nplt.plot(returns)\nplt.plot(running_average(returns, 1000))\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"Returns\")\nplt.show()\n\n# Test the agent for 1000 episodes\ntest_returns = []\ntest_steps = []\nfor episode in range(1000):\n    return_episode, nb_steps = agent.test(render=False)\n    test_returns.append(return_episode)\n    test_steps.append(nb_steps)\nprint(\"Test performance\", np.mean(test_returns))\n\nplt.figure(figsize=(15, 6))\nplt.subplot(121)\nplt.hist(test_returns)\nplt.xlabel(\"Returns\")\nplt.subplot(122)\nplt.hist(test_steps)\nplt.xlabel(\"Number of steps\")\nplt.show()\n\nEpisode 19900 :\n return: 8.0\n steps: 13\n epsilon: 0.012423419219646946\n\n\n\n\n\n\n\n\n\nTest performance 8.019\n\n\n\n\n\n\n\n\n\n\n# Visualize some episodes\nreturns = []\nlength_episode = []\n\nfor episode in range(5):\n    \n    # Perform one episode\n    return_episode, nb_steps = agent.test(render=True)\n    \n    # Store info\n    returns.append(return_episode)\n    length_episode.append(nb_steps)\n\n\nprint(\"Returns:\", returns)\nprint(\"Number of steps\", length_episode)\n\n+---------+\n|R: | : :G|\n| : | : : |\n| : : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n  (North)\nReturns: [9, 11, 5, 8, 4]\nNumber of steps [12, 10, 16, 13, 17]\n\n\nA: Q-learning accepts much higher values of gamma than MC, because the returns have a much lower variance. With the right parameters, Q-learning can learn much faster than MC."
  },
  {
    "objectID": "exercises/10-Eligibilitytraces-solution.html",
    "href": "exercises/10-Eligibilitytraces-solution.html",
    "title": "Eligibility traces",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport gym\nrng = np.random.default_rng()\nimport time\nfrom IPython.display import clear_output\n\ndef running_average(x, N):\n    cumsum = np.cumsum(np.insert(np.array(x), 0, 0)) \n    return (cumsum[N:] - cumsum[:-N]) / N"
  },
  {
    "objectID": "exercises/10-Eligibilitytraces-solution.html#q-learning-in-gridworld",
    "href": "exercises/10-Eligibilitytraces-solution.html#q-learning-in-gridworld",
    "title": "Eligibility traces",
    "section": "Q-learning in Gridworld",
    "text": "Q-learning in Gridworld\n\nRandom interaction with the environment\nThe goal of this exercise is to solve the Gridworld problem using Q-learning. The code is adapted from https://github.com/rlcode/reinforcement-learning-kr\nThe agent is represented by the red square: the state s of the agent is its position in the 5x5 grid, i.e. a number between 0 and 24.\nThe agent can move either to the left, right, top or bottom. When the agent tries to move outside of the environment, it stays at its current position. There are four actions a available, which are deterministic.\nIts goal is to reach the blue circle, while avoiding the green triangles. Actions leading to the blue circle receive a reward r of +100, actions leading to a green triangle receive a reward of -100. The episode ends in those states. All other actions have a reward of -1.\nThe following code allows you to run a random agent in this environment for 10 episodes: at each time step, the action is selected randomly between 0 and 3 with env.action_space.sample().\nThe environment is created with:\nenv = Gridworld()\nenv.render() allows to display the Gridworld. If you pass it a Q-table (one row per state, one column per action) env.render(Q), it will print the Q_value of each action at the corresponding location.\nstate = env.reset() allows to start an episode. The agent is placed at the top-left of the grid.\nnext_state, reward, done, info = env.step(action) allows to perform an action on the environment. action must be a number between 0 and 3. It return the next state, the reward received during the transition, as well as a boolean done which is True when the episode is terminated (the agent moved to a blue circle or a green triangle, or 100 steps have been done), False otherwise. info contains the current undiscounted return of the episode.\nQ: Understand and run the code. Does the agent succeed often? How complex is the task compared to Taxi?\nTip: If the window is too small or too big, modify the parameter K at the beginning of render().\n\nclass GridWorld(gym.Env):\n    \n    def __init__(self, rewards=[100, -100, -1], size=5):\n        \"Initialize the environment, can accept additional parameters such as the number of states and actions.\"\n        \n        self.size = size\n        self.rewards = rewards\n        \n        # State space\n        self.observation_space = gym.spaces.Discrete(self.size**2)\n        \n        # Action space\n        self.action_space = gym.spaces.Discrete(4)    \n        \n        super().__init__()\n        \n        self.state = self.reset()\n        self.t_max = 100\n        \n        self.target = (3, 2)\n        self.distractor1 = (2, 2)\n        self.distractor2 = (3, 3)\n    \n    def reset(self):\n        \"Resets the environment and starts from an initial state.\"\n        \n        # Sample one state randomly \n        self.state = self.rank((0, self.size-1))\n        self._t = 0\n        self.sum_rewards = 0.0\n        \n        return self.state\n    \n    def step(self, action):\n        \"\"\"\n        Takes an action and returns a new state, a reward, a boolean (True for terminal states) \n        and a dictionary with additional info (optional).\n        \"\"\"\n        self._t +=1\n        self.done = False\n        \n        x, y = self.coordinates(self.state)\n        if action == 0: # up\n            y = max(min(y + 1, self.size-1), 0)\n        elif action == 1: # down\n            y = max(min(y - 1, self.size-1), 0)\n        elif action == 2: # left\n            x = max(min(x - 1, self.size-1), 0)\n        elif action == 3: # right\n            x = max(min(x + 1, self.size-1), 0)\n        \n        self.state = self.rank((x, y))\n        \n        if self.state == self.rank(self.target):\n            self.reward = self.rewards[0]\n            self.done = True\n        elif self.state == self.rank(self.distractor1) or self.state == self.rank(self.distractor2):\n            self.reward = self.rewards[1]\n            self.done = True\n        else:\n            self.reward = self.rewards[2]\n            \n        if self._t >= self.t_max:\n            self.done = True\n            \n        self.sum_rewards += self.reward\n        self.info = {'return': self.sum_rewards}\n        \n        return self.state, self.reward, self.done, self.info\n\n    def render(self, Q):\n        \"Displays the current state of the environment.\"\n        \n        K = 1.5\n        plt.figure(figsize=(K*self.size, K*self.size))\n        \n        # grid\n        for i in range(1, self.size):\n            plt.plot([i, i], [0, self.size], color=\"black\")\n        for i in range(1, self.size):\n            plt.plot([0, self.size], [i, i], color=\"black\")\n            \n        # Q-values\n        for s in range(self.size*self.size):\n            c = self.coordinates(s)\n            plt.text(c[0] + 0.5, c[1] + 0.9, f\"{Q[s, 0]:.2f}\", horizontalalignment='center', verticalalignment='center')\n            plt.text(c[0] + 0.5, c[1] + 0.1, f\"{Q[s, 1]:.2f}\", horizontalalignment='center', verticalalignment='center')\n            plt.text(c[0] + 0.05, c[1] + 0.5, f\"{Q[s, 2]:.2f}\", horizontalalignment='left', verticalalignment='center')\n            plt.text(c[0] + 0.95, c[1] + 0.5, f\"{Q[s, 3]:.2f}\", horizontalalignment='right', verticalalignment='center')\n            \n        # target\n        plt.scatter([self.target[0] +0.5], [self.target[1] + 0.5], s=5000, marker=\"o\", color=\"blue\")\n        plt.scatter([self.distractor1[0] +0.5], [self.distractor1[1] + 0.5], s=5000, marker=\"^\", color=\"green\")\n        plt.scatter([self.distractor2[0] +0.5], [self.distractor2[1] + 0.5], s=5000, marker=\"^\", color=\"green\")\n        \n        # state\n        s = self.coordinates(self.state)\n        plt.scatter([s[0] + 0.5], [s[1] + 0.5], s=5000, marker=\"s\", color=\"red\")\n        \n        plt.xticks([])\n        plt.yticks([])\n        plt.xlim((0, self.size))\n        plt.ylim((0, self.size))\n        plt.title(\"Step \" + str(self._t) + \" - undiscounted return \" + str(self.sum_rewards))\n        \n        clear_output(wait=True)\n        plt.show()\n    \n    def coordinates(self, state):\n        \"Returns coordinates of a state.\"\n        return (state % self.size, int(state/self.size))\n    \n    def rank(self, coord):\n        \"Returns rank from coordinates.\"\n        return coord[0] + self.size*coord[1]\n    \n\n\nclass RandomAgent:\n    \n    def __init__(self, env):\n        self.env = env\n        self.Q = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n        \n    def act(self, state):\n        \"Selects an action randomly\"\n        return self.env.action_space.sample()\n    \n    def train(self, nb_episodes, render=True):\n        \"Runs the agent on the environment for nb_episodes. Returns the list of obtained returns.\"\n\n        # Returns\n        returns = []\n\n        # Fixed number of episodes\n        for episode in range(nb_episodes):\n\n            # Reset\n            state = self.env.reset()\n            done = False\n            nb_steps = 0\n\n            # Store rewards\n            rewards = []\n\n            # Sample the episode\n            while not done:\n                \n                # Render the current state\n                if render:\n                    self.env.render(self.Q)\n                    \n                # Select an action \n                action = self.act(state)\n\n                # Perform the action\n                next_state, reward, done, info = self.env.step(action)\n                \n                # Append reward\n                rewards.append(reward)\n\n                # Go in the next state\n                state = next_state\n\n                # Increment time\n                nb_steps += 1\n                \n                if done: \n                    self.env.render(self.Q)\n\n            # Compute the discounted return of the episode.\n            return_episode = np.sum(rewards)    \n\n            # Store info\n            returns.append(return_episode)\n            \n        return returns\n\n\n# Create the environment\nenv = GridWorld()\n\n# Create the agent\nagent = RandomAgent(env)\n\n# Perform random episodes\nreturns = agent.train(2, render=True)\n        \nprint(returns)\n\n\n\n\n\n\n\n\n[-135, -106]\n\n\nQ: Adapt your Q-learning agent from last exercise to the problem. The main difference is the call to render(), the rest is similar. Train it for 100 episodes with the right hyperparameters and without rendering.\n\nclass QLearningAgent:\n    \"\"\"\n    Q-learning agent.\n    \"\"\"\n    \n    def __init__(self, env, gamma, exploration, decay, alpha):\n        \"\"\"\n        :param env: gym-like environment\n        :param gamma: discount factor\n        :param exploration: exploration parameter\n        :param decay: exploration decay parameter\n        :param alpha: learning rate\n        \"\"\"\n        self.env = env\n        self.gamma = gamma\n        self.exploration = exploration\n        self.decay = decay\n        self.alpha = alpha\n        \n        # Q_table\n        self.Q = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n    \n    def act(self, state):\n        \"Returns an action using epsilon-greedy action selection.\"\n        \n        action = rng.choice(np.where(self.Q[state, :] == self.Q[state, :].max())[0])\n        \n        if rng.random() < self.exploration:\n            action = self.env.action_space.sample() \n        \n        return action\n    \n    def update(self, state, action, reward, next_state, done):\n        \"Updates the agent using a single transition.\"\n        \n        # Bellman target\n        target = reward\n        \n        if not done:\n            target += self.gamma * self.Q[next_state, :].max()\n        \n        # Update the Q-value\n        self.Q[state, action] += self.alpha * (target - self.Q[state, action])\n            \n        # Decay exploration parameter\n        self.exploration = self.exploration * (1 - self.decay)\n            \n    \n    def train(self, nb_episodes, render=True):\n        \"Runs the agent on the environment for nb_episodes. Returns the list of obtained returns.\"\n\n        # Returns\n        returns = []\n\n        # Fixed number of episodes\n        for episode in range(nb_episodes):\n\n            # Reset\n            state = self.env.reset()\n            done = False\n            nb_steps = 0\n\n            # Store rewards\n            rewards = []\n\n            # Sample the episode\n            while not done:\n                \n                # Render the current state\n                if render:\n                    self.env.render(self.Q)\n\n                # Select an action \n                action = self.act(state)\n\n                # Perform the action\n                next_state, reward, done, info = self.env.step(action)\n                \n                # Append reward\n                rewards.append(reward)\n\n                # Learn from the transition\n                self.update(state, action, reward, next_state, done)\n\n                # Go in the next state\n                state = next_state\n\n                # Increment time\n                nb_steps += 1\n                \n                if done and render:\n                    self.env.render(self.Q)\n\n            # Compute the discounted return of the episode.\n            return_episode = self.discounted_return(rewards)    \n\n            # Store info\n            returns.append(return_episode)\n            \n        return returns\n    \n    def test(self, render=True):\n        \"Performs a test episode without exploration.\"\n        previous_epsilon = self.epsilon\n        self.epsilon = 0.0\n        \n        # Reset\n        state = self.env.reset()\n        done = False\n        nb_steps = 0\n        rewards= 0\n\n        # Sample the episode\n        while not done:\n            if render:\n                clear_output(wait=True)\n                self.env.render(self.Q)\n            action = self.act(state)\n            next_state, reward, done, info = self.env.step(action)\n            rewards += reward\n            state = next_state\n            nb_steps += 1\n            \n        self.epsilon = previous_epsilon\n            \n        return rewards, nb_steps\n    \n    def discounted_return(self, rewards):\n        \"Computes the discounted return of an episode based on the list of rewards.\"\n        ret = 0.0\n        for reward in reversed(rewards):\n            ret = reward + self.gamma*ret\n        return ret\n\n\n# Parameters\ngamma = 0.99\nepsilon = 0.1\ndecay_epsilon = 0\nalpha = 0.1\nnb_episodes = 100\n\n# Create the environment\nenv = GridWorld(rewards=[100, -100, -1])\n\n# Create the agent\nagent = QLearningAgent(env, gamma, epsilon, decay_epsilon, alpha)\n\n# Train the agent\nreturns = agent.train(nb_episodes, render=False)\n\nplt.figure(figsize=(12, 5))\nplt.plot(returns)\nplt.show()\n\n\n\n\n\n\n\n\nQ: Train a Q-learning agent with rendering on. Observe in particular which Q-values are updates when the agent reaches the target. Is it efficient?\n\n# Parameters\ngamma = 0.99\nepsilon = 0.1\ndecay_epsilon = 0\nalpha = 0.1\nnb_episodes = 10\n\n# Create the environment\nenv = GridWorld(rewards=[100, -100, -1])\n\n# Create the agent\nagent = QLearningAgent(env, gamma, epsilon, decay_epsilon, alpha)\n\n# Train the agent\nreturns = agent.train(nb_episodes, render=True)\n\n\n\n\n\n\n\n\nQ: Modify your agent so that it uses softmax action selection, with a temperature \\tau = 1.0 and a suitable decay. What does it change?\nIf you have time, write a generic class for the Q-learning agent where you can select the action selection method flexibly.\n\nclass SoftQLearningAgent(QLearningAgent):\n    \"\"\"\n    Q-learning agent with softmax or e-greedy AS.\n    \"\"\"\n    \n    def __init__(self, env, gamma, action_selection, alpha):\n        \"\"\"\n        :param env: gym-like environment\n        :param gamma: discount factor\n        :param action selection: exploration mechanism\n        :param alpha: learning rate\n        \"\"\"\n        self.action_selection = action_selection\n        \n        super().__init__(env, gamma, action_selection['param'], action_selection['decay'], alpha)\n    \n    def act(self, state):\n        \"Returns an action using epsilon-greedy or softmax  action selection.\"\n            \n        if self.action_selection['type'] == \"egreedy\":\n            # epsilon-greedy\n            if rng.uniform(0, 1, 1) < self.exploration:\n                action = self.env.action_space.sample() \n            else:\n                action = rng.choice(np.where(self.Q[state, :] == self.Q[state, :].max())[0])\n        else: \n            # softmax\n            logits = np.exp((self.Q[state, :] - self.Q[state, :].max())/self.exploration)\n            probas = logits / np.sum(logits)\n            action = rng.choice(range(4), p=probas) \n        \n        return action\n        \n\n\n# Parameters\ngamma = 0.99\n#action_selection  = {'type': \"egreedy\", \"param\": 0.1, \"decay\": 0.0}\naction_selection  = {'type': \"softmax\", \"param\": 1.0, \"decay\": 0.0}\nalpha = 0.1\nnb_episodes = 100\n\n# Create the environment\nenv = GridWorld()\n\n# Create the agent\nagent = SoftQLearningAgent(env, gamma, action_selection, alpha)\n\n# Train the agent for 200 episodes\nreturns = agent.train(nb_episodes, render=False)\n\nplt.figure(figsize=(12, 5))\nplt.plot(returns)\nplt.show()\n\n\n\n\n\n\n\n\nA: The agent explores much less at the end of training, as the difference between the Q-values becomes high enough to become greedy. In particular, it quickly stops to go to the green triangles. In this environment, there is no real need to decay tau."
  },
  {
    "objectID": "exercises/10-Eligibilitytraces-solution.html#eligibility-traces",
    "href": "exercises/10-Eligibilitytraces-solution.html#eligibility-traces",
    "title": "Eligibility traces",
    "section": "Eligibility traces",
    "text": "Eligibility traces\nThe main drawback of Q-learning is that it needs many episodes to converge (sample complexity).\nOne way to speed up learning is to use eligibility traces, one per state-action pair:\ntraces = np.zeros((nb_states, nb_actions))\nAfter each transition (s_t, a_t), Q(\\lambda) updates a trace e(s_t, a_t) and modifies all Q-values as:\n\nThe trace of the last transition is incremented from 1:\n\ne(s_t, a_t) = e(s_t, a_t) +1\n\nQ(\\lambda)-learning is applied on ALL Q-values, using the TD error at time t:\n\nQ(s, a) = Q(s, a) + \\alpha \\, (r_{t+1} + \\gamma \\, \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)) \\, e(s, a)\n\nAll traces are exponentially decreased using the trace parameter \\lambda (e.g. 0.7):\n\n\ne(s, a) = \\lambda \\, \\gamma \\, e(s, a)\n\nAll traces are reset to 0 at the beginning of an episode.\nQ: Implement eligibility traces in your Q(\\lambda)-learning agent and see if it improves convergence. Train it with rendering on and observe how all Q-values are updated.\n\nclass QLambdaLearningAgent(SoftQLearningAgent):\n    \"\"\"\n    Q(lambda)-learning agent with softmax or e-greedy AS and eligibility traces.\n    \"\"\"\n    \n    def __init__(self, env, gamma, lbda, action_selection, alpha):\n        \"\"\"\n        :param env: gym-like environment\n        :param gamma: discount factor\n        :param lbda: trace\n        :param action selection: exploration mechanism\n        :param alpha: learning rate\n        \"\"\"\n        self.lbda = lbda\n        \n        # Traces\n        self.traces = np.zeros([env.observation_space.n, env.action_space.n])\n        \n        super().__init__(env, gamma, action_selection, alpha)\n        \n\n    \n    def update(self, state, action, reward, next_state, done):\n        \n        # Bellman target\n        target = reward\n        if not done:\n            target += self.gamma * self.Q[next_state, :].max()\n        \n        # Update ALL Q-values\n        self.Q += self.alpha * (target - self.Q[state, action]) * self.traces\n            \n        # Decay exploration parameter\n        self.exploration = self.exploration * (1 - self.decay)\n        \n        \n    def train(self, nb_episodes, render=True):\n        \"Runs the agent on the environment for nb_episodes. Returns the list of obtained returns.\"\n\n        # Returns\n        returns = []\n\n        # Fixed number of episodes\n        for episode in range(nb_episodes):\n\n            # Reset\n            state = self.env.reset()\n            done = False\n            nb_steps = 0\n            \n            # Reset traces\n            self.traces *= 0.0\n\n            # Store rewards\n            rewards = []\n\n            # Sample the episode\n            while not done:\n                \n                # Render the current state\n                if render:\n                    self.env.render(self.Q)\n\n                # Select an action \n                action = self.act(state)\n\n                # Perform the action\n                next_state, reward, done, info = self.env.step(action)\n                \n                # Append reward\n                rewards.append(reward)\n                \n                # Increment trace\n                self.traces[state, action] += 1\n\n                # Learn from the transition\n                self.update(state, action, reward, next_state, done)\n                \n                # Update all traces\n                self.traces *= self.gamma * self.lbda\n\n                # Go in the next state\n                state = next_state\n\n                # Increment time\n                nb_steps += 1\n                \n                if done and render:\n                    self.env.render(self.Q)\n\n            # Compute the discounted return of the episode.\n            return_episode = self.discounted_return(rewards)    \n\n            # Store info\n            returns.append(return_episode)\n            \n        return returns\n\n\n# Parameters\ngamma = 0.99\nlbda = 0.7\n#action_selection  = {'type': \"egreedy\", \"param\": 0.1, \"decay\": 0.0}\naction_selection  = {'type': \"softmax\", \"param\": 1.0, \"decay\": 0.0}\nalpha = 0.1\nnb_episodes = 100\n\n# Create the environment\nenv = GridWorld()\n\n# Create the agent\nagent = QLambdaLearningAgent(env, gamma, lbda, action_selection, alpha)\n\n# Train the agent\nreturns = agent.train(nb_episodes, render=False)\n\nplt.figure(figsize=(12, 5))\nplt.plot(returns)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Parameters\ngamma = 0.99\nlbda = 0.7\n#action_selection  = {'type': \"egreedy\", \"param\": 0.1, \"decay\": 0.0}\naction_selection  = {'type': \"softmax\", \"param\": 1.0, \"decay\": 0.0}\nalpha = 0.1\nnb_episodes = 10\n\n# Create the environment\nenv = GridWorld()\n\n# Create the agent\nagent = QLambdaLearningAgent(env, gamma, lbda, action_selection, alpha)\n\n# Train the agent\nreturns = agent.train(nb_episodes, render=True)\n\n\n\n\n\n\n\n\nQ: Vary the trace parameter \\lambda and discuss its influence.\n\n# Parameters\ngamma = 0.99\n#action_selection  = {'type': \"egreedy\", \"param\": 0.1, \"decay\": 0.0}\naction_selection  = {'type': \"softmax\", \"param\": 1.0, \"decay\": 0.0}\nalpha = 0.1\nnb_episodes = 100\n\nlist_returns = []\n\nfor lbda in np.linspace(0.1, 1.0, 10):\n\n    # Create the environment\n    env = GridWorld()\n\n    # Create the agent\n    agent = QLambdaLearningAgent(env, gamma, lbda, action_selection, alpha)\n\n    # Train the agent\n    returns = agent.train(nb_episodes, render=False)\n    \n    list_returns.append(returns)\n    \nplt.figure(figsize=(15, 6))\nfor idx, lbda in enumerate(np.linspace(0.1, 1.0, 10)):\n    plt.plot(list_returns[idx], label=str(lbda))\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nA: \\lambda should not be too high nor too low in order to speed up learning. It controls the bias/variance trade-off.\nQ: Increase the size of Gridworld to 10x10 and observe how long it takes to learn the optimal strategy.\nenv = GridWorld(size=10)\nComment on the curse of dimensionality and the interest of tabular RL for complex tasks with large state spaces and sparse rewards (e.g. robotics).\n\n# Parameters\ngamma = 0.99\nlbda = 0.7\n#action_selection  = {'type': \"egreedy\", \"param\": 0.1, \"decay\": 0.0}\naction_selection  = {'type': \"softmax\", \"param\": 1.0, \"decay\": 0.0}\nalpha = 0.1\nnb_episodes = 100\n\n# Create the environment\nenv = GridWorld(size=10)\n\n# Create the agent\nagent = QLambdaLearningAgent(env, gamma, lbda, action_selection, alpha)\n\n# Train the agent\nreturns = agent.train(nb_episodes, render=False)\n\nplt.figure(figsize=(12, 5))\nplt.plot(returns)\nplt.show()\n\n\n\n\n\n\n\n\nA: When the Gridworld is too big, the likelihood to hit the target per chance when exploring is very low. There are a lot of unsuccessful trials before learning starts to occur. But it happens after a while."
  },
  {
    "objectID": "exercises/11-Keras-solution.html",
    "href": "exercises/11-Keras-solution.html",
    "title": "Keras tutorial",
    "section": "",
    "text": "The goal of this tutorial is to very quickly present keras, the high-level API of tensorflow, as it has already been seen in the Neurocomputing exercises. We will train a small fully-connected network on MNIST and observe what happens when the inputs or outputs are correlated, by training successively on the 0 digits, then the 1, etc."
  },
  {
    "objectID": "exercises/11-Keras-solution.html#keras",
    "href": "exercises/11-Keras-solution.html#keras",
    "title": "Keras tutorial",
    "section": "Keras",
    "text": "Keras\nThe first step is to install tensorflow. The easiest way is to use pip:\npip install tensorflow\nkeras is now available as a submodule of tensorflow (you can also install it as a separate package):\nimport tensorflow as tf\nKeras provides a lot of ready-made layer types, activation functions, optimizers and so on. Do not hesitate to read its documentation on https://keras.io.\n\nimport tensorflow as tf\n\nThe most important object in keras is Sequential. It is a container where you sequentially add layers of neurons (fully-connected, convolutional, recurrent, etc) and other stuff. It represents your model, i.e. the neural network itself.\nmodel = tf.keras.models.Sequential()\nYou can then add() layers to the model. A fully-connected layer is called Dense in keras.\nLet’s create a MLP with 10 input neurons, two hidden layers with 100 hidden neurons each and 3 output neurons.\nThe input layer is represented by the Input layer:\nmodel.add(tf.keras.layers.Input((10,)))\nThe first hidden layer can be added to the model with:\nmodel.add(tf.keras.layers.Dense(100, activation=\"relu\"))\nThe layer has 100 neurons and uses the ReLU activation function. One could optionally define the activation function as an additional “layer”, but it is usually not needed:\nmodel.add(tf.keras.layers.Dense(100))\nmodel.add(tf.keras.layers.Activation('relu'))\nAdding more layers is straightforward:\nmodel.add(tf.keras.layers.Dense(100, activation=\"relu\"))\nFinally, we can add the output layer. The activation function depends on the problem:\n\nFor regression problems, a linear activation function should be used when the targets can take any value (e.g. Q-values):\n\nmodel.add(tf.keras.layers.Dense(3, activation=\"linear\"))\nIf the targets are bounded between 0 and 1, a logistic/sigmoid function can be used:\nmodel.add(tf.keras.layers.Dense(3, activation=\"sigmoid\"))\n\nFor multi-class classification problems, a softmax activation function should be used:\n\nmodel.add(tf.keras.layers.Dense(3, activation=\"softmax\"))\nThis defines fully the structure of your desired neural network.\nQ: Implement a neural network for classification with 10 input neurons, two hidden layers with 100 neurons each (using ReLU) and 3 output neurons.\nHint: print(model.summary()) gives you a summary of the architecture of your model. Note in particular the number of trainable parameters (weights and biases).\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Input((10,)))\nmodel.add(tf.keras.layers.Dense(100, activation=\"relu\"))\nmodel.add(tf.keras.layers.Dense(100, activation='relu'))\nmodel.add(tf.keras.layers.Dense(3, activation='softmax'))\nprint(model.summary())\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_9 (Dense)             (None, 100)               1100      \n                                                                 \n dense_10 (Dense)            (None, 100)               10100     \n                                                                 \n dense_11 (Dense)            (None, 3)                 303       \n                                                                 \n=================================================================\nTotal params: 11,503\nTrainable params: 11,503\nNon-trainable params: 0\n_________________________________________________________________\nNone\n\n\nThe next step is to choose an optimizer for the neural network, i.e. a variant of gradient descent that will be used to iteratively modify the parameters.\nkeras provides an extensive list of optimizers: https://keras.io/optimizers/. The most useful in practice are:\n\nSGD, the vanilla stochastic gradient descent.\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n\nRMSprop, using second moments:\n\noptimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n\nAdam:\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\nChoosing a optimizer is a matter of taste and trial-and-error. In deep RL, a good choice is Adam: the default values for its other parameters are usually good, it converges well, so your only job is to find the right learning rate.\nFinally, the model must be compiled by defining:\n\nA loss function. For multi-class classification, it should be 'categorical_crossentropy'. For regression, it can be 'mse'. See the list of built-in loss functions here: https://keras.io/losses/ but know that you can also simply define your own.\nThe chosen optimizer.\nThe metrics, i.e. what you want tensorflow to print during training. By default it only prints the current value of the loss function. For classification tasks, it usually makes more sense to also print the accuracy.\n\nmodel.compile(\n    loss='categorical_crossentropy', \n    optimizer=optimizer,\n    metrics=['accuracy']\n)\nQ: Compile the model for classification, using the Adam optimizer and a learning rate of 0.01.\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n\nmodel.compile(\n    loss='categorical_crossentropy', \n    optimizer=optimizer,\n    metrics=['accuracy']\n)\n\nLet’s now train the model on some dummy data. To show the power of deep neural networks, we will try to learn noise by heart.\nThe following cell creates an input tensor X with 1000 random vectors of 10 elements, with values sampled between -1 and 1. The targets (desired outputs) t are class indices (0, 1 or 2), also randomly selected.\nHowever, neural networks expect one-hot encoded vectors for the target, i.e. (1, 0, 0), (0, 1, 0), (0, 0, 1) instead of 0, 1, 2. The method tf.keras.utils.to_categorical allows you to do that.\n\nX = np.random.uniform(-1.0, 1.0, (1000, 10))\nt = np.random.randint(0, 3, (1000, ))\nT = tf.keras.utils.to_categorical(t, 3)\n\nLet’s learn it. The Sequential model has a method called fit() where you simply pass the training data (X, T) and some other parameters, such as:\n\nthe batch size,\nthe total number of epochs,\nthe proportion of training examples to keep in order to compute the validation loss/accuracy (optional but recommmended).\n\n# Training\nhistory = tf.keras.callbacks.History()\n\nmodel.fit(\n    X, T,\n    batch_size=100, \n    epochs=50,\n    validation_split=0.1,\n    callbacks=[history]\n)\nQ: Train the model on the data, using a batch size of 100 for 50 epochs. Explain why you obtained this result.\n\nhistory = model.fit(\n    X, T,\n    batch_size=100, \n    epochs=50,\n    validation_split=0.1,\n    verbose=2\n)\n\nEpoch 1/50\n\n\n2022-11-15 13:09:40.852114: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n\n\n9/9 - 0s - loss: 1.1150 - accuracy: 0.3489 - val_loss: 1.1119 - val_accuracy: 0.3500 - 395ms/epoch - 44ms/step\nEpoch 2/50\n9/9 - 0s - loss: 1.0725 - accuracy: 0.4256 - val_loss: 1.0961 - val_accuracy: 0.3900 - 88ms/epoch - 10ms/step\nEpoch 3/50\n\n\n2022-11-15 13:09:41.054430: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n\n\n9/9 - 0s - loss: 1.0460 - accuracy: 0.4667 - val_loss: 1.1063 - val_accuracy: 0.4000 - 93ms/epoch - 10ms/step\nEpoch 4/50\n9/9 - 0s - loss: 1.0227 - accuracy: 0.4778 - val_loss: 1.1037 - val_accuracy: 0.4000 - 82ms/epoch - 9ms/step\nEpoch 5/50\n9/9 - 0s - loss: 1.0023 - accuracy: 0.4911 - val_loss: 1.1187 - val_accuracy: 0.4000 - 83ms/epoch - 9ms/step\nEpoch 6/50\n9/9 - 0s - loss: 0.9796 - accuracy: 0.5222 - val_loss: 1.1354 - val_accuracy: 0.4100 - 83ms/epoch - 9ms/step\nEpoch 7/50\n9/9 - 0s - loss: 0.9424 - accuracy: 0.5444 - val_loss: 1.1634 - val_accuracy: 0.3800 - 85ms/epoch - 9ms/step\nEpoch 8/50\n9/9 - 0s - loss: 0.9101 - accuracy: 0.5856 - val_loss: 1.1859 - val_accuracy: 0.3800 - 84ms/epoch - 9ms/step\nEpoch 9/50\n9/9 - 0s - loss: 0.8892 - accuracy: 0.5933 - val_loss: 1.2218 - val_accuracy: 0.3700 - 82ms/epoch - 9ms/step\nEpoch 10/50\n9/9 - 0s - loss: 0.8288 - accuracy: 0.6411 - val_loss: 1.2725 - val_accuracy: 0.3800 - 80ms/epoch - 9ms/step\nEpoch 11/50\n9/9 - 0s - loss: 0.7997 - accuracy: 0.6500 - val_loss: 1.3129 - val_accuracy: 0.3900 - 81ms/epoch - 9ms/step\nEpoch 12/50\n9/9 - 0s - loss: 0.7529 - accuracy: 0.6922 - val_loss: 1.3128 - val_accuracy: 0.3500 - 82ms/epoch - 9ms/step\nEpoch 13/50\n9/9 - 0s - loss: 0.6836 - accuracy: 0.7289 - val_loss: 1.3802 - val_accuracy: 0.4000 - 82ms/epoch - 9ms/step\nEpoch 14/50\n9/9 - 0s - loss: 0.6284 - accuracy: 0.7467 - val_loss: 1.4751 - val_accuracy: 0.3600 - 82ms/epoch - 9ms/step\nEpoch 15/50\n9/9 - 0s - loss: 0.5829 - accuracy: 0.7867 - val_loss: 1.4785 - val_accuracy: 0.3800 - 83ms/epoch - 9ms/step\nEpoch 16/50\n9/9 - 0s - loss: 0.5330 - accuracy: 0.7967 - val_loss: 1.7002 - val_accuracy: 0.3700 - 81ms/epoch - 9ms/step\nEpoch 17/50\n9/9 - 0s - loss: 0.4989 - accuracy: 0.8278 - val_loss: 1.6326 - val_accuracy: 0.4000 - 81ms/epoch - 9ms/step\nEpoch 18/50\n9/9 - 0s - loss: 0.4842 - accuracy: 0.8189 - val_loss: 1.9201 - val_accuracy: 0.3500 - 83ms/epoch - 9ms/step\nEpoch 19/50\n9/9 - 0s - loss: 0.4380 - accuracy: 0.8356 - val_loss: 1.7465 - val_accuracy: 0.3900 - 81ms/epoch - 9ms/step\nEpoch 20/50\n9/9 - 0s - loss: 0.3821 - accuracy: 0.8744 - val_loss: 1.9561 - val_accuracy: 0.3500 - 85ms/epoch - 9ms/step\nEpoch 21/50\n9/9 - 0s - loss: 0.3353 - accuracy: 0.8911 - val_loss: 2.0040 - val_accuracy: 0.2600 - 90ms/epoch - 10ms/step\nEpoch 22/50\n9/9 - 0s - loss: 0.2960 - accuracy: 0.9100 - val_loss: 2.1072 - val_accuracy: 0.3400 - 82ms/epoch - 9ms/step\nEpoch 23/50\n9/9 - 0s - loss: 0.2388 - accuracy: 0.9456 - val_loss: 2.1638 - val_accuracy: 0.3500 - 83ms/epoch - 9ms/step\nEpoch 24/50\n9/9 - 0s - loss: 0.2336 - accuracy: 0.9322 - val_loss: 2.2757 - val_accuracy: 0.2900 - 83ms/epoch - 9ms/step\nEpoch 25/50\n9/9 - 0s - loss: 0.1806 - accuracy: 0.9633 - val_loss: 2.4390 - val_accuracy: 0.3400 - 82ms/epoch - 9ms/step\nEpoch 26/50\n9/9 - 0s - loss: 0.1815 - accuracy: 0.9544 - val_loss: 2.4587 - val_accuracy: 0.3400 - 81ms/epoch - 9ms/step\nEpoch 27/50\n9/9 - 0s - loss: 0.1586 - accuracy: 0.9656 - val_loss: 2.5464 - val_accuracy: 0.3000 - 84ms/epoch - 9ms/step\nEpoch 28/50\n9/9 - 0s - loss: 0.1457 - accuracy: 0.9722 - val_loss: 2.6396 - val_accuracy: 0.3300 - 83ms/epoch - 9ms/step\nEpoch 29/50\n9/9 - 0s - loss: 0.1386 - accuracy: 0.9789 - val_loss: 2.5508 - val_accuracy: 0.3300 - 82ms/epoch - 9ms/step\nEpoch 30/50\n9/9 - 0s - loss: 0.1091 - accuracy: 0.9833 - val_loss: 2.8272 - val_accuracy: 0.3400 - 87ms/epoch - 10ms/step\nEpoch 31/50\n9/9 - 0s - loss: 0.0816 - accuracy: 0.9944 - val_loss: 2.8505 - val_accuracy: 0.3500 - 90ms/epoch - 10ms/step\nEpoch 32/50\n9/9 - 0s - loss: 0.0709 - accuracy: 0.9967 - val_loss: 2.8556 - val_accuracy: 0.3400 - 92ms/epoch - 10ms/step\nEpoch 33/50\n9/9 - 0s - loss: 0.0604 - accuracy: 0.9978 - val_loss: 2.9199 - val_accuracy: 0.3400 - 89ms/epoch - 10ms/step\nEpoch 34/50\n9/9 - 0s - loss: 0.0495 - accuracy: 0.9989 - val_loss: 2.9985 - val_accuracy: 0.3300 - 90ms/epoch - 10ms/step\nEpoch 35/50\n9/9 - 0s - loss: 0.0407 - accuracy: 1.0000 - val_loss: 3.2302 - val_accuracy: 0.3200 - 88ms/epoch - 10ms/step\nEpoch 36/50\n9/9 - 0s - loss: 0.0325 - accuracy: 1.0000 - val_loss: 3.1978 - val_accuracy: 0.3300 - 89ms/epoch - 10ms/step\nEpoch 37/50\n9/9 - 0s - loss: 0.0313 - accuracy: 0.9989 - val_loss: 3.1832 - val_accuracy: 0.3500 - 99ms/epoch - 11ms/step\nEpoch 38/50\n9/9 - 0s - loss: 0.0322 - accuracy: 0.9978 - val_loss: 3.3745 - val_accuracy: 0.3200 - 87ms/epoch - 10ms/step\nEpoch 39/50\n9/9 - 0s - loss: 0.0271 - accuracy: 1.0000 - val_loss: 3.4600 - val_accuracy: 0.3100 - 85ms/epoch - 9ms/step\nEpoch 40/50\n9/9 - 0s - loss: 0.0230 - accuracy: 1.0000 - val_loss: 3.4435 - val_accuracy: 0.3200 - 88ms/epoch - 10ms/step\nEpoch 41/50\n9/9 - 0s - loss: 0.0179 - accuracy: 1.0000 - val_loss: 3.5294 - val_accuracy: 0.3400 - 88ms/epoch - 10ms/step\nEpoch 42/50\n9/9 - 0s - loss: 0.0156 - accuracy: 1.0000 - val_loss: 3.5217 - val_accuracy: 0.3200 - 85ms/epoch - 9ms/step\nEpoch 43/50\n9/9 - 0s - loss: 0.0139 - accuracy: 1.0000 - val_loss: 3.5561 - val_accuracy: 0.3300 - 88ms/epoch - 10ms/step\nEpoch 44/50\n9/9 - 0s - loss: 0.0126 - accuracy: 1.0000 - val_loss: 3.5808 - val_accuracy: 0.3300 - 89ms/epoch - 10ms/step\nEpoch 45/50\n9/9 - 0s - loss: 0.0119 - accuracy: 1.0000 - val_loss: 3.6306 - val_accuracy: 0.3200 - 87ms/epoch - 10ms/step\nEpoch 46/50\n9/9 - 0s - loss: 0.0108 - accuracy: 1.0000 - val_loss: 3.6863 - val_accuracy: 0.3200 - 87ms/epoch - 10ms/step\nEpoch 47/50\n9/9 - 0s - loss: 0.0100 - accuracy: 1.0000 - val_loss: 3.6891 - val_accuracy: 0.3200 - 87ms/epoch - 10ms/step\nEpoch 48/50\n9/9 - 0s - loss: 0.0094 - accuracy: 1.0000 - val_loss: 3.7300 - val_accuracy: 0.3200 - 85ms/epoch - 9ms/step\nEpoch 49/50\n9/9 - 0s - loss: 0.0089 - accuracy: 1.0000 - val_loss: 3.7520 - val_accuracy: 0.3200 - 84ms/epoch - 9ms/step\nEpoch 50/50\n9/9 - 0s - loss: 0.0084 - accuracy: 1.0000 - val_loss: 3.8023 - val_accuracy: 0.3200 - 86ms/epoch - 10ms/step\n\n\nA: The final training is 100%, the validation accuracy is 33% (may vary depending on initialization). The network has learned the training examples by heart, although they are totally random, but totally fails to generalize.\nThe main is reason is that we have only 1000 training examples, with a total number of free parameters (VC dimension) around 11500. By definition, the model can learn this training set perfectly, although it is totally random. Its VC dimension is however way to high to generalize anything. It is even worse here: as the data is random, there is nothing to generalize. A nice example to understand why NN overfit…"
  },
  {
    "objectID": "exercises/11-Keras-solution.html#training-a-mlp-on-mnist",
    "href": "exercises/11-Keras-solution.html#training-a-mlp-on-mnist",
    "title": "Keras tutorial",
    "section": "Training a MLP on MNIST",
    "text": "Training a MLP on MNIST\nLet’s now try to learn something a bit more serious, the MNIST dataset. The following cell load the MNIST data (training set 60000 28x28 monochrome images, test set of 10000 images), normalizes it (values betwen 0 and 1 for each pixel), removes the mean image from the training set and transforms the targets to one-hot encoded vectors for the 10 classes. See the neurocomputing exercise for more details.\n\n# Load the MNIST dataset\n(X_train, t_train), (X_test, t_test) = tf.keras.datasets.mnist.load_data()\nprint(\"Training data:\", X_train.shape, t_train.shape)\nprint(\"Test data:\", X_test.shape, t_test.shape)\n\n# Reshape the images to vectors and normalize\nX_train = X_train.reshape(X_train.shape[0], 784).astype('float32') / 255.\nX_test = X_test.reshape(X_test.shape[0], 784).astype('float32') / 255.\n\n# Mean removal\nX_mean = np.mean(X_train, axis=0)\nX_train -= X_mean\nX_test -= X_mean\n\n# One-hot encoded outputs\nT_train = tf.keras.utils.to_categorical(t_train, 10)\nT_test = tf.keras.utils.to_categorical(t_test, 10)\n\nTraining data: (60000, 28, 28) (60000,)\nTest data: (10000, 28, 28) (10000,)\n\n\nQ: Create a fully connected neural network with 784 input neurons (one per pixel), 10 softmax output neurons and whatever you want in the middle, so that it can reach around 98% validation accuracy after 20 epochs.\n\nPut the network creation (including compile()) in a method create_model(), so that you can create a model multiple times.\nChoose a good value for the learning rate.\nDo not exagerate with the number of layers and neurons. Two or there hidden layers with 100 to 300 neurons are more than enough.\nYou will quickly observe that the network overfits: the training accuracy is higher than the validation accuracy. The training accuracy actually goes to 100% if your network is too big. In that case, feel free to add a dropout layer after each fully-connected layer:\n\nmodel.add(tf.keras.layers.Dropout(0.5))\n\ndef create_model():\n    # Create the model\n    model = tf.keras.models.Sequential()\n    \n    # Input layer with 784 pixels\n    model.add(tf.keras.layers.Input((784,)))\n\n    # Hidden layer with 150 neurons\n    model.add(tf.keras.layers.Dense(150, activation=\"relu\"))\n    model.add(tf.keras.layers.Dropout(0.5))\n\n    # Second hidden layer with 100 neurons\n    model.add(tf.keras.layers.Dense(100, activation=\"relu\"))\n    model.add(tf.keras.layers.Dropout(0.5))\n\n    # Softmax output layer with 10 neurons\n    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n\n    # Learning rule\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n\n    # Loss function\n    model.compile(\n        loss='categorical_crossentropy', # loss\n        optimizer=optimizer, # learning rule\n        metrics=['accuracy'] # show accuracy\n    )\n    print(model.summary())\n    \n    return model\n\n\nmodel = create_model()\n\n# Training\nhistory = tf.keras.callbacks.History()\n\nmodel.fit(\n    X_train, T_train,\n    batch_size=100, \n    epochs=20,\n    validation_split=0.1,\n    callbacks=[history]\n)\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_3 (Dense)             (None, 150)               117750    \n                                                                 \n dropout (Dropout)           (None, 150)               0         \n                                                                 \n dense_4 (Dense)             (None, 100)               15100     \n                                                                 \n dropout_1 (Dropout)         (None, 100)               0         \n                                                                 \n dense_5 (Dense)             (None, 10)                1010      \n                                                                 \n=================================================================\nTotal params: 133,860\nTrainable params: 133,860\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/20\n\n\n/Users/vitay/Applications/miniforge3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super().__init__(name, **kwargs)\n\n\n  1/540 [..............................] - ETA: 3:03 - loss: 2.6045 - accuracy: 0.0500\n\n\n2022-11-15 13:05:05.914474: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n\n\n539/540 [============================>.] - ETA: 0s - loss: 0.5274 - accuracy: 0.8360\n\n\n2022-11-15 13:05:10.488618: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n\n\n540/540 [==============================] - 5s 9ms/step - loss: 0.5271 - accuracy: 0.8361 - val_loss: 0.1576 - val_accuracy: 0.9532\nEpoch 2/20\n540/540 [==============================] - 5s 9ms/step - loss: 0.2489 - accuracy: 0.9263 - val_loss: 0.1201 - val_accuracy: 0.9650\nEpoch 3/20\n540/540 [==============================] - 5s 9ms/step - loss: 0.2015 - accuracy: 0.9406 - val_loss: 0.1029 - val_accuracy: 0.9680\nEpoch 4/20\n540/540 [==============================] - 5s 9ms/step - loss: 0.1756 - accuracy: 0.9461 - val_loss: 0.0901 - val_accuracy: 0.9732\nEpoch 5/20\n540/540 [==============================] - 5s 9ms/step - loss: 0.1529 - accuracy: 0.9546 - val_loss: 0.0822 - val_accuracy: 0.9772\nEpoch 6/20\n540/540 [==============================] - 5s 9ms/step - loss: 0.1419 - accuracy: 0.9568 - val_loss: 0.0832 - val_accuracy: 0.9758\nEpoch 7/20\n540/540 [==============================] - 5s 9ms/step - loss: 0.1312 - accuracy: 0.9599 - val_loss: 0.0786 - val_accuracy: 0.9765\nEpoch 8/20\n540/540 [==============================] - 5s 9ms/step - loss: 0.1219 - accuracy: 0.9629 - val_loss: 0.0735 - val_accuracy: 0.9772\nEpoch 9/20\n540/540 [==============================] - 5s 9ms/step - loss: 0.1145 - accuracy: 0.9643 - val_loss: 0.0768 - val_accuracy: 0.9767\nEpoch 10/20\n540/540 [==============================] - 5s 9ms/step - loss: 0.1094 - accuracy: 0.9659 - val_loss: 0.0681 - val_accuracy: 0.9803\nEpoch 11/20\n540/540 [==============================] - 5s 9ms/step - loss: 0.1031 - accuracy: 0.9670 - val_loss: 0.0698 - val_accuracy: 0.9793\nEpoch 12/20\n540/540 [==============================] - 5s 9ms/step - loss: 0.1032 - accuracy: 0.9685 - val_loss: 0.0679 - val_accuracy: 0.9793\nEpoch 13/20\n540/540 [==============================] - 5s 9ms/step - loss: 0.0983 - accuracy: 0.9682 - val_loss: 0.0683 - val_accuracy: 0.9788\nEpoch 14/20\n540/540 [==============================] - 5s 9ms/step - loss: 0.0911 - accuracy: 0.9710 - val_loss: 0.0687 - val_accuracy: 0.9790\nEpoch 15/20\n540/540 [==============================] - 5s 9ms/step - loss: 0.0918 - accuracy: 0.9714 - val_loss: 0.0641 - val_accuracy: 0.9818\nEpoch 16/20\n540/540 [==============================] - 5s 9ms/step - loss: 0.0881 - accuracy: 0.9727 - val_loss: 0.0645 - val_accuracy: 0.9800\nEpoch 17/20\n540/540 [==============================] - 5s 9ms/step - loss: 0.0841 - accuracy: 0.9734 - val_loss: 0.0652 - val_accuracy: 0.9805\nEpoch 18/20\n540/540 [==============================] - 5s 9ms/step - loss: 0.0826 - accuracy: 0.9738 - val_loss: 0.0644 - val_accuracy: 0.9798\nEpoch 19/20\n540/540 [==============================] - 5s 9ms/step - loss: 0.0803 - accuracy: 0.9742 - val_loss: 0.0671 - val_accuracy: 0.9792\nEpoch 20/20\n540/540 [==============================] - 5s 9ms/step - loss: 0.0826 - accuracy: 0.9742 - val_loss: 0.0603 - val_accuracy: 0.9810\n\n\n<keras.callbacks.History at 0x1598b4f40>\n\n\nAfter training, one should evaluate the model on the test set. keras provides an evaluate() method that computes the different metrics (in our case the loss) on the data:\nscore = model.evaluate(X_test, T_test)\nAnother solution would be to predict() labels on the test set and manually compare them to the ground truth:\nY = model.predict(X_test)\nloss = - np.mean(T_test * np.log(Y))\npredicted_classes = np.argmax(Y, axis=1)\naccuracy = 1.0 - np.sum(predicted_classes != t_test)/t_test.shape[0]\nAnother important thing to visualize after training is how the training and validation loss (or accuracy) evolved during training. The fit() method updates a History object which contains the history of your metrics (loss and accuracy) after each epoch of training. These are simple numpy arrays, accessible with:\nhistory.history['loss']\nhistory.history['val_loss']\nhistory.history['accuracy']\nhistory.history['val_accuracy']\nQ: Compute the test loss and accuracy of your model. Plot the history of the training and validation loss/accuracy.\n\n# Testing\nscore = model.evaluate(X_test, T_test)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nplt.figure(figsize=(15, 6))\nplt.subplot(121)\nplt.plot(history.history['loss'], '-r', label=\"Training\")\nplt.plot(history.history['val_loss'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\nplt.subplot(122)\nplt.plot(history.history['accuracy'], '-r', label=\"Training\")\nplt.plot(history.history['val_accuracy'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n 15/313 [>.............................] - ETA: 2s - loss: 0.0563 - accuracy: 0.9833\n\n\n2022-11-15 13:06:45.877919: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n\n\n313/313 [==============================] - 2s 7ms/step - loss: 0.0717 - accuracy: 0.9789\nTest loss: 0.07174411416053772\nTest accuracy: 0.9789000749588013"
  },
  {
    "objectID": "exercises/11-Keras-solution.html#correlated-inputs",
    "href": "exercises/11-Keras-solution.html#correlated-inputs",
    "title": "Keras tutorial",
    "section": "Correlated inputs",
    "text": "Correlated inputs\nNow that we have a basic NN working on MNIST, let’s investigate why deep NN hate sequentially correlated inputs (which is the main justification for the experience replay memory in DQN). Is that really true, or is just some mathematical assumption that does not matter in practice?\nThe idea of this part is the following: we will train the same network as before for 20 epochs, but each epoch will train the network on all the 0s first, then all the 1s, etc. Each epoch will contain the same number of training examples as before, but the order of presentation will simply be different.\nTo get all examples of the training set which have the target 3 (for example), you just have to slice the matrices accordingly:\nX = X_train[t_train==3, :]\nT = T_train[t_train==3]\nQ: Train the same network as before (but reinitialize it!) for 20 epochs, with each epoch sequentially iterating over the classes 0, 1, 2, 3, etc. Plot the loss and accurary during training. What do you observe?\nHint: you will have two for loops to write: one over the epochs, one over the digits.\nfor e in range(20):\n    for c in range(10):\n        model.fit(...)\nYou should only do one epoch for each call to fit(). Set verbose=0 in fit() to avoid printing too much info.\n\nmodel = create_model()\n\nhistory = tf.keras.callbacks.History()\n\nfor e in range(20):\n    print(\"Epoch: \", e+1)\n    for c in range(10):\n        # Training\n        model.fit(\n            X_train[t_train==c, :], T_train[t_train==c, :],\n            batch_size=100, \n            epochs=1,\n            validation_split=0.1,\n            verbose = 0,\n            callbacks=[history]\n        )\n    print(\" Training loss:\", history.history[\"loss\"][-1])\n    print(\" Training accuracy:\", history.history[\"accuracy\"][-1])\n    print(\" Validation loss:\", history.history[\"val_loss\"][-1])\n    print(\" Validation accuracy:\", history.history[\"val_accuracy\"][-1])\n        \nplt.figure(figsize=(15, 6))\nplt.subplot(121)\nplt.plot(history.history['loss'], '-r', label=\"Training\")\nplt.plot(history.history['val_loss'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.legend()\nplt.subplot(122)\nplt.plot(history.history['accuracy'], '-r', label=\"Training\")\nplt.plot(history.history['val_accuracy'], '-b', label=\"Validation\")\nplt.xlabel('Epoch #')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_6 (Dense)             (None, 150)               117750    \n                                                                 \n dropout_2 (Dropout)         (None, 150)               0         \n                                                                 \n dense_7 (Dense)             (None, 100)               15100     \n                                                                 \n dropout_3 (Dropout)         (None, 100)               0         \n                                                                 \n dense_8 (Dense)             (None, 10)                1010      \n                                                                 \n=================================================================\nTotal params: 133,860\nTrainable params: 133,860\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch:  1\n\n\n2022-11-15 13:06:51.648645: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n2022-11-15 13:06:52.409437: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n\n\n Training loss: 2.029949426651001\n Training accuracy: 0.4409787058830261\n Validation loss: 0.06199435144662857\n Validation accuracy: 0.9848739504814148\nEpoch:  2\n Training loss: 0.9748570322990417\n Training accuracy: 0.7392603754997253\n Validation loss: 0.028808292001485825\n Validation accuracy: 0.9899159669876099\nEpoch:  3\n Training loss: 0.6735117435455322\n Training accuracy: 0.7979081273078918\n Validation loss: 0.041012201458215714\n Validation accuracy: 0.9865546226501465\nEpoch:  4\n Training loss: 0.6233999133110046\n Training accuracy: 0.7984684705734253\n Validation loss: 0.046713244169950485\n Validation accuracy: 0.9865546226501465\nEpoch:  5\n Training loss: 0.644547700881958\n Training accuracy: 0.8083676099777222\n Validation loss: 0.046737030148506165\n Validation accuracy: 0.9882352948188782\nEpoch:  6\n Training loss: 0.5482434034347534\n Training accuracy: 0.8408666849136353\n Validation loss: 0.04225711524486542\n Validation accuracy: 0.9865546226501465\nEpoch:  7\n Training loss: 0.48833325505256653\n Training accuracy: 0.8541277647018433\n Validation loss: 0.041813675314188004\n Validation accuracy: 0.9815126061439514\nEpoch:  8\n Training loss: 0.48677706718444824\n Training accuracy: 0.8425476551055908\n Validation loss: 0.04213647544384003\n Validation accuracy: 0.9865546226501465\nEpoch:  9\n Training loss: 0.45263180136680603\n Training accuracy: 0.8576765060424805\n Validation loss: 0.03567739576101303\n Validation accuracy: 0.9882352948188782\nEpoch:  10\n Training loss: 0.43333420157432556\n Training accuracy: 0.862906277179718\n Validation loss: 0.036885034292936325\n Validation accuracy: 0.9899159669876099\nEpoch:  11\n Training loss: 0.42501774430274963\n Training accuracy: 0.8658946752548218\n Validation loss: 0.03803431615233421\n Validation accuracy: 0.9899159669876099\nEpoch:  12\n Training loss: 0.42635124921798706\n Training accuracy: 0.8709376454353333\n Validation loss: 0.03939447179436684\n Validation accuracy: 0.9865546226501465\nEpoch:  13\n Training loss: 0.3924403488636017\n Training accuracy: 0.8813971281051636\n Validation loss: 0.04728272929787636\n Validation accuracy: 0.9848739504814148\nEpoch:  14\n Training loss: 0.3904794454574585\n Training accuracy: 0.8800897002220154\n Validation loss: 0.03570760041475296\n Validation accuracy: 0.9915966391563416\nEpoch:  15\n Training loss: 0.3419549763202667\n Training accuracy: 0.8940979242324829\n Validation loss: 0.03474016487598419\n Validation accuracy: 0.9915966391563416\nEpoch:  16\n Training loss: 0.33528366684913635\n Training accuracy: 0.8946582078933716\n Validation loss: 0.042798422276973724\n Validation accuracy: 0.9882352948188782\nEpoch:  17\n Training loss: 0.34141793847084045\n Training accuracy: 0.8899888396263123\n Validation loss: 0.03723347559571266\n Validation accuracy: 0.9882352948188782\nEpoch:  18\n Training loss: 0.3221873641014099\n Training accuracy: 0.9064251184463501\n Validation loss: 0.038093291223049164\n Validation accuracy: 0.9882352948188782\nEpoch:  19\n Training loss: 0.30312439799308777\n Training accuracy: 0.9045573472976685\n Validation loss: 0.04427587613463402\n Validation accuracy: 0.9848739504814148\nEpoch:  20\n Training loss: 0.30541884899139404\n Training accuracy: 0.9013821482658386\n Validation loss: 0.03604322671890259\n Validation accuracy: 0.9882352948188782\n\n\n\n\n\n\n\n\n\nA: The training accuracy slowly increases (with some oscillations, some numbers are harder to learn than others), but the validation accuracy is suspiciously high right from the start…\nQ: Evaluate the model after training on the whole test set. What happens?\n\n# Testing\nscore = model.evaluate(X_test, T_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nTest loss: 0.9087441563606262\nTest accuracy: 0.7581000328063965\n\n\nA: Horror! The test accuracy is now awful, although the training and validation accuracies were fine after 20 epochs.\nQ: To better understand what happened, compute the test accuracy of the network on each class of the test set individually: all the 0s of the test set, then all the 1s, etc. What happens?\n\nfor c in range(10):\n    score = model.evaluate(X_test[t_test==c, :], T_test[t_test==c, :], verbose=0)\n    print(\"Class\", c)\n    print('Test loss:', score[0])\n    print('Test accuracy:', score[1])\n\nClass 0\nTest loss: 0.3669024407863617\nTest accuracy: 0.9132653474807739\nClass 1\nTest loss: 0.7315441966056824\nTest accuracy: 0.8651982545852661\nClass 2\nTest loss: 0.9564369320869446\nTest accuracy: 0.7509689927101135\nClass 3\nTest loss: 1.3005502223968506\nTest accuracy: 0.5534653663635254\nClass 4\nTest loss: 1.6715565919876099\nTest accuracy: 0.5274949073791504\nClass 5\nTest loss: 1.3488402366638184\nTest accuracy: 0.6502242684364319\nClass 6\nTest loss: 0.165551096200943\nTest accuracy: 0.960334062576294\nClass 7\nTest loss: 2.2395598888397217\nTest accuracy: 0.42704281210899353\nClass 8\nTest loss: 0.25550350546836853\nTest accuracy: 0.9414784908294678\nClass 9\nTest loss: 0.042239751666784286\nTest accuracy: 0.9871159791946411\n\n\nA: The last digits to be seen during training are the 9s: they have a good test accuracy. The 8s were seen not too long ago, they are also OK. But the other digits have been forgotten! The memory has been erased. This explains why you cannot train a deep network on-policy: the last episode would be remembered, but all the previous ones would be erased (catastrophic forgetting).\nA notable exception is for the 6s, which look like 9s, and the 0s, which look like 8s: they share features with the digits which are well recognized, so they perform OK.\nQ: Increase and decrease the learning rate of the optimizer. What do you observe? Is there a solution to this problem?\nA: Increasing the learning rate worsens the problem. Decreasing does help, but then learning is very slow. This is classical example of catastrophic forgetting: learning a new task erases the previous ones. There is no solution to this problem for now, apart from taking i.i.d samples in each minibatch."
  },
  {
    "objectID": "exercises/12-DQN-solution.html",
    "href": "exercises/12-DQN-solution.html",
    "title": "DQN",
    "section": "",
    "text": "The goal of this exercise is to implement DQN and to apply it to the cartpole balancing problem.\nLet’s import eveything we need to run gym on Colab:"
  },
  {
    "objectID": "exercises/12-DQN-solution.html#cartpole-balancing-task",
    "href": "exercises/12-DQN-solution.html#cartpole-balancing-task",
    "title": "DQN",
    "section": "Cartpole balancing task",
    "text": "Cartpole balancing task\nWe are going to use the Cartpole balancing problem, which can be loaded with:\nenv = wrap_env(gym.make('CartPole-v0'))\nStates have 4 continuous values (position and speed of the cart, angle and speed of the pole) and 2 discrete outputs (going left or right). The reward is +1 for each transition where the pole is still standing (angle of less than 30° with the vertical). The episode ends when the pole fails or after 200 steps. The maximal (undiscounted) return is therefore 200. Can DQN learn this?\n\n# Create the environment\nenv = wrap_env(gym.make('CartPole-v0'))\n\n# Sample the initial state\nstate = env.reset()\n\n# One episode:\ndone = False\nreturn_episode = 0\nwhile not done:\n    \n    # Render the current state\n    env.render()\n\n    # Select an action randomly\n    action = env.action_space.sample()\n    \n    # Sample a single transition\n    next_state, reward, done, info = env.step(action)\n\n    # Update undiscounted return\n    return_episode += reward\n    \n    # Go in the next state\n    state = next_state\n\nprint(\"Return:\", return_episode)\n\n# Exit cleanly\nenv.close()\nshow_video()\n\n/Users/vitay/Applications/miniforge3/lib/python3.9/site-packages/gym/envs/registration.py:555: UserWarning: WARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\n  logger.warn(\n\n\nValueError: too many values to unpack (expected 4)\n\n\nAs the problem is quite simple (4 state variables, 2 actions), DQN can run on a single CPU. However, we advise that you run the notebook on a GPU in Colab to avoid emptying the battery of your laptop too fast or making it too warm as training takes quite a long time.\nWe will stop from now on to display the cartpole on colab, it does not work well."
  },
  {
    "objectID": "exercises/12-DQN-solution.html#creating-the-model",
    "href": "exercises/12-DQN-solution.html#creating-the-model",
    "title": "DQN",
    "section": "Creating the model",
    "text": "Creating the model\nThe first step is to create the value network using keras. We will not need anything fancy: a simple fully connected network with 4 input neurons, two hidden layers of 64 neurons each and 2 output neurons will do the trick. ReLU activation functions all along and the Adam optimizer.\nQ: Which loss function should we use? Think about which arguments have to passed to model.compile() and what activation function is required in the output layer.\nWe will need to create two identical networks: the trained network and the target network. You should therefore create a method that returns a compiled model, so it can be called two times. You should pass it the environment (so the network can know how many input and output neurons it needs) and the learning rate for the Adam optimizer.\ndef create_model(env, lr):\n    \n    model = Sequential()\n\n    # ...\n\n    return model\nQ: Implement the method accordingly.\n\ndef create_model(env, lr):\n    \n    model = tf.keras.models.Sequential()\n    \n    model.add(tf.keras.layers.Input(env.observation_space.shape))\n    model.add(tf.keras.layers.Dense(64, activation='relu'))\n    model.add(tf.keras.layers.Dense(64, activation='relu'))\n    model.add(tf.keras.layers.Dense(env.action_space.n, activation='linear'))\n    \n    model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=lr))\n    \n    print(model.summary())\n\n    return model\n\nLet’s test this method by creating the trained and target networks.\nImportant: every time you call create_model, a new neural network will be instantiated but the previous ones will not be deleted. During this exercise, you may have to create hundreds of networks because of the incremental implementation of DQN: all networks will stay instantiated in the RAM, and your computer/colab tab will freeze after a while. Before creating new networks, delete all existing ones with:\ntf.keras.backend.clear_session()\nQ: Create the trained and target networks. The learning rate does not matter for now. Instantiate the Cartpole environment and print the output of both networks for the initial state (state = env.reset()). Are they the same?\nHint: model.predict() expects an array of shape (N, 4), with N the number of examples. Here, we have only one example, so make sure to reshape state so it has the shape (1, 4) (otherwise tf will complain).\n\nenv = gym.make('CartPole-v0')\n\nstate = env.reset()\nprint(\"State:\", state)\n\ntf.keras.backend.clear_session()\ntrained_model = create_model(env, 0.001)\ntarget_model = create_model(env, 0.001)\n\ntrained_prediction = trained_model.predict(state.reshape((1, env.observation_space.shape[0])))[0]\ntarget_prediction = target_model.predict(state.reshape((1, env.observation_space.shape[0])))[0]\nprint(\"Predictions:\", trained_prediction, target_prediction)\n\nState: [-0.01228574  0.00488752  0.02812964 -0.00934043]\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 64)                320       \n_________________________________________________________________\ndense_1 (Dense)              (None, 64)                4160      \n_________________________________________________________________\ndense_2 (Dense)              (None, 2)                 130       \n=================================================================\nTotal params: 4,610\nTrainable params: 4,610\nNon-trainable params: 0\n_________________________________________________________________\nNone\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_3 (Dense)              (None, 64)                320       \n_________________________________________________________________\ndense_4 (Dense)              (None, 64)                4160      \n_________________________________________________________________\ndense_5 (Dense)              (None, 2)                 130       \n=================================================================\nTotal params: 4,610\nTrainable params: 4,610\nNon-trainable params: 0\n_________________________________________________________________\nNone\nPredictions: [-0.0029409   0.00551502] [0.00421481 0.0009347 ]\n\n\nThe target network has the same structure as the trained network, but not the same weights, as they are randomly initialized. We want the target network \\theta' to have exactly the same weights as the trained weights \\theta. You can obtain the weights of a network with:\nw = model.get_weights()\nand set weights using:\nmodel.set_weights(w)\nQ: Transfer the weights of the trained model to the target model. Compare their predictions for the current state.\n\ntarget_model.set_weights(trained_model.get_weights())\n\ntrained_prediction = trained_model.predict(state.reshape((1, env.observation_space.shape[0])))[0]\ntarget_prediction = target_model.predict(state.reshape((1, env.observation_space.shape[0])))[0]\nprint(\"Predictions:\", trained_prediction, target_prediction)\n\nPredictions: [-0.0029409   0.00551502] [-0.0029409   0.00551502]"
  },
  {
    "objectID": "exercises/12-DQN-solution.html#experience-replay-memory",
    "href": "exercises/12-DQN-solution.html#experience-replay-memory",
    "title": "DQN",
    "section": "Experience replay memory",
    "text": "Experience replay memory\nThe second thing that we need is the experience replay memory (or replay buffer). We need a container like a python list where we append (s, a, r, s’, done) transitions (as in Q-learning), but with a maximal capacity: when there are already C transitions in the list, one should stop appending to the list, but rather start writing at the beginning of the list.\nThis would not be very hard to write, but it would take a lot of time and the risk is high to have hard-to-notice bugs.\nHere is a basic implementation of the replay buffer using double-ended queues (deque). A deque is list with a maximum capacity. If the deque is full, it starts writing again at the beginnning. Exactly what we need. This implementation uses one deque per element in (s, a, r, s’, done), but one could also append the whole transition to a single deque.\nQ: Read the code of the ReplayBuffer and understand what it does.\n\nclass ReplayBuffer:\n    \"Basic implementation of the experience replay memory using separated deques.\"\n    def __init__(self, max_capacity):\n        self.max_capacity = max_capacity\n        \n        # deques for each element\n        self.states = deque(maxlen=max_capacity)\n        self.actions = deque(maxlen=max_capacity)\n        self.rewards = deque(maxlen=max_capacity)\n        self.next_states = deque(maxlen=max_capacity)\n        self.dones = deque(maxlen=max_capacity)\n        \n    def append(self, state, action, reward, next_state, done):\n        # Store data\n        self.states.append(state)\n        self.actions.append(action)\n        self.rewards.append(reward)\n        self.next_states.append(next_state)\n        self.dones.append(done)\n        \n    def sample(self, batch_size):\n        # Do not return samples if we do not have at least 2*batch_size transitions\n        if len(self.states) < 2*batch_size: \n            return []\n            \n        # Randomly choose the indices of the samples.\n        indices = sorted(np.random.choice(np.arange(len(self.states)), batch_size, replace=False))\n\n        # Return the corresponding\n        return [np.array([self.states[i] for i in indices]), \n                np.array([self.actions[i] for i in indices]), \n                np.array([self.rewards[i] for i in indices]), \n                np.array([self.next_states[i] for i in indices]), \n                np.array([self.dones[i] for i in indices])]\n\nQ: Run a random agent on Cartpole for a few episodes and append each transition to a replay buffer with small capacity (e.g. 100) and sample batches from time to time. Check that everything makes sense.\n\nenv = gym.make('CartPole-v0')\n\nbuffer = ReplayBuffer(100)\n\nfor episode in range(10):\n            \n    # Reset\n    state = env.reset()\n    done = False\n    \n    # Sample the episode\n    while not done:\n        \n        # Select an action randomly\n        action = env.action_space.sample()\n            \n        # Perform the action\n        next_state, reward, done, info = env.step(action)\n                \n        # Store the transition\n        buffer.append(state, action, reward, next_state, done)\n                \n        # Go in the next state\n        state = next_state\n    \n    # Sample a minibatch\n    batch = buffer.sample(10)\n    print(batch)\n\n[]\n[array([[-0.04588702,  0.17571342, -0.04359217, -0.26982999],\n       [-0.03494416,  0.56720267, -0.06050751, -0.88364164],\n       [-0.02360011,  0.37295228, -0.07818034, -0.61057763],\n       [-0.01614106,  0.56907494, -0.0903919 , -0.92682465],\n       [ 0.01054631,  0.96163079, -0.13385819, -1.57121202],\n       [ 0.04514565,  0.57564238, -0.19174444, -1.08637116],\n       [-0.07998055, -0.42091214,  0.07157347,  0.65326576],\n       [-0.0883988 , -0.226856  ,  0.08463878,  0.38395145],\n       [-0.13009838, -1.01217643,  0.15373449,  1.67594304],\n       [-0.15034191, -1.20871176,  0.18725335,  2.01228927]]), array([1, 0, 1, 1, 0, 1, 1, 0, 0, 1]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([[-0.04237275,  0.37142947, -0.04898877, -0.5759372 ],\n       [-0.02360011,  0.37295228, -0.07818034, -0.61057763],\n       [-0.01614106,  0.56907494, -0.0903919 , -0.92682465],\n       [-0.00475956,  0.76529356, -0.10892839, -1.24649   ],\n       [ 0.02977892,  0.76833619, -0.16528243, -1.32310072],\n       [ 0.0566585 ,  0.77270422, -0.21347187, -1.43257474],\n       [-0.0883988 , -0.226856  ,  0.08463878,  0.38395145],\n       [-0.09293592, -0.42307124,  0.09231781,  0.70207482],\n       [-0.15034191, -1.20871176,  0.18725335,  2.01228927],\n       [-0.17451615, -1.01596628,  0.22749914,  1.78295639]]), array([False, False, False, False, False,  True, False, False, False,\n        True])]\n[array([[-0.03494416,  0.56720267, -0.06050751, -0.88364164],\n       [-0.01614106,  0.56907494, -0.0903919 , -0.92682465],\n       [ 0.01054631,  0.96163079, -0.13385819, -1.57121202],\n       [-0.03861068, -0.41916791,  0.01007019,  0.61341107],\n       [-0.06767495, -0.61528014,  0.05299678,  0.92883467],\n       [-0.11378421, -0.81570892,  0.12680595,  1.34642685],\n       [ 0.02946412,  0.00944244, -0.02332485, -0.04617419],\n       [ 0.03375079,  0.40034929, -0.03117082, -0.64635371],\n       [ 0.04175778,  0.59589138, -0.04409789, -0.94868711],\n       [ 0.0536756 ,  0.79157841, -0.06307163, -1.25489287]]), array([0, 1, 0, 0, 1, 0, 1, 1, 1, 1]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([[-0.02360011,  0.37295228, -0.07818034, -0.61057763],\n       [-0.00475956,  0.76529356, -0.10892839, -1.24649   ],\n       [ 0.02977892,  0.76833619, -0.16528243, -1.32310072],\n       [-0.04699404, -0.61442913,  0.02233841,  0.90924863],\n       [-0.07998055, -0.42091214,  0.07157347,  0.65326576],\n       [-0.13009838, -1.01217643,  0.15373449,  1.67594304],\n       [ 0.02965297,  0.20489096, -0.02424833, -0.3461241 ],\n       [ 0.04175778,  0.59589138, -0.04409789, -0.94868711],\n       [ 0.0536756 ,  0.79157841, -0.06307163, -1.25489287],\n       [ 0.06950717,  0.98744873, -0.08816949, -1.56664493]]), array([False, False, False, False, False, False, False, False, False,\n       False])]\n[array([[-0.07998055, -0.42091214,  0.07157347,  0.65326576],\n       [-0.09293592, -0.42307124,  0.09231781,  0.70207482],\n       [-0.13009838, -1.01217643,  0.15373449,  1.67594304],\n       [ 0.04456579, -0.18728104, -0.04528   ,  0.28217656],\n       [ 0.0331856 , -0.18607367, -0.02843163,  0.25533923],\n       [ 0.10512582,  0.98990177, -0.14555671, -1.6302918 ],\n       [-0.02366513,  0.22517243, -0.04587956, -0.30802066],\n       [-0.01916168,  0.03073322, -0.05203997, -0.03015261],\n       [-0.07011939, -0.54787848,  0.01115384,  0.69896567],\n       [-0.1743693 , -0.74925887,  0.15544077,  1.14156613]]), array([1, 0, 0, 0, 1, 0, 0, 1, 0, 1]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([[-0.0883988 , -0.226856  ,  0.08463878,  0.38395145],\n       [-0.10139734, -0.61934327,  0.10635931,  1.0223321 ],\n       [-0.15034191, -1.20871176,  0.18725335,  2.01228927],\n       [ 0.04082017, -0.38172886, -0.03963647,  0.56024157],\n       [ 0.02946412,  0.00944244, -0.02332485, -0.04617419],\n       [ 0.12492386,  0.79675917, -0.17816255, -1.38628425],\n       [-0.01916168,  0.03073322, -0.05203997, -0.03015261],\n       [-0.01854701,  0.22656133, -0.05264303, -0.33878997],\n       [-0.08107695, -0.74315328,  0.02513316,  0.99513882],\n       [-0.18935448, -0.5564717 ,  0.17827209,  0.9013877 ]]), array([False, False, False, False, False, False, False, False, False,\n       False])]\n[array([[ 0.08925615,  0.79348391, -0.11950239, -1.30271612],\n       [-0.01337126, -0.16199547, -0.06068205,  0.21019852],\n       [-0.01661117, -0.35619964, -0.05647808,  0.48313902],\n       [-0.03721756, -0.35365612, -0.0314414 ,  0.42638794],\n       [-0.07011939, -0.54787848,  0.01115384,  0.69896567],\n       [-0.12959367, -0.54997232,  0.09129524,  0.74776884],\n       [-0.15551766, -0.94258199,  0.12760519,  1.39177889],\n       [ 0.02423362,  0.35477442, -0.02756646, -0.56215783],\n       [ 0.05267667, -0.03268853, -0.07391405, -0.03808703],\n       [ 0.03940372, -0.01677189, -0.13481805, -0.39240767]]), array([1, 0, 1, 0, 0, 0, 1, 1, 0, 1]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([[ 0.10512582,  0.98990177, -0.14555671, -1.6302918 ],\n       [-0.01661117, -0.35619964, -0.05647808,  0.48313902],\n       [-0.02373516, -0.16032796, -0.0468153 ,  0.17320423],\n       [-0.04429068, -0.54831896, -0.02291364,  0.70899563],\n       [-0.08107695, -0.74315328,  0.02513316,  0.99513882],\n       [-0.14059312, -0.74622723,  0.10625061,  1.06772877],\n       [-0.1743693 , -0.74925887,  0.15544077,  1.14156613],\n       [ 0.03132911,  0.55027215, -0.03880962, -0.86339652],\n       [ 0.0520229 , -0.226677  , -0.07467579,  0.23039023],\n       [ 0.03906829,  0.17997992, -0.1426662 , -0.72437389]]), array([False, False, False, False, False, False, False, False, False,\n       False])]\n[array([[-0.11378421, -0.81570892,  0.12680595,  1.34642685],\n       [ 0.02946412,  0.00944244, -0.02332485, -0.04617419],\n       [ 0.06950717,  0.98744873, -0.08816949, -1.56664493],\n       [-0.07011939, -0.54787848,  0.01115384,  0.69896567],\n       [ 0.03411469, -0.00360845, -0.19010635, -0.69103416],\n       [-0.01561388,  0.24260873, -0.01121911, -0.27438396],\n       [-0.0107617 ,  0.43788894, -0.01670679, -0.5705842 ],\n       [ 0.03019177,  0.83091222, -0.07015847, -1.21931716],\n       [ 0.06840585,  0.24965211, -0.12729061, -0.43298024],\n       [ 0.08232539,  0.25331111, -0.15120872, -0.51592219]]), array([0, 1, 0, 0, 1, 1, 0, 0, 1, 0]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([[-0.13009838, -1.01217643,  0.15373449,  1.67594304],\n       [ 0.02965297,  0.20489096, -0.02424833, -0.3461241 ],\n       [ 0.08925615,  0.79348391, -0.11950239, -1.30271612],\n       [-0.08107695, -0.74315328,  0.02513316,  0.99513882],\n       [ 0.03404252,  0.19357148, -0.20392703, -1.03703072],\n       [-0.0107617 ,  0.43788894, -0.01670679, -0.5705842 ],\n       [-0.00200392,  0.24300522, -0.02811847, -0.28321098],\n       [ 0.04681002,  0.63676127, -0.09454481, -0.94941686],\n       [ 0.07339889,  0.44632481, -0.13595022, -0.76292497],\n       [ 0.08739161,  0.06060563, -0.16152716, -0.27444836]]), array([False, False, False, False, False, False, False, False, False,\n       False])]\n[array([[-0.02373516, -0.16032796, -0.0468153 ,  0.17320423],\n       [-0.09594002, -0.93860222,  0.04503593,  1.295608  ],\n       [-0.01656031,  0.04732154, -0.0116582 ,  0.02195434],\n       [-0.01561388,  0.24260873, -0.01121911, -0.27438396],\n       [ 0.00869979,  0.43947173, -0.03982674, -0.6058149 ],\n       [ 0.03019177,  0.83091222, -0.07015847, -1.21931716],\n       [ 0.08739161,  0.06060563, -0.16152716, -0.27444836],\n       [ 0.02318629, -0.16544129,  0.06638922,  0.36720994],\n       [ 0.02252386,  0.21795011,  0.0882486 , -0.06800942],\n       [ 0.01640814, -0.17726995,  0.12111231,  0.6315977 ]]), array([0, 1, 1, 1, 1, 0, 1, 1, 0, 0]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([[-0.02694172, -0.3547497 , -0.04335122,  0.45075865],\n       [-0.11471206, -0.74408039,  0.07094809,  1.01735721],\n       [-0.01561388,  0.24260873, -0.01121911, -0.27438396],\n       [-0.0107617 ,  0.43788894, -0.01670679, -0.5705842 ],\n       [ 0.01748923,  0.6351273 , -0.05194304, -0.91077149],\n       [ 0.04681002,  0.63676127, -0.09454481, -0.94941686],\n       [ 0.08860372,  0.25761936, -0.16701613, -0.61340485],\n       [ 0.01987746,  0.02867756,  0.07373342,  0.09617723],\n       [ 0.02688286,  0.02168106,  0.08688841,  0.25115996],\n       [ 0.01286274, -0.37385492,  0.13374427,  0.95983516]]), array([False, False, False, False, False, False, False, False, False,\n       False])]\n[array([[ 0.0520229 , -0.226677  , -0.07467579,  0.23039023],\n       [ 0.02847391,  0.37003179, -0.10387816, -0.90265178],\n       [ 0.03813067, -0.20079893, -0.18317346, -0.3466444 ],\n       [-0.02140737,  0.24235326, -0.00628339, -0.26874016],\n       [ 0.01748923,  0.6351273 , -0.05194304, -0.91077149],\n       [ 0.08860372,  0.25761936, -0.16701613, -0.61340485],\n       [ 0.10284877,  0.65165459, -0.19835798, -1.2969111 ],\n       [-0.01357656, -0.18525485,  0.1997725 ,  0.81854805],\n       [-0.01315511,  0.14671572, -0.03682447, -0.31321735],\n       [ 0.00486409,  0.74280442, -0.11908856, -1.4356092 ]]), array([0, 0, 1, 0, 1, 1, 0, 1, 0, 1]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([[ 0.04748936, -0.42065683, -0.07006798,  0.49861478],\n       [ 0.03587455,  0.17645877, -0.1219312 , -0.64434272],\n       [ 0.03411469, -0.00360845, -0.19010635, -0.69103416],\n       [-0.01656031,  0.04732154, -0.0116582 ,  0.02195434],\n       [ 0.03019177,  0.83091222, -0.07015847, -1.21931716],\n       [ 0.09375611,  0.45463285, -0.17928422, -0.95368777],\n       [ 0.11588186,  0.4595259 , -0.2242962 , -1.07230467],\n       [-0.01728166,  0.00665442,  0.21614346,  0.59475248],\n       [-0.01022079, -0.04786282, -0.04308882, -0.03237137],\n       [ 0.01972017,  0.93917647, -0.14780074, -1.76301036]]), array([False, False, False, False, False, False,  True,  True, False,\n       False])]\n[array([[-0.01561388,  0.24260873, -0.01121911, -0.27438396],\n       [-0.0107617 ,  0.43788894, -0.01670679, -0.5705842 ],\n       [ 0.01987746,  0.02867756,  0.07373342,  0.09617723],\n       [ 0.02045101, -0.16741948,  0.07565696,  0.41118155],\n       [ 0.01763368,  0.2203797 ,  0.08674614, -0.12180875],\n       [ 0.02731648, -0.17456726,  0.09191161,  0.569935  ],\n       [-0.01307542,  0.35042117, -0.08086043, -0.79721063],\n       [ 0.04680089, -0.16049118, -0.03509216,  0.27523426],\n       [-0.02027473,  0.22240778,  0.07877649, -0.14882592],\n       [ 0.0113827 , -0.37571906,  0.1455837 ,  1.02055041]]), array([1, 0, 0, 1, 0, 0, 1, 1, 1, 0]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([[-0.0107617 ,  0.43788894, -0.01670679, -0.5705842 ],\n       [-0.00200392,  0.24300522, -0.02811847, -0.28321098],\n       [ 0.02045101, -0.16741948,  0.07565696,  0.41118155],\n       [ 0.01710263,  0.02655296,  0.08388059,  0.14327715],\n       [ 0.02204128,  0.02412895,  0.08430996,  0.19693188],\n       [ 0.02382514, -0.37084988,  0.10331031,  0.89010017],\n       [-0.00606699,  0.54655402, -0.09680465, -1.1141957 ],\n       [ 0.04359106,  0.03511342, -0.02958747, -0.02830697],\n       [-0.01582658,  0.41631846,  0.07579997, -0.41565355],\n       [ 0.00386832, -0.57244916,  0.16599471,  1.35517444]]), array([False, False, False, False, False, False, False, False, False,\n       False])]\n[array([[-6.02428909e-03, -3.77613571e-01,  1.78768312e-01,\n         1.05020945e+00],\n       [-1.40961151e-02, -4.97950406e-02, -3.10058632e-02,\n         1.02872809e-02],\n       [-1.60248718e-02, -4.66227879e-02, -3.88080202e-02,\n        -5.97399823e-02],\n       [ 4.42933311e-02, -1.59572005e-01, -3.01536094e-02,\n         2.54895909e-01],\n       [ 4.89776485e-02, -3.70983886e-01,  5.85792870e-02,\n         9.07854348e-01],\n       [ 1.13826996e-02, -3.75719058e-01,  1.45583701e-01,\n         1.02055041e+00],\n       [-4.59710778e-03,  2.41846810e-01, -6.59756969e-02,\n        -3.76268034e-01],\n       [ 2.39828428e-04,  4.37840767e-01, -7.35010576e-02,\n        -6.89002147e-01],\n       [ 1.20013721e-02, -3.37180309e-01, -9.67229348e-02,\n         3.64580237e-01],\n       [ 5.25776591e-03, -5.30804141e-01, -8.94313301e-02,\n         6.25266258e-01]]), array([1, 1, 1, 0, 0, 0, 1, 0, 0, 0]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([[-1.35765605e-02, -1.85254846e-01,  1.99772501e-01,\n         8.18548046e-01],\n       [-1.50920159e-02,  1.45757540e-01, -3.08001176e-02,\n        -2.92014867e-01],\n       [-1.69573275e-02,  1.49033483e-01, -4.00028198e-02,\n        -3.64410108e-01],\n       [ 4.11018910e-02, -3.54250735e-01, -2.50556912e-02,\n         5.37917438e-01],\n       [ 4.15579708e-02, -5.66847830e-01,  7.67363739e-02,\n         1.21835879e+00],\n       [ 3.86831841e-03, -5.72449163e-01,  1.65994709e-01,\n         1.35517444e+00],\n       [ 2.39828428e-04,  4.37840767e-01, -7.35010576e-02,\n        -6.89002147e-01],\n       [ 8.99664377e-03,  2.43811687e-01, -8.72811005e-02,\n        -4.20334199e-01],\n       [ 5.25776591e-03, -5.30804141e-01, -8.94313301e-02,\n         6.25266258e-01],\n       [-5.35831690e-03, -7.24571315e-01, -7.69260049e-02,\n         8.88497710e-01]]), array([False, False, False, False, False, False, False, False, False,\n       False])]"
  },
  {
    "objectID": "exercises/12-DQN-solution.html#dqn-agent",
    "href": "exercises/12-DQN-solution.html#dqn-agent",
    "title": "DQN",
    "section": "DQN agent",
    "text": "DQN agent\nHere starts the fun part. There are a lot of things to do here, but you will now whether it works or not only when everything has been (correctly) implemented. So here is a lot of text to read carefully, and then you are on your own.\nReminder from the lecture:\n\nInitialize value network Q_{\\theta} and target network Q_{\\theta'}.\nInitialize experience replay memory \\mathcal{D} of maximal size N.\nfor t \\in [0, T_\\text{total}]:\n\nSelect an action a_t based on Q_\\theta(s_t, a), observe s_{t+1} and r_{t+1}.\nStore (s_t, a_t, r_{t+1}, s_{t+1}) in the experience replay memory.\nEvery T_\\text{train} steps:\n\nSample a minibatch \\mathcal{D}_s randomly from \\mathcal{D}.\nFor each transition (s_k, a_k, r_k, s'_k) in the minibatch:\n\nCompute the target value t_k = r_k + \\gamma \\, \\max_{a'} Q_{\\theta'}(s'_k, a') using the target network.\n\nUpdate the value network Q_{\\theta} on \\mathcal{D}_s to minimize:\n\n\\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathcal{D}_s}[(t_k - Q_\\theta(s_k, a_k))^2]\nEvery T_\\text{target} steps:\n\nUpdate target network: \\theta' \\leftarrow \\theta.\n\n\n\nHere is the skeleton of the DQNAgent class that you have to write:\nclass DQNAgent:\n    \n    def __init__(self, env, create_model, some_parameters):\n        \n        self.env = env\n        \n        # TODO: copy the parameters\n\n        # TODO: Create the trained and target networks, copy the weights.\n\n        # TODO: Create an instance of the replay memory\n        \n    def act(self, state):\n\n        # TODO: Select an action using epsilon-greedy on the output of the trained model\n\n        return action\n    \n    def update(self, batch):\n        \n        # TODO: train the model using the batch of transitions\n        \n        return loss # mse on the batch\n\n    def train(self, nb_episodes):\n\n        returns = []\n        losses = []\n\n        # TODO: Train the network for the given number of episodes\n\n        return returns, losses\n\n    def test(self):\n\n        # TODO: one episode with epsilon temporarily set to 0\n\n        return nb_steps # Should be 200 after learning\nWith this structure, it will be very simple to actually train the DQN on Cartpole:\n# Create the environment\nenv = gym.make('CartPole-v0')\n\n# Create the agent\nagent = DQNAgent(env, create_model, other_parameters)\n\n# Train the agent\nreturns, losses = agent.train(nb_episodes)\n\n# Plot the returns\nplt.figure(figsize=(10, 6))\nplt.plot(returns)\nplt.plot(running_mean(returns, 10))\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"Returns\")\n\n# Plot the losses\nplt.figure(figsize=(10, 6))\nplt.plot(losses)\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"Training loss\")\n\nplt.show()\n\n# Test the network\nnb_steps = agent.test()\nprint(\"Number of steps:\", nb_steps)\nSo you “just” have to fill the holes.\n\n1 - __init__(): Initializing the agent\nIn this method, you should first copy the value of the parameters as attributes: learning rate, epsilon, gamma and so on.\nSuggested values: gamma = 0.99, learning_rate = 0.001\nThe second thing to do is to create the trained and target networks (with the same weights) and save them as attributes (the other methods will use them). Do not forget to clear the keras session first, otherwise the RAM will be quickly filled.\nThe third thing is to create an instance of the ERM. Use a buffer limit of 5000 transitions (should be passed as a parameter).\nDo not hesitate to add other stuff as you implementing the other methods (e.g. counters).\n\n\n2 - act(): action selection\nWe will use a simple \\epsilon-greedy method for the action selection, as in the previous exercises.\nThe only difference is that we have to use the trained model to get the greedy action, using trained_model.predict(). This will return the Q-value of the two actions left and right. Use argmax() to return the greedy action (with probability 1 - \\epsilon). env.action_space.sample() should be used for the exploration (do not use the Q-network in that case, it is slow!).\n\\epsilon will be scheduled with an initial value of 1.0 and an exponential decay rate of 0.0005 after each action. It is always better to keep a little exploration, even if \\epsilon has decayed to 0. Keep a minimal value of 0.05 for epsilon.\nQ: Once this has been implemented, run your very slow random agent for 100 episodes to check everything works correctly.\n\n\n3 - train(): training loop\nThis method will be very similar to the Q-learning agent that you implemented previously. Do not hesitate to copy and paste.\nHere is the parts of the DQN algorithm that should be implemented:\n\nfor t \\in [0, T_\\text{total}]:\n\nSelect an action a_t based on Q_\\theta(s_t, a), observe s_{t+1} and r_{t+1}.\nStore (s_t, a_t, r_{t+1}, s_{t+1}) in the experience replay memory.\nEvery T_\\text{train} steps:\n\nSample a minibatch \\mathcal{D}_s randomly from \\mathcal{D}.\nUpdate the trained network using \\mathcal{D}_s.\n\nEvery T_\\text{target} steps:\n\nUpdate target network: \\theta' \\leftarrow \\theta.\n\n\n\nThe main difference with Q-learning is that update() will be called only every T_train = 4 steps: the number of updates to the trained network will be 4 times smaller that the number of steps made in the environment. Beware that if the ERM does not have enough transitions yet (less than the batch size), you should not call update().\nUpdating the target network (copying the weights of the trained network) should happen every 100 steps. Pass these parameters to the constructor of the agent.\nThe batch size can be set to 32.\n\n\n4 - update(): training the value network\nUsing the provided minibatch, one should implement the following part of the DQN algorithm:\n\nFor each transition (s_k, a_k, r_k, s'_k) in the minibatch:\n\nCompute the target value t_k = r_k + \\gamma \\, \\max_{a'} Q_{\\theta'}(s'_k, a') using the target network.\n\nUpdate the value network Q_{\\theta} on \\mathcal{D}_s to minimize:\n\\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathcal{D}_s}[(t_k - Q_\\theta(s_k, a_k))^2]\n\nSo we just need to define the targets for each transition in the minibatch, and call model.fit() on the trained network to minimize the mse between the current predictions Q_\\theta(s_k, a_k) and the target.\nBut we have a problem: the network has two outputs for the actions left and right, but we have only one target for the action that was executed. We cannot compute the mse between a vector with 2 elements and a single value… They must have the same size.\nAs we want only the train the output neuron corresponding to the action a_k, we are going to:\n\nUse the trained network to predict the Q-value of both actions [Q_\\theta(s_k, 0), Q_\\theta(s_k, 1)].\nReplace one of the values with the target, for example [Q_\\theta(s_k, 0), t_k] if the second action was chosen.\nMinimize the mse between [Q_\\theta(s_k, 0), Q_\\theta(s_k, 1)] and [Q_\\theta(s_k, 0), t_k].\n\nThat way, the first output neuron has a squared error of 0, so it won’t learn anything. Only the second output neuron will have a non-zero mse and learn.\nThere are more efficient ways to do this (using masks), but this will do the trick, the drawback being that we have to make a forward pass on the minibatch before calling fit().\nThe rest is pretty much the same as for your Q-learning agent. Do not forget that actions leading to a terminal state should only use the reward as a target, not the complete Bellman target r + \\gamma \\max Q.\nHint: as we sample a minibatch of 32 transitions, it is faster to call:\nQ_values = np.array(training_model.predict_on_batch(states))\nthan:\nQ_values = training_model.predict(states)\nfor reasons internal to tensorflow. Note that with tf2, you need to cast the result to numpy arrays as eager mode is now the default.\nThe method should return the training loss, which is contained in the History object returned by model.fit(). model.fit() should be called for one epoch only, a batch size of 32, and verbose set to 0.\n\n\n5 - test()\nThis method should run one episode with epsilon set to 0, without learning. The number of steps should be returned (do not bother discounting with gamma, the goal is to be up for 200 steps).\nQ: Let’s go! Run the agent for 100 episodes and observe how fast it manages to keep the pole up for 200 steps.\nBeware that running the same network twice can lead to very different results. In particular, policy collapse (the network was almost perfect, but suddenly crashes and becomes random) can happen. Just be patient.\n\nclass DQNAgent:\n    \n    def __init__(self, env, create_model, learning_rate, epsilon, epsilon_decay, gamma, batch_size, target_update_period, training_update_period, buffer_limit):\n        self.env = env\n\n        self.learning_rate = learning_rate\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.gamma = gamma\n        self.batch_size = batch_size\n        self.target_update_period = target_update_period\n        self.training_update_period = training_update_period\n        \n        # Create the Q-network and the target network\n        tf.keras.backend.clear_session() # start by deleting all existing models to be gentle on the RAM\n        self.model = create_model(self.env, self.learning_rate)\n        self.target_model = create_model(self.env, self.learning_rate)\n        self.target_model.set_weights(self.model.get_weights())\n\n        # Create the replay memory\n        self.buffer = ReplayBuffer(buffer_limit)\n                \n    def act(self, state):\n\n        # epsilon-greedy\n        if np.random.rand() < self.epsilon: # Random selection\n            action = self.env.action_space.sample()\n        else: # Use the Q-network to get the greedy action\n            action = self.model.predict(state.reshape((1, env.observation_space.shape[0])))[0].argmax()\n\n        # Decay epsilon\n        self.epsilon *= 1 - self.epsilon_decay\n        self.epsilon = max(0.05, self.epsilon)\n\n        return action\n    \n    def update(self, batch):\n        \n        # Get the minibatch\n        states, actions, rewards, next_states, dones = batch \n        \n        # Predict the Q-values in the current state\n        targets = np.array(self.model.predict_on_batch(states))\n        \n        # Predict the Q-values in the next state using the target model\n        next_Q_value = np.array(self.target_model.predict_on_batch(next_states)).max(axis=1)\n        \n        # Terminal states have a value of 0\n        next_Q_value[dones] = 0.0\n        \n        # Compute the target\n        for i in range(self.batch_size):\n            targets[i, actions[i]] = rewards[i] + self.gamma * next_Q_value[i]\n            \n        # Train the model on the minibatch\n        history = self.model.fit(states, targets, epochs=1, batch_size=self.batch_size, verbose=0)\n        \n        return history.history['loss'][0]\n\n    def train(self, nb_episodes):\n\n        steps = 0\n        returns = []\n        losses = []\n\n        for episode in range(nb_episodes):\n            \n            # Reset\n            state = self.env.reset()\n            done = False\n            steps_episode = 0\n            return_episode = 0\n\n            loss_episode = []\n            \n            # Sample the episode\n            while not done:\n\n                # Select an action \n                action = self.act(state)\n            \n                # Perform the action\n                next_state, reward, done, info = self.env.step(action)\n                \n                # Store the transition\n                self.buffer.append(state, action, reward, next_state, done)\n            \n                # Sample a minibatch\n                batch = self.buffer.sample(batch_size)\n                \n                # Train the NN on the minibatch\n                if len(batch) > 0 and steps % self.training_update_period == 0:\n                    loss = self.update(batch)\n                    loss_episode.append(loss)\n\n                # Update the target model\n                if steps > self.target_update_period and steps % self.target_update_period == 0:\n                    self.target_model.set_weights(self.model.get_weights())\n            \n                # Go in the next state\n                state = next_state\n                \n                # Increment time\n                steps += 1\n                steps_episode += 1\n                return_episode += reward\n                    \n                if done:\n                    break\n            \n            # Store info\n            returns.append(return_episode)\n            losses.append(np.mean(loss_episode))\n\n            # Print info\n            clear_output(wait=True)\n            print('Episode', episode+1)\n            print(' total steps:', steps)\n            print(' length of the episode:', steps_episode)\n            print(' return of the episode:', return_episode)\n            print(' current loss:', np.mean(loss_episode))\n            print(' epsilon:', self.epsilon)\n\n        return returns, losses\n\n    def test(self, render=True):\n\n        old_epsilon = self.epsilon\n        self.epsilon = 0.0\n        \n        state = self.env.reset()\n        nb_steps = 0\n        done = False\n        \n        while not done:\n            action = self.act(state)\n            next_state, reward, done, info = self.env.step(action)\n            state = next_state\n            nb_steps += 1\n        \n        self.epsilon = old_epsilon\n        return nb_steps\n\n\n# Parameters\nnb_episodes = 100\nbatch_size = 32\n\nepsilon = 1.0\nepsilon_decay = 0.0005\n\ngamma = 0.99\n\nlearning_rate = 0.005 \nbuffer_limit = 5000\ntarget_update_period = 100\ntraining_update_period = 4\n\n\n# Create the environment\nenv = gym.make('CartPole-v0')\n\n# Create the agent\nagent = DQNAgent(env, create_model, learning_rate, epsilon, epsilon_decay, gamma, batch_size, target_update_period, training_update_period, buffer_limit)\n\n# Train the agent\nreturns, losses = agent.train(nb_episodes)\n\n# Plot the returns\nplt.figure(figsize=(10, 6))\nplt.plot(returns)\nplt.plot(running_mean(returns, 10))\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"Returns\")\n\n# Plot the losses\nplt.figure(figsize=(10, 6))\nplt.plot(losses)\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"Training loss\")\nplt.show()\n\nEpisode 100\n total steps: 8599\n length of the episode: 174\n return of the episode: 174.0\n current loss: 5.679146097149959\n epsilon: 0.05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Test the network\nnb_steps = agent.test()\nprint(\"Number of steps:\", nb_steps)\n\nNumber of steps: 131\n\n\nQ: How does the loss evolve? Does it make sense?\nA: The Q-values are non-stationary: the initial Q-values are very small (the agent fails almost immediately), while they are around 40 after training (200 steps, but discounted with gamma). The mse increases with the magnitude of the Q-values, so the loss is a poor indicator of the convergence of the network."
  },
  {
    "objectID": "exercises/12-DQN-solution.html#reward-scaling",
    "href": "exercises/12-DQN-solution.html#reward-scaling",
    "title": "DQN",
    "section": "Reward scaling",
    "text": "Reward scaling\nQ: Do a custom test trial after training (i.e. do not call test(), but copy and adapt its code) and plot the Q-value of the selected action at each time step. Do you think it is a good output for the network? Could it explain why learning is so slow?\n\nagent.epsilon = 0.0\n        \nstate = agent.env.reset()\ndone = False\n\nQ_values = []\n\nwhile not done:\n    action = agent.act(state)\n    Q_values.append(agent.model.predict(state.reshape((1, 4)))[0][action])\n    next_state, reward, done, info = agent.env.step(action)\n    state = next_state\n\nplt.figure(figsize=(10, 6))\nplt.plot(Q_values)\nplt.xlabel(\"Steps\")\nplt.ylabel(\"Q-value\")\nplt.show()\n\n\n\n\n\n\n\n\nA: The predicted Q-values at the beginning of learning are close to 0, as the weights are randomly initialized. They must grow to around 40, which takes a lot of time. If the target Q-values were around 1, learning might be much faster.\nQ: Implement reward scaling by dividing the received rewards by a fixed factor of 100 when computing the Bellman targets. That way, the final Q-values will be around 1, what may be much easier to learned.\nTip: in order to avoid a huge copy and paste, you can inherit from your DQNAgent and ony reimplement the desired function:\nclass ScaledDQNAgent (DQNAgent):\n    def update(self, batch):\n        # Change the content of this function only\nYou should reduce a bit the learning rate (e.g. 0.001) as the magnitude of the targets has changed.\n\nclass ScaledDQNAgent(DQNAgent):\n    \n    def update(self, batch):\n        \n        # Get the minibatch\n        states, actions, rewards, next_states, dones = batch \n        \n        # Predict the Q-values in the current state\n        targets = np.array(self.model.predict_on_batch(states))\n        \n        # Predict the Q-values in the next state using the target model\n        next_Q_value = np.array(self.target_model.predict_on_batch(next_states)).max(axis=1)\n        \n        # Terminal states have a value of 0\n        next_Q_value[dones] = 0.0\n        \n        # Compute the target\n        for i in range(self.batch_size):\n            targets[i, actions[i]] = rewards[i]/100. + self.gamma * next_Q_value[i]\n            \n        # Train the model on the minibatch\n        history = self.model.fit(states, targets, epochs=1, batch_size=self.batch_size, verbose=0)\n        \n        return history.history['loss'][0]\n\n\n# Create the environment\nenv = gym.make('CartPole-v0')\n\n# Create the agent\nlearning_rate = 0.001\nagent = ScaledDQNAgent(env, create_model, learning_rate, epsilon, epsilon_decay, gamma, batch_size, target_update_period, training_update_period, buffer_limit)\n\n# Train the agent\nreturns, losses = agent.train(nb_episodes)\n\n# Test the network\nagent.epsilon = 0.0\n\n# Q-values     \nstate = agent.env.reset()\ndone = False\nQ_values = []\n\nwhile not done:\n    action = agent.act(state)\n    Q_values.append(agent.model.predict(state.reshape((1, 4)))[0][action])\n    next_state, reward, done, info = agent.env.step(action)\n    state = next_state\n\n# Plot the returns\nplt.figure(figsize=(10, 6))\nplt.plot(returns)\nplt.plot(running_mean(returns, 10))\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"Returns\")\n\n# Plot the losses\nplt.figure(figsize=(10, 6))\nplt.plot(losses)\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"Training loss\")\n\n# Plot the Q-values\nplt.figure(figsize=(10, 6))\nplt.plot(Q_values)\nplt.xlabel(\"Steps\")\nplt.ylabel(\"Q-value\")\nplt.show()\n\nEpisode 100\n total steps: 9986\n length of the episode: 200\n return of the episode: 200.0\n current loss: 0.0003223761088884203\n epsilon: 0.05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Test the network\nnb_steps = agent.test()\nprint(\"Number of steps:\", nb_steps)\n\nNumber of steps: 200\n\n\nQ: Depending on the time left and your motivation, vary the different parameters to understand their influence: learning rate, target update frequency, training update frequency, epsilon decay, gamma, etc. Change the size of the network. If you find better hyperparameters than what is proposed, please report them for next year!"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Agarwal, A., Hsu, D., Kale, S., Langford, J., Li, L., and Schapire, R.\nE. (2014). Taming the Monster: A Fast and\nSimple Algorithm for Contextual Bandits. in\nProceedings of the 31 st International Conference on\nMachine Learning (Beijing, China), 9. https://arxiv.org/abs/1402.0555.\n\n\nAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R.,\nWelinder, P., et al. (2017). Hindsight Experience Replay.\nhttp://arxiv.org/abs/1707.01495.\n\n\nArora, S., and Doshi, P. (2019). A Survey of Inverse\nReinforcement Learning: Challenges,\nMethods and Progress. http://arxiv.org/abs/1806.06877.\n\n\nBarreto, A., Dabney, W., Munos, R., Hunt, J. J., Schaul, T., van\nHasselt, H., et al. (2016). Successor Features for\nTransfer in Reinforcement Learning. http://arxiv.org/abs/1606.05312.\n\n\nBarth-Maron, G., Hoffman, M. W., Budden, D., Dabney, W., Horgan, D., TB,\nD., et al. (2018). Distributed Distributional Deterministic Policy\nGradients. http://arxiv.org/abs/1804.08617.\n\n\nBarto, A. G. (2013). “Intrinsic Motivation and\nReinforcement Learning,” in Intrinsically\nMotivated Learning in Natural and\nArtificial Systems, eds. G. Baldassarre and M. Mirolli\n(Berlin, Heidelberg: Springer), 17–47. doi:10.1007/978-3-642-32375-1_2.\n\n\nBelkhale, S., Li, R., Kahn, G., McAllister, R., Calandra, R., and\nLevine, S. (2021). Model-Based Meta-Reinforcement Learning\nfor Flight with Suspended Payloads. IEEE\nRobot. Autom. Lett., 1–1. doi:10.1109/LRA.2021.3057046.\n\n\nBellemare, M. G., Dabney, W., and Munos, R. (2017). A\nDistributional Perspective on Reinforcement\nLearning. http://arxiv.org/abs/1707.06887.\n\n\nBurda, Y., Edwards, H., Pathak, D., Storkey, A., Darrell, T., and Efros,\nA. A. (2018). Large-Scale Study of Curiosity-Driven\nLearning. http://arxiv.org/abs/1808.04355.\n\n\nChou, P.-W., Maturana, D., and Scherer, S. (2017). Improving\nStochastic Policy Gradients in Continuous\nControl with Deep Reinforcement Learning using the\nBeta Distribution. in International\nConference on Machine Learning http://proceedings.mlr.press/v70/chou17a/chou17a.pdf.\n\n\nDabney, W., Ostrovski, G., Silver, D., and Munos, R. (2018). Implicit\nQuantile Networks for Distributional Reinforcement\nLearning. http://arxiv.org/abs/1806.06923.\n\n\nDabney, W., Rowland, M., Bellemare, M. G., and Munos, R. (2017).\nDistributional Reinforcement Learning with Quantile\nRegression. http://arxiv.org/abs/1710.10044.\n\n\nDayan, P. (1993). Improving Generalization for\nTemporal Difference Learning: The Successor\nRepresentation. Neural Computation 5, 613–624. doi:10.1162/neco.1993.5.4.613.\n\n\nDayan, P., and Niv, Y. (2008). Reinforcement learning: The\nGood, The Bad and The Ugly. Current\nOpinion in Neurobiology 18, 185–196. doi:10.1016/j.conb.2008.08.003.\n\n\nDegris, T., White, M., and Sutton, R. S. (2012). Linear Off-Policy\nActor-Critic. in Proceedings of the 2012 International\nConference on Machine Learning http://arxiv.org/abs/1205.4839.\n\n\nDuan, Y., Schulman, J., Chen, X., Bartlett, P. L., Sutskever, I., and\nAbbeel, P. (2016). RL$2̂$: Fast Reinforcement\nLearning via Slow Reinforcement Learning. http://arxiv.org/abs/1611.02779.\n\n\nFeinberg, V., Wan, A., Stoica, I., Jordan, M. I., Gonzalez, J. E., and\nLevine, S. (2018). Model-Based Value Estimation for\nEfficient Model-Free Reinforcement Learning. http://arxiv.org/abs/1803.00101.\n\n\nFortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves,\nA., et al. (2017). Noisy Networks for\nExploration. http://arxiv.org/abs/1706.10295.\n\n\nFrans, K., Ho, J., Chen, X., Abbeel, P., and Schulman, J. (2017). Meta\nLearning Shared Hierarchies. http://arxiv.org/abs/1710.09767.\n\n\nFujimoto, S., van Hoof, H., and Meger, D. (2018). Addressing\nFunction Approximation Error in Actor-Critic\nMethods. http://arxiv.org/abs/1802.09477.\n\n\nGehring, C. A. (2015). Approximate Linear Successor\nRepresentation. in, 5. http://people.csail.mit.edu/gehring/publications/clement-gehring-rldm-2015.pdf.\n\n\nGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep\nLearning. MIT Press http://www.deeplearningbook.org.\n\n\nGruslys, A., Dabney, W., Azar, M. G., Piot, B., Bellemare, M., and\nMunos, R. (2017). The Reactor: A fast and\nsample-efficient Actor-Critic agent for Reinforcement\nLearning. http://arxiv.org/abs/1704.04651.\n\n\nGu, S., Lillicrap, T., Sutskever, I., and Levine, S. (2016). Continuous\nDeep Q-Learning with Model-based\nAcceleration. http://arxiv.org/abs/1603.00748.\n\n\nHa, D., and Eck, D. (2017). A Neural Representation of\nSketch Drawings. http://arxiv.org/abs/1704.03477.\n\n\nHa, D., and Schmidhuber, J. (2018). World Models. doi:10.5281/zenodo.1207631.\n\n\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement\nLearning with Deep Energy-Based Policies. http://arxiv.org/abs/1702.08165.\n\n\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018). Soft\nActor-Critic: Off-Policy Maximum Entropy Deep\nReinforcement Learning with a Stochastic Actor. http://arxiv.org/abs/1801.01290.\n\n\nHafner, D., Lillicrap, T., Ba, J., and Norouzi, M. (2020). Dream to\nControl: Learning Behaviors by Latent\nImagination. http://arxiv.org/abs/1912.01603.\n\n\nHafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H.,\net al. (2019). Learning Latent Dynamics for\nPlanning from Pixels. http://arxiv.org/abs/1811.04551.\n\n\nHausknecht, M., and Stone, P. (2015). Deep Recurrent\nQ-Learning for Partially Observable MDPs. http://arxiv.org/abs/1507.06527.\n\n\nHessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G.,\nDabney, W., et al. (2017). Rainbow: Combining Improvements\nin Deep Reinforcement Learning. http://arxiv.org/abs/1710.02298.\n\n\nHorgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., van\nHasselt, H., et al. (2018). Distributed Prioritized Experience\nReplay. http://arxiv.org/abs/1803.00933.\n\n\nKakade, S., and Langford, J. (2002). Approximately Optimal\nApproximate Reinforcement Learning. Proc. 19th International\nConference on Machine Learning, 267–274. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.7601.\n\n\nKapturowski, S., Ostrovski, G., Quan, J., Munos, R., and Dabney, W.\n(2019). Recurrent experience replay in distributed reinforcement\nlearning. in, 19. https://openreview.net/pdf?id=r1lyTjAqYX.\n\n\nKendall, A., Hawke, J., Janz, D., Mazur, P., Reda, D., Allen, J.-M., et\nal. (2018). Learning to Drive in a Day. http://arxiv.org/abs/1807.00412.\n\n\nKulkarni, T. D., Saeedi, A., Gautam, S., and Gershman, S. J. (2016).\nDeep Successor Reinforcement Learning. http://arxiv.org/abs/1606.02396.\n\n\nKurutach, T., Clavera, I., Duan, Y., Tamar, A., and Abbeel, P. (2018).\nModel-Ensemble Trust-Region Policy Optimization. http://arxiv.org/abs/1802.10592.\n\n\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa,\nY., et al. (2015). Continuous control with deep reinforcement learning.\nCoRR. http://arxiv.org/abs/1509.02971.\n\n\nMao, H., Alizadeh, M., Menache, I., and Kandula, S. (2016). Resource\nManagement with Deep Reinforcement Learning.\nin Proceedings of the 15th ACM Workshop on Hot\nTopics in Networks - HotNets ’16\n(Atlanta, GA, USA: ACM Press), 50–56. doi:10.1145/3005745.3005750.\n\n\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley,\nT., et al. (2016). Asynchronous Methods for Deep\nReinforcement Learning. in Proc. ICML http://arxiv.org/abs/1602.01783.\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I.,\nWierstra, D., et al. (2013). Playing Atari with Deep\nReinforcement Learning. http://arxiv.org/abs/1312.5602.\n\n\nMomennejad, I., Russek, E. M., Cheong, J. H., Botvinick, M. M., Daw, N.\nD., and Gershman, S. J. (2017). The successor representation in human\nreinforcement learning. Nature Human Behaviour 1, 680–692.\ndoi:10.1038/s41562-017-0180-8.\n\n\nMoore, A. W., and Atkeson, C. G. (1993). Prioritized sweeping:\nReinforcement learning with less data and less time.\nMach Learn 13, 103–130. doi:10.1007/BF00993104.\n\n\nNagabandi, A., Kahn, G., Fearing, R. S., and Levine, S. (2017). Neural\nNetwork Dynamics for Model-Based Deep Reinforcement\nLearning with Model-Free Fine-Tuning. http://arxiv.org/abs/1708.02596.\n\n\nNair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De\nMaria, A., et al. (2015). Massively Parallel Methods for\nDeep Reinforcement Learning. https://arxiv.org/pdf/1507.04296.pdf.\n\n\nNiu, F., Recht, B., Re, C., and Wright, S. J. (2011).\nHOGWILD!: A Lock-Free Approach to\nParallelizing Stochastic Gradient Descent. in Proc.\nAdvances in Neural Information Processing\nSystems, 21–21. http://arxiv.org/abs/1106.5730.\n\n\nPathak, D., Agrawal, P., Efros, A. A., and Darrell, T. (2017).\nCuriosity-driven Exploration by Self-supervised Prediction. http://arxiv.org/abs/1705.05363.\n\n\nPlappert, M., Houthooft, R., Dhariwal, P., Sidor, S., Chen, R. Y., Chen,\nX., et al. (2018). Parameter Space Noise for\nExploration. http://arxiv.org/abs/1706.01905.\n\n\nPong, V., Gu, S., Dalal, M., and Levine, S. (2018). Temporal\nDifference Models: Model-Free Deep RL for\nModel-Based Control. http://arxiv.org/abs/1802.09081.\n\n\nPopov, I., Heess, N., Lillicrap, T., Hafner, R., Barth-Maron, G.,\nVecerik, M., et al. (2017). Data-efficient Deep Reinforcement\nLearning for Dexterous Manipulation. http://arxiv.org/abs/1704.03073.\n\n\nRussek, E. M., Momennejad, I., Botvinick, M. M., Gershman, S. J., and\nDaw, N. D. (2017). Predictive representations can link model-based\nreinforcement learning to model-free mechanisms. PLOS Computational\nBiology 13, e1005768. doi:10.1371/journal.pcbi.1005768.\n\n\nRusu, A. A., Colmenarejo, S. G., Gulcehre, C., Desjardins, G.,\nKirkpatrick, J., Pascanu, R., et al. (2016). Policy\nDistillation. http://arxiv.org/abs/1511.06295.\n\n\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. (2015). Prioritized\nExperience Replay. http://arxiv.org/abs/1511.05952.\n\n\nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L.,\nSchmitt, S., et al. (2019). Mastering Atari,\nGo, Chess and Shogi by\nPlanning with a Learned Model. http://arxiv.org/abs/1911.08265.\n\n\nSchulman, J., Chen, X., and Abbeel, P. (2017). Equivalence Between\nPolicy Gradients and Soft Q-Learning. http://arxiv.org/abs/1704.06440.\n\n\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.\n(2015a). Trust Region Policy Optimization. in\nProceedings of the 31 st International Conference on\nMachine Learning, 1889–1897. http://proceedings.mlr.press/v37/schulman15.html.\n\n\nSchulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P.\n(2015b). High-Dimensional Continuous Control Using Generalized\nAdvantage Estimation. http://arxiv.org/abs/1506.02438.\n\n\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den\nDriessche, G., et al. (2016). Mastering the game of Go with\ndeep neural networks and tree search. Nature 529, 484–489.\ndoi:10.1038/nature16961.\n\n\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M.,\nGuez, A., et al. (2018). A general reinforcement learning algorithm that\nmasters chess, shogi, and Go through self-play.\nScience 362, 1140–1144. doi:10.1126/science.aar6404.\n\n\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and\nRiedmiller, M. (2014). Deterministic Policy Gradient\nAlgorithms. in Proc. ICML Proceedings of\nMachine Learning Research., eds. E. P. Xing and T. Jebara\n(PMLR), 387–395. http://proceedings.mlr.press/v32/silver14.html.\n\n\nStachenfeld, K. L., Botvinick, M. M., and Gershman, S. J. (2017). The\nhippocampus as a predictive map. Nature Neuroscience 20,\n1643–1653. doi:10.1038/nn.4650.\n\n\nSutton, R. S. (1990). Integrated Architectures for\nLearning, Planning, and Reacting\nBased on Approximating Dynamic Programming.\nMachine Learning Proceedings 1990, 216–224. doi:10.1016/B978-1-55860-141-3.50030-4.\n\n\nSutton, R. S., and Barto, A. G. (1998). Reinforcement\nLearning: An introduction.\nCambridge, MA: MIT press.\n\n\nSutton, R. S., and Barto, A. G. (2017). Reinforcement\nLearning: An Introduction. 2nd ed.\nCambridge, MA: MIT Press http://incompleteideas.net/book/the-book-2nd.html.\n\n\nSutton, R. S., McAllester, D., Singh, S., and Mansour, Y. (1999). Policy\ngradient methods for reinforcement learning with function approximation.\nin Proceedings of the 12th International Conference on\nNeural Information Processing Systems (MIT\nPress), 1057–1063. https://dl.acm.org/citation.cfm?id=3009806.\n\n\nTeh, Y. W., Bapst, V., Czarnecki, W. M., Quan, J., Kirkpatrick, J.,\nHadsell, R., et al. (2017). Distral: Robust Multitask\nReinforcement Learning. http://arxiv.org/abs/1707.04175.\n\n\nTesauro, G. (1995). “TD-Gammon: A Self-Teaching\nBackgammon Program,” in Applications of Neural\nNetworks, ed. A. F. Murray (Boston, MA:\nSpringer US), 267–285. doi:10.1007/978-1-4757-2379-3_11.\n\n\nTodorov, E. (2008). General duality between optimal control and\nestimation. in 2008 47th IEEE Conference on\nDecision and Control, 4286–4292. doi:10.1109/CDC.2008.4739438.\n\n\nToussaint, M. (2009). Robot Trajectory Optimization Using\nApproximate Inference. in Proceedings of the 26th\nAnnual International Conference on Machine\nLearning ICML ’09. (New York, NY,\nUSA: ACM), 1049–1056. doi:10.1145/1553374.1553508.\n\n\nUhlenbeck, G. E., and Ornstein, L. S. (1930). On the Theory\nof the Brownian Motion. Physical Review 36. doi:10.1103/PhysRev.36.823.\n\n\nvan Hasselt, H., Guez, A., and Silver, D. (2015). Deep\nReinforcement Learning with Double\nQ-learning. http://arxiv.org/abs/1509.06461.\n\n\nWang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z.,\nMunos, R., et al. (2017a). Learning to reinforcement learn. http://arxiv.org/abs/1611.05763.\n\n\nWang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R., Kavukcuoglu, K., et\nal. (2017b). Sample Efficient Actor-Critic with\nExperience Replay. http://arxiv.org/abs/1611.01224.\n\n\nWang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and de\nFreitas, N. (2016). Dueling Network Architectures for\nDeep Reinforcement Learning. http://arxiv.org/abs/1511.06581.\n\n\nWeber, T., Racanière, S., Reichert, D. P., Buesing, L., Guez, A.,\nRezende, D. J., et al. (2017). Imagination-Augmented Agents\nfor Deep Reinforcement Learning. http://arxiv.org/abs/1707.06203.\n\n\nWilliams, R. J. (1992). Simple statistical gradient-following algorithms\nfor connectionist reinforcement learning. Machine Learning 8,\n229–256.\n\n\nWilliams, R. J., and Peng, J. (1991). Function optimization using\nconnectionist reinforcement learning algorithms. Connection\nScience 3, 241–268.\n\n\nZhang, J., Springenberg, J. T., Boedecker, J., and Burgard, W. (2016).\nDeep Reinforcement Learning with Successor\nFeatures for Navigation across Similar\nEnvironments. http://arxiv.org/abs/1612.05533.\n\n\nZhu, Y., Gordon, D., Kolve, E., Fox, D., Fei-Fei, L., Gupta, A., et al.\n(2017). Visual Semantic Planning using Deep Successor\nRepresentations. http://arxiv.org/abs/1705.08080."
  }
]