[
  {
    "objectID": "webpage/Installation.html",
    "href": "webpage/Installation.html",
    "title": "Python installation",
    "section": "",
    "text": "In the B202 (FRIZ), conda is already installed. You first need to initialize it by typing this command in a terminal:\nconda init tcsh\nClose the terminal, open a new one, and type:\nconda create --name deeprl python=3.9\nconda activate deeprl\nconda install -c conda-forge numpy matplotlib jupyterlab\nThis can take a while, be patient.\nBefore every session, or when you open a new terminal, you will need to type:\nconda activate deeprl\nHere are the main Python dependencies necessary for the exercises:\nIf you are using Linux, you can probably install all the dependencies (except gym) from your package manager. For the others, use either Anaconda or Colab."
  },
  {
    "objectID": "webpage/Installation.html#anaconda",
    "href": "webpage/Installation.html#anaconda",
    "title": "Python installation",
    "section": "Anaconda",
    "text": "Anaconda\n\nInstalling Anaconda\nPython should be already installed if you use Linux, a very old version if you use MacOS, and probably nothing under Windows. Moreover, Python 2.7 became obsolete in December 2019 but is still the default on some distributions.\nFor these reasons, we strongly recommend installing Python 3 using the Anaconda distribution, or even better the community-driven fork Miniforge:\nhttps://github.com/conda-forge/miniforge\nAnaconda offers all the major Python packages in one place, with a focus on data science and machine learning. To install it, simply download the installer / script for your OS and follow the instructions. Beware, the installation takes quite a lot of space on the disk (around 1 GB), so choose the installation path wisely.\n\n\nInstalling packages\nTo install packages (for example numpy), you just have to type in a terminal:\nconda install numpy\nRefer to the docs (https://docs.anaconda.com/anaconda/) to know more.\nIf you prefer your local Python installation, or if a package is not available or outdated with Anaconda, the pip utility allows to also install virtually any Python package:\npip install numpy\n\n\nVirtual environments\nIt is a good idea to isolate the required packages from the rest of your Python installation, otherwise conflicts between package versions may arise.\nVirtual environments allow to create an isolated Python distribution for a project. The Python ecosystem offers many tools for that:\n\nvenv, the default Python 3 module.\nvirtualenv\npyenv\npipenv\n\nAs we advise to use Anaconda, we focus here on conda environments, but the logic is always the same.\nTo create a conda environment with the name deeprl using Python 3.9, type in a terminal:\nconda create --name deeprl python=3.9\nYou should see that it installs a bunch of basic packages along python:\n(base) ~/ conda create --name deeprl python=3.9\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /Users/vitay/Applications/miniforge3/envs/deeprl\n\n  added / updated specs:\n    - python=3.9\n\n\nThe following NEW packages will be INSTALLED:\n\n  bzip2              conda-forge/osx-arm64::bzip2-1.0.8-h3422bc3_4 None\n  ca-certificates    conda-forge/osx-arm64::ca-certificates-2022.9.24-h4653dfc_0 None\n  libffi             conda-forge/osx-arm64::libffi-3.4.2-h3422bc3_5 None\n  libsqlite          conda-forge/osx-arm64::libsqlite-3.39.4-h76d750c_0 None\n  libzlib            conda-forge/osx-arm64::libzlib-1.2.12-h03a7124_4 None\n  ncurses            conda-forge/osx-arm64::ncurses-6.3-h07bb92c_1 None\n  openssl            conda-forge/osx-arm64::openssl-3.0.5-h03a7124_2 None\n  pip                conda-forge/noarch::pip-22.2.2-pyhd8ed1ab_0 None\n  python             conda-forge/osx-arm64::python-3.9.13-h96fcbfb_0_cpython None\n  readline           conda-forge/osx-arm64::readline-8.1.2-h46ed386_0 None\n  setuptools         conda-forge/noarch::setuptools-65.4.1-pyhd8ed1ab_0 None\n  sqlite             conda-forge/osx-arm64::sqlite-3.39.4-h2229b38_0 None\n  tk                 conda-forge/osx-arm64::tk-8.6.12-he1e0b03_0 None\n  tzdata             conda-forge/noarch::tzdata-2022d-h191b570_0 None\n  wheel              conda-forge/noarch::wheel-0.37.1-pyhd8ed1ab_0 None\n  xz                 conda-forge/osx-arm64::xz-5.2.6-h57fd34a_0 None\n\n\nProceed ([y]/n)?\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n#\n# To activate this environment, use\n#\n#     $ conda activate deeprl\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\nRetrieving notices: ...working... done\nAs indicated at the end of the message, you need to activate the environment to use its packages:\nconda activate deeprl\nWhen you are done, you can deactivate it, or simply close the terminal.\n\n\n\n\n\n\nYou need to activate the environment every time you start an exercise or open a new terminal!\n\n\n\nYou can then install all the required packages to their latest versions, alternating between conda and pip:\nconda install numpy matplotlib jupyterlab\npip install tensorflow\npip install gym[all]\n\nIf you installed the regular Anaconda and not miniforge, we strongly advise to force using the conda forge channel:\nconda install -c conda-forge numpy matplotlib jupyterlab\n\nAlternatively, you can use one of the following files and install everything in one shot:\n\nconda-linux.yml for Linux and (possibly) Windows.\nconda-macos.yml for MacOS arm64 (M1). Untested on Intel-based macs.\n\nconda env create -f conda-linux.yml\nconda env create -f conda-macos.yml\n\n\n\n\n\n\nIf you have a CUDA-capable NVIDIA graphical card, follow these instructions to install tensorflow:\nhttps://www.tensorflow.org/install/pip\n\n\n\n\n\nUsing notebooks\nWhen the installation is complete, you just need to download the Jupyter notebook (.ipynb), activate your environment, and type:\njupyter lab name_of_the_notebook.ipynb\nto open a browser tab with the notebook."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "You will find below the links to the slides for each lecture (html and pdf). Outdated videos can be found in this playlist.\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n1.1 - Introduction\nIntroduction to the main concepts of reinforcement learning and showcasing of the current applications.\nhtml, pdf\n\n\n1.2 - Basics in mathematics (optional)\nThis lecture quickly reminds the main mathematical concepts needed to understand this course: linear algebra, calculus, probabilities, statistics and information theory. It is only intended for students who are not sure of the prerequisites.\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n2.1 - Bandits\nn-armed bandits, the simplest RL setting that can be solved by sampling.\nhtml, pdf\n\n\n2.2 - Markov Decision Processes\nMDPs are the basic RL framework. The value functions and the Bellman equations fully characterize a MDP.\nhtml, pdf\n\n\n2.3 - Dynamic Programming\nDynamic programming is a model-based method allowing to iteratively solve the Bellman equations.\nhtml, pdf\n\n\n2.4 - Monte Carlo control\nMonte Carlo control estimates value functions through sampling of complete episodes and infers the optimal policy using action selection, either on- or off-policy.\nhtml, pdf\n\n\n2.5 - Temporal Difference\nTD algorithms allow the learning of value functions using single transitions. Q-learning is the famous off-policy variant.\nhtml, pdf\n\n\n2.6 - Function Approximation\nThe value functions can actually be approximated by any function approximator, allowing to apply RL to continuous state of action spaces.\nhtml, pdf\n\n\n2.7 - Deep Neural Networks\nQuick overview of the main neural network architectures needed for the rest of the course.\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n3.1 - DQN: Deep Q-Network\nDQN (Mnih et al. 2013) was the first successful application of deep networks to the RL problem. It has been applied to Atari video games and started the interest for deep RL methods.\nhtml, pdf\n\n\n3.2 - Beyond DQN\nVarious extensions to the DQN algorithms have been proposed in the following years: distributional learning, parameter noise, distributed learning or recurrent architectures.\nhtml, pdf\n\n\n3.3 - PG: Policy Gradient\nPolicy gradient methods allow to directly learn the policy without requiring action selection over value functions.\nhtml, pdf\n\n\n3.4 - A3C: Asynchronous Advantage Actor-Critic\nA3C (Mnih et al., 2016) is an actor-critic architecture estimating the policy gradient from multiple parallel workers.\nhtml, pdf\n\n\n3.5 - DDPG: Deep Deterministic Policy Gradient\nDDPG (Lillicrap et al., is an off-policy actor-critic architecture particularly suited for continuous control problems such as robotics.\nhtml, pdf\n\n\n3.6 - PPO: Proximal Policy Optimization\nPPO (Schulman et al., 2017) allows stable learning by estimating trust regions for the policy updates.\nhtml, pdf\n\n\n3.7 - SAC: Soft Actor-Critic\nMaximum Entropy RL modifies the RL objective by learning optimal policies that also explore the environment as much as possible.. SAC (Haarnoja et al., 2018) is an off-policy actor-critic architecture for soft RL.\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\nIn WS2023-24, the lectures 3.4 to 3.6 were compressed into these slides: html. The lecture 3.7 was skipped.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n4.1 - Model-based RL\nTwo main paradigms in model-based RL: model-based augmentation of model-free learning (Dyna architectures) and planning (model predictive control, MPC)\nhtml, pdf\n\n\n4.2 - Learned World models\nLearning a world model from data is much easier than learning the optimal policy, as it is just supervised learning. Modern model-based algorithms (TDM, World models, PlaNet, Dreamer) make use of this property to reduce the sample complexity.\nhtml, pdf\n\n\n4.3 - AlphaGo\nAlphaGo surprised the world in 2016 by beating Lee Seedol, the world champion of Go. It combines model-free learning through policy gradient and self-play with model-based planning using MCTS (Monte Carlo Tree Search).\nhtml, pdf\n\n\n4.4 - Successor representations\nSuccessor representations provide a trade-off between model-free and model-based learning.\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n5.1 - Outlook\nCurrent RL research investigates many different directions: inverse RL, intrinsic motivation, hierarchical RL, meta RL, offline RL, multi-agent RL, etc.\nhtml, pdf"
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "You will find below the links to the slides for each lecture (html and pdf). Outdated videos can be found in this playlist.\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n1.1 - Introduction\nIntroduction to the main concepts of reinforcement learning and showcasing of the current applications.\nhtml, pdf\n\n\n1.2 - Basics in mathematics (optional)\nThis lecture quickly reminds the main mathematical concepts needed to understand this course: linear algebra, calculus, probabilities, statistics and information theory. It is only intended for students who are not sure of the prerequisites.\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n2.1 - Bandits\nn-armed bandits, the simplest RL setting that can be solved by sampling.\nhtml, pdf\n\n\n2.2 - Markov Decision Processes\nMDPs are the basic RL framework. The value functions and the Bellman equations fully characterize a MDP.\nhtml, pdf\n\n\n2.3 - Dynamic Programming\nDynamic programming is a model-based method allowing to iteratively solve the Bellman equations.\nhtml, pdf\n\n\n2.4 - Monte Carlo control\nMonte Carlo control estimates value functions through sampling of complete episodes and infers the optimal policy using action selection, either on- or off-policy.\nhtml, pdf\n\n\n2.5 - Temporal Difference\nTD algorithms allow the learning of value functions using single transitions. Q-learning is the famous off-policy variant.\nhtml, pdf\n\n\n2.6 - Function Approximation\nThe value functions can actually be approximated by any function approximator, allowing to apply RL to continuous state of action spaces.\nhtml, pdf\n\n\n2.7 - Deep Neural Networks\nQuick overview of the main neural network architectures needed for the rest of the course.\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n3.1 - DQN: Deep Q-Network\nDQN (Mnih et al. 2013) was the first successful application of deep networks to the RL problem. It has been applied to Atari video games and started the interest for deep RL methods.\nhtml, pdf\n\n\n3.2 - Beyond DQN\nVarious extensions to the DQN algorithms have been proposed in the following years: distributional learning, parameter noise, distributed learning or recurrent architectures.\nhtml, pdf\n\n\n3.3 - PG: Policy Gradient\nPolicy gradient methods allow to directly learn the policy without requiring action selection over value functions.\nhtml, pdf\n\n\n3.4 - A3C: Asynchronous Advantage Actor-Critic\nA3C (Mnih et al., 2016) is an actor-critic architecture estimating the policy gradient from multiple parallel workers.\nhtml, pdf\n\n\n3.5 - DDPG: Deep Deterministic Policy Gradient\nDDPG (Lillicrap et al., is an off-policy actor-critic architecture particularly suited for continuous control problems such as robotics.\nhtml, pdf\n\n\n3.6 - PPO: Proximal Policy Optimization\nPPO (Schulman et al., 2017) allows stable learning by estimating trust regions for the policy updates.\nhtml, pdf\n\n\n3.7 - SAC: Soft Actor-Critic\nMaximum Entropy RL modifies the RL objective by learning optimal policies that also explore the environment as much as possible.. SAC (Haarnoja et al., 2018) is an off-policy actor-critic architecture for soft RL.\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\nIn WS2023-24, the lectures 3.4 to 3.6 were compressed into these slides: html. The lecture 3.7 was skipped.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n4.1 - Model-based RL\nTwo main paradigms in model-based RL: model-based augmentation of model-free learning (Dyna architectures) and planning (model predictive control, MPC)\nhtml, pdf\n\n\n4.2 - Learned World models\nLearning a world model from data is much easier than learning the optimal policy, as it is just supervised learning. Modern model-based algorithms (TDM, World models, PlaNet, Dreamer) make use of this property to reduce the sample complexity.\nhtml, pdf\n\n\n4.3 - AlphaGo\nAlphaGo surprised the world in 2016 by beating Lee Seedol, the world champion of Go. It combines model-free learning through policy gradient and self-play with model-based planning using MCTS (Monte Carlo Tree Search).\nhtml, pdf\n\n\n4.4 - Successor representations\nSuccessor representations provide a trade-off between model-free and model-based learning.\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n5.1 - Outlook\nCurrent RL research investigates many different directions: inverse RL, intrinsic motivation, hierarchical RL, meta RL, offline RL, multi-agent RL, etc.\nhtml, pdf"
  },
  {
    "objectID": "index.html#exercises",
    "href": "index.html#exercises",
    "title": "Deep Reinforcement Learning",
    "section": "Exercises",
    "text": "Exercises\nYou will find below links to download the notebooks for the exercises (which you have to fill) and their solution (which you can look at after you have finished the exercise). It is recommended not to look at the solution while doing the exercise unless you are lost. Alternatively, you can run the notebooks directly on Colab (https://colab.research.google.com/) if you have a Google account.\nFor instructions on how to install a Python distribution on your computer, check this page.\n\n\n\n\n\n\nNotebook\nSolution\n\n\n\n\n1 - Introduction to Python\nIntroduction to the Python programming language. Optional for students already knowing Python.\nipynb, colab\nipynb, colab\n\n\n2 - Numpy and Matplotlib\nPresentation of the numpy library for numerical computations and matplotlib for visualization. Also optional for students already familiar.\nipynb, colab\nipynb, colab\n\n\n3 - Sampling\nSimple exercise to investigate random sampling and its properties.\nipynb, colab\nipynb, colab\n\n\n4 - Bandits\nImplementation of various action selection methods to the n-armed bandit.\nipynb, colab\nipynb, colab\n\n\n5 - Bandits (part 2)\nAdvanced bandit methods.\nipynb, colab\nipynb, colab\n\n\n6 - Dynamic programming\nCalculation of the Bellman equations for the recycling robot and application of policy iteration and value iteration.\nipynb, colab\nipynb, colab\n\n\n7 - Gym environments\nIntrodcution to the gym(nasium) RL environments.\nipynb, colab\nipynb, colab\n\n\n8 - Monte Carlo control\nStudy of on-policy Monte Carlo control on the Taxi environment.\nipynb, colab\nipynb, colab\n\n\n9 - Temporal Difference, Q-learning\nQ-learning on the Taxi environment.\nipynb, colab\nipynb, colab\n\n\n10 - Eligibility traces\nInvestigation of eligibility traces for Q-learning in a gridworld environment.\nipynb, colab\nipynb, colab\n\n\n11 - Keras\nQuick tutorial for Keras.\nipynb, colab\nipynb, colab\n\n\n12 - DQN\nImplementation of the DQN algorithm for Cartpole from scratch.\nipynb, colab\nipynb, colab\n\n\n13 - PPO\nDQN and PPO on cartpole using the tianshou library.\nipynb, colab\nipynb, colab"
  },
  {
    "objectID": "index.html#recommended-readings",
    "href": "index.html#recommended-readings",
    "title": "Deep Reinforcement Learning",
    "section": "Recommended readings",
    "text": "Recommended readings\n\nRichard Sutton and Andrew Barto (2017). Reinforcement Learning: An Introduction. MIT press.\n\nhttp://incompleteideas.net/book/the-book-2nd.html\n\nCS294 course of Sergey Levine at Berkeley.\n\nhttp://rll.berkeley.edu/deeprlcourse/\n\nReinforcement Learning course by David Silver at UCL.\n\nhttp://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"
  }
]