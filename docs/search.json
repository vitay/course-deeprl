[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "You will find below the links to the slides for each lecture (html and pdf). Outdated videos can be found in this playlist.\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n1.1 - Introduction\nThis lecture introduces the main concepts of reinforcement learning and showcases the current applications.\nhtml, pdf\n\n\n1.2 - Basics in mathematics (optional)\nThis lecture quickly reminds the main mathematical concepts needed to understand this course: linear algebra, calculus, probabilities, statistics and information theory. It is only intended for students who are not sure of the prerequisites.\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n2.1 - Bandits\nThis lecture introduces bandits, the simplest RL setting that can be solved by sampling.\nhtml, pdf\n\n\n2.2 - Markov Decision Processes\nTODO\nhtml, pdf\n\n\n2.3 - Dynamic Programming\nTODO\nhtml, pdf\n\n\n2.4 - Monte Carlo control\nTODO\nhtml, pdf\n\n\n2.5 - Temporal Difference\nTODO\nhtml, pdf\n\n\n2.6 - Function Approximation\nTODO\nhtml, pdf\n\n\n2.7 - Deep Neural Networks\nTODO\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n3.1 - DQN: Deep Q-Network\nTODO\nhtml, pdf\n\n\n3.2 - Beyond DQN\nTODO\nhtml, pdf\n\n\n3.3 - PG: Policy Gradient\nTODO\nhtml, pdf\n\n\n3.4 - A3C: Asynchronous Advantage Actor-Critic\nTODO\nhtml, pdf\n\n\n3.5 - DDPG: Deep Deterministic Policy Gradient\nTODO\nhtml, pdf\n\n\n3.6 - PPO: Proximal Policy Optimization\nTODO\nhtml, pdf\n\n\n3.7 - SAC: Soft Actor-Critic\nTODO\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\nIn WS2023-24, the lectures 3.4 to 3.6 were compressed into these slides: html, pdf. The lecture 3.7 was skipped."
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "You will find below the links to the slides for each lecture (html and pdf). Outdated videos can be found in this playlist.\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n1.1 - Introduction\nThis lecture introduces the main concepts of reinforcement learning and showcases the current applications.\nhtml, pdf\n\n\n1.2 - Basics in mathematics (optional)\nThis lecture quickly reminds the main mathematical concepts needed to understand this course: linear algebra, calculus, probabilities, statistics and information theory. It is only intended for students who are not sure of the prerequisites.\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n2.1 - Bandits\nThis lecture introduces bandits, the simplest RL setting that can be solved by sampling.\nhtml, pdf\n\n\n2.2 - Markov Decision Processes\nTODO\nhtml, pdf\n\n\n2.3 - Dynamic Programming\nTODO\nhtml, pdf\n\n\n2.4 - Monte Carlo control\nTODO\nhtml, pdf\n\n\n2.5 - Temporal Difference\nTODO\nhtml, pdf\n\n\n2.6 - Function Approximation\nTODO\nhtml, pdf\n\n\n2.7 - Deep Neural Networks\nTODO\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n3.1 - DQN: Deep Q-Network\nTODO\nhtml, pdf\n\n\n3.2 - Beyond DQN\nTODO\nhtml, pdf\n\n\n3.3 - PG: Policy Gradient\nTODO\nhtml, pdf\n\n\n3.4 - A3C: Asynchronous Advantage Actor-Critic\nTODO\nhtml, pdf\n\n\n3.5 - DDPG: Deep Deterministic Policy Gradient\nTODO\nhtml, pdf\n\n\n3.6 - PPO: Proximal Policy Optimization\nTODO\nhtml, pdf\n\n\n3.7 - SAC: Soft Actor-Critic\nTODO\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\nIn WS2023-24, the lectures 3.4 to 3.6 were compressed into these slides: html, pdf. The lecture 3.7 was skipped."
  },
  {
    "objectID": "index.html#exercises",
    "href": "index.html#exercises",
    "title": "Deep Reinforcement Learning",
    "section": "Exercises",
    "text": "Exercises\nYou will find below links to download the notebooks for the exercises (which you have to fill) and their solution (which you can look at after you have finished the exercise). It is recommended not to look at the solution while doing the exercise unless you are lost. Alternatively, you can run the notebooks directly on Colab (https://colab.research.google.com/) if you have a Google account.\nFor instructions on how to install a Python distribution on your computer, check this page.\n\n\n\n\n\n\nNotebook\nSolution\n\n\n\n\n1 - Introduction to Python\nIntroduction to the Python programming language. Optional for students already knowing Python.\n.ipynb, colab\n.ipynb, colab\n\n\n2 - Numpy and Matplotlib\nTODO.\n.ipynb, colab\n.ipynb, colab\n\n\n3 - Sampling\nTODO.\n.ipynb, colab\n.ipynb, colab\n\n\n4 - Bandits\nTODO.\n.ipynb, colab\n.ipynb, colab\n\n\n5 - Bandits (part 2)\nTODO.\n.ipynb, colab\n.ipynb, colab\n\n\n6 - Dynamic programming\nTODO.\n.ipynb, colab\n.ipynb, colab\n\n\n7 - Gym environments\nTODO.\n.ipynb, colab\n.ipynb, colab\n\n\n8 - Monte Carlo control\nTODO.\n.ipynb, colab\n.ipynb, colab\n\n\n9 - Temporal Difference\nTODO.\n.ipynb, colab\n.ipynb, colab\n\n\n10 - Eligibility traces\nTODO.\n.ipynb, colab\n.ipynb, colab\n\n\n11 - Keras\nTODO.\n.ipynb, colab\n.ipynb, colab\n\n\n12 - DQN\nTODO.\n.ipynb, colab\n.ipynb, colab\n\n\n13 - PPO\nTODO.\n.ipynb, colab\n.ipynb, colab"
  },
  {
    "objectID": "index.html#recommended-readings",
    "href": "index.html#recommended-readings",
    "title": "Deep Reinforcement Learning",
    "section": "Recommended readings",
    "text": "Recommended readings\n\nRichard Sutton and Andrew Barto (2017). Reinforcement Learning: An Introduction. MIT press.\n\nhttp://incompleteideas.net/book/the-book-2nd.html\n\nCS294 course of Sergey Levine at Berkeley.\n\nhttp://rll.berkeley.edu/deeprlcourse/\n\nReinforcement Learning course by David Silver at UCL.\n\nhttp://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"
  },
  {
    "objectID": "Installation.html",
    "href": "Installation.html",
    "title": "Python installation",
    "section": "",
    "text": "In the B202 (FRIZ), conda is already installed. You first need to initialize it by typing this command in a terminal:\nconda init tcsh\nClose the terminal, open a new one, and type:\nconda create --name deeprl python=3.9\nconda activate deeprl\nconda install -c conda-forge numpy matplotlib jupyterlab\nThis can take a while, be patient.\nBefore every session, or when you open a new terminal, you will need to type:\nconda activate deeprl\nHere are the main Python dependencies necessary for the exercises:\nIf you are using Linux, you can probably install all the dependencies (except gym) from your package manager. For the others, use either Anaconda or Colab."
  },
  {
    "objectID": "Installation.html#anaconda",
    "href": "Installation.html#anaconda",
    "title": "Python installation",
    "section": "Anaconda",
    "text": "Anaconda\n\nInstalling Anaconda\nPython should be already installed if you use Linux, a very old version if you use MacOS, and probably nothing under Windows. Moreover, Python 2.7 became obsolete in December 2019 but is still the default on some distributions.\nFor these reasons, we strongly recommend installing Python 3 using the Anaconda distribution, or even better the community-driven fork Miniforge:\nhttps://github.com/conda-forge/miniforge\nAnaconda offers all the major Python packages in one place, with a focus on data science and machine learning. To install it, simply download the installer / script for your OS and follow the instructions. Beware, the installation takes quite a lot of space on the disk (around 1 GB), so choose the installation path wisely.\n\n\nInstalling packages\nTo install packages (for example numpy), you just have to type in a terminal:\nconda install numpy\nRefer to the docs (https://docs.anaconda.com/anaconda/) to know more.\nIf you prefer your local Python installation, or if a package is not available or outdated with Anaconda, the pip utility allows to also install virtually any Python package:\npip install numpy\n\n\nVirtual environments\nIt is a good idea to isolate the required packages from the rest of your Python installation, otherwise conflicts between package versions may arise.\nVirtual environments allow to create an isolated Python distribution for a project. The Python ecosystem offers many tools for that:\n\nvenv, the default Python 3 module.\nvirtualenv\npyenv\npipenv\n\nAs we advise to use Anaconda, we focus here on conda environments, but the logic is always the same.\nTo create a conda environment with the name deeprl using Python 3.9, type in a terminal:\nconda create --name deeprl python=3.9\nYou should see that it installs a bunch of basic packages along python:\n(base) ~/ conda create --name deeprl python=3.9\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /Users/vitay/Applications/miniforge3/envs/deeprl\n\n  added / updated specs:\n    - python=3.9\n\n\nThe following NEW packages will be INSTALLED:\n\n  bzip2              conda-forge/osx-arm64::bzip2-1.0.8-h3422bc3_4 None\n  ca-certificates    conda-forge/osx-arm64::ca-certificates-2022.9.24-h4653dfc_0 None\n  libffi             conda-forge/osx-arm64::libffi-3.4.2-h3422bc3_5 None\n  libsqlite          conda-forge/osx-arm64::libsqlite-3.39.4-h76d750c_0 None\n  libzlib            conda-forge/osx-arm64::libzlib-1.2.12-h03a7124_4 None\n  ncurses            conda-forge/osx-arm64::ncurses-6.3-h07bb92c_1 None\n  openssl            conda-forge/osx-arm64::openssl-3.0.5-h03a7124_2 None\n  pip                conda-forge/noarch::pip-22.2.2-pyhd8ed1ab_0 None\n  python             conda-forge/osx-arm64::python-3.9.13-h96fcbfb_0_cpython None\n  readline           conda-forge/osx-arm64::readline-8.1.2-h46ed386_0 None\n  setuptools         conda-forge/noarch::setuptools-65.4.1-pyhd8ed1ab_0 None\n  sqlite             conda-forge/osx-arm64::sqlite-3.39.4-h2229b38_0 None\n  tk                 conda-forge/osx-arm64::tk-8.6.12-he1e0b03_0 None\n  tzdata             conda-forge/noarch::tzdata-2022d-h191b570_0 None\n  wheel              conda-forge/noarch::wheel-0.37.1-pyhd8ed1ab_0 None\n  xz                 conda-forge/osx-arm64::xz-5.2.6-h57fd34a_0 None\n\n\nProceed ([y]/n)?\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n#\n# To activate this environment, use\n#\n#     $ conda activate deeprl\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\nRetrieving notices: ...working... done\nAs indicated at the end of the message, you need to activate the environment to use its packages:\nconda activate deeprl\nWhen you are done, you can deactivate it, or simply close the terminal.\n\n\n\n\n\n\nYou need to activate the environment every time you start an exercise or open a new terminal!\n\n\n\nYou can then install all the required packages to their latest versions, alternating between conda and pip:\nconda install numpy matplotlib jupyterlab\npip install tensorflow\npip install gym[all]\n\nIf you installed the regular Anaconda and not miniforge, we strongly advise to force using the conda forge channel:\nconda install -c conda-forge numpy matplotlib jupyterlab\n\nAlternatively, you can use one of the following files and install everything in one shot:\n\nconda-linux.yml for Linux and (possibly) Windows.\nconda-macos.yml for MacOS arm64 (M1). Untested on Intel-based macs.\n\nconda env create -f conda-linux.yml\nconda env create -f conda-macos.yml\n\n\n\n\n\n\nIf you have a CUDA-capable NVIDIA graphical card, follow these instructions to install tensorflow:\nhttps://www.tensorflow.org/install/pip\n\n\n\n\n\nUsing notebooks\nWhen the installation is complete, you just need to download the Jupyter notebook (.ipynb), activate your environment, and type:\njupyter lab name_of_the_notebook.ipynb\nto open a browser tab with the notebook."
  }
]