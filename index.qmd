---
title: Deep Reinforcement Learning
author: 
  - name: Julien Vitay 
    email: julien.vitay@informatik.tu-chemnitz.de
    url: https://julien-vitay.net
    affiliation: Chemnitz University of Technology
    affiliation-url: https://tu-chemnitz.de
    orcid: 0000-0001-5229-2349

abstract: This website contains the materials for the module **Deep Reinforcement Learning** (573140) taught by Dr. Julien Vitay at the Technische Universit√§t Chemnitz. 

echo: false
---


## Information

Registration on [OPAL](https://bildungsportal.sachsen.de/opal/auth/RepositoryEntry/21637267457) to receive updates per email.

:::callout-note
### Summer semester 2024

No lectures in the summer semester. There will be a (retake) exam in July/August.
:::

## Lectures

You will find below the links to the slides for each lecture (html and pdf). Outdated videos can be found in this [playlist](https://www.youtube.com/playlist?list=PLIEjdhhAF7UJwegwyWUcDrUNJTQfxMcUw).

#### 1 - Introduction


```{python}
from IPython.display import Markdown

def table_lecture(lectures):
    # header
    text = """
|          | Slides |
|----------|--------|"""

    # fields
    for key, desc in lectures.items():
        text += f"""
| {desc}  | [html](slides/{key}.qmd){{target="_blank"}}, [pdf](slides/pdf/{key}.pdf){{target="_blank"}} |"""

    # finish
    text += """
: {tbl-colwidths="[90, 10]"}
"""

    return Markdown(text)
```

```{python} 
lecs = {
    '1.1-Introduction': """**1.1 - Introduction**\\
Introduction to the main concepts of reinforcement learning and showcasing of the current applications.""",
    '1.2-Basics': """**1.2 - Basics in mathematics** (optional)\\
This lecture quickly reminds the main mathematical concepts needed to understand this course: linear algebra, calculus, probabilities, statistics and information theory. It is only intended for students who are not sure of the prerequisites.""",
}
table_lecture(lecs)
```


#### 2 - Tabular RL

```{python} 
lecs = {
    '2.1-Bandits': """**2.1 - Bandits**\\
n-armed bandits, the simplest RL setting that can be solved by sampling.""",
    '2.2-MDP': """**2.2 - Markov Decision Processes**\\
MDPs are the basic RL framework. The value functions and the Bellman equations fully characterize a MDP.""",
    '2.3-DP': """**2.3 - Dynamic Programming**\\
Dynamic programming is a model-based method allowing to iteratively solve the Bellman equations.""",
    '2.4-MC': """**2.4 - Monte Carlo control**\\
Monte Carlo control estimates value functions through sampling of complete episodes and infers the optimal policy using action selection, either on- or off-policy.""",
    '2.5-TD': """**2.5 - Temporal Difference**\\
TD algorithms allow the learning of value functions using single transitions. Q-learning is the famous off-policy variant.""",
    '2.6-FunctionApproximation': """**2.6 - Function Approximation**\\
The value functions can actually be approximated by any function approximator, allowing to apply RL to continuous state of action spaces.""",
    '2.7-DeepNetworks': """**2.7 - Deep Neural Networks**\\
Quick overview of the main neural network architectures needed for the rest of the course.""",
}
table_lecture(lecs)
```


#### 3 - Model-free RL


```{python} 
lecs = {
    '3.1-DQN': """**3.1 - DQN: Deep Q-Network**\\
DQN (Mnih et al. 2013) was the first successful application of deep networks to the RL problem. It has been applied to Atari video games and started the interest for deep RL methods.""",
    '3.2-BeyondDQN': """**3.2 - Beyond DQN**\\
Various extensions to the DQN algorithms have been proposed in the following years: distributional learning, parameter noise, distributed learning or recurrent architectures.""",
    '3.3-PG': """**3.3 - PG: Policy Gradient**\\
Policy gradient methods allow to directly learn the policy without requiring action selection over value functions.""",
    '3.4-A3C': """**3.4 - A3C: Asynchronous Advantage Actor-Critic**\\
A3C (Mnih et al., 2016) is an actor-critic architecture estimating the policy gradient from multiple parallel workers.""",
    '3.5-DDPG': """**3.5 - DDPG: Deep Deterministic Policy Gradient**\\
DDPG (Lillicrap et al., is an off-policy actor-critic architecture particularly suited for continuous control problems such as robotics.""",
    '3.6-PPO': """**3.6 - PPO: Proximal Policy Optimization**\\
PPO (Schulman et al., 2017) allows stable learning by estimating trust regions for the policy updates.""",
    '3.7-SAC': """**3.7 - SAC: Soft Actor-Critic**\\
Maximum Entropy RL modifies the RL objective by learning optimal policies that also explore the environment as much as possible.. SAC (Haarnoja et al., 2018) is an off-policy actor-critic architecture for soft RL.""",
}
table_lecture(lecs)
```

:::callout-note

In WS2023-24, the lectures 3.4 to 3.6 were compressed into these slides: [html](slides/3.8-A3C-DDPG-PPO.qmd){target="_blank"}. The lecture 3.7 was skipped.
:::



#### 4 - Model-based RL


```{python} 
lecs = {
    '4.1-ModelBased': """**4.1 - Model-based RL**\\
Two main paradigms in model-based RL: model-based augmentation of model-free learning (Dyna architectures) and planning (model predictive control, MPC)""",
    '4.2-LearnedModels': """**4.2 - Learned World models**\\
Learning a world model from data is much easier than learning the optimal policy, as it is just supervised learning. Modern model-based algorithms (TDM, World models, PlaNet, Dreamer) make use of this property to reduce the sample complexity.""",
    '4.3-AlphaGo': """**4.3 - AlphaGo**\\
AlphaGo surprised the world in 2016 by beating Lee Seedol, the world champion of Go. It combines model-free learning through policy gradient and self-play with model-based planning using MCTS (Monte Carlo Tree Search).""",
    '4.4-SR': """**4.4 - Successor representations**\\
Successor representations provide a trade-off between model-free and model-based learning.""",
}
table_lecture(lecs)
```



#### 5 - Outlook


```{python} 
lecs = {
    '5.1-Outlook': """**5.1 - Outlook**\\
Current RL research investigates many different directions: inverse RL, intrinsic motivation, hierarchical RL, meta RL, offline RL, multi-agent RL, etc.""",
}
table_lecture(lecs)
```

## Exercises

You will find below links to download the notebooks for the exercises (which you have to fill) and their solution (which you can look at after you have finished the exercise). It is recommended not to look at the solution while doing the exercise unless you are lost.  Alternatively, you can run the notebooks directly on Colab (<https://colab.research.google.com/>) if you have a Google account. 

For instructions on how to install a Python distribution on your computer, check this [page](webpage/Installation.qmd).


```{python}
from IPython.display import Markdown

repo = "https://raw.githubusercontent.com/vitay/notebooks-deeprl/main"
colab = "https://colab.research.google.com/github/vitay/notebooks-deeprl/blob/main"

def table_exercise(exs):
    text = """
|           | Notebook  | Solution |
|-----------|-----------|----------|"""

    for key, val in exs.items():
        text += f"""
| {val}  | [ipynb]({repo}/{key}.ipynb){{target="_blank"}}, [colab]({colab}/{key}.ipynb){{target="_blank"}}   | [ipynb]({repo}/{key}-solution.ipynb){{target="_blank"}}, [colab]({colab}/{key}-solution.ipynb){{target="_blank"}} |"""


    text += """
: {tbl-colwidths="[80, 10, 10]", .striped, .hover}
"""

    return Markdown(text)
```

```{python} 
exs = {
  '1-Python' : """**1 - Introduction to Python**\\
Introduction to the Python programming language. Optional for students already knowing Python.""",
  '2-Numpy' : """**2 - Numpy and Matplotlib**\\
Presentation of the numpy library for numerical computations and matplotlib for visualization. Also optional for students already familiar.""",
  '3-Sampling': """**3 - Sampling**\\
Simple exercise to investigate random sampling and its properties.""",
  '4-Bandits': """**4 - Bandits**\\
Implementation of various action selection methods to the n-armed bandit.""",
  '5-Bandits2': """**5 - Bandits (part 2)**\\
Advanced bandit methods.""",
  '6-DP': """**6 - Dynamic programming**\\
Calculation of the Bellman equations for the recycling robot and application of policy iteration and value iteration.""",
  '7-Gym': """**7 - Gym environments**\\
Introdcution to the gym(nasium) RL environments.""",
  '8-MonteCarlo': """**8 - Monte Carlo control**\\
Study of on-policy Monte Carlo control on the Taxi environment.""",
  '9-TD': """**9 - Temporal Difference, Q-learning**\\
Q-learning on the Taxi environment.""",
  '10-Eligibilitytraces': """**10 - Eligibility traces**\\
Investigation of eligibility traces for Q-learning in a gridworld environment.""",
  '11-Keras': """**11 - Keras**\\
Quick tutorial for Keras.""",
  '12-DQN': """**12 - DQN**\\
Implementation of the DQN algorithm for Cartpole from scratch.""",
  '13-PPO': """**13 - PPO**\\
DQN and PPO on cartpole using the tianshou library.""",

}

table_exercise(exs)
```

## Recommended readings

* Richard Sutton and Andrew Barto (2017). Reinforcement Learning: An Introduction. MIT press. 

<http://incompleteideas.net/book/the-book-2nd.html>

* CS294 course of Sergey Levine at Berkeley. 

<http://rll.berkeley.edu/deeprlcourse/>

* Reinforcement Learning course by David Silver at UCL.

<http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html>