{"title":"Eligibility traces","markdown":{"headingText":"Eligibility traces","containsRefs":false,"markdown":"\n\n## Q-learning in Gridworld\n\n### Random interaction with the environment\n\nThe goal of this exercise is to solve the **Gridworld** problem using Q-learning. The code is adapted from  <https://github.com/rlcode/reinforcement-learning-kr>\n\nThe agent is represented by the red square: the **state** $s$ of the agent is its position in the 5x5 grid, i.e. a number between 0 and 24.\n\nThe agent can move either to the left, right, top or bottom. When the agent tries to move outside of the environment, it stays at its current position. There are four **actions** $a$ available, which are deterministic.    \n\nIts goal is to reach the blue circle, while avoiding the green triangles. Actions leading to the blue circle receive a reward $r$ of +100, actions leading to a green triangle receive a reward of -100. The episode ends in those states. All other actions have a reward of -1.\n\nThe following code allows you to run a **random agent** in this environment for 10 episodes: at each time step, the action is selected randomly between 0 and 3 with `env.action_space.sample()`.\n\nThe environment is created with:\n\n```python\nenv = Gridworld()\n```\n\n`env.render()` allows to display the Gridworld. If you pass it a Q-table (one row per state, one column per action) `env.render(Q)`, it will print the Q_value of each action at the corresponding location.\n\n`state = env.reset()` allows to start an episode. The agent is placed at the top-left of the grid. \n\n`next_state, reward, done, info = env.step(action)` allows to perform an action on the environment. `action` must be a number between 0 and 3. It return the next state, the reward received during the transition, as well as a boolean `done` which is `True` when the episode is terminated (the agent moved to a blue circle or a green triangle, or 100 steps have been done), `False` otherwise. `info` contains the current undiscounted return of the episode.\n\n**Q:** Understand and run the code. Does the agent succeed often? How complex is the task compared to Taxi?\n\n**Q:** Adapt your Q-learning agent from last exercise to the problem. The main difference is the call to `render()`, the rest is similar. Train it for 100 episodes with the right hyperparameters and without rendering.\n\n**Q:** Train a Q-learning agent with rendering on. Observe in particular which Q-values are updates when the agent reaches the target. Is it efficient?\n\n**Q:** Modify your agent so that it uses **softmax action selection**, with a temperature $\\tau = 1.0$ and a suitable decay. What does it change?\n\nIf you have time, write a generic class for the Q-learning agent where you can select the action selection method flexibly.\n\n**A:** The agent explores much less at the end of training, as the difference between the Q-values becomes high enough to become greedy. In particular, it quickly stops to go to the green triangles. In this environment, there is no real need to decay tau.\n\n## Eligibility traces\n\nThe main drawback of Q-learning is that it needs many episodes to converge (**sample complexity**).\n\nOne way to speed up learning is to use eligibility traces, one per state-action pair:\n\n```python\ntraces = np.zeros((nb_states, nb_actions))\n```\n\nAfter each transition $(s_t, a_t)$, Q($\\lambda$) updates a **trace** $e(s_t, a_t)$ and modifies all Q-values as:\n\n1.  The trace of the last transition is incremented from 1:\n    \n$$e(s_t, a_t) = e(s_t, a_t) +1$$\n    \n2. Q($\\lambda$)-learning is applied on **ALL** Q-values, using the TD error at time $t$:\n    \n$$Q(s, a) = Q(s, a) + \\alpha \\, (r_{t+1} + \\gamma \\, \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)) \\, e(s, a)$$\n    \n3. All traces are exponentially decreased using the trace parameter $\\lambda$ (e.g. 0.7):\n\n$$\ne(s, a) = \\lambda \\, \\gamma \\, e(s, a)\n$$\n\nAll traces are reset to 0 at the beginning of an episode.\n\n**Q:** Implement eligibility traces in your Q($\\lambda$)-learning agent and see if it improves convergence. Train it with rendering on and observe how all Q-values are updated.\n\n**Q:** Vary the trace parameter $\\lambda$ and discuss its influence.\n\n**A:** $\\lambda$ should not be too high nor too low in order to speed up learning. It controls the bias/variance trade-off.\n\n**Q:** Increase the size of Gridworld to 10x10 and observe how long it takes to learn the optimal strategy.\n\n```python\nenv = GridWorld(size=10)\n```\n\nComment on the **curse of dimensionality** and the interest of tabular RL for complex tasks with large state spaces and sparse rewards (e.g. robotics).\n\n**A:** When the Gridworld is too big, the likelihood to hit the target per chance when exploring is very low. There are a lot of unsuccessful trials before learning starts to occur. But it happens after a while.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"10-Eligibilitytraces-solution.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}