{"title":"Sampling","markdown":{"headingText":"Sampling","containsRefs":false,"markdown":"\n\nIn this first exercise, we will investigate how to evaluate the Q-value of each action available in a 5-armed bandit. It is mostly to give you intuition about the limits of sampling and the central limit theorem.\n\nLet's start with importing numpy and matplotlib:\n\n## Sampling a n-armed bandit\n\nLet's now create the n-armed bandit. The only thing we need to do is to randomly choose 5 true Q-values $Q^*(a)$.\n\n![](../img/bandit-example.png)\n\nTo be generic, let's define `nb_actions=5` and create an array corresponding to the index of each action (0, 1, 2, 3, 4) for plotting purpose.\n\n**Q:** Create a numpy array `Q_star` with `nb_actions` values, normally distributed with a mean of 0 and standard deviation of 1 (as in the lecture).  \n\n**Q:** Plot the Q-values. Identify the optimal action $a^*$.\n\n*Tip:* you could plot the array `Q_star` with `plt.plot`, but that would be ugly. Check the documentation of the `plt.bar` method.\n\nGreat, now let's start evaluating these Q-values with random sampling.\n\n**Q:** Define an action sampling method `get_reward` taking as arguments:\n* The array `Q_star`.\n* The index `a` of the action you want to sample (between 0 and 4).\n* An optional variance argument `var`, which should have the value 1.0 by default.\n   \nIt should return a single value, sampled from the normal distribution with mean `Q_star[a]` and variance `var`.\n\n**Q:** For each possible action `a`, take `nb_samples=10` out of the reward distribution and store them in a numpy array. Compute the mean of the samples for each action separately in a new array `Q_t`. Make a bar plot of these estimated Q-values.\n\n**Q:** Make a bar plot of the difference between the true values `Q_star` and the estimates `Q_t`. Conclude. Re-run the sampling cell with different numbers of samples.\n\n**Q:** To better understand the influence of the number of samples on the accuracy of the sample average, create a `for` loop over the preceding code, with a number of samples increasing from 1 to 100. For each value, compute the **mean square error** (mse) between the estimates `Q_t` and the true values `Q^*`.\n\nThe mean square error is simply defined over the `nb_actions` actions as:\n\n$$\\epsilon = \\frac{1}{\\text{nb_actions}} \\, \\sum_{a=0}^\\text{nb_actions-1} (Q_t(a) - Q^*(a))^2$$\n\nAt the end of the loop, plot the evolution of the mean square error with the number of samples. You can append each value of the mse in an empty list and then plot it with `plt.plot`, for example. \n\nThe plot should give you an indication of how many samples you at least need to correctly estimate each action (30 or so). But according to the central limit theorem (CLT), the variance of the sample average also varies with the variance of the distribution itself.\n\n> The distribution of sample averages is normally distributed with mean $\\mu$ and variance $\\frac{\\sigma^2}{N}$.\n\n$$S_N \\sim \\mathcal{N}(\\mu, \\frac{\\sigma}{\\sqrt{N}})$$\n\n**Q:** Vary the variance of the reward distribution (as an argument to `get_reward`) and re-run the previous experiment. Do not hesitate to take more samples.  Conclude.\n\n**A:** the higher the variance of the distribution, the more samples we need to get correct estimates.  \n\n## Bandit environment\n\nIn order to prepare the next exercise, let's now implement the n-armed bandit in a Python class. As reminded in the tutorial on Python, a class is defined using this structure:\n\n```python\nclass MyClass:\n    \"\"\"\n    Documentation of the class.\n    \"\"\"\n    def __init__(self, param1, param2):\n        \"\"\"\n        Constructor of the class.\n        \n        :param param1: first parameter.\n        :param param2: second parameter.\n        \"\"\"\n        self.param1 = param1\n        self.param2 = param2\n        \n    def method(self, another_param):\n        \"\"\"\n        Method to do something.\n        \n        :param another_param: another parameter.\n        \"\"\"\n        return (another_param + self.param1)/self.param2\n```\n\nYou can then create an object of the type `MyClass`:\n\n```python\nmy_object = MyClass(param1= 1.0, param2=2.0)\n```\n\nand call any method of the class on the object:\n\n```python\nresult = my_object.method(3.0)\n```\n\n**Q:** Create a `Bandit` class taking as arguments:\n\n* nb_actions: number of arms.\n* mean: mean of the normal distribution for $Q^*$.\n* std_Q: standard deviation of the normal distribution for $Q^*$.\n* std_r: standard deviation of the normal distribution for the sampled rewards.\n\nThe constructor should initialize a `Q_star` array accordingly and store it as an attribute. It should also store the optimal action.\n\nAdd a method `step(action)` that samples a reward for a particular action and returns it.\n\n**Q:** Create a 5-armed bandits and sample each action multiple times. Compare the mean reward to the ground truth as before.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"3-Sampling-solution.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}