{"title":"Bandits","markdown":{"headingText":"Bandits","containsRefs":false,"markdown":"\n\nIn this part, we will investigate the properties of the action selection schemes seen in the lecture and compare their properties:\n\n1. greedy action selection\n2. $\\epsilon$-greedy action selection\n3. softmax action selection\n\nLet's re-use the definitions of the last exercise:\n\n## Greedy action selection\n\nIn **greedy action selection**, we systematically chose the action with the highest estimated Q-value at each play (or randomly when there are ties):\n\n$$a_t = \\text{argmax}_a Q_t(a)$$\n\nWe maintain estimates $Q_t$ of the action values (initialized to 0) using the online formula:\n\n$$Q_{t+1}(a_t) = Q_t(a_t) + \\alpha \\, (r_{t} - Q_t(a_t))$$\n\nwhen receiving the sampled reward $r_t$ after taking the action $a_t$. The learning rate $\\alpha$ can be set to 0.1 at first.\n\nThe algorithm simply alternates between these two steps for 1000 plays (or steps): take an action, update its Q-value. \n\n**Q:** Implement the greedy algorithm on the 5-armed bandit.\n\nYour algorithm will look like this:\n\n* Create a 5-armed bandit (mean of zero, variance of 1).\n* Initialize the estimated Q-values to 0 with an array of the same size as the bandit.\n* **for** 1000 plays:\n    * Select the greedy action $a_t^*$ using the current estimates.\n    * Sample a reward from $\\mathcal{N}(Q^*(a_t^*), 1)$.\n    * Update the estimated Q-value of the action taken.\n    \nAdditionally, you will store the received rewards at each step in an initially empty list or a numpy array of the correct size and plot it in the end. You will also plot the true Q-values and the estimated Q-values at the end of the 1000 plays. \n\n*Tip:* to implement the argmax, do not rely on `np.argmax()`. If there are ties in the array, for example at the beginning:\n\n```python\nx = np.array([0, 0, 0, 0, 0])\n```\n\n`x.argmax()` will return you the **first occurrence** of the maximum 0.0 of the array. In this case it will be the index 0, so you will always select the action 0 first. \n\nIt is much more efficient to retrieve the indices of **all** maxima and randomly select one of them:\n\n```python\na = rng.choice(np.where(x == x.max())[0])\n```\n\n`np.where(x == x.max())` returns a list of indices where `x` is maximum. `rng.choice()` randomly selects one of them.\n\n**Q:** Re-run your algorithm multiple times with different values of $Q^*$ (simply recreate the `Bandit`) and observe:\n\n1. How much reward you get.\n2. How your estimated Q-values in the end differ from the true Q-values.\n3. Whether greedy action action selection finds the optimal action or not.\n\n**A:** The plot with rewards is very noisy, you do not really see whether you have learned something because of the randomness of the rewards. More often than not, greedy action selection finds the optimal action, or least a not-that-bad action. The estimates `Q_t` have however nothing to see with the true Q-values, as you quickly select the same action and never update the other ones. \n\nBefore going further, let's turn the agent into a class for better reusability. \n\n**Q:** Create a `GreedyAgent` class taking the bandit as an argument as well as the learning rate `alpha=0.1`:\n\n```python\nbandit = Bandit(nb_actions)\n\nagent = GreedyAgent(bandit, alpha=0.1)\n```\n\nThe constructor should initialize the array of estimated Q-values `Q_t` and store it as an attribute.\n\nDefine a method `act(self)` that returns the index of the greedy action based on the current estimates, as well as a method `update(self, action, reward)` that allows to update the estimated Q-value of the action given the obtained reward. Define also a `train(self, nb_steps)` method that implements the complete training process for `nb_steps=1000` plays and returns the list of obtained rewards.\n\n```python\nclass GreedyAgent:\n    def __init__(self, bandit, alpha):\n        # TODO\n        \n    def act(self):      \n        action = # TODO\n        return action\n        \n    def update(self, action, reward):\n        # TODO\n        \n    def train(self, nb_steps):\n        # TODO\n```\n\nRe-run the experiment using this Greedy agent.\n\n**Q:** Modify the `train()` method so that it also returns a list of binary values (0 and 1) indicating for each play whether the agent chose the optimal action. Plot this list and observe the lack of exploration.\n\n*Hint:* the index of the optimal action is already stored in the bandit: `bandit.a_star`.\n\nThe evolution of the received rewards and optimal actions does not give a clear indication of the successful learning, as it is strongly dependent on the true Q-values. To truly estimate the performance of the algorithm, we have to average these results over many runs, e.g. 200.\n\n**Q:** Run the learning procedure 200 times (new bandit and agent every time) and average the results. Give a unique name to these arrays (e.g. `rewards_greedy` and `optimal_greedy`) as we will do comparisons later. Compare the results with the lecture, where a 10-armed bandit was used.\n\n**A:** the greedy agent selects the optimal action around 80% of the time, vs. 50% for the 10-armed bandits. It is really not bad knowing that it starts at chance level (20% for 5 actions).\n\n## $\\epsilon$-greedy action selection\n\nThe main drawback of greedy action selection is that it does not explore: as soon as it finds an action better than the others (with a sufficiently positive true Q-value, i.e. where the sampled rewards are mostly positive), it will keep selecting that action and avoid exploring the other options. \n\nThe estimated Q-value of the selected action will end up being quite correct, but those of the other actions will stay at 0.\n\nIn $\\epsilon$-greedy action selection, the greedy action $a_t^*$ (with the highest estimated Q-value) will be selected with a probability $1-\\epsilon$, the others with a probability of $\\epsilon$ altogether. \n\n$$\n    \\pi(a) = \\begin{cases} 1 - \\epsilon \\; \\text{if} \\; a = a_t^* \\\\ \\frac{\\epsilon}{|\\mathcal{A}| - 1} \\; \\text{otherwise.} \\end{cases}\n$$\n\nIf you have $|\\mathcal{A}| = 5$ actions, the four non-greedy actions will be selected with a probability of $\\frac{\\epsilon}{4}$.\n\n**Q:** Create a `EpsilonGreedyAgent` (possibly inheriting from `GreedyAgent` to reuse code) to implement $\\epsilon$-greedy action selection (with $\\epsilon=0.1$ at first). Do not overwrite the arrays previously calculated (mean reward and optimal actions), as you will want to compare the two methods in a single plot.\n\nTo implement $\\epsilon-$greedy, you need to:\n\n1. Select the greedy action $a = a^*_t$.\n2. Draw a random number between 0 and 1 (`rng.random()`).\n3. If this number is smaller than $\\epsilon$, you need to select another action randomly in the remaining ones (`rng.choice()`).\n4. Otherwise, keep the greedy action.\n\n**Q:** Compare the properties of greedy and $\\epsilon$-greedy (speed, optimality, etc). Vary the value of the parameter $\\epsilon$ (0.0001 until 0.5) and conclude.\n\n**A:** Depending on the value of $\\epsilon$, $\\epsilon$-greedy can perform better that greedy in the end, but will necessitate more time at the beginning. If there is too much exploration, $\\epsilon$-greedy can be even worse than greedy. \n\n## Softmax action selection\n\nTo avoid exploring actions which are clearly not optimal, another useful algorithm is **softmax action selection**. In this scheme, the estimated Q-values are ransformed into a probability distribution using the softmax opertion:\n\n$$\n    \\pi(a) = \\frac{\\exp \\frac{Q_t(a)}{\\tau}}{ \\sum_b \\exp \\frac{Q_t(b)}{\\tau}}\n$$ \n\nFor each action, the term $\\exp \\frac{Q_t(a)}{\\tau}$ is proportional to $Q_t(a)$ but made positive. These terms are then normalized by the denominator in order to obtain a sum of 1, i.e. they are the parameters of a discrete probability distribution. The temperature $\\tau$ controls the level of exploration just as $\\epsilon$ for $\\epsilon$-greedy.\n\nIn practice, $\\exp \\frac{Q_t(a)}{\\tau}$ can be very huge if the Q-values are high or the temperature is small, creating numerical instability (NaN). It is much more stable to substract the maximal Q-value from all Q-values before applying the softmax:\n\n$$\n    \\pi(a) = \\frac{\\exp \\displaystyle\\frac{Q_t(a) - \\max_a Q_t(a)}{\\tau}}{ \\sum_b \\exp \\displaystyle\\frac{Q_t(b) - \\max_b Q_t(b)}{\\tau}}\n$$ \n\nThis way, $Q_t(a) - \\max_a Q_t(a)$ is always negative, so its exponential is between 0 and 1.\n\n**Q:** Implement the softmax action selection (with $\\tau=0.5$ at first) and compare its performance to greedy and $\\epsilon$-greedy. Vary the temperature $\\tau$ and find the best possible value. Conclude.\n\n*Hint:* To select actions with different probabilities, check the doc of `rng.choice()`.\n\n**A:** softmax loses less time than $\\epsilon$-greedy exploring the really bad solutions, so it is optimal earlier. It can be more efficient and optimal than the other methods, but finding the right value for $\\tau$ (0.1 works well) is difficult: its optimum value depends on the scaling of Q, you cannot know it in advance...\n\n## Exploration scheduling\n\nThe problem with this version of softmax (with a constant temperature) is that even after it has found the optimal action, it will still explore the other ones (although more rarely than at the beginning). The solution is to **schedule** the exploration parameter so that it explores a lot at the beginning (high temperature) and gradually switches to more exploitation (low temperature).\n\nMany schemes are possible for that, the simplest one (**exponential decay**) being to multiply the value of $\\tau$ by a number very close to 1 after **each** play:\n\n$$\\tau = \\tau \\times (1 - \\tau_\\text{decay})$$\n\n**Q:** Implement in a class `SoftmaxScheduledAgent` temperature scheduling for the softmax algorithm ($\\epsilon$-greedy would be similar) with $\\tau=1$ initially and $\\tau_\\text{decay} = 0.01$ (feel free to change these values). Plot the evolution of `tau` and of the standard deviation of the choices of the optimal action. Conclude.\n\n**A:** Scheduling drastically improves how often the optimal action is selected. In terms of mean reward, the difference is not that big, as there is often a \"second best\" action whose expected reward is close. We can see that the variance of the optimal action selection follows the parameter $\\tau$.\n\n**Q:** Experiment with different schedules (initial values, decay rate) and try to find the best setting.\n\n**A:** no unique answer here, but a very high exploration parameter initially which decreases quite fast leads to very performant solutions. Take-home message: scheduling is very important, but it is quite difficult to find the optimal schedule.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"4-Bandits-solution.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}