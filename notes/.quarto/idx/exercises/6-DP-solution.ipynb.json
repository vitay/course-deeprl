{"title":"Dynamic programming","markdown":{"headingText":"Dynamic programming","containsRefs":false,"markdown":"\n\nThe goal of this exercise is to find the optimal policy for the recycling robot.\n\n![](../img/recyclingrobot.png)\n \nIn this problem, a recycling robot has to search for empty cans to collect (each can defines a \"reward\" given to the robot). It can also decide to stay where it is to save its battery and wait that somebody brings it a can (which gives less cans in average than actively searching for them). \n\nThe robot has two battery levels, *high* and *low*. \n\n* In the *high* level, the robot can either search or wait. \n\n* In the *low* state, three actions are possible: search, wait and recharge. \n\nState-action transitions are probabilistic, i.e. they bring the robot in different states based on different probabilities $\\alpha$ and $\\beta$.\n\nThis problem defines a finite MDP, with two states *high* and *low* corresponding to the battery level. The actions *search* and *wait* are possible in the *high* and *low* states, while the action *recharge* is only possible in the *low* state.\n\n$$\n\\begin{aligned}\n    \\mathcal{S} &=& \\{ \\text{high}, \\text{low} \\} \\\\\n    \\mathcal{A}(\\text{high} ) &=& \\{ \\text{search}, \\text{wait} \\} \\\\\n    \\mathcal{A}(\\text{low} ) &=& \\{ \\text{search}, \\text{wait}, \\text{recharge} \\}\n\\end{aligned}\n$$\n\nThe action *search* brings on average a reward of $\\mathcal{R}^\\text{search}$, the action *wait* a reward of $\\mathcal{R}^\\text{wait}$, the action *recharge* brings no reward, but allows to get in the *high* state.\n\nNote that if the robot decides to search in the *low* state, there is a probability $1 - \\beta$ that it totally empties its battery, requiring human intervention. This is punished with a negative reward of -3.\n\nThe transition and reward probabilities of each transition is defined in the following table, completely defining a MDP.\n\n| s             |  s'           |  a           | p(s' / s, a)      |  r(s, a, s')                 |\n|:-------------:|:-------------:|:------------:|:-----------------:|:-----------------------------:|\n| high          | high          | search       | $\\alpha$          | $\\mathcal{R}^\\text{search}$   |\n| high          | low           | search       | $1 - \\alpha$      | $\\mathcal{R}^\\text{search}$   |\n| low           | high          | search       | $1 - \\beta$       | $-3$                          |\n| low           | low           | search       | $\\beta$           | $\\mathcal{R}^\\text{search}$   |\n| high          | high          | wait         | $1$               | $\\mathcal{R}^\\text{wait}$     |\n| high          | low           | wait         | $0$               | $\\mathcal{R}^\\text{wait}$     |\n| low           | high          | wait         | $0$               | $\\mathcal{R}^\\text{wait}$     |\n| low           | low           | wait         | $1$               | $\\mathcal{R}^\\text{wait}$     |\n| low           | high          | recharge     | $1$               | $0$                           |\n| low           | low           | recharge     | $0$               | $0$                           |\n\nThe goal of this exercise is to find the optimal policy $\\pi^*$ of the robot, i.e to find for each state the action that should be performed systematically in order to gather the maximum of reward on the long term. \n\nWe will apply here two **dynamic programming** methods, policy iteration and value iteration, to solve the Bellman equations.\n\nThe Bellman equation for the state function is:\n\n$$V^{\\pi} (s)  = \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{\\pi} (s') ]$$\n\n**Q:** On paper, adapt the Bellman equation to the problem. First, for every state $s$ and possible action $a$, find the optimal value of the action with the form:\n\n$$Q^{\\pi} (s, a) = f( V^\\pi (\\text{high}), V^\\pi (\\text{low}), \\alpha, \\beta, \\gamma, \\mathcal{R}^{\\text{search}}, \\mathcal{R}^{\\text{wait}} )$$\n\nDeduce the Bellman equation for the two states $V^\\pi (\\text{high})$ and $V^\\pi (\\text{low})$.\n\n**A:**\n\n$$Q^\\pi(\\text{high}, \\text{search}) = \\alpha \\, (\\mathcal{R }^\\text{search} + \\gamma \\, V^\\pi(\\text{high} )) + (1- \\alpha)\\, (\\mathcal{R}^\\text{search} + \\gamma \\, V^\\pi(\\text{low}))$$\n\n$$Q^\\pi(\\text{high}, \\text{wait}) = \\mathcal{R}^\\text{wait} + \\gamma \\, V^\\pi(\\text{high})$$\n\n$$Q^\\pi(\\text{low}, \\text{search}) = \\beta * (\\mathcal{R }^\\text{search} + \\gamma \\,  V^\\pi(\\text{low})) + (1- \\beta) \\, (-3 + \\gamma \\, V^\\pi(\\text{high}))$$\n\n$$Q^\\pi(\\text{low}, \\text{wait}) = \\mathcal{R}^\\text{wait} + \\gamma \\, V^\\pi(\\text{low})$$\n\n$$Q^\\pi(\\text{low}, \\text{recharge}) = \\gamma \\, V^\\pi(\\text{high})$$\n\n$$V^\\pi(\\text{high}) = \\pi(\\text{high}, \\text{search}) \\, Q^\\pi(\\text{high}, \\text{search}) +  \\pi(\\text{high}, \\text{wait}) \\, Q^\\pi(\\text{high}, \\text{wait})$$\n\n$$V^\\pi(\\text{low}) = \\pi(\\text{low}, \\text{search}) \\, Q^\\pi(\\text{low}, \\text{search}) +  \\pi(\\text{low}, \\text{wait}) \\, Q^\\pi(\\text{low}, \\text{wait}) + \\pi(\\text{low}, \\text{recharge}) \\,  Q^\\pi(\\text{low}, \\text{recharge})$$\n\n\n## Policy Iteration\n\nNow that we have the Bellman equations for the two states high and low, we can solve them using **iterative policy evaluation** for a fixed policy $\\pi$. \n\n### Iterative policy evaluation\n\nLet's start by setting the parameters of the MDP. In the rest of the exercise, you will modify these parameters to investigate how it changes the optimal policy.\n\nThere are many ways to represent states and actions in a MDP. The suggestion for this exercise is to use dictionaries here the keys are the actions' name and the vaues are indices:\n\nUsing dictionaries, you can access numpy arrays with `s['high']` or `a['recharge']` instead of 0 and 2, what will make the code readable.\n\nThe next step is to initialize numpy arrays where we will store the V and Q values. `V` will have only two elements for high and low, while `Q` will be a 2x3 matrix with one element for each state-action pair. Notice that (high, recharge) is not a possible action, so this element will not be be updated.\n\nYou can then access the individual values with `V[s['high']]` or `Q[s['low'], a['wait']]`.\n\nWe can now evaluate a policy $\\pi$. In dynamic programming, the policies are deterministic, as we want to estimate the optimal policy.\n\nTo implement the policy, we just need to assign the index of an action to each state, i.e. $\\pi(s)$. The following cell creates an initial policy $\\pi$ where the agent **searches** in both states high and low. We here make sure that the array contains integers (0, 1 or 2), but that is not even necessary.\n\n**Q:** Evaluate this policy using iterative policy evaluation.\n\nWe would normally only need to update the V-value of the two states using:\n\n$$\n     V (s) \\leftarrow \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V (s') ] \\quad \\forall s \\in \\mathcal{S}\n$$\n\nThe code will be more readable if you first update the Q-values of the 5 state-action pairs:\n\n$$\n     Q (s, a) \\leftarrow  \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V (s') ] \\quad \\forall s \\in \\mathcal{S}\n$$\n\nand only then update the two V-values:\n\n$$\n     V (s) \\leftarrow \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, Q(s, a)\n$$\n\nThese updates should normally be applied until the V-values converge. For simplicity, we could decide to simply apply 50 updates or so, and hope that it is enough.\n\nRecord the V-value of the two states after each update and plot them. \n\n**Q:** Do the V-values converge? How fast? What do the final values represent? Change the value of $\\gamma$ and conclude on its importance (do not forget to reset the V and Q arrays to 0!). \n\n**A:** The V-values converge quite fast (~15 iterations) to their true value. The high state has a higher value than the low state, as there is no risk in that state to receive the punishment of -3.\n\nThe final value is the expected return in that state, that is:\n\n$$R_t = \\sum_{k=0}^\\infty \\gamma^k \\, r_{t+k+1}$$\n\n$\\gamma$ completely changes the scale of the return. Small values of $\\gamma$ lead to small returns (only a couple of rewards count in the sum), while high values lead to high returns (there are a lot of rewards to be summed, especially because the task is continuing).\n\n**Q:** Print the Q-values at the end of the policy evaluation. What would the greedy policy with respect to these Q-values? \n\n**Q:** Change the initial policy to this policy and evaluate it. What happens? Compare the final value of the states under both policies. Which one is the best?\n\n**A:** The greedy policy w.r.t. the Q-values is searching in high, recharging in low, as the Q-values are maximal for these actions.\n\nIf we evaluate this policy, we observe that:\n\n* the value of both states is higher: this is a better policy, as we collect more return on average.\n* the greedy policy does not change after the evaluation: we have found the optimal policy already!\n\n### Policy iteration\n\nImproving the policy is now straightforward. We just to look at the Q-values in each state, and change the policy so that it takes the action with the maximal Q-value. If this does not change the policy (we still take the same actions), we have found the optimal policy, we can stop.\n\n**Q:** Implement policy iteration.\n\nDo not forget to reset the V and Q arrays at the beginning of the cell, as well as the original policy.\n\nUse an infinite loop that you will quit when the policy has not changed between two iterations. Something like:\n\n```python\nwhile True:\n    # 1 - Policy evaluation\n    for k in range(50):\n        # Update the values\n    \n    # 2 - Policy improvement\n    \n    if pi != pi_old:\n        break\n```\n\n**Beware:** if you simply assign the policy to another array and modify the policy:\n\n```python\npi_old = pi\npi[s['high']] = a['search']\n```\n\n`pi_old` will also change! You need to `.copy()` the policy.\n\n## Value iteration\n\nIn value iteration, we merge the policy evaluation and improvement in a single update rule: \n\n$$\n    V (s) \\leftarrow \\max_{a \\in \\mathcal{A}(s)} \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V (s') ]\n$$\n\nThe value of state takes the value of its greedy action. The policy is therefore implicitly greedy w.r.t the Q-values.\n\nThe algorithm becomes:\n\n* while not converged:\n\n    * for all states $s$:\n    \n        * Update the value estimates with:\n        \n        $$\n            V (s)  \\leftarrow \\max_{a \\in \\mathcal{A}(s)} \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V (s') ]\n        $$\n        \n**Q:** Modify your previous code to implement value iteration. Use a fixed number of iterations (e.g. 50) as in policy evaluation. Visualize the evolution of the V-values and print the greedy policy after each iteration. Conclude.\n\n**Q:**  Change the value of the discount factor $\\gamma =0.3$ so that the agent becomes short-sighted: it only takes into account the immediate rewards, but forgets about the long-term. Does it change the strategy? Explain why.\n\n**A:** the agent now decides to wait in the low state (r=2) instead of recharging (r=0) and then be in the high state (r=6). The agent is so greedy that it cannot stand not getting reward for one step, although he will collect much more reard later. \n\n**Q:** Change $\\gamma$ to 0.99 (far-sighted agent). What does it change and why?  \n\n**A:** The optimal policy stays the same (search in high, recharge in low) but the V values grow very high. The difference between the values of the high and low state is comparatively very small: the high state is always only one action away from the low state, it is nothing with such a high gamma.\n\n**Q:**  Change the parameters to:\n\n$$\\alpha = 0.01 \\quad \\beta = 0.2 \\quad \\gamma = 0.7 \\quad \\mathcal{R}^{\\text{search}} = 6 \\quad  \\mathcal{R}^{\\text{wait}} = 5$$\n\nFind the optimal policy. What is the optimal action to be taken in the state *high*, although the probability to stay in this state is very small? Why?\n\n**A:** the agent now decides to wait in the low state and accumulate quite a lot of rewards (r=5, compared to 6 while searching). But it is still worth searching in the high state: even if we transition immediately into low with 99% probability, one still gets 6 instead of 5, so the return is higher than when waiting in high.\n\n**Q:** Find a set of parameters where it would be optimal to search while in the *low* state.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"6-DP-solution.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}