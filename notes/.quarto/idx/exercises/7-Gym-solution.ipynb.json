{"title":"Gym environments","markdown":{"headingText":"Gym environments","containsRefs":false,"markdown":"\n\n## Installing gym\n\nIn this course, we will mostly address RL environments available in the **OpenAI Gym** framework:\n\n<https://gym.openai.com>\n\nIt provides a multitude of RL problems, from simple text-based problems with a few dozens of states (Gridworld, Taxi) to continuous control problems (Cartpole, Pendulum) to Atari games (Breakout, Space Invaders) to complex robotics simulators (Mujoco, but the license is expensive):\n\n<https://gym.openai.com/envs>\n\nWe start by installing gym and its dependencies. Unfortunately, gym only works fully under Linux or MacOS. You can `pip install gym` on Windows, but will be limited to the most simple environments, excluding for example Atari games (but you can try these instructions: <https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30>). This isn't a big issue as we will only use the simple environments in the exercises, but less fun.\n\nMoreover, it does not work out-of-the-box on colab, as gym opens graphical windows for some environments (e.g. Atari games), what is not possible in the browser. The first cell provides a workaround.\n\n### On your computer\n\nOn your computer, you first need to make sure you have the right dependencies, especially `cmake`:\n\n**Ubuntu Linux:**\n\n```bash\napt-get install libglu1-mesa-dev libgl1-mesa-dev libosmesa6-dev xvfb ffmpeg curl patchelf libglfw3 libglfw3-dev cmake zlib1g zlib1g-dev swig\n```\n\n**MacOSX:**\n\n```bash\nbrew install cmake swig boost boost-python sdl2 wget\n```\n\nOn Windows, you may have to have Microsoft Visual C++ build tools installed. The link <https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30> suggests the 2017 version, but Visual C++ 2015-2019 Redistributable seems to also work.\n\nYou can then install gym in two steps, installing the `atari_py` dependency in between to be able to play Atari games.\n\n```bash\npip install gym\npip install atari_py\npip install gym[atari]\n```\n\n### On Colab\n\nOn Colab, we need to install a bunch of dependencies (Colab runs a Debian VM under the hood) and define a visualization function that can be called at the end of an episode to visualize it. \n\nThe following cell detects whether you run this notebook on colab or not, and imports lots of additional stuff if needed.\n\n## Gym interface \n\nThe main interest of gym is that all problems have a common interface defined by the class `gym.Env`. There are only three methods that have to be implemented and used when creating a new environment:\n\n* `state = env.reset()` restarts the environment and returns an initial state $s_0$.\n\n* `state, reward, done, info = env.step(action)` takes an action $a_t$ and returns the new state $s_{t+1}$, the reward $r_{t+1}$, a boolean flag indicating whether the current state is terminal and a dictionary containing additional info for debugging (you can ignore it most of the time).\n\n* `env.render()` displays the current state of the MDP, either text-based or in a graphical window. When training, this should not be called to save some time.\n\nAdditionally, you override the constructor `__init__()` to setup the state space (called observation space) and the action space. \n\nState and action space can either be :\n\n* discrete (`gym.spaces.Discrete(nb_states)`), with states being an integer between 0 and `nb_states` -1.\n\n* feature-based (`gym.spaces.Box(low=0, high=255, shape=(SCREEN_HEIGHT, SCREEN_WIDTH, 3))`) for pixel frames.\n\n* continuous:\n\n```python\ngym.spaces.Tuple(\n    gym.spaces.Box(-180.0, 180.0, 1), # First joint\n    gym.spaces.Box(-180.0, 180.0, 1)  # Second joint    \n)\n```\n\nHere is an example of a dummy environment with discrete states and actions, where the transition probabilities and rewards are completely random:\n\nWith this interface, we can interact with the environment in a standardized way:\n\n* We first create the environment.\n\n* We pick an initial state with `reset()`.\n\n* For a fixed number of steps (or until the episode terminates):\n\n    * We render the current state with `render()`.\n    \n    * We select an action using our RL algorithm or randomly.\n    \n    * We take that action (`step()`), observe the new state and the reward.\n    \n    * We go into the new state.\n\nThat's it. To use any of the gym environments, replace the creation of the `FooEnv` instance by:\n\n```python\nenv = wrap_env(gym.make('CartPole-v0'))\n```\n\nif you want to interact with the Cartpole environment. `wrap_env()` is obligatory on Colab, otherwise you will not be able to visualize it. You can omit it on your computer. For text-based environments such as `FooEnv`, you do not need to wrap the environment.\n\nEach environment has a unique key, with a version number (`v0`). To have a list of the available environments, call:\n\n**Q:** Interact randomly with many gym environments (Cartpole, Pendulum, Breakout, SpaceInvaders, etc). Print the rewards you obtain.\n\n*Note 1:* If you run a fixed number of steps, you should reset the environment when a terminal state is encountered, other wise you will be stuck in that terminal state:\n\n```python\nif done:\n    state = env.reset()\n```\n\n*Note 2:* If you stop the execution of a cell but the window does not close, run `env.close()` in a separate cell.\n\n*Note 3:* Some environments are very fast, especially Atari games. A simple solution is to have python **sleep** a bit after rendering a frame, so that you can see something:\n\n```python\nimport time\n# ...\nfor t in range(1000):\n    env.render()\n    time.sleep(0.01) # sleep 10 milliseconds\n    # ...\n```\n\n*Note 4:* On Colab, `env.render()` will do nothing. You need to call `show_video()` after `env.close()` to see the movie of what happened.\n\n## Recycling robot\n\n**Q:** Create a `RecyclingRobot` gym-like environment using last week's exercise.\n\nThe parameters `alpha`, `beta`, `r_wait` and `r_search` should be passed to the constructor of the environment and saved as attributes.\n\nThe state space is discrete, with two states `high` and `low` which will have indices 0 and 1. The three discrete actions `search`, `wait` and `recharge` have indices 0, 1, and 2.\n\nThe initial state of the MDP (`reset()`) should be the high state.\n\nThe `step()` should generate transitions according to the dynamics of the MDP. Depending on the current state and the chosen action, make a transition to another state. For the actions `search` and `wait`, sample the reward from the normal distribution with mean `r_search` (resp. `r_wait`) and variance 0.5. \n\nIf the random agent selects `recharge` in `high`, do nothing (next state is high, reward is 0).\n\nRendering is just printing the current state. There is nothing to close, so you do not even need to redefine the function.\n\nInteract randomly with the MDP for several steps and observe the rewards. \n\nTo be complete, let's implement the random agent as a class. The class should look like:\n\n```python\nclass RandomAgent:\n    \"\"\"\n    Random agent exploring uniformly the environment.\n    \"\"\"\n    \n    def __init__(self, env):\n        \"\"\"\n        :param env: gym-like environment\n        \"\"\"\n        self.env = env\n    \n    def act(self, state):\n        \"Returns a random action by sampling the action space.\"\n        action = # TODO\n        return action\n    \n    def update(self, state, action, reward, next_state):\n        \"Updates the agent using the transition (s, a, r, s').\"\n        pass\n    \n    def train(self, nb_steps, render=True):\n        \"Runs the agent on the environment for nb_steps. Returns the list of obtained rewards.\"\n        \n        # List of rewards\n        rewards = []\n\n        # TODO\n            \n        return rewards\n```\n\nThe environment is passed to the constructor. `act(state)` should sample a random action. `update(state, action, reward, next_state)` does nothing for the random agent (`pass` is a Python command doing nothing), but we will implement it in the next exercises. \n\n`train(nb_steps, render)` implements the interaction loop between the agent and the environment for a fixed number of steps. It should return the list of obtained rewards. You can use the flag `render` to switch rendering or printing on and off.\n\n**Q:** Implement the random agent and have it interact with the environment for a fixed number of steps.\n\nThat's it! We now \"only\" need to define classes for all the sampling-based RL algorithms (MC, TD, deep RL) and we can interact with any environment using the previous cell!\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"7-Gym-solution.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}