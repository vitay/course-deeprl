{"title":"Q-learning","markdown":{"headingText":"Q-learning","containsRefs":false,"markdown":"\n\nIn this short exercise, we are going to apply **Q-learning** on the Taxi environment used last time for MC control.\n\nAs a reminder, Q-learning updates the Q-value of a state-action pair **after each transition**, using the update rule:\n\n$$\\Delta Q(s_t, a_t) = \\alpha \\, (r_{t+1} + \\gamma \\, \\max_{a'} \\, Q(s_{t+1}, a') - Q(s_t, a_t))$$\n\n**Q:** Update the class you designed for online MC in the last exercise so that it implements Q-learning. \n\nThe main difference is that the `update()` method has to be called after each step of the episode, not at the end. It simplifies a lot the code too (no need to iterate backwards on the episode).\n\nYou can use the following parameters at the beginning, but feel free to change them:\n\n* Discount factor $\\gamma = 0.9$. \n* Learning rate $\\alpha = 0.1$.\n* Epsilon-greedy action selection, with an initial exploration parameter of 1.0 and an exponential decay of $10^{-5}$ after each update (i.e. every step!).\n* A total number of episodes of 20000.\n\nKeep the general structure of the class: `train()` for the main loop, `test()` to run one episode without exploration, etc. Add a method to compute the discounted return of each episode, as it will not be done automatically by the `update()` method anymore. Plot the training and test performance in the end and render the learned deterministic policy for 10 episodes.\n\n*Note:* if $s_{t+1}$ is terminal (`done` is true after the transition), the target should not be $r_{t+1} + \\gamma \\, \\max_{a'} \\, Q(s_{t+1}, a')$, but simply $r_{t+1}$ as there is no next action.\n\n**Q:** Compare the performance of Q-learning to online MC. Experiment with parameters (gamma, epsilon, alpha, etc.).\n\n**A:** Q-learning accepts much higher values of gamma than MC, because the returns have a much lower variance. With the right parameters, Q-learning can learn much faster than MC.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"9-TD-solution.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}