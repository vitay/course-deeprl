{"title":"Overview","markdown":{"headingText":"Overview","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"\nThis website contains the materials for the module **Deep Reinforcement Learning** taught by Dr. Julien Vitay at the Technische Universit√§t Chemnitz, Faculty of Computer Science, Professorship for Artificial Intelligence. \n\nEach section/lecture is accompanied by a set of videos, the slides and a written version of the content. Some sections are optional in the sense that no questions will be asked at the exam, but those interested in becoming neural network experts should feel free to study them. The (slightly outdated) videos are integrated in the lecture notes, but you can also access the complete playlist on [Youtube](https://www.youtube.com/playlist?list=PLIEjdhhAF7UJwegwyWUcDrUNJTQfxMcUw).\n\nExercises are provided in the form of Jupyter notebooks, allowing to implement in Python at your own pace the algorithms seen in the lectures and to learn to use reinforcement learning libraries such as `gym`. A notebook to work on (locally or on Colab) and the solution are available in the [Exercises section](exercises/Content.qmd). \n\n\n## Syllabus\n\n### Lectures\n\n::: {.columns}\n::: {.column width=50%}\n\n1. **Introduction**\n    1. [Introduction](notes/1.1-Introduction.qmd)\n    2. [Math basics](notes/1.2-Math.qmd)\n2. **Tabular RL**\n    1. [Bandits](notes/2.1-Bandits.qmd)\n    2. [Markov Decision Processes](notes/2.2-MDP.qmd)\n    3. [Dynamic Programming](notes/2.3-DP.qmd)\n    4. [Monte Carlo control](notes/2.4-MC.qmd)\n    5. [Temporal Difference](notes/2.5-TD.qmd)\n    6. [Function approximation](notes/2.6-FA.qmd)\n    7. [Deep learning](notes/2.7-NN.qmd)\n\n:::\n::: {.column width=50%}\n\n3. **Model-free RL**\n    1. [Deep Q-network](notes/3.1-DQN.qmd)\n    2. [Beyond DQN](notes/3.2-BeyondDQN.qmd)\n    3. [Policy Gradient](notes/3.3-PG.qmd)\n    4. [A2C / A3C](notes/3.4-A3C.qmd)\n    5. [DDPG](notes/3.5-DDPG.qmd)\n    6. [TRPO / PPO](notes/3.6-PPO.qmd)\n    7. [SAC](notes/3.7-SAC.qmd)\n\n4. **Model-based RL**\n    1. [Model-based RL](notes/4.1-MB.qmd)\n    2. [Learned models](notes/4.2-LearnedModels.qmd)\n    3. [AlphaGo](notes/4.3-AlphaGo.qmd)\n    4. [Successor representations](notes/4.4-SR.qmd)\n\n5. **Outlook**\n    1. [Outlook](notes/5.1-Outlook.qmd)\n\n:::\n:::\n\n### Exercises\n\nNotebooks and videos are in the [List of Exercises](exercises/Content.qmd). Below are links to the rendered solutions.\n\n::: {.columns}\n::: {.column width=50%}\n\n1. [Introduction to Python](exercises/1-Python-solution.ipynb)\n2. [Numpy and Matplotlib](exercises/2-Numpy-solution.ipynb)\n\n:::\n::: {.column width=50%}\n\n:::\n:::\n\n## Recommended readings\n\n* [@Sutton2017]  Richard Sutton and Andrew Barto (2017). Reinforcement Learning: An Introduction. MIT press. \n<http://incompleteideas.net/book/the-book-2nd.html>\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["DeepLearning.bib","ReinforcementLearning.bib"],"csl":"frontiers.csl","theme":["cosmo","custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}