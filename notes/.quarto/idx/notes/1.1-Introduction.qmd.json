{"title":"Introduction","markdown":{"headingText":"Introduction","containsRefs":false,"markdown":"\nSlides: [html](../slides/1.1-Introduction.html){target=\"_blank\"} [pdf](../slides/pdf/1.1-Introduction.pdf){target=\"_blank\"}\n\nDeep reinforcement learning (deep RL or DRL) is the integration of deep learning methods, classically used in supervised or unsupervised learning contexts, with reinforcement learning (RL), a well-studied adaptive control method used in problems with delayed and partial feedback.\n\n\n## History of RL\n\n{{< youtube  m2Y_k8A4iHU >}}\n\n\n* **Early 20th century**: animal behavior, psychology, operant conditioning \n\n    * Ivan Pavlov, Edward Thorndike, B.F. Skinner\n\n* **1950s**: optimal control, Markov Decision Process, dynamic programming\n\n    * Richard Bellman, Ronald Howard\n\n* **1970s**: trial-and-error learning\n\n    * Marvin Minsky, Harry Klopf, Robert Rescorla, Allan Wagner\n\n* **1980s**: temporal difference learning, Q-learning\n\n    * Richard Sutton, Andrew Barto, Christopher Watkins, Peter Dayan\n\n* **2013-now**: deep reinforcement learning\n\n    * Deepmind (Mnih, Silver, Graves...)\n    * OpenAI (Sutskever, Schulman...)\n    * Sergey Levine (Berkeley)\n\n\n\nReinforcement learning comes from animal behavior studies, especially **operant conditioning / instrumental learning**. **Thorndike’s Law of Effect** (1874–1949) suggested that behaviors followed by satisfying consequences tend to be repeated and those that produce unpleasant consequences are less likely to be repeated. Positive reinforcements (**rewards**) or negative reinforcements (**punishments**) can be used to modify behavior (**Skinner's box, 1936**). This form of learning applies to all animals, including humans:\n\n* Training (animals, children...)\n\n* Addiction, economics, gambling, psychological manipulation... \n\n**Behaviorism:** only behavior matters, not mental states.\n\n{{< youtube  y-g2OmRXb0g >}}\n\n\nThe key concept of RL is **trial and error** learning. The agent (rat, robot, algorithm) tries out an **action** and observes the **outcome**.\n\n* If the outcome is positive (reward), the action is reinforced (more likely to occur again).\n* If the outcome is negative (punishment), the action will be avoided.\n\nAfter enough interactions, the agent has **learned** which action to perform in a given situation.\n\nRL is merely a formalization of the trial-and-error learning paradigm. The agent has to **explore** its environment via trial-and-error in order to gain knowledge. The biggest issue with this approach is that exploring large action spaces might necessitate a **lot** of trials (**sample complexity**). The modern techniques we will see in this course try to reduce the sample complexity.\n\n\n## The agent-environment interface\n\n{{< youtube  QjhKJmFV8T4 >}}\n\n\n![Agent-environment interface. Source: [@Sutton2017].](../slides/img/rl-agent.jpg){width=80%}\n\nThe agent and the environment interact at discrete time steps:  $t$=0, 1, ... The agent observes its state at time t:  $s_t \\in \\mathcal{S}$, produces an action at time t, depending on the available actions in the current state: $a_t \\in \\mathcal{A}(s_t)$ and  receives a reward according to this action at time t+1: $r_{t+1} \\in \\Re$. It then updates its state: $s_{t+1} \\in \\mathcal{S}$. The behavior of the agent is therefore is a sequence of **state-action-reward-state** $(s, a, r, s')$ transitions.\n\n![State-action-reward-state sequences. Source: [@Sutton2017].](../slides/img/rl-sequence.jpg){width=100%}\n\nSequences $\\tau = (s_0, a_0, r_1, s_1, a_1, \\ldots, s_T)$ are called **episodes**, **trajectories**, **histories** or **rollouts**.\n\n![Agent-environment interface for video games. Source: David Silver <https://www.davidsilver.uk/teaching/>.](../slides/img/rl-loop.png){width=100%}\n\nThe state $s_t$ can relate to:\n\n* the **environment state**, i.e. all information external to the agent (position of objects, other agents, etc).\n\n* the **internal state**, information about the agent itself (needs, joint positions, etc).\n\nGenerally, the state represents all the information necessary to solve the task. The agent generally has no access to the states directly, but to **observations** $o_t$:\n\n$$\n    o_t = f(s_t)\n$$\n\nExample: camera inputs do not contain all the necessary information such as the agent's position. Imperfect information define **partially observable problems**.\n\n\nWhat we search in RL is the optimal **policy**: which action $a$ should the agent perform in a state $s$? The policy $\\pi$ maps states into actions. It is defined as a **probability distribution** over states and actions:\n\n$$\n\\begin{align}\n    \\pi &: \\mathcal{S} \\times \\mathcal{A} \\rightarrow P(\\mathcal{S})\\\\\n    \\\\\n    & (s, a) \\rightarrow \\pi(s, a)  = P(a_t = a | s_t = s) \\\\\n\\end{align}\n$$\n\n$\\pi(s, a)$ is the probability of selecting the action $a$ in $s$. We have of course: \n\n$$\\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) = 1$$\n\nPolicies can be **probabilistic** / **stochastic**. **Deterministic policies** select a single action $a^*$in $s$:\n\n$$\\pi(s, a) = \\begin{cases} 1 \\; \\text{if} \\; a = a^* \\\\ 0 \\; \\text{if} \\; a \\neq a^* \\\\ \\end{cases}$$\n\n\nThe only teaching signal in RL is the **reward function**. The reward is a scalar value $r_{t+1}$ provided to the system after each transition $(s_t,a_t, s_{t+1})$. Rewards can also be probabilistic (casino). The mathematical expectation of these rewards defines the **expected reward** of a transition: \n\n$$\n    r(s, a, s') = \\mathbb{E}_t [r_{t+1} | s_t = s, a_t = a, s_{t+1} = s']\n$$\n\nRewards can be:\n\n* **dense**: a non-zero value is provided after each time step (easy).\n\n* **sparse**: non-zero rewards are given very seldom (difficult).\n\nThe goal of the agent is to find a policy that **maximizes** the sum of future rewards at each timestep. The discounted sum of future rewards is called the **return**:\n\n$$\n    R_t = \\sum_{k=0}^\\infty \\gamma ^k \\, r_{t+k+1}\n$$\n\nRewards can be delayed w.r.t to an action: we care about all future rewards to select an action, not only the immediate ones.  Example: in chess, the first moves are as important as the last ones in order to win, but they do not receive reward.\n\n\nThe **expected return** in a state $s$ is called its **value**:\n\n$$\n    V^\\pi(s) = \\mathbb{E}_\\pi(R_t | s_t = s)\n$$\n\nThe value of a state defines how good it is to be in that state. If a state has a high value, it means we will be able to collect a lot of rewards **on the long term** and **on average**.  Value functions are central to RL: if we know the value of all states, we can infer the policy. The optimal action is the one that leads to the state with the highest value. Most RL methods deal with estimating the value function from experience (trial and error).\n\n\n**Simple maze**\n\n![Simple maze. Source: David Silver <http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html>.](../slides/img/maze.png){width=80%}\n\nThe goal is to find a path from start to goal as fast as possible.\n\n* **States**: position in the maze (1, 2, 3...).\n\n* **Actions**: up, down, left, right.\n\n* **Rewards**: -1 for each step until the exit.\n\n\nThe value of each state indicates how good it is to be in that state. It can be learned by trial-and-error given a policy.\n\n![Value of each state. Source: David Silver <http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html>.](../slides/img/maze-value.png){width=80%}\n\nWhen the value of all states is known, we can infer the optimal policy by choosing actions leading to the states with the highest value.\n\n![Optimal policy. Source: David Silver <http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html>.](../slides/img/maze-policy.png){width=80%}\n:::{.callout-note}\nAs we will see, the story is actually much more complicated...\n:::\n\n## Difference between supervised and reinforcement learning\n\n**Supervised learning**\n\n* Correct input/output samples are provided by a **superviser** (training set).\n\n* Learning is driven by **prediction errors**, the difference between the prediction and the target.\n\n* Feedback is **instantaneous**: the target is immediately known.\n\n* **Time** does not matter: training samples are randomly sampled from the training set.\n\n\n**Reinforcement learning**\n\n* Behavior is acquired through **trial and error**, no supervision.\n\n* **Reinforcements** (rewards or punishments) change the probability of selecting particular actions.\n\n* Feedback is **delayed**: which action caused the reward? Credit assignment.\n\n* **Time** matters: as behavior gets better, the observed data changes.\n\n\n## Applications of tabular RL\n\n{{< youtube  EGI89ypJiv4 >}}\n\n### Pendulum\n\n{{< youtube  v6IEpH4vYq0 >}}\n\n\n### Cartpole\n\n![Cartpole before training. Source: <https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288>.](../slides/img/cartpole-before.gif){width=60%}\n\n![Cartpole after training. Source: <https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288>.](../slides/img/cartpole-after.gif){width=60%}\n\n{{< youtube  XiigTGKZfks >}}\n\n### Backgammon\n\nTD-Gammon [@Tesauro1995] was one of the first AI to beat human experts at a complex game, Backgammon.\n\n![Backgammon. Source: [@Tesauro1995].](../slides/img/tdgammon.png){width=50%}\n\n![TD-Gammon. Source: [@Tesauro1995].](../slides/img/tdgammon2.png){width=60%}\n\n## Deep Reinforcement Learning (DRL)\n\n![In deep RL, the policy is approximated by a deep neural network.](../slides/img/deeprl.jpg){width=80%}\n\nClassical tabular RL was limited to toy problems, with few states and actions. It is only when coupled with **deep neural networks** that interesting applications of RL became possible. Deepmind (now Google) started the deep RL hype in 2013 by learning to solve 50+ Atari games with a CNN, the **deep Q-network** (DQN) [@Mnih2013].\n\n![Architecture of the deep Q-network. Source: [@Mnih2013].](../slides/img/discrete.png){width=100%}\n\n{{< youtube  rQIShnTz1kU >}}\n\n\nDeep RL methods we since then improved and applied to a variety of control tasks, including simulated cars:\n\n{{< youtube  0xo1Ldx3L5Q >}}\n\nor Parkour:\n\n{{< youtube  faDKMMwOS2Q >}}\n\nOne very famous success of deep RL is when **AlphaGo** managed to beat Lee Sedol at the ancient game of Go:\n\n\n{{< youtube  8tq1C8spV_g >}}\n\nDeepRL has since been applied to real-world robotics:\n\n{{< youtube  l8zKZLqkfII >}}\n\n\n{{< youtube  jwSbzNHGflM >}}\n\nor even autonomous driving (<https://wayve.ai/blog/learning-to-drive-in-a-day-with-reinforcement-learning>):\n\n{{< youtube  eRwTbRtnT1I >}}\n\nIt is also used for more complex video games, such as **DotA II**:\n\n{{< youtube  eHipy_j29Xw >}}\n\nor Starcraft II (AlphaStar, <https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii>)\n\n![](../img/alphastar.gif)\n\n\nDeep RL is gaining a lot of importance in AI research, with lots of applications in control: video games, robotics, industrial applications... It may be AI's best shot at producing intelligent behavior, as it does not rely on annotated data. A lot of problems have to be solved before becoming as mainstream as deep learning.\n\n* Sample complexity is often prohibitive.\n* Energy consumption and computing power simply crazy (AlphaGo: 1 MW, Dota2: 800 petaflop/s-days)\n* The correct reward function is hard to design, ethical aspects. (*inverse RL*)\n* Hard to incorporate expert knowledge. (*model-based RL*)\n* Learns single tasks, does not generalize (*hierarchical RL*, *meta-learning*)\n\n\n## Suggested readings\n\n* Sutton and Barto (1998, 2017). Reinforcement Learning: An Introduction. MIT Press. \n\n<http://incompleteideas.net/sutton/book/the-book.html>\n\n* Szepesvari (2010). Algorithms for Reinforcement Learning. Morgan and Claypool.\n\n<http://www.ualberta.ca/∼szepesva/papers/RLAlgsInMDPs.pdf>\n\n* CS294 course of Sergey Levine at Berkeley. \n\n<http://rll.berkeley.edu/deeprlcourse/>\n\n* Reinforcement Learning course by David Silver at UCL.\n\n<https://www.davidsilver.uk/teaching/>\n\n* My notes on Deep Reinforcement Learning.\n\n<https://julien-vitay.net/deeprl>\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"1.1-Introduction.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}