{"title":"Math basics (optional)","markdown":{"headingText":"Math basics (optional)","containsRefs":false,"markdown":"\nSlides: [html](../slides/1.2-Basics.html){target=\"_blank\"} [pdf](../slides/pdf/1.2-Basics.pdf){target=\"_blank\"}\n\nThis chapter is not part of the course itself (there will not be questions at the exam on basic mathematics) but serves as a reminder of the important mathematical notions that are needed to understand this course. Students who have studied mathematics as a major can safely skip this part, as there is nothing fancy (although the section on information theory could be worth a read). \n\nIt is not supposed to replace any course in mathematics (we won't show any proof and will skip what we do not need) but rather to provide a high-level understanding of the most important concepts and set the notations. Nothing should be really new to you, but it may be useful to have everything summarized at the same place.\n\n**References:** Part I of [@Goodfellow2016]. Any mathematics textbook can be used in addition.\n\n\n## Linear algebra\n\n\n{{< youtube gE3W7pymA2k >}}\n\nSeveral mathematical objects are manipulated in linear algebra:\n\n* **Scalars** $x$ are 0-dimensional values (single numbers, so to speak). They can either take real values ($x \\in \\Re$, e.g. $x = 1.4573$, floats in CS) or natural values ($x \\in \\mathbb{N}$, e.g. $x = 3$, integers in CS). \n\n* **Vectors** $\\mathbf{x}$ are 1-dimensional arrays of length $d$. The bold notation $\\mathbf{x}$ will be used in this course, but you may also be accustomed to the arrow notation $\\overrightarrow{x}$ used on the blackboard. When using real numbers, the **vector space** with $d$ dimensions is noted $\\Re^d$, so we can note $\\mathbf{x} \\in \\Re^d$. Vectors are typically represented vertically to outline their $d$ elements $x_1, x_2, \\ldots, x_d$:\n\n$$\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix}$$\n\n\n* **Matrices** $A$ are 2-dimensional arrays of size (or shape) $m \\times n$ ($m$ rows, $n$ columns, $A \\in \\Re^{m \\times n}$). They are represented by a capital letter to distinguish them from scalars (classically also in bold $\\mathbf{A}$ but not here). The element $a_{ij}$ of a matrix $A$ is the element on the $i$-th row and $j$-th column.\n\n$$A = \\begin{bmatrix}\n a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix}$$\n\n* **Tensors** $\\mathcal{A}$ are arrays with more than two dimensions. We will not really do math on these objects, but they are useful internally (hence the name of the `tensorflow` library).\n\n### Vectors\n\nA vector can be thought of as the **coordinates of a point** in an Euclidean space (such the 2D space), relative to the origin. A vector space relies on two fundamental operations, which are that:\n\n* Vectors can be added:\n\n$$\\mathbf{x} + \\mathbf{y} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix} + \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_d \\end{bmatrix} = \\begin{bmatrix} x_1 + y_1 \\\\ x_2 + y_2 \\\\ \\vdots \\\\ x_d + y_d \\end{bmatrix}$$\n\n* Vectors can be multiplied by a scalar:\n\n$$a \\, \\mathbf{x} = a \\, \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix} = \\begin{bmatrix} a \\, x_1 \\\\ a \\, x_2 \\\\ \\vdots \\\\ a \\, x_d \\end{bmatrix}$$\n\n\n![Vector spaces allow additions of vectors. Source: <https://mathinsight.org/image/vector_2d_add>](../slides/img/vectorspace.png)\n\nThese two operations generate a lot of nice properties (see <https://en.wikipedia.org/wiki/Vector_space> for a full list), including:\n\n* associativity: $\\mathbf{x} + (\\mathbf{y} + \\mathbf{z}) = (\\mathbf{x} + \\mathbf{y}) + \\mathbf{z}$\n* commutativity: $\\mathbf{x} + \\mathbf{y} = \\mathbf{y} + \\mathbf{x}$\n* the existence of a zero vector $\\mathbf{x} + \\mathbf{0} = \\mathbf{x}$\n* inversion: $\\mathbf{x} + (-\\mathbf{x}) = \\mathbf{0}$\n* distributivity: $a \\, (\\mathbf{x} + \\mathbf{y}) = a \\, \\mathbf{x} + a \\, \\mathbf{y}$\n\nVectors have a **norm** (or length) $||\\mathbf{x}||$. The most intuitive one (if you know the Pythagoras theorem) is the **Euclidean norm** or $L^2$-norm, which sums the square of each element:\n\n$$||\\mathbf{x}||_2 = \\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}$$\n\nOther norms exist, distinguished by the subscript. The **$L^1$-norm** (also called Taxicab or Manhattan norm) sums the absolute value of each element:\n\n$$||\\mathbf{x}||_1 = |x_1| + |x_2| + \\ldots + |x_d|$$\n\nThe **p-norm** generalizes the Euclidean norm to other powers $p$: \n\n$$||\\mathbf{x}||_p = (|x_1|^p + |x_2|^p + \\ldots + |x_d|^p)^{\\frac{1}{p}}$$\n\nThe **infinity norm** (or maximum norm) $L^\\infty$ returns the maximum element of the vector:\n\n$$||\\mathbf{x}||_\\infty = \\max(|x_1|, |x_2|, \\ldots, |x_d|)$$\n\n\nOne important operation for vectors is the **dot product** (also called scalar product or inner product) between two vectors:\n\n$$\\langle \\mathbf{x} \\cdot \\mathbf{y} \\rangle = \\langle \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix} \\cdot \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_d \\end{bmatrix} \\rangle = x_1 \\, y_1 + x_2 \\, y_2 + \\ldots + x_d \\, y_d$$\n\nThe dot product basically sums one by one the product of the elements of each vector. The angular brackets are sometimes omitted ($\\mathbf{x} \\cdot \\mathbf{y}$) but we will use them in this course for clarity.\n\n\nOne can notice immediately that the dot product is **symmetric**: \n\n$$\\langle \\mathbf{x} \\cdot \\mathbf{y} \\rangle = \\langle \\mathbf{y} \\cdot \\mathbf{x} \\rangle$$\n\nand **linear**:\n\n$$\\langle (a \\, \\mathbf{x} + b\\, \\mathbf{y}) \\cdot \\mathbf{z} \\rangle = a\\, \\langle \\mathbf{x} \\cdot \\mathbf{z} \\rangle + b \\, \\langle \\mathbf{y} \\cdot \\mathbf{z} \\rangle$$\n\nThe dot product is an indirect measurement of the **angle** $\\theta$ between two vectors:\n\n\n$$\\langle \\mathbf{x} \\cdot \\mathbf{y} \\rangle = ||\\mathbf{x}||_2 \\, ||\\mathbf{y}||_2 \\, \\cos(\\theta)$$\n\n\n![The dot product between two vectors is proportional to the cosine of the angle between the two vectors. Source: <https://mathinsight.org/image/dot_product_projection_unit_vector>](../slides/img/dot_product_projection_unit_vector.png)\n\nIf you normalize the two vectors by dividing them by their norm (which is a scalar), we indeed have the cosine of the angle between them: The higher the normalized dot product, the more the two vectors point towards the same direction (**cosine distance** between two vectors).\n\n\n$$\\langle \\displaystyle\\frac{\\mathbf{x}}{||\\mathbf{x}||_2} \\cdot \\frac{\\mathbf{y}}{||\\mathbf{y}||_2} \\rangle =  \\cos(\\theta)$$\n\n### Matrices\n\nMatrices are derived from vectors, so most of the previous properties will be true. Let's consider this 4x3 matrix:\n\n$$A = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\na_{41} & a_{42} & a_{43} \\\\\n\\end{bmatrix}$$\n\nEach column of the matrix is a vector with 4 elements:\n\n$$\\mathbf{a}_1 = \\begin{bmatrix}\na_{11} \\\\\na_{21} \\\\\na_{31} \\\\\na_{41} \\\\\n\\end{bmatrix} \\qquad\n\\mathbf{a}_2 = \\begin{bmatrix}\na_{12} \\\\\na_{22} \\\\\na_{32} \\\\\na_{42} \\\\\n\\end{bmatrix} \\qquad\n\\mathbf{a}_3 = \\begin{bmatrix}\na_{13} \\\\\na_{23} \\\\\na_{33} \\\\\na_{43} \\\\\n\\end{bmatrix} \\qquad\n$$\n\nA $m \\times n$ matrix is therefore a collection of $n$ vectors of size $m$ put side by side column-wise:\n\n$$A = \\begin{bmatrix}\n\\mathbf{a}_1 & \\mathbf{a}_2 & \\mathbf{a}_3\\\\\n\\end{bmatrix}$$\n\nSo all properties of the vector spaces (associativity, commutativity, distributivity) also apply to matrices, as additions and multiplications with a scalar are defined.\n\n$$\\alpha \\, A + \\beta \\, B = \\begin{bmatrix}\n\\alpha\\, a_{11} + \\beta \\, b_{11} & \\alpha\\, a_{12} + \\beta \\, b_{12} & \\alpha\\, a_{13} + \\beta \\, b_{13} \\\\\n\\alpha\\, a_{21} + \\beta \\, b_{21} & \\alpha\\, a_{22} + \\beta \\, b_{22} & \\alpha\\, a_{23} + \\beta \\, b_{23} \\\\\n\\alpha\\, a_{31} + \\beta \\, b_{31} & \\alpha\\, a_{32} + \\beta \\, b_{32} & \\alpha\\, a_{33} + \\beta \\, b_{33} \\\\\n\\alpha\\, a_{41} + \\beta \\, b_{41} & \\alpha\\, a_{42} + \\beta \\, b_{42} & \\alpha\\, a_{43} + \\beta \\, b_{43} \\\\\n\\end{bmatrix}$$\n\n:::{.callout-note}\nBeware, you can only add matrices of the same dimensions $m\\times n$. You cannot add a $2\\times 3$ matrix to a $5 \\times 4$ one.\n:::\n\nThe **transpose** $A^T$ of a $m \\times n$ matrix $A$ is a $n \\times m$ matrix, where the row and column indices are swapped:\n\n\n$$A = \\begin{bmatrix}\n a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix}, \\qquad\nA^T = \\begin{bmatrix}\n a_{11} & a_{21} & \\cdots & a_{m1} \\\\\n a_{12} & a_{22} & \\cdots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n a_{1n} & a_{2n} & \\cdots & a_{mn} \\\\\n\\end{bmatrix}\n$$\n\nThis is also true for vectors, which become horizontal after transposition:\n\n$$\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix}, \\qquad\n\\mathbf{x}^T = \\begin{bmatrix} x_1 & x_2 & \\ldots & x_d \\end{bmatrix}\n$$\n\n\nA very important operation is the **matrix multiplication**. If $A$ is a $m\\times n$ matrix and $B$ a $n \\times p$ matrix:\n\n$$\nA=\\begin{bmatrix}\n a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix},\\quad\nB=\\begin{bmatrix}\n b_{11} & b_{12} & \\cdots & b_{1p} \\\\\n b_{21} & b_{22} & \\cdots & b_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n b_{n1} & b_{n2} & \\cdots & b_{np} \\\\\n\\end{bmatrix}\n$$\n\nwe can multiply them to obtain a $m \\times p$ matrix:\n\n$$\nC = A \\times B =\\begin{bmatrix}\n c_{11} & c_{12} & \\cdots & c_{1p} \\\\\n c_{21} & c_{22} & \\cdots & c_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n c_{m1} & c_{m2} & \\cdots & c_{mp} \\\\\n\\end{bmatrix}\n$$\n\nwhere each element $c_{ij}$ is the dot product of the $i$th row of $A$ and $j$th column of $B$:\n\n$$c_{ij} = \\langle A_{i, :} \\cdot B_{:, j} \\rangle = a_{i1}b_{1j} + a_{i2}b_{2j} +\\cdots + a_{in}b_{nj}= \\sum_{k=1}^n a_{ik}b_{kj}$$\n\n\n:::{.callout-note}\n$n$, the number of columns of $A$ and rows of $B$, must be the same!\n:::\n\n![The element $c_{ij}$ of $C = A \\times B$ is the dot product between the $i$th row of $A$ and the $j$th column of $B$. Source: [CC BY-NC-SA; Marcia Levitus](https://chem.libretexts.org/Bookshelves/Physical_and_Theoretical_Chemistry_Textbook_Maps/Book%3A_Mathematical_Methods_in_Chemistry_(Levitus)/15%3A_Matrices/15.03%3A_Matrix_Multiplication)](../slides/img/matrixmultiplication.jpg){width=60%}\n\nThinking of vectors as $n \\times 1$ matrices, we can multiply a matrix $m \\times n$ with a vector:\n\n$$\n\\mathbf{y} = A \\times \\mathbf{x} = \\begin{bmatrix}\n a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix} \\times \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix}\n$$\n\nThe result $\\mathbf{y}$ is a vector of size $m$. In that sense, a matrix $A$ can transform a vector of size $n$ into a vector of size $m$: $A$ represents a **projection** from $\\Re^n$ to $\\Re^m$.\n\n![A $2 \\times 3$ projection matrix allows to project any 3D vector onto a 2D plane. This is for example what happens inside a camera. Source: <https://en.wikipedia.org/wiki/Homogeneous_coordinate>](../slides/img/projection.png){width=60%}\n\nNote that the **dot product** between two vectors of size $n$ is the matrix multiplication between the transpose of the first vector and the second one:\n\n$$\\mathbf{x}^T \\times \\mathbf{y} = \\begin{bmatrix} x_1 & x_2 & \\ldots & x_n \\end{bmatrix} \\times \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = x_1 \\, y_1 + x_2 \\, y_2 + \\ldots + x_n \\, y_n = \\langle \\mathbf{x} \\cdot \\mathbf{y} \\rangle$$\n\nSquare matrices of size $n \\times n$ can be inverted. The **inverse** $A^{-1}$ of a matrix $A$ is defined by:\n\n$$A \\times A^{-1} = A^{-1} \\times A = I$$\n\nwhere $I$ is the identity matrix (a matrix with ones on the diagonal and 0 otherwise). Not all matrices have an inverse (those who don't are called singular or degenerate). There are plenty of conditions for a matrix to be invertible (for example its determinant is non-zero, see <https://en.wikipedia.org/wiki/Invertible_matrix>), but they will not matter in this course. Non-square matrices are generally not invertible, but see the pseudoinverse (<https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse>).\n\nMatrix inversion allows to solve linear systems of equations. Given the problem:\n\n$$\n\\begin{cases}\n    a_{11} \\, x_1 + a_{12} \\, x_2 + \\ldots + a_{1n} \\, x_n = b_1 \\\\\n    a_{21} \\, x_1 + a_{22} \\, x_2 + \\ldots + a_{2n} \\, x_n = b_2 \\\\\n    \\ldots \\\\\n    a_{n1} \\, x_1 + a_{n2} \\, x_2 + \\ldots + a_{nn} \\, x_n = b_n \\\\\n\\end{cases}\n$$\n\nwhich is equivalent to:\n\n$$A \\times \\mathbf{x} = \\mathbf{b}$$\n\nwe can multiply both sides to the left with $A^{-1}$ (if it exists) and obtain:\n\n$$\\mathbf{x} = A^{-1} \\times \\mathbf{b}$$\n\n## Calculus\n\n{{< youtube  RsQVNOTgYwk >}}\n\n### Functions\n\nA **univariate function** $f$ associates to any real number $x \\in \\Re$ (or a subset of $\\Re$ called the support of the function) another (unique) real number $f(x)$:\n\n$$\n\\begin{align}\nf\\colon \\quad \\Re &\\to \\Re\\\\\nx &\\mapsto f(x)\n\\end{align}\n$$\n\n\n![Example of univariate function, here the quadratic function $f(x) = x^2 - 2 \\, x + 1$.](../slides/img/function.png){width=60%}\n\n\nA **multivariate function** $f$ associates to any vector $\\mathbf{x} \\in \\Re^n$ (or a subset) a real number $f(\\mathbf{x})$:\n\n$$\n\\begin{align}\nf\\colon \\quad \\Re^n &\\to \\Re\\\\\n\\mathbf{x} &\\mapsto f(\\mathbf{x})\n\\end{align}\n$$\n\nThe variables of the function are the elements of the vector. For low-dimensional vector spaces, it is possible to represent each element explicitly, for example:\n\n$$\n\\begin{align}\nf\\colon \\quad\\Re^3 &\\to \\Re\\\\\nx, y, z &\\mapsto f(x, y, z),\\end{align}\n$$\n\n![Example of a multivariate function $f(x_1, x_2)$ mapping $\\Re^2$ to $\\Re$. Source: <https://en.wikipedia.org/wiki/Function_of_several_real_variables>](../slides/img/multivariatefunction.png){width=50%}\n\n\n**Vector fields** associate to any vector $\\mathbf{x} \\in \\Re^n$ (or a subset) another vector (possibly of different size):\n\n$$\n\\begin{align}\n\\overrightarrow{f}\\colon \\quad \\Re^n &\\to \\Re^m\\\\\n\\mathbf{x} &\\mapsto \\overrightarrow{f}(\\mathbf{x}),\\end{align}\n$$\n\n![Vector field associating to each point of $\\Re^2$ another vector. Source: <https://en.wikipedia.org/wiki/Vector_field>](../slides/img/vectorfield.png){width=40%}\n\n:::{.callout-note}\nThe matrix-vector multiplication $\\mathbf{y} = A \\times \\mathbf{x}$ is a linear vector field, mapping any vector $\\mathbf{x}$ into another vector $\\mathbf{y}$.\n:::\n\n### Differentiation\n\n#### Derivatives\n\n\n\nDifferential calculus deals with the **derivative** of a function, a process called differentiation.\n\nThe derivative $f'(x)$ or $\\displaystyle\\frac{d f(x)}{dx}$ of a univariate function $f(x)$ is defined as the local *slope* of the tangent to the function for a given value of $x$:\n\n$$f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}$$\n\nThe line passing through the points $(x, f(x))$ and $(x + h, f(x + h))$ becomes tangent to the function when $h$ becomes very small:\n\n![The derivative of the function $f(x)$ can be approximated by the slope of the line passing through $(x, f(x))$ and $(x + h, f(x + h))$ when $h$ becomes very small.](../slides/img/derivative-approx.png){width=60%}\n\nThe sign of the derivative tells you how the function behaves locally:\n\n* If the derivative is positive, increasing a little bit $x$ increases the function $f(x)$, so the function is **locally increasing**.\n* If the derivative is negative, increasing a little bit $x$ decreases the function $f(x)$, so the function is **locally decreasing**.\n\nIt basically allows you to measure the local influence of $x$ on $f(x)$: if I change a little bit the value $x$, what happens to $f(x)$? This will be very useful in machine learning.\n\nA special case is when the derivative is equal to 0 in $x$. $x$ is then called an **extremum** (or optimum) of the function, i.e. it can be a maximum or minimum. \n\n:::{.callout-note}\nIf you differentiate $f'(x)$ itself, you obtain the **second-order derivative** $f''(x)$. You can repeat that process and obtain higher order derivatives. \n\nFor example, if $x(t)$ represents the position $x$ of an object depending on time $t$, the first-order derivative $x'(t)$ denotes the **speed** of the object and the second-order derivative $x''(t)$ its **acceleration**.\n:::\n\nYou can tell whether an extremum is a maximum or a minimum by looking at its second-order derivative:\n\n* If $f''(x) > 0$, the extremum is a **minimum**.\n* If $f''(x) < 0$, the extremum is a **maximum**.\n* If $f''(x) = 0$, the extremum is a **saddle point**.\n\n![Quadratic functions have only one extremum (here a minimum in -1), as their derivative is linear and is equal to zero for only one value.](../slides/img/optimization-example.png){width=80%}\n\nThe derivative of a **multivariate function** $f(\\mathbf{x})$ is a vector of partial derivatives called the **gradient of the function** $\\nabla_\\mathbf{x} \\, f(\\mathbf{x})$:\n\n$$\n    \\nabla_\\mathbf{x} \\, f(\\mathbf{x}) = \\begin{bmatrix}\n        \\displaystyle\\frac{\\partial f(\\mathbf{x})}{\\partial x_1} \\\\\n        \\displaystyle\\frac{\\partial f(\\mathbf{x})}{\\partial x_2} \\\\\n        \\ldots \\\\\n        \\displaystyle\\frac{\\partial f(\\mathbf{x})}{\\partial x_n} \\\\\n    \\end{bmatrix}\n$$\n\n\nThe subscript to the $\\nabla$ operator denotes *with respect to* (w.r.t) which variable the differentiation is done.\n\nA **partial derivative** w.r.t. to particular variable (or element of the vector) is simply achieved by differentiating the function while considering all other variables to be **constant**. For example the function:\n\n$$f(x, y) = x^2 + 3 \\, x \\, y + 4 \\, x \\, y^2 - 1$$\n\ncan be partially differentiated w.r.t. $x$ and $y$ as:\n\n$$\\begin{cases}\n\\displaystyle\\frac{\\partial f(x, y)}{\\partial x} = 2 \\, x + 3\\, y + 4 \\, y^2 \\\\\n\\\\\n\\displaystyle\\frac{\\partial f(x, y)}{\\partial y} = 3 \\, x + 8\\, x \\, y\n\\end{cases}\n$$\n\nThe gradient can be generalized to **vector fields**, where the **Jacobian** or **Jacobi matrix** is a matrix containing all partial derivatives.\n\n$$\nJ = \\begin{bmatrix}\n    \\dfrac{\\partial \\mathbf{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\mathbf{f}}{\\partial x_n} \\end{bmatrix}\n= \\begin{bmatrix}\n    \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_n}\\\\\n    \\vdots & \\ddots & \\vdots\\\\\n    \\dfrac{\\partial f_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_m}{\\partial x_n} \\end{bmatrix}\n$$\n\n#### Analytical properties\n\nThe analytical form of the derivative of most standard mathematical functions is known. The following table lists the most useful ones in this course:\n\n| Function $f(x)$  | Derivative $f'(x)$ |\n|---|---|\n| $x$ | $1$ |\n| $x^p$ | $p \\, x^{p-1}$ |\n|$\\displaystyle\\frac{1}{x}$ | $- \\displaystyle\\frac{1}{x^2}$ |\n| $e^x$ | $e^x$ |\n|$\\ln x$ | $\\displaystyle\\frac{1}{x}$ |\n\nDifferentiation is linear, which means that if we define the function:\n\n$$h(x) = a \\, f(x) + b \\, g(x)$$\n\nits derivative is:\n\n$$h'(x) = a \\, f'(x) + b \\, g'(x)$$\n\nA product of functions can also be differentiated analytically:\n\n$$(f(x) \\times g(x))' = f'(x) \\times g(x) + f(x) \\times g'(x)$$\n\n\n:::{.callout-tip}\n## Example\n$$f(x) = x^2 \\, e^x$$\n\n$$f'(x) = 2 \\, x \\, e^x + x^2 \\cdot e^x$$\n:::\n\n#### Chain rule\n\nA very important concept for neural networks is the **chain rule**, which tells how to differentiate **function compositions** (functions of a function) of the form:\n\n$$(f \\circ g) (x) = f(g(x))$$\n\nThe derivative of $f \\circ g$ is:\n\n$$(f \\circ g)' (x) = (f' \\circ g) (x) \\times g'(x)$$\n\nThe chain rule may be more understandable using Leibniz's notation:\n\n$$\\frac{d f \\circ g (x)}{dx} = \\frac{d f (g (x))}{d g(x)} \\times \\frac{d g (x)}{dx}$$\n\nBy posing $y = g(x)$ as an intermediary variable, it becomes:\n\n\n$$\\frac{d f(y)}{dx} = \\frac{d f(y)}{dy} \\times \\frac{dy}{dx}$$\n\n\n:::{.callout-tip}\n## Example\n\nThe function :\n\n$$h(x) = \\frac{1}{2 \\, x + 1}$$\n\nis the function composition of $g(x) = 2 \\, x + 1$ and $f(x) = \\displaystyle\\frac{1}{x}$, whose derivatives are known:\n\n$$g'(x) = 2$$\n$$f'(x) = -\\displaystyle\\frac{1}{x^2}$$\n\nIts derivative according to the **chain rule** is:\n\n$$h'(x) = f'(g(x)) \\times g'(x) = -\\displaystyle\\frac{1}{(2 \\, x + 1)^2} \\times 2$$\n:::\n\nThe chain rule also applies to partial derivatives:\n\n$$\n    \\displaystyle\\frac{\\partial f \\circ g (x, y)}{\\partial x} = \\frac{\\partial f \\circ g (x, y)}{\\partial g (x, y)} \\times \\frac{\\partial g (x, y)}{\\partial x}\n$$\n\nand gradients:\n\n$$\n    \\nabla_\\mathbf{x} \\, f \\circ g (\\mathbf{x}) = \\nabla_{g(\\mathbf{x})} \\, f \\circ g (\\mathbf{x}) \\times \\nabla_\\mathbf{x} \\, g (\\mathbf{x})\n$$\n\n### Integration\n\nThe opposite operation of differentation is **integration**. Given a function $f(x)$, we search a function $F(x)$ whose *derivative* is $f(x)$:\n\n$$F'(x) = f(x)$$\n\nThe **integral** of $f$ is noted:\n\n$$F(x) = \\int f(x) \\, dx$$\n\n$dx$ being an infinitesimal interval (similar $h$ in the definition of the derivative). There are tons of formal definitions of integrals (Riemann, Lebesgue, Darboux...) and we will not get into details here as we will not use integrals a lot.\n\nThe most important to understand for now is maybe that the integral of a function is the **area under the curve**. The area under the curve of a function $f$ on the interval $[a, b]$ is:\n\n$$\\mathcal{S} = \\int_a^b f(x) \\, dx$$\n\n![The integral of $f$ on $[a, b]$ is the area of the surface between the function and the x-axis. Note that it can become negative when the function is mostly negative on $[a, b]$. Source: <https://www.math24.net/riemann-sums-definite-integral/>](../slides/img/riemann-sum1.svg){width=60%}\n\nOne way to approximate this surface is to split the interval $[a, b]$ into $n$ intervals of width $dx$ with the points $x_1, x_2, \\ldots, x_n$. This defines $n$ rectangles of width $dx$ and height $f(x_i)$, so their surface is $f(x_i) \\, dx$. The area under the curve can then be approximated by the sum of the surfaces of all these rectangles.\n\n![The interval$[a, b]$ can be split in $n$ small intervals of width $dx$, defining $n$ rectangles whose sum is close to the area under the curve. Source: <https://www.math24.net/riemann-sums-definite-integral/>](../slides/img/riemann-sum.svg){width=60%}\n\nWhen $n \\to \\infty$, or equivalently $dx \\to 0$, the sum of these rectangular areas (called the Riemann sum) becomes exactly the area under the curve. This is the definition of the definite integral:\n\n$$\\int_a^b f(x) \\, dx = \\lim_{dx \\to 0} \\sum_{i=1}^n f(x_i) \\, dx$$\n\nVery roughly speaking, the integral can be considered as the equivalent of a sum for continuous functions.\n\n\n## Probability theory\n\n{{< youtube  X5uSbrFe82Q >}}\n\n### Discrete probability distributions\n\nLet's note $X$ a **discrete random variable** with $n$ realizations (or outcomes) $x_1, \\ldots, x_n$. \n\n* A coin has two outcomes: head and tails.\n* A dice has six outcomes: 1, 2, 3, 4, 5, 6.\n\nThe **probability** that $X$ takes the value $x_i$ is defined in the frequentist sense by the **relative frequency of occurrence**, i.e. the proportion of samples having the value $x_i$, when the total number $N$ of samples tends to infinity:\n\n$$\n    P(X = x_i) = \\frac{\\text{Number of favorable cases}}{\\text{Total number of samples}}\n$$\n\nThe set of probabilities $\\{P(X = x_i)\\}_{i=1}^n$ define the **probability distribution** for the random variable (or probability mass function, pmf). By definition, we have $0 \\leq P(X = x_i) \\leq 1$ and the probabilities **have** to respect:\n\n$$\n    \\sum_{i=1}^n P(X = x_i) = 1\n$$\n\nAn important metric for a random variable is its **mathematical expectation** or expected value, i.e. its \"mean\" realization weighted by the probabilities:\n\n$$\n    \\mathbb{E}[X] = \\sum_{i=1}^n P(X = x_i) \\, x_i\n$$\n\nThe expectation does not even need to be a valid realization:\n\n$$\n    \\mathbb{E}[\\text{Coin}] = \\frac{1}{2} \\, 0 + \\frac{1}{2} \\, 1 = 0.5\n$$\n\n$$\n    \\mathbb{E}[\\text{Dice}] = \\frac{1}{6} \\, (1 + 2 + 3 + 4 + 5 + 6) = 3.5\n$$\n\nWe can also compute the mathematical expectation of **functions of** a random variable:\n\n$$\n    \\mathbb{E}[f(X)] = \\sum_{i=1}^n P(X = x_i) \\, f(x_i)\n$$\n\nThe **variance** of a random variable is the squared deviation around the mean:\n\n$$\n    \\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\sum_{i=1}^n P(X = x_i) \\, (x_i - \\mathbb{E}[X])^2\n$$\n\nVariance of a coin:\n\n$$\n    \\text{Var}(\\text{Coin}) = \\frac{1}{2} \\, (0 - 0.5)^2 + \\frac{1}{2} \\, (1 - 0.5)^2 = 0.25\n$$\n\nVariance of a dice: \n\n$$\n    \\text{Var}(\\text{Dice}) = \\frac{1}{6} \\, ((1-3.5)^2 + (2-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (5-3.5)^2 + (6-3.5)^2) = \\frac{105}{36}\n$$\n\n### Continuous probability distributions\n\n**Continuous random variables** can take infinitely many values in a continuous interval, e.g. $\\Re$ or some subset. The closed set of values they can take is called the **support** $\\mathcal{D}_X$ of the probability distribution. The probability distribution is described by a **probability density function** (pdf) $f(x)$.\n\n![Normal distributions are continuous distributions. The area under the curve is always 1.](../slides/img/normaldistribution.png){width=60%}\n\nThe pdf of a distribution must be positive ($f(x) \\geq 0 \\, \\forall x \\in \\mathcal{D}_X$) and its integral (area under the curve) must be equal to 1:\n\n$$\n    \\int_{x \\in \\mathcal{D}_X} f(x) \\, dx = 1\n$$\n\nThe pdf does not give the probability of taking a particular value $x$ (it is 0), but allows to get the probability that a value lies in a specific interval:\n\n$$\n    P(a \\leq X \\leq b) = \\int_{a}^b f(x) \\, dx \n$$\n\n\nOne can however think of the pdf as the **likelihood** that a value $x$ comes from that distribution.\n\n\nFor continuous distributions, the mathematical expectation is now defined by an integral instead of a sum:\n\n$$\n    \\mathbb{E}[X] = \\int_{x \\in \\mathcal{D}_X} f(x) \\, x \\, dx\n$$\n\nthe variance also:\n\n$$\n    \\text{Var}(X) = \\int_{x \\in \\mathcal{D}_X} f(x) \\, (x - \\mathbb{E}[X])^2 \\, dx\n$$\n\nor a function of the random variable:\n\n$$\n    \\mathbb{E}[g(X)] = \\int_{x \\in \\mathcal{D}_X} f(x) \\, g(x) \\, dx\n$$\n\nNote that the expectation operator is **linear**:\n\n$$\n    \\mathbb{E}[a \\, X + b \\, Y] = a \\, \\mathbb{E}[X] + b \\, \\mathbb{E}[Y]\n$$\n\nbut not the variance, even when the distributions are independent:\n\n$$\n    \\text{Var}[a \\, X + b \\, Y] = a^2 \\, \\text{Var}[X] + b^2 \\, \\text{Var}[Y]\n$$\n\n### Standard distributions\n\nProbability distributions can in principle have any form: $f(x)$ is unknown. However, specific parameterized distributions can be very useful: their pmf/pdf is fully determined by a couple of parameters.\n\n* The **Bernouilli** distribution is a binary (discrete, 0 or 1) distribution with a parameter $p$ specifying the probability to obtain the outcome 1 (e.g. a coin):\n\n$$\n    P(X = 1) = p \\; \\text{and} \\; P(X=0) = 1 - p \n$$\n$$P(X=x) = p^x \\, (1-p)^{1-x}$$\n$$\\mathbb{E}[X] = p$$\n\n* The **Multinouilli** or **categorical** distribution is a discrete distribution with $k$ realizations (e.g. a dice). Each realization $x_i$ is associated with a parameter $p_i >0$ representing its probability. We have $\\sum_i p_i = 1$.\n\n$$P(X = x_i) = p_i$$\n\n\n* The **uniform distribution** has an equal and constant probability of returning values between $a$ and $b$, never outside this range. It is parameterized by the start of the range $a$ and the end of the range $b$. Its support is $[a, b]$. The pdf of the uniform distribution $\\mathcal{U}(a, b)$ is defined on $[a, b]$ as:\n\n$$\n    f(x; a, b) = \\frac{1}{b - a}\n$$\n\n* The **normal distribution** is the most frequently encountered continuous distribution. It is parameterized by two parameters: the mean $\\mu$ and the variance $\\sigma^2$ (or standard deviation $\\sigma$). Its support is $\\Re$. The pdf of the normal distribution $\\mathcal{N}(\\mu, \\sigma)$ is defined on $\\Re$ as:\n\n$$\n    f(x; \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\,\\pi\\,\\sigma^2}} \\, e^{-\\displaystyle\\frac{(x - \\mu)^2}{2\\,\\sigma^2}}\n$$\n\n\n* The **exponential distribution** is the probability distribution of the time between events in a Poisson point process, i.e., a process in which events occur continuously and independently at a constant average rate. It is parameterized by one parameter: the rate $\\lambda$. Its support is $\\Re^+$ ($x > 0$).\nThe pdf of the exponential distribution is defined on $\\Re^+$ as:\n\n$$\n    f(x; \\lambda) = \\lambda \\, e^{-\\lambda \\, x}\n$$\n\n\n### Joint and conditional probabilities\n\nLet's now suppose that we have two random variables $X$ and $Y$ with different probability distributions $P(X)$ and $P(Y)$.  The **joint probability** $P(X, Y)$ denotes the probability of observing the realizations $x$ **and** $y$ at the same time:\n\n$$P(X=x, Y=y)$$\n\nIf the random variables are **independent**, we have:\n\n$$P(X=x, Y=y) = P(X=x) \\, P(Y=y)$$\n\nIf you know the joint probability, you can compute the **marginal probability distribution** of each variable:\n\n$$P(X=x) = \\sum_y P(X=x, Y=y)$$\n\nThe same is true for continuous probability distributions:\n\n$$\n    f(x) = \\int f(x, y) \\, dy\n$$\n\nSome useful information between two random variables is the **conditional probability**. $P(X=x | Y=y)$ is the conditional probability that $X=x$, **given** that $Y=y$ is observed.\n\n* $Y=y$ is not random anymore: it is a **fact** (at least theoretically).\n\n* You wonder what happens to the probability distribution of $X$ now that you know the value of $Y$.\n\nConditional probabilities are linked to the joint probability by:\n\n$$\n    P(X=x | Y=y) = \\frac{P(X=x, Y=y)}{P(Y=y)}\n$$\n\nIf $X$ and $Y$ are **independent**, we have $P(X=x | Y=y) = P(X=x)$ (knowing $Y$ does not change anything to the probability distribution of $X$). We can use the same notation for the complete probability distributions:\n\n$$\n    P(X | Y) = \\frac{P(X, Y)}{P(Y)}\n$$\n\n**Example**\n\n\n![Source: <https://www.elevise.co.uk/g-e-m-h-5-u.html>.](../slides/img/conditionalprobability.png){width=60%}\n\nYou ask 50 people whether they like cats or dogs:\n\n* 18 like both cats and dogs.\n* 21 like only dogs.\n* 5 like only cats.\n* 6 like none of them.\n\nWe consider loving cats and dogs as random variables (and that our sample size is big enough to use probabilities...). Among the 23 who love cats, which proportion also loves dogs?\n\n\n:::{.callout-tip}\n## Answer \n\nWe have $P(\\text{dog}) = \\displaystyle\\frac{18+21}{50}= \\displaystyle\\frac{39}{50}$ and $P(\\text{cat}) = \\displaystyle\\frac{18+5}{50} = \\frac{23}{50}$.\n\nThe joint probability of loving both cats and dogs is $P(\\text{cat}, \\text{dog}) = \\displaystyle\\frac{18}{50}$.\n\nThe conditional probability of loving dogs given one loves cats is:\n\n$$P(\\text{dog} | \\text{cat}) = \\displaystyle\\frac{P(\\text{cat}, \\text{dog})}{P(\\text{cat})} = \\frac{\\frac{18}{50}}{\\frac{23}{50}} = \\frac{18}{23}$$\n:::\n\n\n### Bayes' rule\n\nNoticing that the definition of conditional probabilities is symmetric:\n\n$$\n    P(X, Y) = P(X | Y) \\, P(Y) = P(Y | X) \\, P(X)\n$$\n\nwe can obtain the **Bayes' rule**:\n\n$$\n    P(Y | X) = \\frac{P(X|Y) \\, P(Y)}{P(X)}\n$$\n\nIt is very useful when you already know $P(X|Y)$ and want to obtain $P(Y|X)$ (**Bayesian inference**).\n\n* $P(Y | X)$ is called the **posterior probability**.\n\n* $P(X | Y)$ is called the **likelihood**.\n\n* $P(Y)$ is called the **prior probability** (belief).\n\n* $P(X)$ is called the **model evidence** or **marginal likelihood**.\n\n\n**Example**\n\nLet's consider a disease $D$ (binary random variable) and a medical test $T$ (also binary). The disease affects 10% of the general population: \n\n$$P(D=1)= 0.1 \\qquad \\qquad P(D=0)=0.9$$\n\nWhen a patient has the disease, the test is positive 80% of the time (true positives):\n\n$$P(T=1 | D=1) = 0.8 \\qquad \\qquad P(T=0 | D=1) = 0.2$$\n\nWhen a patient does not have the disease, the test is still positive 10% of the time (false positives):\n\n$$P(T=1 | D=0) = 0.1 \\qquad \\qquad P(T=0 | D=0) = 0.9$$\n\nGiven that the test is positive, what is the probability that the patient is ill?\n\n\n:::{.callout-tip}\n## Answer\n$$\n\\begin{aligned}\n    P(D=1|T=1) &= \\frac{P(T=1 | D=1) \\, P(D=1)}{P(T=1)} \\\\\n               &\\\\\n               &= \\frac{P(T=1 | D=1) \\, P(D=1)}{P(T=1 | D=1) \\, P(D=1) + P(T=1 | D=0) \\, P(D=0)} \\\\\n               &\\\\\n               &= \\frac{0.8 \\times 0.1}{0.8 \\times 0.1 + 0.1 \\times 0.9} \\\\\n               &\\\\\n               & = 0.47 \\\\\n\\end{aligned}\n$$\n:::\n\n## Statistics\n\n{{< youtube  Q7u_iMZ89K8 >}}\n\n### Monte Carlo sampling\n\n**Random sampling** or **Monte Carlo sampling** (MC) consists of taking $N$ samples $x_i$ out of the distribution $X$ (discrete or continuous) and computing the **sample average**:\n\n$$\n    \\mathbb{E}[X] = \\mathbb{E}_{x \\sim X} [x] \\approx \\frac{1}{N} \\, \\sum_{i=1}^N x_i\n$$\n\n![Samples taken from a normal distribution will mostly be around the mean.](../slides/img/normaldistribution.svg){width=60%}\n\nMore samples will be obtained where $f(x)$ is high ($x$ is probable), so the average of the sampled data will be close to the expected value of the distribution.\n\n**Law of big numbers**\n\n> As the number of identically distributed, randomly generated variables increases, their sample mean (average) approaches their theoretical mean.\n\n\nMC estimates are only correct when: \n\n* the samples are **i.i.d** (independent and identically distributed):\n\n    * independent: the samples must be unrelated with each other.\n\n    * identically distributed: the samples must come from the same distribution $X$.\n\n* the number of samples is large enough. Usually $N > 30$ for simple distributions.\n\nOne can estimate any function of the random variable with random sampling:\n\n$$\n    \\mathbb{E}[f(X)] = \\mathbb{E}_{x \\sim X} [f(x)] \\approx \\frac{1}{N} \\, \\sum_{i=1}^N f(x_i)\n$$\n\n![Sampling can be used to estimate $\\pi$: when sampling $x$ and $y$ uniformly in $[0, 1]$, the proportion of points with a norm smaller than tends to $\\pi/4$. Source: <https://towardsdatascience.com/an-overview-of-monte-carlo-methods-675384eb1694>](img/montecarlo.svg){width=100%}\n\n\n### Central limit theorem\n\nSuppose we have an unknown distribution $X$ with expected value $\\mu = \\mathbb{E}[X]$ and variance $\\sigma^2$. We can take randomly $N$ samples from $X$ to compute the sample average:\n\n$$\n    S_N = \\frac{1}{N} \\, \\sum_{i=1}^N x_i\n$$ \n\nThe **Central Limit Theorem** (CLT) states that:\n\n> The distribution of sample averages is normally distributed with mean $\\mu$ and variance $\\frac{\\sigma^2}{N}$.\n\n$$S_N \\sim \\mathcal{N}(\\mu, \\frac{\\sigma}{\\sqrt{N}})$$\n\nIf we perform the sampling multiple times, even with few samples, the average of the sampling averages will be very close to the expected value. The more samples we get, the smaller the variance of the estimates. Although the distribution $X$ can be anything, the sampling averages are normally distributed.\n\n![Source: <https://en.wikipedia.org/wiki/Central_limit_theorem>](../slides/img/IllustrationCentralTheorem.png){width=100%}\n\n### Estimators\n\nCLT shows that the sampling average is an **unbiased estimator** of the expected value of a distribution:\n\n$$\\mathbb{E}[S_N] = \\mathbb{E}[X]$$\n\nAn estimator is a random variable used to measure parameters of a distribution (e.g. its expectation). The problem is that estimators can generally be **biased**.\n\nTake the example of a thermometer $M$ measuring the temperature $T$. $T$ is a random variable (normally distributed with $\\mu=20$ and $\\sigma=10$) and the measurements $M$ relate to the temperature with the relation:\n\n$$\n    M = 0.95 \\, T + 0.65\n$$\n\n![Left: measurement as a function of the temperature. Right: distribution of temperature.](../slides/img/estimators-temperature.png){width=100%}\n\nThe thermometer is not perfect, but do random measurements allow us to estimate the expected value of the temperature?\n\nWe could repeatedly take 100 random samples of the thermometer and see how the distribution of sample averages look like:\n\n![Sampled measurements.](../slides/img/estimators-temperature2.png){width=60%}\n\nBut, as the expectation is linear, we actually have:\n\n$$\n    \\mathbb{E}[M] = \\mathbb{E}[0.95 \\, T + 0.65] = 0.95 \\, \\mathbb{E}[T] + 0.65 = 19.65 \\neq \\mathbb{E}[T]\n$$\n\nThe thermometer is a **biased estimator** of the temperature.\n\nLet's note $\\theta$ a parameter of a probability distribution $X$ that we want to estimate (it does not have to be its mean). An **estimator** $\\hat{\\theta}$ is a random variable mapping the sample space of $X$ to a set of sample estimates.\n\n* The **bias** of an estimator is the mean error made by the estimator:\n\n$$\n    \\mathcal{B}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta} - \\theta] = \\mathbb{E}[\\hat{\\theta}] - \\theta\n$$\n\n* The **variance** of an estimator is the deviation of the samples around the expected value:\n\n$$\n    \\text{Var}(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}] )^2]\n$$\n\nIdeally, we would like estimators with:\n\n* **low bias**: the estimations are correct on average (= equal to the true parameter).\n\n* **low variance**: we do not need many estimates to get a correct estimate (CLT: $\\frac{\\sigma}{\\sqrt{N}}$)\n\n\n![Bias-variance trade-off.](../slides/img/biasvariance3.png){width=60%}\n\nUnfortunately, the perfect estimator does not exist in practice.  One usually talks of a **bias/variance** trade-off: if you have a small bias, you will have a high variance, or vice versa. In machine learning, bias corresponds to underfitting, variance to overfitting.\n\n\n\n## Information theory\n\n{{< youtube  -WZKHdKDtTY >}}\n\n\n### Entropy\n\n**Information theory** (a field founded by Claude Shannon) asks how much information is contained in a probability distribution. Information is related to **surprise** or **uncertainty**: are the outcomes of a random variable surprising? \n\n* Almost certain outcomes ($P \\sim 1$) are not surprising because they happen all the time.\n\n* Almost impossible outcomes ($P \\sim 0$) are very surprising because they are very rare.\n\n![Self-information.](../slides/img/selfinformation.png){width=80%}\n\nA useful measurement of how surprising is an outcome $x$ is the **self-information**:\n\n$$\n    I (x) = - \\log P(X = x)\n$$\n\nDepending on which log is used, self-information has different units, but it is just a rescaling, the base never matters:\n\n* $\\log_2$: bits or shannons.\n* $\\log_e = \\ln$: nats.\n\n\nThe **entropy** (or Shannon entropy) of a probability distribution is the expected value of the self-information of its outcomes:\n\n$$\n    H(X) = \\mathbb{E}_{x \\sim X} [I(x)] = \\mathbb{E}_{x \\sim X} [- \\log P(X = x)] \n$$\n\nIt measures the **uncertainty**, **randomness** or **information content** of the random variable.\n\nIn the discrete case:\n\n$$\n    H(X) = - \\sum_x P(x) \\, \\log P(x)\n$$\n\nIn the continuous case:\n\n$$\n    H(X) = - \\int_x f(x) \\, \\log f(x) \\, dx\n$$\n\nThe entropy of a Bernouilli variable is maximal when both outcomes are **equiprobable**. If a variable is **deterministic**, its entropy is minimal and equal to zero.\n\n\n![The entropy of a Bernouilli distribution is maximal when the two outcomes are equiprobable.](../slides/img/entropy-binomial.png){width=80%}\n\nThe **joint entropy** of two random variables $X$ and $Y$ is defined by:\n\n$$\n    H(X, Y) = \\mathbb{E}_{x \\sim X, y \\sim Y} [- \\log P(X=x, Y=y)] \n$$\n\nThe **conditional entropy** of two random variables $X$ and $Y$ is defined by:\n\n$$\n    H(X | Y) = \\mathbb{E}_{x \\sim X, y \\sim Y} [- \\log P(X=x | Y=y)]  = \\mathbb{E}_{x \\sim X, y \\sim Y} [- \\log \\frac{P(X=x , Y=y)}{P(Y=y)}] \n$$\n\nIf the variables are **independent**, we have:\n\n$$\n    H(X, Y) = H(X) + H(Y)\n$$\n$$\n    H(X | Y) = H(X)\n$$\n\nBoth are related by:\n\n$$\n    H(X | Y) = H(X, Y) - H(Y)\n$$\n\nThe equivalent of Bayes' rule is:\n\n$$\n    H(Y |X) = H(X |Y) + H(Y) - H(X)\n$$\n\n\n### Mutual Information, cross-entropy and Kullback-Leibler divergence\n\nThe most important information measurement between two variables is the **mutual information** MI (or information gain):\n\n$$\n    I(X, Y) = H(X) - H(X | Y) = H(Y) - H(Y | X)\n$$\n\nIt measures how much information the variable $X$ holds on $Y$:\n\n* If the two variables are **independent**, the MI is 0 : $X$ is as random, whether you know $Y$ or not.\n\n$$\n        I (X, Y) = 0\n$$\n\n* If the two variables are **dependent**, knowing $Y$ gives you information on $X$, which becomes less random, i.e. less uncertain / surprising.\n\n$$\n        I (X, Y) > 0\n$$\n\nIf you can fully predict $X$ when you know $Y$, it becomes deterministic ($H(X|Y)=0$) so the mutual information is maximal ($I(X, Y) = H(X)$).\n\n\nThe **cross-entropy** between two distributions $X$ and $Y$ is defined as:\n\n$$\n    H(X, Y) = \\mathbb{E}_{x \\sim X}[- \\log P(Y=x)]\n$$\n\n:::{.callout-note}\nBeware that the notation $H(X, Y)$ is the same as the joint entropy, but it is a different concept!\n:::\n\n![The cross-entropy measures the overlap between two probability distributions.](../slides/img/crossentropy.svg){width=100%}\n\nThe cross-entropy measures the **negative log-likelihood** that a sample $x$ taken from the distribution $X$ could also come from the distribution $Y$. More exactly, it measures how many bits of information one would need to distinguish the two distributions $X$ and $Y$.\n\n$$\n    H(X, Y) = \\mathbb{E}_{x \\sim X}[- \\log P(Y=x)]\n$$\n\nIf the two distributions are the same *almost anywhere*, one cannot distinguish samples from the two distributions, the cross-entropy is the same as the entropy of $X$. If the two distributions are completely different, one can tell whether a sample $Z$ comes from $X$ or $Y$, the cross-entropy is higher than the entropy of $X$.\n\n\nIn practice, the **Kullback-Leibler divergence** $\\text{KL}(X ||Y)$ is a better measurement of the similarity (statistical distance) between two probability distributions:\n\n$$\n    \\text{KL}(X ||Y) = \\mathbb{E}_{x \\sim X}[- \\log \\frac{P(Y=x)}{P(X=x)}]\n$$\n\nIt is linked to the cross-entropy by:\n\n$$\n    \\text{KL}(X ||Y) = H(X, Y) - H(X)\n$$\n\nIf the two distributions are the same *almost anywhere*, the KL divergence is zero. If the two distributions are different, the KL divergence is positive. Minimizing the KL between two distributions is the same as making the two distributions \"equal\". But remember: the KL is not a metric, as it is not symmetric.\n\n:::{.callout-note}\nRefer \n<https://towardsdatascience.com/entropy-cross-entropy-and-kl-divergence-explained-b09cdae917a> for nice visual explanations of the cross-entropy.\n:::\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"1.2-Math.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}