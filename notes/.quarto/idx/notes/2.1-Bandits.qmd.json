{"title":"Bandits","markdown":{"headingText":"Bandits","containsRefs":false,"markdown":"\nSlides: [html](../slides/2.1-Bandits.html){target=\"_blank\"} [pdf](../slides/pdf/2.1-Bandits.pdf){target=\"_blank\"}\n\n## n-armed bandits\n\n{{< youtube  PVKWm3rrIOc >}}\n\nRL evaluates actions through **trial-and-error** rather than comparing its predictions to the correct actions. This is called **evaluative feedback**, as the feedback depends completely on the action taken. Oppositely, supervised learning is a form of **instructive feedback**, as the targets (labels / ground truth) do not depend at all on the prediction. \n\nEvaluative feedback indicates how good the action is, but not whether it is the best or worst action possible. Two forms of RL learning can be distinguished:\n\n* **Associative learning**: inputs are mapped to the best possible outputs (general RL). \n\n* **Non-associative learning**: finds one best output, regardless of the current state or past history (bandits).\n        \nThe **n-armed bandit**  (or multi-armed bandit) is a non-associative evaluative feedback procedure. Learning and action selection take place in the same single state, while the $n$ actions have different reward distributions. The goal is to find out through trial and error which action provides the most reward on average.\n\n![10-armed bandit with the true value of each action.](../slides/img/bandit-example.png){width=80%}\n\nWe have the choice between $N$ different actions $(a_1, ..., a_N)$. Each action $a$ taken at time $t$ provides a **reward** $r_t$ drawn from the action-specific probability distribution $r(a)$. The mathematical expectation of that distribution is the **expected reward**, called the **true value** of the action $Q^*(a)$.\n\n$$\n    Q^*(a) = \\mathbb{E} [r(a)]\n$$\n\nThe reward distribution also has a **variance**: we usually ignore it in RL, as all we care about is the **optimal action** (but see distributional RL later). If we take the optimal action an infinity of times, we maximize the reward intake **on average**. The question is how to find out the optimal action through **trial and error**, i.e. without knowing the exact reward distribution $r(a)$.\n\n\nThe **true value** (or **utility**) of an action is: \n\n$$Q^*(a) = \\mathbb{E} [r(a)]$$\n\nWe only have access to **samples** of $r(a)$ by taking the action at time $t$ (a **trial**, **play** or **step**). The received rewards $r_t$ vary around the true value over time. We need to build **estimates** $Q_t(a)$ of the value of each action based on the samples. These estimates will be very wrong at the beginning, but should get better over time.\n\n\n## Sampling-based evaluation\n\n{{< youtube  mHyySub8wVc >}}\n\n\nThe expectation of the reward distribution can be approximated by the **mean** of its samples (sampling average):\n\n$$\n    \\mathbb{E} [r(a)] \\approx  \\frac{1}{N} \\sum_{t=1}^N r_t |_{a_t = a}\n$$ \n\nSuppose that the action $a$ had been selected $t$ times, producing rewards \n\n$$\n    (r_1, r_2, ..., r_t)\n$$\n\nThe estimated value of action $a$ at play $t$ is then:\n\n$$\n    Q_t (a) = \\frac{r_1 + r_2 + ... + r_t }{t}\n$$\n\nOver time, the estimated action-value converges to the true action-value:\n\n$$\n   \\lim_{t \\to \\infty} Q_t (a) = Q^* (a)\n$$\n\n![The sampling average can correctly approximate the true value of an action given enough samples.](../slides/img/bandit-samples2.png){width=80%}\n\nThe drawback of maintaining the mean of the received rewards is that it consumes a lot of memory:\n\n$$\n    Q_t (a) = \\frac{r_1 + r_2 + ... + r_t }{t} = \\frac{1}{t} \\, \\sum_{i=1}^{t} r_i\n$$\n\nIt is possible to update an estimate of the mean in an **online** or incremental manner:\n\n$$\n\\begin{aligned}\n    Q_{t+1}(a) &= \\frac{1}{t+1} \\, \\sum_{i=1}^{t+1} r_i \\\\\n            &= \\frac{1}{t+1} \\, (r_{t+1} + \\sum_{i=1}^{t} r_i )\\\\\n            &= \\frac{1}{t+1} \\, (r_{t+1} + t \\,  Q_{t}(a) ) \\\\\n            &= \\frac{1}{t+1} \\, (r_{t+1} + (t + 1) \\,  Q_{t}(a) - Q_t(a)) \n\\end{aligned} \n$$\n\nThe estimate at time $t+1$ depends on the previous estimate at time $t$ and the last reward $r_{t+1}$:\n\n$$\n    Q_{t+1}(a) = Q_t(a) + \\frac{1}{t+1} \\, (r_{t+1} - Q_t(a)) \n$$\n\nThe problem with the exact mean is that it is only exact when the reward distribution is **stationary**, i.e. when the probability distribution does not change over time. If the reward distribution is **non-stationary**, the $\\frac{1}{t+1}$ term will become very small and prevent rapid updates of the mean.\n\n\n![The sampling average have problems when the reward distribution is non-stationary.](../slides/img/bandit-nonstationary1.png){width=80%}\n\nThe solution is to replace $\\frac{1}{t+1}$ with a fixed parameter called the **learning rate** (or **step size**) $\\alpha$:\n\n$$\n\\begin{aligned}\n    Q_{t+1}(a) & = Q_t(a) + \\alpha \\, (r_{t+1} - Q_t(a)) \\\\\n                & \\\\\n                & = (1 - \\alpha) \\, Q_t(a) + \\alpha \\, r_{t+1}\n\\end{aligned}\n$$\n\nThe computed value is called a **moving average** (or sliding average), as if one used only a small window of the past history. \n\n![The moving average is much more flexible for non-stationary distributions.](../slides/img/bandit-nonstationary2.png){width=80%}\n\nThe moving average adapts very fast to changes in the reward distribution and should be used in **non-stationary problems**. It is however not exact and sensible to noise. Choosing the right value for $\\alpha$ can be difficult.\n\nThe form of this **update rule** is very important to remember:\n\n$$\n    \\text{new estimate} = \\text{old estimate} + \\alpha \\, (\\text{target} - \\text{old estimate}) \n$$\n\nEstimates following this update rule track the mean of their sampled target values. $\\text{target} - \\text{old estimate}$ is the **prediction error** between the target and the estimate.\n\n## Action selection\n\n{{< youtube  yg3yI7YmEa8 >}}\n\n### Greedy action selection\n\nLet's suppose we have formed reasonable estimates of the Q-values $Q_t(a)$ at time $t$. Which action should we do next? If we select the next action $a_{t+1}$ randomly (**random agent**), we do not maximize the rewards we receive, but we can continue learning the Q-values. Choosing the action to perform next is called **action selection** and several schemes are possible.\n\n![Greedy action selection.](../slides/img/bandit-estimates-greedy.png){width=100%}\n\nThe **greedy action** is the action whose estimated value is **maximal** at time $t$ based on our current estimates: \n\n$$\n    a^*_t = \\text{argmax}_{a} Q_t(a)\n$$\n\nIf our estimates $Q_t$ are correct (i.e. close from $Q^*$), the greedy action is the **optimal action** and we maximize the rewards on average. If our estimates are wrong, the agent will perform **sub-optimally**.\n\n\nThis defines the **greedy policy**, where the probability of taking the greedy action is 1 and the probability of selecting another action is 0:\n\n$$\n    \\pi(a) = \\begin{cases} 1 \\; \\text{if} \\; a = a_t^* \\\\ 0 \\; \\text{otherwise.} \\end{cases}\n$$\n\nThe greedy policy is **deterministic**: the action taken is always the same for a fixed $Q_t$.\n\n\n### Exploration-exploitation dilemma\n\n![Greedy action selection only works when the estimates are good enough.](../slides/img/bandit-estimates-greedy-problem1.png){width=60%}\n\n![Estimates are initially bad (e.g. 0 here), so an action is sampled randomly and a reward is received.](../slides/img/bandit-estimates-greedy-problem2.png){width=60%}\n\n![The Q-value of that action becomes positive, so it becomes the greedy action.](../slides/img/bandit-estimates-greedy-problem3.png){width=60%}\n\n![Greedy action selection will always select that action, although the second one would have been better.](../slides/img/bandit-estimates-greedy-problem4.png){width=60%}\n\nThis **exploration-exploitation** dilemma is the hardest problem in RL:\n\n* **Exploitation** is using the current estimates to select an action: they might be wrong!\n* **Exploration** is selecting non-greedy actions in order to improve their estimates: they might not be optimal!\n\nOne has to balance exploration and exploitation over the course of learning:\n\n* More exploration at the beginning of learning, as the estimates are initially wrong.\n* More exploitation at the end of learning, as the estimates get better.\n\n### $\\epsilon$-greedy action selection\n\n\n![$\\epsilon$-greedy action selection](../slides/img/bandit-estimates-epsilongreedy.png){width=100%}\n\n**$\\epsilon$-greedy action selection** ensures a trade-off between exploitation and exploration. The greedy action is selected with probability $1 - \\epsilon$ (with $0 < \\epsilon <1$), the others with probability $\\epsilon$:\n\n$$\n    \\pi(a) = \\begin{cases} 1 - \\epsilon \\; \\text{if} \\; a = a_t^* \\\\ \\frac{\\epsilon}{|\\mathcal{A}| - 1} \\; \\text{otherwise.} \\end{cases}\n$$\n\nThe parameter $\\epsilon$ controls the level of exploration: the higher $\\epsilon$, the more exploration. One can set $\\epsilon$ high at the beginning of learning and progressively reduce it to exploit more. However, it chooses equally among all actions: the worst action is as likely to be selected as the next-to-best action.\n    \n    \n### Softmax action selection\n\n\n![Softmax action selection](../slides/img/bandit-estimates-softmax.png){width=100%}\n\n**Softmax action selection**  defines the probability of choosing an action using all estimated value. It represents the policy using a Gibbs (or Boltzmann) distribution:\n\n$$\n    \\pi(a) = \\frac{\\exp \\frac{Q_t(a)}{\\tau}}{ \\sum_b \\exp \\frac{Q_t(b)}{\\tau}}\n$$ \n\nwhere $\\tau$ is a positive parameter called the **temperature**.\n\n![](../img/bandit-estimates-softmax2.png)\n\nJust as $\\epsilon$, the temperature $\\tau$ controls the level of exploration:\n\n* High temperature causes the actions to be nearly equiprobable (**random agent**).\n\n* Low temperature causes the greediest actions only to be selected (**greedy agent**).\n    \n    \n### Example of action selection for the 10-armed bandit\n\n**Procedure:**\n    \n* N = 10 possible actions with Q-values $Q^*(a_1), ... , Q^*(a_{10})$ randomly chosen in $\\mathcal{N}(0, 1)$.\n\n* Each reward $r_t$ is drawn from a normal distribution $\\mathcal{N}(Q^*(a), 1)$ depending on the selected action.\n\n* Estimates $Q_t(a)$ are initialized to 0.\n\n* The algorithms run for 1000 plays, and the results are averaged 200 times.\n    \n**Greedy action selection**\n\nGreedy action selection allows to get rid quite early of the actions with negative rewards. However, it may stick with the first positive action it founds, probably not the optimal one. The more actions you have, the more likely you will get stuck in a **suboptimal policy**.\n\n\n![Greedy action selection on a 10-armed bandit for 200 plays.](../slides/img/bandit-greedy.gif){width=80%}\n\n**$\\epsilon$-greedy action selection**\n\n$\\epsilon$-greedy action selection continues to explore after finding a good (but often suboptimal) action. It is not always able to recognize the optimal action (it depends on the variance of the rewards).\n\n![$\\epsilon$-greedy action selection on a 10-armed bandit for 200 plays.](../slides/img/bandit-egreedy.gif){width=80%}\n\n**Softmax action selection**\n\nSoftmax action selection explores more consistently the available actions. The estimated Q-values are much closer to the true values than with ($\\epsilon$-)greedy methods.\n\n\n![Softmax action selection on a 10-armed bandit for 200 plays.](../slides/img/bandit-softmax.gif){width=80%}\n\n**Comparison**\n\n![Average reward per step.](../slides/img/bandit-comparison1.png){width=60%}\n\n![Frequency of selection of the optimal action.](../slides/img/bandit-comparison2.png){width=60%}\n\nThe **greedy** method learns faster at the beginning, but get stuck in the long-term by choosing **suboptimal** actions (50\\% of trials). $\\epsilon$-greedy methods perform better on the long term, because they continue to explore. High values for $\\epsilon$ provide more exploration, hence find the optimal action earlier, but also tend to deselect it more often: with a limited number of plays, it may collect less reward than smaller values of $\\epsilon$.\n\n![Average reward per step.](../slides/img/bandit-comparison3.png){width=60%}\n\n![Frequency of selection of the optimal action.](../slides/img/bandit-comparison4.png){width=60%}\n\nThe softmax does not necessarily find a better solution than $\\epsilon$-greedy, but it tends to find it **faster** (depending on $\\epsilon$ or $\\tau$), as it does not lose time exploring obviously bad solutions. $\\epsilon$-greedy or softmax methods work best when the variance of rewards is high. If the variance is zero (always the same reward value), the greedy method would find the optimal action more rapidly: the agent only needs to try each action once.\n\n### Exploration schedule\n\nA useful technique to cope with the **exploration-exploitation dilemma** is to slowly decrease the value of $\\epsilon$ or $\\tau$ with the number of plays. This allows for more exploration at the beginning of learning and more exploitation towards the end. It is however hard to find the right **decay rate** for the exploration parameters. \n    \n![Exploration parameters such as $\\epsilon$ or $\\tau$ can be scheduled to allow more exploration at the beginning of learning.](../slides/img/bandit-scheduling.png){width=60%}\n\nThe performance is worse at the beginning, as the agent explores with a high temperature. But as the agent becomes greedier and greedier, the performance become more **optimal** than with a fixed temperature.\n\n\n![Average reward per step.](../slides/img/bandit-comparison5.png){width=60%}\n\n![Frequency of selection of the optimal action.](../slides/img/bandit-comparison6.png){width=60%}\n    \n    \n### Optimistic initial values\n\nThe problem with online evaluation is that it depends a lot on the initial estimates $Q_0$.\n\n* If the initial estimates are already quite good (expert knowledge), the Q-values will converge very fast.\n\n* If the initial estimates are very wrong, we will need a lot of updates to correctly estimate the true values.\n\n$$\n\\begin{aligned}\n    &Q_{t+1}(a) = (1 - \\alpha) \\, Q_t(a) + \\alpha \\, r_{t+1}  \\\\\n    &\\\\\n    & \\rightarrow Q_1(a) = (1 - \\alpha) \\, Q_0(a) + \\alpha \\, r_1 \\\\\n    & \\rightarrow Q_2(a) = (1 - \\alpha) \\, Q_1(a) + \\alpha \\, r_2 = (1- \\alpha)^2 \\, Q_0(a) + (1-\\alpha)\\alpha \\, r_1 + \\alpha r_2 \\\\\n\\end{aligned}\n$$\n\nThe influence of $Q_0$ on $Q_t$ **fades** quickly with $(1-\\alpha)^t$, but that can be lost time or lead to a suboptimal policy. However, we can use this at our advantage with **optimistic initialization**.\n\nBy choosing very high initial values for the estimates (they can only decrease), one can ensure that all possible actions will be selected during learning by the greedy method, solving the **exploration problem**. This leads however to an **overestimation** of the value of other actions.\n\n![Optimistic initialization.](../slides/img/bandit-optimistic.gif){width=80%}\n\n### Reinforcement comparison\n\n{{< youtube  C5xXGfgeDOk >}}\n    \nActions followed by large rewards should be made more likely to recur, whereas actions followed by small rewards should be made less likely to recur. But what is a large/small reward? Is a reward of 5 large or small? **Reinforcement comparison** methods only maintain a **preference** $p_t(a)$ for each action, which is not exactly its Q-value. The preference for an action is updated after each play, according to the update rule:\n\n$$\n    p_{t+1}(a_t) =    p_{t}(a_t) + \\beta \\, (r_t - \\tilde{r}_t)\n$$\n\nwhere $\\tilde{r}_t$ is the moving average of the recently received rewards (regardless the action):\n\n$$\n    \\tilde{r}_{t+1} =  \\tilde{r}_t + \\alpha \\, (r_t - \\tilde{r}_t)\n$$\n\n* If an action brings more reward than usual (**good surprise**), we increase the preference for that action.\n\n* If an action brings less reward than usual (**bad surprise**), we decrease the preference for that action.\n\n$\\beta > 0$ and $0 < \\alpha < 1$ are two constant parameters.\n\nThe preferences can be used to select the action using the softmax method just like the Q-values (without temperature):\n\n$$\n    \\pi_t (a) = \\frac{\\exp p_t(a)}{ \\sum_b \\exp p_t(b)}\n$$    \n\n\n![Reinforcement comparison.](../slides/img/bandit-reinforcementcomparison.gif){width=100%}\n\nReinforcement comparison can be very effective, as it does not rely only on the rewards received, but also on their comparison with a **baseline**, the average reward. This idea is at the core of **actor-critic** architectures which we will see later. The initial average reward $\\tilde{r}_{0}$ can be set optimistically to encourage exploration.\n\n![Average reward per step.](../slides/img/bandit-comparison7.png){width=60%}\n\n![Frequency of selection of the optimal action.](../slides/img/bandit-comparison8.png){width=60%}\n\n### Gradient bandit algorithm\n\nOne can even go further than reinforcement comparison: Instead of only increasing the preference for the executed action if it brings more usual than rewards, we could also decrease the preference for the other actions. The preferences are used to select an action $a_t$ *via* softmax:\n\n$$\n    \\pi_t (a) = \\frac{\\exp p_t(a)}{ \\sum_b \\exp p_t(b)}\n$$ \n\nThe update rule for the **action taken** $a_t$ is:\n\n$$\n    p_{t+1}(a_t) =    p_{t}(a_t) + \\beta \\, (r_t - \\tilde{r}_t) \\, (1 - \\pi_t(a_t))\n$$\n\nThe update rule for the **other actions** $a$ is:\n\n$$\n    p_{t+1}(a) =    p_{t}(a) - \\beta \\, (r_t - \\tilde{r}_t) \\, \\pi_t(a)\n$$\n\nThe reward **baseline** is updated with:\n\n$$\n    \\tilde{r}_{t+1} =  \\tilde{r}_t + \\alpha \\, (r_t - \\tilde{r}_t)\n$$\n\n\nThe preference can increase become quite high, making the policy greedy towards the end. No need for a temperature parameter!\n\n![Reinforcement comparison.](../slides/img/bandit-gradientbandit.gif){width=100%}\n\nGradient bandit is not always better than reinforcement comparison, but learns initially faster (depending on the parameters $\\alpha$ and $\\beta$).\n\n![Average reward per step.](../slides/img/bandit-comparison9.png){width=60%}\n\n![Frequency of selection of the optimal action.](../slides/img/bandit-comparison10.png){width=60%}\n\n\n### Upper-Confidence-Bound action selection\n    \nIn the previous methods, **exploration** is controlled by an external parameter ($\\epsilon$ or $\\tau$) which is **global** to each action an must be scheduled. A much better approach would be to decide whether to explore an action based on the **uncertainty** about its Q-value:\n\n* If we are certain about the value of an action, there is no need to explore it further, we only have to exploit it if it is good.\n\nThe **central limit theorem** tells us that the variance of a sampling estimator decreases with the number of samples:\n\n* The distribution of sample averages is normally distributed with mean $\\mu$ and variance $\\frac{\\sigma^2}{N}$.\n\n$$S_N \\sim \\mathcal{N}(\\mu, \\frac{\\sigma}{\\sqrt{N}})$$\n\nThe more you explore an action $a$, the smaller the variance of $Q_t(a)$, the more certain you are about the estimation, the less you need to explore it.\n\n**Upper-Confidence-Bound** (UCB) action selection is a **greedy** action selection method that uses an **exploration** bonus:\n\n$$\n    a^*_t = \\text{argmax}_{a} \\left( Q_t(a) + c \\, \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right)\n$$\n\n$Q_t(a)$ is the current estimated value of $a$ and $N_t(a)$ is the number of times the action $a$ has already been selected. It realizes a balance between trusting the estimates $Q_t(a)$ and exploring uncertain actions which have not been explored much yet.\n\nThe term $\\sqrt{\\frac{\\ln t}{N_t(a)}}$ is an estimate of the variance of $Q_t(a)$. The sum of both terms is an **upper-bound** of the true value $\\mu + \\sigma$. When an action has not been explored much yet, the uncertainty term will dominate and the action be explored, although its estimated value might be low. When an action has been sufficiently explored, the uncertainty term goes to 0 and we greedily follow $Q_t(a)$. The **exploration-exploitation** trade-off is automatically adjusted by counting visits to an action.\n\n\n![Upper-Confidence-Bound action selection.](../slides/img/bandit-ucb.gif){width=100%}\n\nThe \"smart\" exploration of UCB allows to find the optimal action faster.\n\n![Average reward per step.](../slides/img/bandit-comparison11.png){width=60%}\n\n![Frequency of selection of the optimal action.](../slides/img/bandit-comparison12.png){width=60%}\n\n### Summary of evaluative feedback methods\n    \nGreedy, $\\epsilon$-greedy, softmax, reinforcement comparison, gradient bandit and UCB all have their own advantages and disadvantages depending on the type of problem: stationary or not, high or low reward variance, etc...\n\nThese simple techniques are the most useful ones for bandit-like problems: more sophisticated ones exist, but they either make too restrictive assumptions, or are computationally intractable. \n\n**Take home messages:**\n\n1. RL tries to **estimate values** based on sampled rewards.\n2. One has to balance **exploitation and exploration** throughout learning with the right **action selection scheme**.\n3. Methods exploring more find **better policies**, but are initially slower.\n\n\n## Contextual bandits\n\n{{< youtube  hI9CzfE8xtA >}}\n\nIn contextual bandits, the obtained rewards do not only depend on the action $a$, but also on the **state** or **context** $s$:\n\n$$\n    r_{t+1} \\sim r(s, a)\n$$\n\nFor example, the n-armed bandit could deliver rewards with different probabilities depending on who plays, the time of the year or the availability of funds in the casino. The problem is simply to estimate $Q(s, a)$ instead of $Q(a)$...\n\n![Contextual bandits are an intermediary problem between bandits and the full RL setup, as rewards depend on the state, but the state is not influenced by the action. Source: <https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c>](../slides/img/bandit-hierarchy.png){width=90%}\n\nContextual bandits are for example useful for **Recommender systems**: actions are the display of an  advertisement, the context (or state) represents the user features / identity (possibly learned by an autoencoder), and the reward represents whether the user clicks on the ad or not.\n\n![Contextual bandits are used in recommender systems. Source: <https://aws.amazon.com/blogs/machine-learning/power-contextual-bandits-using-continual-learning-with-amazon-sagemaker-rl/>](../slides/img/contextualbandit.gif){width=40%}\n\nSome efficient algorithms have been developed recently, for example [@Agarwal2014], but we will not go into details in this course.\n\n    "},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"2.1-Bandits.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}