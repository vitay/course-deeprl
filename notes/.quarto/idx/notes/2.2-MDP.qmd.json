{"title":"Markov Decision Processes","markdown":{"headingText":"Markov Decision Processes","containsRefs":false,"markdown":"\nSlides: [html](../slides/2.2-MDP.html){target=\"_blank\"} [pdf](../slides/pdf/2.2-MDP.pdf){target=\"_blank\"}\n\n\n## From Finite State Machines to Markov Decision Process\n\n{{< youtube  u8_DgEuqo3g >}}\n\n\nThe kind of task that can be solved by RL is called a **Markov Decision Process** (MDP). For a MDP, the environment is **fully observable**, i.e. the current state $s_t$ completely characterizes the process at time $t$. **Actions** $a_t$ provoke transitions between two states $s_t$ and $s_{t+1}$, according to **transition probabilities**. A **reward**  $r_{t+1}$ is (probabilistically) associated to each transition.\n\n![Agent-environment interface for video games. Source: David Silver <https://www.davidsilver.uk/teaching/>](../slides/img/rl-loop.png){width=100%}\n\n::: {.callout-note}\nn-armed bandits are MDPs with only one state.\n:::\n\n### Finite State Machine (FSM)\n\nA **finite state machine** (or finite state automaton) is a mathematical model of computation. A FSM can only be in a single **state** $s$ at any given time. **Transitions** between states are governed by external inputs, when some condition is met.\n\n![Finite state machine. Source:  <https://web.stanford.edu/class/archive/cs/cs103/cs103.1142/button-fsm/>](../slides/img/fsm.png){width=80%}\n\nA FSM is fully defined by:\n\n* The **state set** $\\mathcal{S} = \\{ s_i\\}_{i=1}^N$.\n* Its initial state $S_0$.\n* A list of **conditions** for each transition.\n\nA FSM is usually implemented by a series of if/then/else statements:\n\n* if state == \"hover\" and press == true:\n    * state = \"pressed\"\n* elif ...\n\n### Markov Chain (MC)\n\n![Markov chain. Credit: David Silver <https://www.davidsilver.uk/teaching/>](../slides/img/student-markovchain.png){width=80%}\n\nA first-order **Markov Chain** (or Markov process) is a stochastic process generated by a FSM, where transitions between states are governed by **state transition probabilities**.\n\nA Markov Chain is defined by:\n\n* The **state set** $\\mathcal{S} = \\{ s_i\\}_{i=1}^N$.\n* The **state transition probability function**:\n\n$$\n\\begin{aligned}\n    \\mathcal{P}: \\mathcal{S} \\rightarrow & P(\\mathcal{S}) \\\\\n    p(s' | s) & =  P (s_{t+1} = s' | s_t = s) \\\\\n\\end{aligned}\n$$\n\n#### Markov property\n\nWhen the states have the **Markov property**, the state transition probabilities fully describe the MC. The Markov property states that:\n\n> The future is independent of the past given the present.\n\nFormally, the state $s_t$ (state at time $t$) is **Markov** (or Markovian) if and only if:\n\n$$\n    P(s_{t+1} | s_t) = P(s_{t+1} | s_t, s_{t-1}, \\ldots, s_0)\n$$\n\nThe knowledge of the current state $s_t$ is **enough** to predict in which state $s_{t+1}$ the system will be at the next time step. We do not need the whole **history** $\\{s_0, s_1, \\ldots, s_t\\}$ of the system to predict what will happen.\n\n:::{.callout-note}\nIf we need only $s_{t-1}$ and $s_t$ to predict $s_{t+1}$, we have a second-order Markov chain.\n:::\n\nFor example, the probability 0.8 of transitioning from \"Class 2\" to \"Class 3\" is the same regardless we were in \"Class 1\" or \"Pub\" before. If this is not the case, the states are not Markov, and this is not a Markov chain. We would need to create two distinct states:\n\n* \"Class 2 coming from Class 1\"\n* \"Class 2 coming from the pub\"\n\nSingle **video frames** are not Markov states: you cannot generally predict what will happen based on a single image.  A simple solution is to **stack** or **concatenate** multiple frames: By measuring the displacement of the ball between two consecutive frames, we can predict where it is going. One can also **learn** state representations containing the history using recurrent neural networks (see later).\n\n\n#### State transition matrix\n\nSupposing that the states have the Markov property, the transitions in the system can be summarized by the **state transition matrix** $\\mathcal{P}$:\n\n![State transition matrix. Credit: David Silver <https://www.davidsilver.uk/teaching/>](../slides/img/student-transitionmatrix.png){width=80%}\n\nEach element of the state transition matrix corresponds to $p(s' | s)$. Each row of the state transition matrix sums to 1:\n\n$$\\sum_{s'} p(s' | s)  = 1$$\n\nThe tuple $<\\mathcal{S}, \\mathcal{P}>$ fully describes the Markov chain.\n\n### Markov Reward Process (MRP)\n\nA **Markov Reward Process** is a Markov Chain where each transition is associated with a scalar **reward** $r$, coming from some probability distribution.\n\n![Markov Reward Process. Credit: David Silver <https://www.davidsilver.uk/teaching/>](../slides/img/student-mrp.png){width=80%}\n\nA Markov Reward Process is defined by the tuple $<\\mathcal{S}, \\mathcal{P}, \\mathcal{R}, \\gamma>$.\n\n* The finite **state set** $\\mathcal{S}$.\n* The **state transition probability function**:\n$$\n\\begin{aligned}\n    \\mathcal{P}: \\mathcal{S} \\rightarrow & P(\\mathcal{S}) \\\\\n    p(s' | s) & =  P (s_{t+1} = s' | s_t = s) \\\\\n\\end{aligned}\n$$\n* The **expected reward function**:\n$$\n\\begin{aligned}\n    \\mathcal{R}: \\mathcal{S} \\times \\mathcal{S} \\rightarrow & \\Re \\\\\n    r(s, s') &=  \\mathbb{E} (r_{t+1} | s_t = s, s_{t+1} = s') \\\\\n\\end{aligned}\n$$\n* The **discount factor** $\\gamma \\in [0, 1]$.\n\n\nAs with n-armed bandits, we only care about the **expected reward** received during a transition $s \\rightarrow s'$ (*on average*), but the actual reward received $r_{t+1}$ may vary around the expected value.\n\n$$r(s, s') =  \\mathbb{E} (r_{t+1} | s_t = s, s_{t+1} = s')$$\n\nThe main difference with n-armed bandits is that the MRP will be in a sequence of states (possibly infinite):\n\n$$s_0 \\rightarrow s_1 \\rightarrow s_2  \\rightarrow \\ldots \\rightarrow s_T$$\n\nand collect a sequence of reward samples:\n\n$$r_1 \\rightarrow r_2 \\rightarrow r_3  \\rightarrow \\ldots \\rightarrow r_{T}$$\n\nIn a MRP, we are interested in estimating the **return** $R_t$, i.e. the discounted sum of **future** rewards after the step $t$:\n\n$$\n    R_t = r_{t+1} + \\gamma \\, r_{t+2} + \\gamma^2 \\, r_{t+3} + \\ldots = \\sum_{k=0}^\\infty \\gamma^k \\, r_{t+k+1}\n$$  \n\nOf course, you never know the return at time $t$: transitions and rewards are probabilistic, so the received rewards in the future are not exactly predictable at $t$. $R_t$ is therefore purely theoretical: RL is all about **estimating** the return.\n\nThe **discount factor** (or discount rate, or discount) $\\gamma \\in [0, 1]$ is a very important parameter in RL: It defines the **present value of future rewards**. Receiving 10 euros now has a higher **value** than receiving 10 euros in ten years, although the reward is the same: you do not have to wait.\n\nThe value of receiving a reward $r$ after $k+1$ time steps is $\\gamma^k \\, r$. **Immediate rewards** are better than **delayed rewards**. When $\\gamma < 1$, $\\gamma^k$ tends to 0 when $k$ goes to infinity: this makes sure that the return is always **finite**. This is particularly important when the MRP is cyclic / periodic. If all sequences terminate at some time step $T$, we can set $\\gamma= 1$.\n\n\n\n### Markov Decision Process (MDP)\n\nA **Markov Decision Process** is a MRP where transitions are influenced by **actions** $a \\in \\mathcal{A}$.\n\n![Markov decision process. Credit: David Silver <https://www.davidsilver.uk/teaching/>](../slides/img/student-mdp.png){width=80%}\n\nA finite MDP is defined by the tuple $<\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma>$:\n\n* The finite **state set** $\\mathcal{S}$.\n* The finite **action set** $\\mathcal{A}$.\n* The **state transition probability function**:\n\n$$\n\\begin{aligned}\n    \\mathcal{P}: \\mathcal{S} \\times \\mathcal{A} \\rightarrow & P(\\mathcal{S}) \\\\\n    p(s' | s, a) & =  P (s_{t+1} = s' | s_t = s, a_t = a) \\\\\n\\end{aligned}\n$$\n\n* The **expected reward function**:\n\n$$\n\\begin{aligned}\n    \\mathcal{R}: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow & \\Re \\\\\n    r(s, a, s') &=  \\mathbb{E} (r_{t+1} | s_t = s, a_t = a, s_{t+1} = s') \\\\\n\\end{aligned}\n$$\n\n* The **discount factor** $\\gamma \\in [0, 1]$.\n\nWhy do we need transition probabilities in RL?\n\n$$ p(s' | s, a) =  P (s_{t+1} = s' | s_t = s, a_t = a)$$\n\nSome RL tasks are **deterministic**: an action $a$ in a state $s$ always leads to the state $s'$ (board games, video games...). Others are **stochastic**: the same action $a$ can lead to different states $s'$: Casino games (throwing a dice, etc), two-opponent games (the next state depends on what the other player chooses), uncertainty (shoot at basketball, slippery wheels, robotic grasping)...\n\nFor a transition $(s, a, s')$, the received reward can be also stochastic: casino games (armed bandit), incomplete information, etc. Most of the problems we will see in this course have deterministic rewards, but we only care about expectations anyway.\n\n#### Markov property\n\nThe state of the agent at step $t$ refers to whatever information is available about its environment or its own \"body\". The state can include immediate \"sensations\", highly processed sensations, and structures built up over time from sequences of sensations. A state should summarize all past sensations so as to retain all essential information, i.e. it should have the **Markov Property**:\n\n$$\\begin{aligned}\n     P( s_{t+1} = s, r_{t+1} = r & | s_t, a_t, r_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = P( s_{t+1} = s, r_{t+1} = r | s_t, a_t ) \\\\\n     &\\text{for all s, r, and past histories} \\quad (s_{t}, a_{t}, ..., s_0, a_0)\n\\end{aligned}\n$$\n\nThis means that the current state representation $s$ contains enough information to predict the probability of arriving in the next state $s'$ given the chosen action $a$. When the Markovian property is not met, we have a **Partially-Observable Markov Decision Process** (POMDP).\n\n\n#### Returns\n\nSuppose the sequence of rewards obtained after step $t$ (after being in state $s_t$ and choosing action $a_t$) is:\n\n$$ r_{t+1}, r_{t+2}, r_{t+3}, ... $$\n\nWhat we want to maximize is the **return** (reward-to-go) at each time step $t$, i.e. the sum of all future rewards:\n\n$$ \n    R_t = r_{t+1} + \\gamma \\, r_{t+2} +  \\gamma^2 \\, r_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k \\, r_{t+k+1}\n$$\n\nMore generally, for a trajectory (episode) $\\tau = (s_0, a_0, r_1, s_1, a_1, \\ldots, s_T)$, one can define its return as:\n\n$$ R(\\tau) = \\sum_{t=0}^{T} \\gamma^t \\, r_{t+1} $$\n\nFor **episodic tasks** (which break naturally into finite episodes of length $T$, e.g. plays of a game, trips through a maze), the return is always finite and easy to compute at the end of the episode. The discount factor can be set to 1:\n\n$$ \n    R_t = \\sum_{k=0}^{T} r_{t+k+1}\n$$\n\nFor **continuing tasks** (which can not be split into episodes), the return could become infinite if $\\gamma = 1$. The discount factor has to be smaller than 1.\n\n$$ \n    R_t = \\sum_{k=0}^{\\infty} \\gamma^k \\, r_{t+k+1}\n$$\n\nThe discount rate $\\gamma$ determines the relative importance of future rewards for the behavior:\n\n* if $\\gamma$ is close to 0, only the immediately available rewards will count: the agent is greedy or **myopic**.\n* if $\\gamma$ is close to 1, even far-distance rewards will be taken into account: the agent is **farsighted**.\n\n\n::: {.callout-note}\n## Why the reward on the long term?\n\n![](../slides/img/return-example.svg)\n\nSelecting the action $a_1$ in $s_1$ does not bring reward immediately ($r_1 = 0$) but allows to reach $s_5$ in the future and get a reward of 10. Selecting $a_2$ in $s_1$ brings immediately a reward of 1, but that will be all. $a_1$ is **better** than $a_2$, because it will bring more reward **on the long term**.\n\nWhen selecting $a_1$ in $s_1$, the discounted return is:\n\n$$\n    R = 0 + \\gamma \\, 0 + \\gamma^2 \\, 0 + \\gamma^3 \\, 10 + \\ldots = 10 \\, \\gamma^3\n$$\n\nwhile it is $R= 1$ for the action $a_2$.\n\nFor small values of $\\gamma$ (e.g. 0.1), $10\\, \\gamma^3$ becomes smaller than one, so the action $a_2$ leads to a higher discounted return. The discount rate $\\gamma$ changes the behavior of the agent. It is usually taken somewhere between 0.9 and 0.999.\n:::\n\n####  Example: the cartpole balancing task\n\n![Cartpole balancing task](../slides/img/cartpole-after.gif){width=60%}\n\n* **State:** Position and velocity of the cart, angle and speed of the pole.\n* **Actions:** Commands to the motors for going left or right.\n* **Reward function:** Depends on whether we consider the task as episodic or continuing.\n\nThe problem can be viewed both as an episodic or continuing task:\n\n* **Episodic** task where episode ends upon failure:\n    * **reward** = +1 for every step before failure, 0 at failure.\n    * **return** = number of steps before failure.\n* **Continuing** task with discounted return:\n    * **reward** = -1 at failure, 0 otherwise.\n    * **return** = $- \\gamma^k$ for $k$ steps before failure.\n\nIn both cases, the goal is to maximize the return by maintaining the pole vertical as long as possible.\n\n#### Example: the recycling robot\n\n![Recycling robot. Source: [@Sutton1998]](../slides/img/recyclingrobot.png){width=60%}\n\nAt each step, the recycling robot has to decide whether it should:\n\n1. actively search for a can,\n2. wait for someone to bring it a can, or\n3. go to home base and recharge.\n\nSearching is better (more reward) but runs down the battery (probability 1-$\\alpha$ to empty the battery): if the robot runs out of power while searching, he has to be rescued (which leads to punishment and should be avoided). Decisions must be made on basis of the current energy level: high, low. This will be the state of the robot. The return is the number of cans collected on the long term.\n\n* $\\mathcal{S} = \\{ \\text{high}, \\text{low} \\}$\n* $\\mathcal{A}(\\text{high} ) = \\{ \\text{search}, \\text{wait} \\}$\n* $\\mathcal{A}(\\text{low} ) = \\{ \\text{search}, \\text{wait}, \\text{recharge} \\}$\n* $R^{\\text{search}}$ = expected number of cans while searching.\n* $R^{\\text{wait}}$ = expected number of cans while waiting.\n* $R^{\\text{search}} > R^{\\text{wait}}$\n\nThe MDP is fully described by the following table:\n\n\n\n| $s$           | $s'$          | $a$          | $p(s' / s, a)$    | $r(s, a, s')$                 |\n|:-------------:|:-------------:|:------------:|:-----------------:|:-----------------------------:|\n| high          | high          | search       | $\\alpha$          | $\\mathcal{R}^\\text{search}$   |\n| high          | low           | search       | $1 - \\alpha$      | $\\mathcal{R}^\\text{search}$   |\n| low           | high          | search       | $1 - \\beta$       | $-3$                          |\n| low           | low           | search       | $\\beta$           | $\\mathcal{R}^\\text{search}$   |\n| high          | high          | wait         | $1$               | $\\mathcal{R}^\\text{wait}$     |\n| high          | low           | wait         | $0$               | $\\mathcal{R}^\\text{wait}$     |\n| low           | high          | wait         | $0$               | $\\mathcal{R}^\\text{wait}$     |\n| low           | low           | wait         | $1$               | $\\mathcal{R}^\\text{wait}$     |\n| low           | high          | recharge     | $1$               | $0$                           |\n| low           | low           | recharge     | $0$               | $0$                           |\n\n\n#### The policy\n\nThe probability that an agent selects a particular action $a$ in a given state $s$ is called the **policy** $\\pi$.\n\n$$\n\\begin{align}\n    \\pi &: \\mathcal{S} \\times \\mathcal{A} \\rightarrow P(\\mathcal{S})\\\\\n    (s, a) &\\rightarrow \\pi(s, a)  = P(a_t = a | s_t = s) \\\\\n\\end{align}\n$$\n\nThe policy can be **deterministic** (one action has a probability of 1, the others 0) or **stochastic**. The goal of an agent is to find a policy that maximizes the sum of received rewards **on the long term**, i.e. the **return** $R_t$ at each each time step. This policy is called the **optimal policy** $\\pi^*$.\n\n$$\n    \\mathcal{J}(\\pi) = \\mathbb{E}_{\\rho_\\pi} [R_t] \\qquad\n    \\pi^* = \\text{argmax} \\, \\mathcal{J}(\\pi)\n$$\n\n#### Goal of Reinforcement Learning\n\nRL is an **adaptive optimal control** method for Markov Decision Processes using (sparse) rewards as a partial feedback. At each time step $t$, the agent observes its Markov state $s_t \\in \\mathcal{S}$, produces an action $a_t \\in \\mathcal{A}(s_t)$, receives a reward according to this action $r_{t+1} \\in \\Re$ and updates its state: $s_{t+1} \\in \\mathcal{S}$.\n\nThe agent generates trajectories $\\tau = (s_0, a_0, r_1, s_1, a_1, \\ldots, s_T)$ depending on its policy $\\pi(s ,a)$.\n\nThe return of a trajectory is the (discounted) sum of rewards accumulated during the sequence:\n\n$$ R(\\tau) = \\sum_{t=0}^{T} \\gamma^t \\, r_{t+1} $$\n\nThe goal is to find the **optimal policy** $\\pi^* (s, a)$ that maximizes in expectation the return of each possible trajectory under that policy:\n\n$$\n    \\mathcal{J}(\\pi) = \\mathbb{E}_{\\tau \\sim \\rho_\\pi} [R(\\tau)] \\qquad\n    \\pi^* = \\text{argmax} \\, \\mathcal{J}(\\pi)\n$$\n\n\n## Bellman equations\n\n{{< youtube  1Z5sMSCEMRo >}}\n\n### Value Functions\n\nA central notion in RL is to estimate the **value** (or **utility**) of every state and action of the MDP. The value of a state $V^{\\pi} (s)$ is the expected return when starting from that state and thereafter following the agent’s current policy $\\pi$. \n\nThe **state-value function** $V^{\\pi} (s)$ of a state $s$ given the policy $\\pi$ is defined as the mathematical expectation of the return after that state:\n\n$$  V^{\\pi} (s) = \\mathbb{E}_{\\rho_\\pi} ( R_t | s_t = s) = \\mathbb{E}_{\\rho_\\pi} ( \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} |s_t=s ) $$\n\n\nThe mathematical expectation operator $\\mathbb{E}(\\cdot)$ is indexed by $\\rho_\\pi$, the probability distribution of states achievable with $\\pi$.\n\nSeveral trajectories are possible after the state $s$:\n\n* The **state transition probability function** $p(s' | s, a)$ leads to different states $s'$, even if the same actions are taken.\n* The **expected reward function** $r(s, a, s')$ provides stochastic rewards, even if the transition $(s, a, s')$ is the same.\n* The **policy** $\\pi$ itself is stochastic.\n\nOnly rewards that are obtained using the policy $\\pi$ should be taken into account, not the complete distribution of states and rewards.\n\nThe value of a state is not intrinsic to the state itself, it depends on the policy: One could be in a state which is very close to the goal (only one action left to win game), but if the policy is very bad, the \"good\" action will not be chosen and the state will have a small value.\n\nThe value of taking an action $a$ in a state $s$ under policy $\\pi$ is the expected return starting from that state, taking that action, and thereafter following the following $\\pi$. The **action-value function** for a state-action pair $(s, a)$ under the policy $\\pi$ (or **Q-value**) is defined as:\n\n$$\n\\begin{align}\n    Q^{\\pi} (s, a)  & = \\mathbb{E}_{\\rho_\\pi} ( R_t | s_t = s, a_t =a) \\\\\n                    & = \\mathbb{E}_{\\rho_\\pi} ( \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} |s_t=s, a_t=a) \\\\\n\\end{align}\n$$\n\nState- and action-value functions are linked to each other. The value of a state $V^{\\pi}(s)$ depends on the value $Q^{\\pi} (s, a)$ of the action that will be chosen by the policy $\\pi$ in $s$:\n\n$$\n    V^{\\pi}(s) = \\mathbb{E}_{a \\sim \\pi(s,a)} [Q^{\\pi} (s, a)] = \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, Q^{\\pi} (s, a)\n$$\n\nIf the policy $\\pi$ is deterministic (the same action is chosen every time), the value of the state is the same as the value of that action (same expected return). If the policy $\\pi$ is stochastic (actions are chosen with different probabilities), the value of the state is the expectation (weighted average) of the value of the actions. If the Q-values are known, the V-values can be found easily.\n\n\nWe can note that the return at time $t$ depends on the **immediate reward** $r_{t+1}$ and the return at the next time step $t+1$:\n\n$$\n\\begin{aligned}\n    R_t &= r_{t+1} + \\gamma \\, r_{t+2} +  \\gamma^2  \\, r_{t+3} + \\dots + \\gamma^k \\, r_{t+k+1} + \\dots \\\\\n        &= r_{t+1} + \\gamma \\, ( r_{t+2} +  \\gamma \\, r_{t+3} + \\dots + \\gamma^{k-1} \\, r_{t+k+1} + \\dots) \\\\\n        &= r_{t+1} + \\gamma \\,  R_{t+1} \\\\\n\\end{aligned}\n$$\n\nWhen taking the mathematical expectation of that identity, we obtain:\n\n$$\n    \\mathbb{E}_{\\rho_\\pi}[R_t] = r(s_t, a_t, s_{t+1}) + \\gamma \\, \\mathbb{E}_{\\rho_\\pi}[R_{t+1}]\n$$\n\nIt becomes clear that the value of an action depends on the immediate reward received just after the action, as well as the value of the next state:\n\n$$\n        Q^{\\pi}(s_t, a_t) = r(s_t, a_t, s_{t+1}) + \\gamma \\,  V^{\\pi} (s_{t+1})\n$$\n\nBut that is only for a fixed $(s_t, a_t, s_{t+1})$ transition. Taking transition probabilities into account, one can obtain the Q-values when the V-values are known:\n\n$$\n        Q^{\\pi}(s, a) = \\mathbb{E}_{s' \\sim p(s'|s, a)} [ r(s, a, s') + \\gamma \\, V^{\\pi} (s') ] = \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{\\pi} (s') ]\n$$\n\nThe value of an action depends on:\n\n* the states $s'$ one can arrive after the action (with a probability $p(s' | s, a)$).\n* the value of that state $V^{\\pi} (s')$, weighted by $\\gamma$ as it is one step in the future.\n* the reward received immediately after taking that action $r(s, a, s')$ (as it is not included in the value of $s'$).\n\n\n### Bellman equations\n\nA fundamental property of value functions used throughout reinforcement learning is that they satisfy a  particular recursive relationship:\n\n$$\n\\begin{aligned}\n        V^{\\pi}(s)  &= \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, Q^{\\pi} (s, a)\\\\\n                    &= \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{\\pi} (s') ]\n\\end{aligned}\n$$\n\nThis equation is called the **Bellman equation** for $V^{\\pi}$. It expresses the relationship between the value of a state and the value of its successors, depending on the dynamics of the MDP ($p(s' | s, a)$ and $r(s, a, s')$) and the current policy $\\pi$. The interesting property of the Bellman equation for RL is that it admits one and only one solution $V^{\\pi}(s)$.\n\nThe same recursive relationship stands for $Q^{\\pi}(s, a)$:\n\n$$\n\\begin{aligned}\n        Q^{\\pi}(s, a)  &= \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{\\pi} (s') ] \\\\\n                    &=  \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, \\sum_{a' \\in \\mathcal{A}(s')} \\pi(s', a') \\, Q^{\\pi} (s', a')]\n\\end{aligned}\n$$\n\nwhich is called the **Bellman equation** for $Q^{\\pi}$.\n\nThe following **backup diagrams** denote these recursive relationships.\n\n![Backup diagrams of the Bellman equations. Left: the value of a state $s$ depends on the policy $\\pi$ and the value of the succeeding states $s'$. Right: the value of an action depends on the value of the action that can be taken in the next states. Source: [@Sutton1998]](../slides/img/backup-V.png){width=80%}\n\n## Bellman optimality equations\n\n{{< youtube  mFucN5K351A >}}\n\n### Optimal policy\n\nThe optimal policy is the policy that gathers the maximum of reward on the long term. Value functions define a partial ordering over policies:\n\n* a policy $\\pi$ is better than another policy $\\pi'$ if its expected return is greater or equal than that of $\\pi'$ for all states $s$.\n\n$$\n        \\pi \\geq \\pi' \\Leftrightarrow V^{\\pi}(s) \\geq V^{\\pi'}(s) \\quad \\forall s \\in \\mathcal{S}\n$$\n\nThere exists at least one policy that is better than all the others: this is the **optimal policy** $\\pi^*$. We note $V^*(s)$ and $Q^*(s, a)$ the optimal value of the different states and actions under $\\pi^*$.\n\n$$\n   V^* (s) = \\max_{\\pi} V^{\\pi}(s) \\quad \\forall s \\in \\mathcal{S}\n$$\n\n$$\n    Q^* (s, a) = \\max_{\\pi} Q^{\\pi}(s, a) \\quad \\forall s \\in \\mathcal{S}, \\quad \\forall a \\in \\mathcal{A}\n$$\n\nWhen the policy is optimal $\\pi^*$, the link between the V and Q values is even easier. The V and Q values are maximal for the optimal policy: there is no better alternative.\n\n\n![Backup diagrams of the Bellman optimality equations. Source: [@Sutton1998]](../slides/img/fullvi.png){width=40%}\n\n**The optimal action $a^*$ to perform in the state $s$ is the one with the highest optimal Q-value $Q^*(s, a)$**.\n\n$$\n    a^* = \\text{argmax}_a \\, Q^*(s, a)\n$$\n\nBy definition, this action will bring the maximal return when starting in $s$.\n\n$$\n    Q^*(s, a) = \\mathbb{E}_{\\rho_{\\pi^*}} [R_t]\n$$\n\nThe optimal policy is **greedy** with respect to $Q^*(s, a)$, i.e. **deterministic**.\n\n$$\n    \\pi^*(s, a) = \\begin{cases}\n                1 \\; \\text{if} \\; a = a^* \\\\\n                0 \\; \\text{otherwise.}\n                \\end{cases}\n$$\n\n\n### Bellman optimality equations\n\n\nAs the optimal policy is deterministic, the optimal value of a state is equal to the value of the optimal action:\n\n$$\n    V^* (s)  = \\max_{a \\in \\mathcal{A}(s)} Q^{\\pi^*} (s, a)\n$$\n\n\nThe expected return after being in $s$ is the same as the expected return after being in $s$ and choosing the optimal action $a^*$, as this is the only action that can be taken. This allows to find the **Bellman optimality equation** for $V^*$:\n\n$$\n    V^* (s)  = \\max_{a \\in \\mathcal{A}(s)} \\sum_{s' \\in \\mathcal{S}}  p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{*} (s') ]\n$$\n\nThe same Bellman optimality equation stands for $Q^*$:\n\n$$\n    Q^* (s, a)  = \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s')  + \\gamma \\max_{a' \\in \\mathcal{A}(s')} Q^* (s', a') ]\n$$\n\nThe optimal value of $(s, a)$ depends on the optimal action in the next state $s'$.\n\nThe Bellman optimality equations for $V^*$ form a system of equations:\n\n* If there are $N$ states $s$, there are $N$ Bellman equations with $N$ unknowns $V^*(s)$.\n\n$$\n    V^* (s)  = \\max_{a \\in \\mathcal{A}(s)} \\sum_{s' \\in \\mathcal{S}}  p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{*} (s') ]\n$$\n\nIf the dynamics of the environment are known ($p(s' | s, a)$ and $r(s, a, s')$), then in principle one can solve this system of equations using linear algebra. For finite MDPs, the Bellman optimality equation for $V^*$ has a unique solution (one and only one): This is the principle of **dynamic programming**.\n\nThe same is true for the Bellman optimality equation for $Q^*$: If there are $N$ states and $M$ actions available, there are $N\\times M$ equations with $N\\times M$ unknowns $Q^*(s, a)$.\n\n$$\n    Q^* (s, a)  = \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s')  + \\gamma \\max_{a' \\in \\mathcal{A}(s')} Q^* (s', a') ]\n$$\n\n\n$V^*$ and $Q^*$ are interdependent: one needs only to compute one of them.\n\n$$V^* (s)  = \\max_{a \\in \\mathcal{A}(s)} \\, Q^{*} (s, a)$$\n\n$$Q^* (s, a)  = \\sum_{s' \\in \\mathcal{S}} \\, p(s' | s, a) \\, [r(s, a, s') + \\gamma V^*(s') ] $$\n\nIf you only have $V^*(s)$, you need to perform a **one-step-ahead** search using the dynamics of the MDP:\n\n$$\n    Q^* (s, a)  = \\sum_{s' \\in \\mathcal{S}} \\, p(s' | s, a) \\, [r(s, a, s') + \\gamma V^*(s') ]\n$$\n\nand then select the optimal action with the highest $Q^*$-value. Using the $V^*(s)$ values is called **model-based**: you need to know the model of the environment to act, at least locally.\n\n\nIf you have all $Q^*(s, a)$, the optimal policy is straightforward:\n\n$$\n    \\pi^*(s, a) = \\begin{cases}\n                1 \\; \\text{if} \\; a = \\text{argmax}_a \\, Q^*(s, a) \\\\\n                0 \\; \\text{otherwise.}\n                \\end{cases}\n$$\n\n\nFinding $Q^*$ makes the selection of optimal actions easy:\n\n* no need to iterate over all actions and to know the dynamics $p(s' | s, a)$ and $r(s, a, s')$.\n* for any state $s$, it can simply find the action that maximizes $Q^*(s,a)$.\n\nThe action-value function effectively **caches** the results of all one-step-ahead searches into a single value: **model-free**. At the cost of representing a function of all state-action pairs, the optimal action-value function allows optimal actions to be selected without having to know anything about the environment's dynamics. But there are $N \\times M$ equations to solve instead of just $N$... \n\nFinding an optimal policy by solving the **Bellman optimality equations** requires the following:\n\n- accurate knowledge of environment dynamics $p(s' | s, a)$ and $r(s, a, s')$ for all transitions;\n\n- enough memory and time to do the computations;\n\n- the Markov property.\n\nHow much space and time do we need? A solution requires an exhaustive search, looking ahead at all possibilities, computing their probabilities of occurrence and their desirability in terms of expected rewards. The number of states is often huge or astronomical (e.g., Go has about $10^{170}$ states). **Dynamic programming** solves exactly the Bellman equations; **Monte-Carlo** and **temporal-difference** methods approximate them.\n\n\n\n## Generalized Policy Iteration\n\n\n![Generalized Policy Iteration. Source: [@Sutton1998]](../slides/img/gpi-scheme.png){width=30%}\n\nRL algorithms iterate over two steps:\n\n1. **Policy evaluation**\n    * For a given policy $\\pi$, the value of all states $V^\\pi(s)$ or all state-action pairs $Q^\\pi(s, a)$ is calculated, either based on:\n        * the Bellman equations (Dynamic Programming)\n        * sampled experience (Monte-Carlo and Temporal Difference)\n2. **Policy improvement**\n    * From the current estimated values $V^\\pi(s)$ or $Q^\\pi(s, a)$, a new **better** policy $\\pi$ is derived.\n\n\nAfter enough iterations, the policy converges to the **optimal policy** (if the states are Markov).\n\n::: {.callout-note}\n## Different notations in RL\n\nNotations can vary depending on the source. The ones used in this course use what you can read in most modern deep RL papers (Deepmind, OpenAI), but beware that you can encounter $G_t$ for the return...\n\n|          | **This course**          | **Sutton and Barto 1998**          | **Sutton and Barto 2017**   |\n|:-------------:|:-------------:|:------------:|:-----------------:\n| Current state | $s_t$ | $s_t$ | $S_t$ |\n| Selected action | $a_t$ | $a_t$ | $A_t$ |\n| Sampled reward | $r_{t+1}$ | $r_{t+1}$ | $R_{t+1}$ |\n| Transition probability    | $p(s' / s,a)$          | $\\mathcal{P}_{ss'}^a$      | $p(s'/s, a)$          |\n| Expected reward    | $r(s,a, s')$          | $\\mathcal{R}_{ss'}^a$      | $r(s, a, s')$          | \n| Return    | $R_t$          | $R_t$      | $G_t$          | \n| State value function    | $V^\\pi(s)$          | $V^\\pi(s)$      | $v_\\pi(s)$          | \n| Action value function    | $Q^\\pi(s, a)$          | $Q^\\pi(s, a)$      | $q_\\pi(s, a)$          | \n\n:::\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"2.2-MDP.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}