{"title":"Monte-Carlo (MC) methods","markdown":{"headingText":"Monte-Carlo (MC) methods","containsRefs":false,"markdown":"\nSlides: [html](../slides/2.4-MC.html){target=\"_blank\"} [pdf](../slides/pdf/2.4-MC.pdf){target=\"_blank\"}\n\n## Monte-Carlo policy evaluation\n\n{{< youtube  XD1h9iMxCTQ >}}\n\nThe value of a state $s$ is defined as the mathematical expectation of the return obtained after that state and thereafter following the policy $\\pi$:\n\n$$V^{\\pi} (s) = \\mathbb{E}_{\\rho_\\pi} ( R_t | s_t = s) = \\mathbb{E}_{\\rho_\\pi} ( \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} | s_t = s )$$\n\n\n**Monte-Carlo methods** (MC) approximate this mathematical expectation by **sampling** $M$ trajectories $\\tau_i$ starting from $s$ and computing the sampling average of the obtained returns:\n\n$$\n    V^{\\pi}(s) = \\mathbb{E}_{\\rho_\\pi} (R_t | s_t = s) \\approx \\frac{1}{M} \\sum_{i=1}^M R(\\tau_i)\n$$\n\nIf you have enough trajectories, the sampling average is an unbiased estimator of the value function. The advantage of Monte-Carlo methods is that they require only **experience**, not the complete dynamics $p(s' | s,a)$ and $r(s, a, s')$.\n\nThe idea of MC policy evaluation is to repeatedly sample **episodes** starting from each possible state $s_0$ and maintain a **running average** of the obtained returns for each state:\n\n::: {.callout-tip}\n## Monte-Carlo policy evaluation of state values\n\n* **while** True:\n    \n    * Start from an initial state $s_0$.\n    \n    * Generate a sequence of transitions according to the current policy $\\pi$ until a terminal state $s_T$ is reached.\n\n    $$\n        \\tau = (s_o, a_o, r_ 1, s_1, a_1, \\ldots, s_T)\n    $$\n\n    * Compute the return $R_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$ for all encountered states $s_0, s_1, \\ldots, s_T$.\n\n    * Update the estimated state value $V(s_t)$ of all encountered states using the obtained return:\n\n    $$\n        V(s_t) \\leftarrow V(s_t) + \\alpha \\, (R_t - V(s_t))\n    $$\n:::\n\n\nThe same method can be used to estimate Q-values.\n\n::: {.callout-tip}\n## Online Monte-Carlo policy evaluation of action values\n\n* **while** True:\n\n    * Start from an initial state $s_0$.\n\n    * Generate a sequence of transitions according to the current policy $\\pi$ until a terminal state $s_T$ is reached.\n\n    $$\n        \\tau = (s_o, a_o, r_ 1, s_1, a_1, \\ldots, s_T)\n    $$\n\n    * Compute the return $R_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$ for all encountered state-action pairs $(s_0, a_0), (s_1, a_1), \\ldots, (s_{T-1}, a_{T-1})$.\n\n    * Update the estimated action value $Q(s_t, a_t)$ of all encountered state-action pairs using the obtained return:\n\n    $$\n        Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\, (R_t - Q(s_t, a_t))\n    $$\n:::\n\nThere are much more values to estimate (one per state-action pair), but the policy will be easier to derive.\n\n## Monte-Carlo policy improvement\n\nAfter each episode, the state or action values of the visited $(s, a)$ pairs have changed, so the current policy might not be optimal anymore. As in DP, the policy can then be improved in a greedy manner:\n\n$$\\begin{aligned}\n        \\pi'(s) & = \\text{argmax}_a Q(s, a)\\\\\n        &\\\\\n        & = \\text{argmax}_a \\sum_{s' \\in \\mathcal{S}} p(s'|s, a) \\, [r(s, a, s') + \\gamma \\, V(s')] \\\\\n\\end{aligned}\n$$\n\nEstimating the Q-values allows to act greedily, while estimating the V-values still requires the dynamics  $p(s' | s,a)$ and $r(s, a, s')$.\n\n## On-policy Monte-Carlo control\n\n**Monte-Carlo control** alternates between **MC policy evaluation** and **policy improvement** until the optimal policy is found: generalized policy iteration (GPI).\n\n::: {.callout-tip}\n## Principle of Monte-Carlo control\n\n* **while** True:\n\n    * Select an initial state $s_0$.\n\n    * Generate a sequence of transitions according to the current policy $\\pi$ until a terminal state $s_T$ is reached.\n\n    $$\n        \\tau = (s_o, a_o, r_ 1, s_1, a_1, \\ldots, s_T)\n    $$\n\n    * Compute the return $R_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$ of all encountered state-action pairs.\n\n    * Update the estimated action value $Q_k(s_t, a_t)$ of all encountered state-action pairs:\n\n    $$\n        Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\, (R_t - Q(s_t, a_t))\n    $$\n\n    * For each state $s_t$ in the episode, **improve** the policy greedily:\n\n    $$\n        \\pi(s_t, a) = \\begin{cases}\n                        1\\; \\text{if} \\; a = \\text{argmax} \\, Q(s_t, a) \\\\\n                        0 \\; \\text{otherwise.} \\\\\n                      \\end{cases}\n    $$\n:::\n\nThe problem with MC control is that we need a policy to generate the sample episodes, but it is that policy that we want to learn. We have the same **exploration/exploitation** problem as in bandits:\n\n* If I trust my estimates too much (**exploitation**), I may miss more interesting solutions by keeping generating the same episodes.\n* If I act randomly (**exploration**), I will find more interesting solutions, but I won't keep doing them.\n\n**Exploitation** is using the current estimated values to select the greedy action: The estimated values represent how good we think an action is, so we have to use this value to update the policy.\n\n**Exploration** is executing non-greedy actions to try to reduce our uncertainty about the true values: The values are only estimates: they may be wrong so we can not trust them completely.\n\nIf you only **exploit** your estimates, you may miss interesting solutions. If you only **explore**, you do not use what you know: you act randomly and do not obtain as much reward as you could. **You can’t exploit all the time; you can’t explore all the time.** You can never stop exploring; but you can reduce it if your performance is good enough.\n\nAn easy solution to ensure exploration is to assume **exploring starts**, where every state-action pair has a non-zero probability to be selected as the start of an episode.\n\nExploration can be ensured by forcing the learned policy to be **stochastic**, aka **$\\epsilon$-soft**.\n\n\n* **$\\epsilon$-Greedy action selection** randomly selects non-greedy actions with a small probability $\\epsilon$:\n\n$$\n    \\pi(s, a) = \\begin{cases} 1 - \\epsilon \\; \\text{if} \\; a = \\text{argmax}\\, Q(s, a) \\\\ \\frac{\\epsilon}{|\\mathcal{A}| - 1} \\; \\text{otherwise.} \\end{cases}\n$$\n\n\n* **Softmax action selection** uses a Gibbs (or Boltzmann) distribution to represent the probability of choosing the action $a$ in state $s$:\n\n$$\n    \\pi(s, a) = \\frac{\\exp Q(s, a) / \\tau}{ \\sum_b \\exp Q(s, b) / \\tau}\n$$ \n\n$\\epsilon$-greedy choses non-greedy actions randomly, while softmax favors the best alternatives.\n\nIn **on-policy** control methods, the learned policy has to be $\\epsilon$-soft, which means all actions have a probability of at least $\\frac{\\epsilon}{|\\mathcal{A}|}$ to be visited. $\\epsilon$-greedy and softmax policies meet this criteria. Each sample episode is generated using this policy, which ensures exploration, while the control method still converges towards the optimal $\\epsilon$-policy.\n\n::: {.callout-tip}\n## On-policy Monte-Carlo control\n\n* **while** True:\n\n    * Generate an episode $\\tau = (s_0, a_0, r_1, \\ldots, s_T)$ using the current **stochastic** policy $\\pi$.\n\n    * For each state-action pair $(s_t, a_t)$ in the episode, update the estimated Q-value:\n\n    $$\n        Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\, (R_t - Q(s_t, a_t))\n    $$\n\n    * For each state $s_t$ in the episode, improve the policy (e.g. $\\epsilon$-greedy):\n\n    $$\n        \\pi(s_t, a) = \\begin{cases}\n                        1 - \\epsilon \\; \\text{if} \\; a = \\text{argmax}\\, Q(s, a) \\\\\n                        \\frac{\\epsilon}{|\\mathcal{A(s_t)}-1|} \\; \\text{otherwise.} \\\\\n                      \\end{cases}\n    $$\n:::\n\nAnother option to ensure exploration is to generate the sample episodes using a policy $b(s, a)$ different from the current policy $\\pi(s, a)$ of the agent. The **behavior policy** $b(s, a)$ used to generate the episodes is only required to select at least occasionally the same actions as the **learned policy** $\\pi(s, a)$ (coverage assumption):\n\n$$ \\pi(s,a) > 0 \\Rightarrow b(s,a) > 0$$\n\nAn $\\epsilon$-greedy behavior policy over the Q-values is often enough, while a deterministic (greedy) policy can be learned. The behavior policy could also come from **expert knowledge**, i.e. known episodes from the MDP generaed by somebody else. But are we mathematically allowed to do this?\n\n\n\n## Importance sampling\n\n{{< youtube  g0qd41d62Gg >}}\n\nWe search for the optimal policy that maximizes in expectation the return of each **trajectory** (episode) possible under the learned policy $\\pi$:\n\n$$\\mathcal{J}(\\pi) = \\mathbb{E}_{\\tau \\sim \\rho_\\pi} [R(\\tau)]$$\n\n$\\rho_\\pi$ denotes the probability distribution of trajectories achievable using the policy $\\pi$. If we generate the trajectories from the behavior policy $b(s, a)$, we end up maximizing something else:\n\n$$\\mathcal{J}'(\\pi) = \\mathbb{E}_{\\tau \\sim \\rho_b} [R(\\tau)]$$\n\nThe policy that maximizes $\\mathcal{J}'(\\pi)$ is **not** the optimal policy of the MDP.\n\n![You cannot use samples from one distribution to estimate something about another.](../slides/img/importancesampling.svg){width=100%}\n\nIf you try to estimate a parameter of a random distribution $\\pi$ using samples of another distribution $b$, the sample average will have a strong **bias**. We need to **correct** the samples from $b$ in order to be able to estimate the parameters of $\\pi$ correctly: this is called **importance sampling** (IS).\n\n\nWe want to estimate the expected return of the trajectories generated by the policy $\\pi$:\n\n$$\\mathcal{J}(\\pi) = \\mathbb{E}_{\\tau \\sim \\rho_\\pi} [R(\\tau)]$$\n\nWe start by using the definition of the mathematical expectation:\n\n$$\\mathcal{J}(\\pi) = \\int_\\tau \\rho_\\pi(\\tau) \\, R(\\tau) \\, d\\tau$$\n\nThe expectation is the integral over all possible trajectories of their return $R(\\tau$), weighted by the likelihood $\\rho_\\pi(\\tau)$ that a trajectory $\\tau$ is generated by the policy $\\pi$.\n\n![RL maximizes the expected return of the trajectories generated by the policy. ](../slides/img/importancesampling2.svg){width=50%}\n\nThe trick is to introduce the behavior policy $b$ in what we want to estimate:\n\n\n$$\\mathcal{J}(\\pi) = \\int_\\tau \\frac{\\rho_b(\\tau)}{\\rho_b(\\tau)} \\, \\rho_\\pi(\\tau) \\, R(\\tau) \\, d\\tau$$\n\n$\\rho_b(\\tau)$ is the likelihood that a trajectory $\\tau$ is generated by the behavior policy $b$. We shuffle a bit the terms:\n\n\n$$\\mathcal{J}(\\pi) = \\int_\\tau \\rho_b(\\tau) \\, \\frac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)} \\,  R(\\tau) \\, d\\tau$$\n\nand notice that it has the form of an expectation over trajectories generated by $b$:\n\n\n$$\\mathcal{J}(\\pi) = \\mathbb{E}_{\\tau \\sim \\rho_b} [\\frac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)} \\, R(\\tau)]$$\n\nThis means that we can sample trajectories from $b$, but we need to **correct** the observed return by the **importance sampling weight** $\\frac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)}$.\n\nThe importance sampling weight corrects the mismatch between $\\pi$ and $b$. \n\n![Importance sampling gives more weight to samples that are likely under the distribution to estimate.](../slides/img/importancesampling3.svg){width=100%}\n\nIf the two distributions are the same (on-policy), the IS weight is 1, there is no need to correct the returns. If a sample is likely under $b$ but not under $\\pi$, we should not care about its return: $\\frac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)} << 1$. If a sample is likely under $\\pi$ but not much under $b$, we increase its importance in estimating the return: $\\frac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)} >> 1$. The sampling average of the corrected samples will be closer from the true estimate (unbiased).\n\nHow do we compute these probability distributions $\\rho_\\pi(\\tau)$ and $\\rho_b(\\tau)$ for a trajectory $\\tau$? A trajectory $\\tau$ is a sequence of state-action transitions $(s_0, a_0, s_1, a_1, \\ldots, s_T)$ whose probability depends on:\n\n* the probability of choosing an action $a_t$ in state $s_t$: the **policy** $\\pi(s, a)$.\n* the probability of arriving in the state $s_{t+1}$ from the state $s_t$ with the action $a_t$: the **transition probability** $p(s_{t+1} | s_t, a_t)$. \n\nThe **likelihood** of a trajectory $\\tau = (s_0, a_0, s_1, a_1, \\ldots, s_T)$ under a policy $\\pi$ depends on the policy and the transition probabilities (Markov property):\n\n$$\n    \\rho_\\pi(\\tau) = p_\\pi(s_0, a_0, s_1, a_1, \\ldots, s_T) = p(s_0) \\, \\prod_{t=0}^{T-1} \\pi_\\theta(s_t, a_t) \\, p(s_{t+1} | s_t, a_t)\n$$ \n\n$p(s_0)$ is the probability of starting an episode in $s_0$, we do not have control over it.\n\nWhat is interesting is that the transition probabilities disappear when calculating the **importance sampling weight**:\n\n$$\n    \\rho_{0:T-1} = \\frac{\\rho_\\pi(\\tau)}{\\rho_b(\\tau)} = \\frac{p_0 (s_0) \\, \\prod_{t=0}^{T-1} \\pi(s_t, a_t) p(s_{t+1} | s_t, a_t)}{p_0 (s_0) \\, \\prod_{t=0}^T b(s_t, a_t) p(s_{t+1} | s_t, a_t)} = \\frac{\\prod_{t=0}^{T-1} \\pi(s_t, a_t)}{\\prod_{t=0}^T b(s_t, a_t)} = \\prod_{t=0}^{T-1} \\frac{\\pi(s_t, a_t)}{b(s_t, a_t)}\n$$\n\nThe importance sampling weight is simply the product over the length of the episode of the ratio between $\\pi(s_t, a_t)$ and $b(s_t, a_t)$. \n\n## Off-policy Monte-Carlo control\n\nIn **off-policy MC control**, we generate episodes using the behavior policy $b$ and update **greedily** the learned policy $\\pi$. For the state $s_t$, the obtained returns just need to be weighted by the relative probability of occurrence of the **rest of the episode** following the policies $\\pi$ and $b$:\n\n$$\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(s_k, a_k)}{b(s_k, a_k)}$$\n\nThis gives us the updates:\n\n$$\n    V(s_t) = V(s_t) + \\alpha  \\, \\rho_{t:T-1} \\, (R_t - V(s_t))\n$$\n\nand:\n\n$$\n    Q(s_t, a_t) = Q(s_t, a_t) + \\alpha  \\, \\rho_{t:T-1} \\, (R_t - Q(s_t, a_t))\n$$\n\nUnlikely episodes under $\\pi$ are barely used for learning, likely ones are used a lot.\n\n::: {.callout-tip}\n## Off-policy Monte-Carlo control\n\n* **while** True:\n\n    * Generate an episode $\\tau = (s_0, a_0, r_1, \\ldots, s_T)$ using the **behavior** policy $b$.\n\n    * For each state-action pair $(s_t, a_t)$ in the episode, update the estimated Q-value:\n\n\n    $$\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(s_k, a_k)}{b(s_k, a_k)}$$\n\n    $$\n        Q(s_t, a_t) = Q(s_t, a_t) + \\alpha  \\, \\rho_{t:T-1} \\, (R_t - Q(s_t, a_t))\n    $$\n\n    * For each state $s_t$ in the episode, update the **learned** deterministic policy (greedy):\n\n    $$\n        \\pi(s_t, a) = \\begin{cases}\n                        1\\; \\text{if} \\; a = \\text{argmax} \\, Q(s_t, a) \\\\\n                        0 \\; \\text{otherwise.} \\\\\n                      \\end{cases}\n    $$\n:::\n\n* **Problem 1:** if the learned policy is greedy, the IS weight becomes quickly 0 for a non-greedy action $a_t$:\n\n$$\\pi(s_t, a_t) = 0 \\rightarrow \\rho_{0:T-1} = \\prod_{k=0}^{T-1} \\frac{\\pi(s_k, a_k)}{b(s_k, a_k)} = 0$$\n\nOff-policy MC control only learns from the last greedy actions, what is slow at the beginning.\n\n**Solution:** $\\pi$ and $b$ should not be very different. Usually $\\pi$ is greedy and $b$ is a softmax (or $\\epsilon$-greedy) over it.\n\n* **Problem 2:** if the learned policy is stochastic, the IS weights can quickly **vanish** to 0 or **explode** to infinity: \n\n$$\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(s_k, a_k)}{b(s_k, a_k)}$$\n\nIf $\\frac{\\pi(s_k, a_k)}{b(s_k, a_k)}$ is smaller than 1, the products go to 0. If it is bigger than 1, it grows to infinity.\n\n**Solution:** one can normalize the IS weight between different episodes (see Sutton and Barto) or **clip** it (e.g. restrict it to [0.9, 1.1], see PPO later in this course).\n\n\nThe main advantage of **off-policy** strategies is that you can learn from other's actions, you don't have to rely on your initially wrong policies to discover the solution by chance. Example: learning to play chess by studying thousands/millions of plays by chess masters. In a given state, only a subset of the possible actions are actually executed by experts: the others may be too obviously wrong. The exploration is then guided by this expert knowledge, not randomly among all possible actions.\n\nOff-policy methods greatly reduce the number of transitions needed to learn a policy: very stupid actions are not even considered, but the estimation policy learns an optimal strategy from the \"classical\" moves. Drawback: if a good move is not explored by the behavior policy, the learned policy will never try it.\n\n::: {.callout-note}\n## Properties of Monte-Carlo methods\n\n* Monte-Carlo evaluation estimates value functions via **sampling** of entire episodes.\n\n* Monte-Carlo control (evaluation-improvement) is a generalized policy iteration method.\n\n* MC for action values is **model-free**: you do not need to know $p(s' | s, a)$ to learn the optimal policy, you just sample transitions (trial and error).\n\n* MC only applies to **episodic tasks**: as you learn at the end of an episode, it is not possible to learn continuing tasks.\n\n* MC suffers from the **exploration-exploitation** problem:\n\n    * **on-policy** MC learns a stochastic policy ($\\epsilon$-greedy, softmax) to ensure exploration.\n\n    * **off-policy** MC learns a greedy policy, but explores via a behavior policy (importance sampling). \n\n* Monte-Carlo methods have:\n\n    * a **small bias**: with enough sampled episodes, the estimated values converge to the true values.\n\n    * a **huge variance**: the slightest change of the policy can completely change the episode and its return. You will need a lot of samples to form correct estimates: **sample complexity**.\n:::\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"2.4-MC.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}