{"title":"Function approximation","markdown":{"headingText":"Function approximation","containsRefs":false,"markdown":"\n\nSlides: [html](../slides/2.6-FunctionApproximation.html){target=\"_blank\"} [pdf](../slides/pdf/2.6-FunctionApproximation.pdf){target=\"_blank\"}\n\n## Limits of tabular RL\n\n{{< youtube  el6F6Drem88 >}}\n\nAll the methods seen so far belong to **tabular RL**. Q-learning necessitates to store in a **Q-table** one Q-value per state-action pair $(s, a)$.\n\n![Tabular Q-learning. Source: <https://towardsdatascience.com/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677>](../slides/img/qtable.gif){width=100%}\n\nIf a state has never been visited during learning, the Q-values will still be at their initial value (0.0), no policy can be derived.\n\n![In high-dimensional state spaces (e.g. images), tabular RL cannot generalize between close states.](../slides/img/tabular-generalization.svg){width=60%}\n\nSimilar states likely have the same optimal action: we want to be able to **generalize** the policy between states. For most realistic problems, the size of the Q-table becomes quickly untractable. If you use black-and-white 256x256 images as inputs, you have $2^{256 * 256} = 10^{19728}$ possible states! **Tabular RL** is therefore limited to toy problems.\n\nTabular RL also only works for small **discrete action spaces**. Robots have **continuous action spaces**, where the actions are changes in **joint angles** or **torques**. A joint angle could for example take any value in $[0, \\pi]$. A solution would be to **discretize** the action space (one action per degree), but we would fall into the **curse of dimensionality**.\n\n![Curse of dimensionality. Source: <https://medium.com/diogo-menezes-borges/give-me-the-antidote-for-the-curse-of-dimensionality-b14bce4bf4d2>](../slides/img/cursedimensionality.png){width=90%}\n\nThe more degrees of freedom, the more discrete actions, the more entries in the Q-table... Tabular RL cannot deal with continuous action spaces, unless we approximate the policy with an **actor-critic** architecture.\n\n## Function approximation\n\n{{< youtube  cATgUO0QBes >}}\n\n### Feature vectors\n\nLet's represent a state $s$ by a vector of $d$ **features** $\\phi(s) = [\\phi_1(s), \\phi_2(s), \\ldots, \\phi_d(s)]^T$. For the cartpole, the feature vector would be:\n\n$$ \\phi(s) = \\begin{bmatrix}x \\\\ \\dot{x} \\\\ \\theta \\\\ \\dot{\\theta} \\end{bmatrix}$$\n\nwhere $x$ is the position, $\\theta$ the angle, $\\dot{x}$ and $\\dot{\\theta}$ their derivatives. We are able to represent **any state** $s$ of the cartpole problem using these four variables.\n\nFor more complex problems, the feature vector should include all the necessary information (Markov property). Example of breakout:\n\n$$\n    \\phi(s) = \\begin{bmatrix}\n        x \\, \\text{position of the paddle} \\\\ \n        x \\, \\text{position of the ball} \\\\ \n        y \\, \\text{position of the ball} \\\\ \n        x \\, \\text{speed of the ball} \\\\ \n        y \\, \\text{speed of the position} \\\\ \n        \\text{presence of brick 1} \\\\ \n        \\text{presence of brick 2} \\\\ \n        \\vdots \\\\\n    \\end{bmatrix}\n$$\n\nIn deep RL, we will **learn** these feature vectors, but let's suppose for now that we have them. \n\nNote that we can always fall back to the tabular case using **one-hot encoding** of the states:\n\n$$\n\\phi(s_1) = \\begin{bmatrix}1\\\\0\\\\0\\\\ \\ldots\\\\ 0\\end{bmatrix} \\qquad\n\\phi(s_2) = \\begin{bmatrix}0\\\\1\\\\0\\\\ \\ldots\\\\ 0\\end{bmatrix}\\qquad\n\\phi(s_3) = \\begin{bmatrix}0\\\\0\\\\1\\\\ \\ldots\\\\ 0\\end{bmatrix} \\qquad \\ldots\n$$ \n\nBut the idea is that we can represent states with much less values than the number of states:\n\n$$d \\ll |\\mathcal{S}|$$\n\nWe can also represent **continuous state spaces** with feature vectors, as in cartpole.\n\n\n### State value approximation\n\nIn **state value approximation**, we want to approximate the state value function $V^\\pi(s)$ with a **parameterized function** $V_\\varphi(s)$:\n\n$$V_\\varphi(s) \\approx V^\\pi(s)$$\n\n![State value approximation.](../slides/img/functionapproximation-state.svg){width=80%}\n\nThe parameterized function can have any form. Its has a set of parameters $\\varphi$ used to transform the feature vector $\\phi(s)$ into an approximated value $V_\\varphi(s)$.\n\nThe simplest function approximator (FA) is the **linear approximator**.\n\n![Linear state value approximation.](../slides/img/functionapproximation-state-linear.svg){width=80%}\n\nThe approximated value is a linear combination of the features:\n\n$$V_\\varphi(s) = \\sum_{i=1}^d w_i \\, \\phi_i(s) = \\mathbf{w}^T \\times \\phi(s)$$\n\nThe **weight vector** $\\mathbf{w} = [w_1, w_2, \\ldots, w_d]^T$is the set of parameters $\\varphi$ of the function. A linear approximator is a single **artificial neuron** (linear regression) without a bias.\n\n\nRegardless the form of the function approximator, we want to find the parameters $\\varphi$ making the approximated values $V_\\varphi(s)$ as close as possible from the true values $V^\\pi(s)$ for all states $s$. \n\nThis is a **regression** problem, so we want to minimize the **mean-square error** between the two quantities:\n\n$$ \\min_\\varphi \\mathcal{L}(\\varphi) = \\mathbb{E}_{s \\in \\mathcal{S}} [ (V^\\pi(s) - V_\\varphi(s))^2]$$\n\nThe **loss function** $\\mathcal{L}(\\varphi)$ is minimal when the predicted values are close to the true ones on average over the state space.\n\nLet's suppose for now that we know the true state values $V^\\pi(s)$ for all states and that the parametrized function is **differentiable**. We can find the minimum of the loss function by applying **gradient descent** (GD) iteratively:\n\n$$\n    \\Delta \\varphi = - \\eta \\, \\nabla_\\varphi \\mathcal{L}(\\varphi)\n$$\n\n$\\nabla_\\varphi \\mathcal{L}(\\varphi)$ is the gradient of the loss function w.r.t to the parameters $\\varphi$.\n\n$$\n    \\nabla_\\varphi \\mathcal{L}(\\varphi) = \\begin{bmatrix}\n        \\frac{\\partial \\mathcal{L}(\\varphi)}{\\partial \\varphi_1} \\\\\n        \\frac{\\partial \\mathcal{L}(\\varphi)}{\\partial \\varphi_2} \\\\\n        \\ldots \\\\\n        \\frac{\\partial \\mathcal{L}(\\varphi)}{\\partial \\varphi_K} \\\\\n    \\end{bmatrix}\n$$\n\nWhen applied repeatedly, GD converges to a local minimum of the loss function.\n\nIn order to minimize the mean square error, we will iteratively modify the parameters $\\varphi$ according to:\n\n$$\n\\begin{aligned}\n    \\Delta \\varphi = \\varphi_{k+1} - \\varphi_n & = - \\eta \\, \\nabla_\\varphi \\mathcal{L}(\\varphi) \\\\\n    &\\\\\n    & = - \\eta \\, \\nabla_\\varphi \\mathbb{E}_{s \\in \\mathcal{S}} [ (V^\\pi(s) - V_\\varphi(s))^2] \\\\\n    &\\\\\n    & = \\mathbb{E}_{s \\in \\mathcal{S}} [- \\eta \\, \\nabla_\\varphi  (V^\\pi(s) - V_\\varphi(s))^2] \\\\\n    &\\\\\n    & = \\mathbb{E}_{s \\in \\mathcal{S}} [\\eta \\,  (V^\\pi(s) - V_\\varphi(s)) \\, \\nabla_\\varphi V_\\varphi(s)] \\\\\n\\end{aligned}\n$$\n\nAs it would be too slow to compute the expectation on the whole state space (**batch algorithm**), we will sample the quantity:\n    \n$$\\delta_\\varphi = \\eta \\,  (V^\\pi(s) - V_\\varphi(s)) \\, \\nabla_\\varphi V_\\varphi(s)$$\n\nand update the parameters with **stochastic gradient descent** (SGD).\n\nIf we sample $K$ states $s_i$ from the state space, we get:\n\n$$\n    \\Delta \\varphi = \\eta \\,  \\frac{1}{K} \\sum_{k=1}^K (V^\\pi(s_k) - V_\\varphi(s_k)) \\, \\nabla_\\varphi V_\\varphi(s_k)\n$$\n\nWe can also sample a single state $s$ (online algorithm):\n\n$$\n    \\Delta \\varphi = \\eta \\, (V^\\pi(s) - V_\\varphi(s)) \\, \\nabla_\\varphi V_\\varphi(s)\n$$\n\nUnless stated otherwise, we will sample single states in this section, but beware that the parameter updates will be noisy (high variance).\n\n\nThe approximated value is a linear combination of the features:\n\n$$V_\\varphi(s) = \\sum_{i=1}^d w_i \\, \\phi_i(s) = \\mathbf{w}^T \\times \\phi(s)$$\n\nThe weights are updated using stochastic gradient descent:\n\n$$\n    \\Delta \\mathbf{w} = \\eta \\, (V^\\pi(s) - V_\\varphi(s)) \\, \\phi(s)\n$$\n\nThis is the **delta learning rule** of linear regression and classification, with $\\phi(s)$ being the input vector and $V^\\pi(s) - V_\\varphi(s)$ the prediction error.\n\nThe rule can be used with any function approximator, we only need to be able to differentiate it:\n\n$$\n    \\Delta \\varphi = \\eta \\, (V^\\pi(s) - V_\\varphi(s)) \\, \\nabla_\\varphi V_\\varphi(s)\n$$\n\nThe problem is that we do not know $V^\\pi(s)$, as it is what we are trying to estimate. We can replace $V^\\pi(s)$ by a sampled estimate using Monte-Carlo or TD:\n\n* **Monte-Carlo** function approximation:\n\n$$\n    \\Delta \\varphi = \\eta \\, (R_t - V_\\varphi(s)) \\, \\nabla_\\varphi V_\\varphi(s)\n$$\n\n* **Temporal Difference** function approximation:\n\n$$\n    \\Delta \\varphi = \\eta \\, (r_{t+1} + \\gamma \\, V_\\varphi(s') - V_\\varphi(s)) \\, \\nabla_\\varphi V_\\varphi(s)\n$$\n\n\n::: {.callout-tip}\n## Gradient Monte Carlo Algorithm for value estimation\n\n* Initialize the parameter $\\varphi$ to 0 or randomly.\n\n* **while** not converged:\n\n    1. Generate an episode according to the current policy $\\pi$ until a terminal state $s_T$ is reached.\n\n    $$\n        \\tau = (s_o, a_o, r_ 1, s_1, a_1, \\ldots, s_T)\n    $$\n\n    2. For all encountered states $s_0, s_1, \\ldots, s_{T-1}$:\n\n        1. Compute the return $R_t = \\sum_k \\gamma^k r_{t+k+1}$ .\n\n        2. Update the parameters using function approximation:\n\n        $$\n            \\Delta \\varphi = \\eta \\, (R_t - V_\\varphi(s_t)) \\, \\nabla_\\varphi V_\\varphi(s_t)\n        $$\n:::\n\n::: {.callout-tip}\n## Semi-gradient Temporal Difference Algorithm for value estimation\n\n* Initialize the parameter $\\varphi$ to 0 or randomly.\n\n* **while** not converged:\n\n    * Start from an initial state $s_0$.\n\n    * **foreach** step $t$ of the episode:\n\n        * Select $a_t$ using the current policy $\\pi$ in state $s_t$.\n\n        * Observe $r_{t+1}$ and $s_{t+1}$.\n\n        * Update the parameters using function approximation:\n\n        $$\n            \\Delta \\varphi = \\eta \\, (r_{t+1} + \\gamma \\, V_\\varphi(s_{t+1}) - V_\\varphi(s_t)) \\, \\nabla_\\varphi V_\\varphi(s_t)\n        $$\n\n        * **if** $s_{t+1}$ is terminal: **break**\n:::\n\nAs in tabular RL, Gradient Monte-Carlo has no bias (real returns) but a high variance.\nSemi-gradient TD has less variance, but a significant bias as $V_\\varphi(s_{t+1})$ is initially wrong. You can never trust these estimates completely.\n\n\nNote that for Temporal Difference, we actually want to minimize the TD reward-prediction error for all states, i.e. the surprise:\n\n$$\\mathcal{L}(\\varphi) = \\mathbb{E}_{s \\in \\mathcal{S}} [ (r_{t+1} + \\gamma \\, V_\\varphi(s') - V_\\varphi(s))^2]= \\mathbb{E}_{s \\in \\mathcal{S}} [ \\delta_t^2]$$\n\n### Action value approximation\n\nQ-values can be approximated by a parameterized function $Q_\\theta(s, a)$ in the same manner. There are basically two options for the structure of the function approximator:\n\n\n* The FA takes a feature vector for both the state $s$ and the action $a$ (which can be continuous) as inputs, and outputs a single Q-value $Q_\\theta(s ,a)$. \n\n![Action value approximation for a single action.](../slides/img/functionapproximation-action1.svg){width=80%}\n\n* The FA takes a feature vector for the state $s$ as input, and outputs one Q-value $Q_\\theta(s ,a)$ per possible action (the action space must be discrete).\n\n![Action value approximation for all actions.](../slides/img/functionapproximation-action2.svg){width=80%}\n\nIn both cases, we minimize the mse between the true value $Q^\\pi(s, a)$ and the approximated value $Q_\\theta(s, a)$.\n\n::: {.callout-tip}\n## Q-learning with function approximation\n\n* Initialize the parameters $\\theta$. \n\n* **while** True:\n\n    * Start from an initial state $s_0$.\n\n    * **foreach** step $t$ of the episode:\n\n        * Select $a_{t}$ using the behavior policy $b$ (e.g. derived from $\\pi$).\n\n        * Take $a_t$, observe $r_{t+1}$ and $s_{t+1}$.\n\n        * Update the parameters $\\theta$:\n\n        $$\\Delta \\theta = \\eta \\, (r_{t+1} + \\gamma \\, \\max_a Q_\\theta(s_{t+1}, a) - Q_\\theta(s_t, a_t)) \\, \\nabla_\\theta Q_\\theta(s_t, a_t)$$\n\n        * Improve greedily the learned policy:\n        \n        $$\\pi(s_t, a) = \\text{Greedy}(Q_\\theta(s_t, a))$$\n\n        * **if** $s_{t+1}$ is terminal: **break**\n:::\n\n\n## Feature construction\n\n{{< youtube  t39QwC_5vXI >}}\n\n### Linear features\n\nBefore we dive into deep RL (i.e. RL with non-linear FA), let's see how we can design good **feature vectors** for linear function approximation. The problem with deep NN is that they need a lot of samples to converge, what worsens the fundamental problem of RL: **sample efficiency**. By engineering the right features, we could use linear approximators, which converge much faster. The convergence of linear FA is **guaranteed**, not (always) non-linear ones.\n\nWhy do we need to choose features? For the cartpole, the feature vector $\\phi(s)$ could be:\n\n$$ \\phi(s) = \\begin{bmatrix}x \\\\ \\dot{x} \\\\ \\theta \\\\ \\dot{\\theta} \\end{bmatrix}$$\n\nwhere $x$ is the position, $\\theta$ the angle, $\\dot{x}$ and $\\dot{\\theta}$ their derivatives. Can we predict the value of a state **linearly**?\n\n$$V_\\varphi(s) = \\sum_{i=1}^d w_i \\, \\phi_i(s) = \\mathbf{w}^T \\times \\phi(s)$$\n\nThis answer is no, as a high angular velocity $\\dot{\\theta}$ is good when the pole is horizontal (going up) but bad if the pole is vertical (will not stop). The value would depends linearly on something like $\\dot{\\theta} \\, \\sin \\theta$, which is a non-linear combination of features.\n\nLet's suppose we have a simple problem where the state $s$ is represented by two continuous variables $x$ and $y$. The true value function $V^\\pi(s)$ is a non-linear function of $x$ and $y$.\n\n![State values $V^\\pi(s)$ for a two-dimensional state space.](../slides/img/featurecoding-data.png){width=60%}\n\nIf we apply linear FA directly on the feature vector $[x, y]$, we catch the tendency of $V^\\pi(s)$ but we make a lot of bad predictions: **high bias** (underfitting).\n\n![Linear approximation of the state value function.](../slides/img/featurecoding-linear.png){width=60%}\n\n### Polynomial features\n\nTo introduce non-linear relationships between continuous variables, a simple method is to construct the feature with **polynomials** of the variables.\n\nExample with polynomials of order 2:\n\n$$\n    \\phi(s) = \\begin{bmatrix}1 & x & y & x\\, y & x^2 & y^2 \\end{bmatrix}^T\n$$\n\nWe transform the two input variables $x$ and $y$ into a vector with 6 elements. The 1 (order 0) is there to learn the offset / bias.\n\nExample with polynomials of order 3:\n\n$$\n    \\phi(s) = \\begin{bmatrix}1 & x & y & x\\, y & x^2 & y^2 & x^2 \\, y & x \\, y^2 & x^3 & y^3\\end{bmatrix}^T\n$$\n\nWe then just need to apply linear FA on these feature vectors (**polynomial regression**).\n\n$$\n    V_\\varphi(s) = w_0 + w_1 \\, x + w_2 \\, y + w_3 \\, x \\, y + w_4 \\, x^2 + w_5 \\, y^2 + \\ldots\n$$\n\n![Polynomial approximation of the state value function with order 2.](../slides/img/featurecoding-polynomial2.png){width=60%}\n\n![Polynomial approximation of the state value function with order 6.](../slides/img/featurecoding-polynomial6.png){width=60%}\n\nThe higher the degree of the polynomial, the better the fit, but the number of features grows exponentially. This adds to the computational complexity and leads to **overfitting**: if we only sample some states, high-order polynomials will not interpolate correctly.\n\n\n### Fourier transforms\n\nInstead of approximating a state variable $x$ by a polynomial:\n\n$$\n    V_\\varphi(s) = w_0 + w_1 \\, x + w_2 \\, x^2 + w_3 \\, x^3 + \\ldots\n$$\n\nwe could also use its **Fourier decomposition** (here DCT, discrete cosine transform):\n\n$$\n    V_\\varphi(s) = w_0 + w_1 \\, \\cos(\\pi \\, x) + w_2 \\, \\cos( 2 \\, \\pi \\, x) + w_3 \\, \\cos(3 \\, \\pi \\, x) + \\ldots\n$$\n\nThe Fourier theorem tells us that, if we take enough frequencies, we can reconstruct the signal $V_\\varphi(s)$ perfectly.\n\n![Fourier transform in 1D. Source: [@Sutton1998].](../slides/img/featurecoding-fourier1.png){width=100%}\n\nIt is just a change of basis, the problem stays a linear regression to find $w_0, w_1, w_2$, etc.\n\nFourier transforms can be applied on multivariate functions as well.\n\n![Fourier transform in 2D. Source: [@Sutton1998].](../slides/img/featurecoding-fourier2.png){width=100%}\n\n![Comparison of polynomial and Fourier features. Source: [@Sutton1998].](../slides/img/featurecoding-fourier3.png){width=100%}\n\nA Fourier basis tends to work better than a polynomial basis. The main problem is that the number of features increases very fast with the number of input dimensions and  the desired precision (higher-order polynomials, more frequencies).\n\n### Discrete coding\n\nAn obvious solution for continuous state variables is to **discretize** the input space. The input space is divided into a grid of non-overlapping **tiles**.\n\n![Linear approximation of the state value function using discrete coding.](../slides/img/featurecoding-tile1.png){width=60%}\n\nThe feature vector is a **binary** vector with a 1 when the input is inside a tile, 0 otherwise.\n\n$$\\phi(s) = \\begin{bmatrix}0 & 0 & \\ldots & 0 & 1 & 0 & \\ldots & 0 \\\\ \\end{bmatrix}^T$$\n\nThis ensures **generalization** inside a tile: you only need a couple of samples inside a tile to know the mean value of all the states. Drawbacks: the value function is step-like (discontinuous), the correct size of a tile is not known, we fall into the **curse of dimensionality**.\n\n\n### Coarse coding\n\nA more efficient solution is **coarse coding**. The tiles (rectangles, circles, or what you need) need to **overlap**.\n\n![Coarse coding uses overlapping tiles. Source: [@Sutton1998].](../slides/img/featurecoding-tile2.png){width=60%}\n\nA state $s$ is encoded by a **binary vector**, but with several 1, for each tile it belongs.\n\n$$\\phi(s) = \\begin{bmatrix}0 & 1 & 0 & \\ldots & 1 & 1 & 0 & \\ldots & 0 \\\\ \\end{bmatrix}^T$$\n\nThis allows generalization inside a tile, but also **across tiles**. The size and shape of the **\"receptive field\"** influences the generalization properties.\n\n![The overlap between tiles defines the generalization. Source: [@Sutton1998].](../slides/img/featurecoding-tile4.png){width=90%}\n\n\n### Tile coding\n\nA simple way to ensure that tiles overlap is to use several regular grids with an **offset**. Each tiling will be **coarse**, but the location of a state will be quite precise as it may belong to many tiles.\n\n![Tile coding. Source: [@Sutton1998].](../slides/img/featurecoding-tile3.png){width=100%}\n\nThis helps against the curse of dimensionality: high precision, but the number of tiles does not grow exponentially.\n\n### Radial-basis functions (RBF)\n\nThe feature vector in tile coding is a binary vector: there will be **discontinuous jumps** in the approximated value function when moving between tiles. We can use **radial-basis functions** (RBF) such as Gaussians to map the state space.\n\n![Radial-basis functions.](../slides/img/featurecoding-rbf.png){width=100%}\n\nWe set a set of centers $\\{c_i\\}_{i=1}^K$ in the input space on a regular grid (or randomly). Each element of the feature vector will be a Gaussian function of the distance between the state $s$ and one center:\n\n$$\\phi_i(s) = \\exp \\frac{-(s - c_i)^2}{2\\, \\sigma_i^2}$$\n\nThe approximated value function now represents **continuously** the states:\n\n$$V_\\varphi(s) = \\sum_{i=1}^d w_i \\, \\phi_i(s) = \\sum_{i=1}^d w_i \\, \\exp \\frac{-(s - c_i)^2}{2\\, \\sigma_i^2}$$\n\nIf you have enough centers and they overlap sufficiently, you can even **decode** the original state perfectly:\n\n$$\\hat{s} = \\sum_{i=1}^d \\phi_i(s) \\, c_i$$\n\n::: {.callout-note}\n## Summary of function approximation\n\n![](../slides/img/functionapproximation-state.svg){width=60%}\n\nIn FA, we project the state information into a **feature space** to get a better representation. We then apply a linear approximation algorithm to estimate the value function: \n\n$$V_\\varphi(s) = \\mathbf{w}^T \\, \\phi(s)$$\n\nThe linear FA is trained using some variant of gradient decent: \n\n$$\\Delta \\mathbf{w} = \\eta \\, (V^\\pi(s) - V_\\varphi(s)) \\, \\phi(s)$$\n\n**Deep neural networks** are the most powerful function approximators in supervised learning.  Do they also work with RL?\n:::"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"2.6-FA.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}