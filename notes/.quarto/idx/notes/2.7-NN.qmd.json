{"title":"Deep learning","markdown":{"headingText":"Deep learning","containsRefs":false,"markdown":"\nSlides: [html](../slides/2.7-DeepNetworks.html){target=\"_blank\"} [pdf](../slides/pdf/2.7-DeepNetworks.pdf){target=\"_blank\"}\n\n## Feedforward neural networks\n\n{{< youtube  pVneu-1EYdI >}}\n\nAn **artificial neural network** (ANN) is a cascade of **fully-connected** (FC) layers of artificial neurons.\n\n![Shallow vs. deep neural networks.](../slides/img/shallowdeep.png){width=100%}\n\nEach layer $k$ transforms an input vector $\\mathbf{h}_{k-1}$ into an output vector $\\mathbf{h}_{k}$ using a weight matrix $W_k$, a bias vector $\\mathbf{b}_k$ and an activation function $f()$.\n\n$$\\mathbf{h}_{k} = f(W_k \\times \\mathbf{h}_{k-1} + \\mathbf{b}_k)$$\n\nOverall, ANNs are **non-linear parameterized function estimators** from the inputs $\\mathbf{x}$ to the outputs $\\mathbf{y}$ with parameters $\\theta$ (all weight matrices and biases).\n\n$$\\mathbf{y} = F_\\theta (\\mathbf{x})$$\n\nANNs can be used for both **regression** (continuous outputs) and **classification** (discrete outputs) tasks. In supervised learning, we have a fixed **training set** $\\mathcal{D}$ of $N$ samples $(\\mathbf{x}_t, \\mathbf{t}_i)$, where $t_i$ is the **desired output** or **target**.\n\n* **Regression:**\n\n    * The output layer uses a **linear** activation function: $f(x) = x$\n\n    * The network minimizes the **mean square error** (mse) of the model on the training set:\n\n    $$\\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\in \\mathcal{D}} [ ||\\mathbf{t} - \\mathbf{y}||^2 ]$$ \n\n\n* **Classification:**\n\n    * The output layer uses the **softmax** operator to produce a probabilty distribution: $y_j = \\frac{e^{z_j}}{\\sum_k e^{z_k}}$\n\n    * The network minimizes the **cross-entropy** or **negative log-likelihood** of the model on the training set:\n\n    $$\\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\in \\mathcal{D}} [ - \\mathbf{t} \\, \\log \\mathbf{y} ]$$\n\n\nThe cross-entropy between two probability distributions $X$ and $Y$ measures their similarity:\n\n$$\n    H(X, Y) = \\mathbb{E}_{x \\sim X}[- \\log P(Y=x)]\n$$\n\nIt measures whether samples from $X$ are likely under $Y$? Minimizing the cross-entropy makes the two distributions equal almost anywhere.\n\n![The cross-entropy measures the similarity of the two probability distributions.](../slides/img/crossentropy.svg){width=100%}\n\nIn supervised learning, the targets $\\mathbf{t}$ are fixed **one-hot encoded vectors**.\n\n$$\\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\in \\mathcal{D}} [ - \\sum_j t_j \\, \\log y_{j} ]$$\n\nBut it could be any target distribution.\n\n![Cross-entropy between multinomial distributions.](../slides/img/crossentropy-animation.gif){width=70%}\n\nIn both cases, we want to minimize the loss variant by applying **Stochastic Gradient Descent** (SGD) or a variant (Adam, RMSprop).\n\n$$\n    \\Delta \\theta = - \\eta \\, \\nabla_\\theta \\mathcal{L}(\\theta)\n$$\n\nThe question is how to compute the **gradient of the loss function** w.r.t the parameters $\\theta$. For both the mse and cross-entropy loss functions, we have:\n\n$$\\nabla_\\theta \\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [- (\\mathbf{t} - \\mathbf{y}) \\, \\nabla_\\theta \\, \\mathbf{y}]$$\n\nThere is an algorithm to compute efficiently the gradient of the output w.r.t the parameters: **backpropagation** (see Neurocomputing). In deep RL, we do not care about backprop: tensorflow or pytorch do it for us. \n\n![Principle of deep reinforcement learning.](../slides/img/deeprl.jpg){width=80%}\n\nThere are three aspects to consider when building a neural network:\n\n1. **Architecture:**  how many layers, what type of layers, how many neurons, etc.\n\n    * Task-dependent: each RL task will require a different architecture. Not our focus.\n\n2. **Loss function:** what should the network do?\n\n    * Central to deep RL!\n\n3. **Update rule** how should we update the parameters $\\theta$ to minimize the loss function? SGD, backprop.\n\n    * Not really our problem, but see *natural gradients* later.\n\n## Convolutional neural networks\n\n{{< youtube  t244PS_tZtY >}}\n\nWhen using images as inputs, **fully-connected networks** (FCN) would have too many weights: slow learning and overfitting.\n\n![Fully-connected layers necessitate too many parameters on images.](../slides/img/fullyconnected.png){width=50%}\n\n**Convolutional layers** reduce the number of weights by **reusing** weights at different locations. This the principle of a convolution, which leads to fast and efficient learning.\n\n\n![Convolutional layers share weights on the image.](../slides/img/convolutional.png){width=50%}\n\nA **convolutional layer** extracts **features** of its inputs. $d$ filters are defined with very small sizes (3x3, 5x5...). Each filter is convoluted over the input image (or the previous layer) to create a **feature map**. The set of $d$ feature maps becomes a new 3D structure: a **tensor**. If the input image is 32x32x3, the resulting tensor will be 32x32xd. The convolutional layer has only very few parameters: each feature map has 3x3 values in the filter and a bias, i.e. 10 parameters. The convolution operation is **differentiable**: backprop will work.\n\n![Convolutional layer. Source: <http://cs231n.github.io/convolutional-networks/>](../slides/img/depthcol.jpeg){width=50%}\n\nThe number of elements in a convolutional layer is still too high. We need to reduce the spatial dimension of a convolutional layer by **downsampling** it. For each feature, a **max-pooling** layer takes the maximum value of a feature for each subregion of the image (mostly 2x2). Pooling allows translation invariance: the same input pattern will be detected whatever its position in the input image. Max-pooling is also differentiable.\n\n![Convolutional layer. Source: <http://cs231n.github.io/convolutional-networks/>](../slides/img/maxpooling.jpg){width=50%}\n\nA **convolutional neural network** (CNN) is a cascade of convolution and pooling operations, extracting layer by layer increasingly  complex features. The spatial dimensions decrease after each pooling operation, but the number of extracted features increases after each convolution. One usually stops when the spatial dimensions are around 7x7. The last layers are fully connected. Can be used for regression and classification depending on the output layer and the loss function. Training a CNN uses backpropagation all along: the convolution and pooling operations are differentiable.\n\n![Convolutional neural network.](../slides/img/lenet.png){width=100%}\n\nThe only thing we need to know is that CNNs are non-linear function approximators that work well with images.\n\n$$\\mathbf{y} = F_\\theta (\\mathbf{x})$$\n\nThe convolutional layers **extract complex features** from the images through learning. The last FC layers allow to approximate values (regression) or probability distributions (classification). \n\n\n## Autoencoders\n\n{{< youtube  IqrEdV9ejO8 >}}\n\n\nThe problem with FCN and CNN is that they **extract features** in supervised learning tasks: Need for a lot of annotated data (image, label). **Autoencoders** allows **unsupervised learning**, as they only need inputs (images). Their task is to **reconstruct** the input:\n\n$$\\mathbf{y} = \\mathbf{\\tilde{x}} \\approx \\mathbf{x}$$\n\nThe **reconstruction loss** is simply the **mse** between the input and its reconstruction.\n\n$$\n    \\mathcal{L}_\\text{autoencoder}(\\theta) = \\mathbb{E}_{\\mathbf{x} \\in \\mathcal{D}} [ ||\\mathbf{\\tilde{x}} - \\mathbf{x}||^2 ]\n$$\n\nApart from the loss function, they are trained as regular NNs. Autoencoders consists of:\n\n* the **encoder**: from the input $\\mathbf{x}$ to the **latent space** $\\mathbf{z}$.\n\n* the **decoder**: from the latent space $\\mathbf{z}$ to the reconstructed input $\\mathbf{\\tilde{x}}$.\n\n![Autoencoder. Source: <https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html>](../slides/img/autoencoder-architecture.png){width=100%}\n\nThe **latent space** $\\mathbf{z}$ is a **compressed representation** (bottleneck) of the inputs $\\mathbf{x}$. It has to learn to compress efficiently the inputs without losing too much information, in order to reconstruct the inputs: Dimensionality reduction, unsupervised feature extraction.\n\n\nIn deep RL, we can construct the feature vector with an autoencoder. The autoencoder can be trained offline with a random agent or online with the current policy (auxiliary loss).\n\n\n![Autoencoders can be used in RL to find a feature space where linear FA can apply easily.](../slides/img/autoencoder-RL.svg){width=100%}\n\n\n## Recurrent neural networks\n\n{{< youtube  dkxIqBjldtY >}}\n\nFCN, CNN and AE are **feedforward neural networks**: they transform an input $\\mathbf{x}$ into an output $\\mathbf{y}$:\n\n$$\\mathbf{y} = F_\\theta(\\mathbf{x})$$\n\nIf you present a sequence of inputs $\\mathbf{x}_0, \\mathbf{x}_1, \\ldots, \\mathbf{x}_t$ to a feedforward network, the outputs will be independent from each other:\n\n$$\\mathbf{y}_0 = F_\\theta(\\mathbf{x}_0)$$\n$$\\mathbf{y}_1 = F_\\theta(\\mathbf{x}_1)$$\n$$\\dots$$\n$$\\mathbf{y}_t = F_\\theta(\\mathbf{x}_t)$$\n\nThe output $\\mathbf{y}_t$ does **not** depend on the history of inputs $\\mathbf{x}_0, \\mathbf{x}_1, \\ldots, \\mathbf{x}_{t-1}$. This not always what you want. If your inputs are frames of a video, the correct response at time $t$ might also depend on previous frames.\nThe task of the NN could be to explain what happens at each frame. As we saw, a single frame is often not enough to predict the future (**Markov property**).\n\n![Recurrent neural network. Source: <https://colah.github.io/posts/2015-08-Understanding-LSTMs/>](../slides/img/RNN-rolled.png){width=30%}\n\nA **recurrent neural network** (RNN) uses it previous output as an additional input (*context*). All vectors have a time index $t$ denoting the time at which this vector was computed. The input vector at time $t$ is $\\mathbf{x}_t$, the output vector is $\\mathbf{h}_t$:\n\n$$\n    \\mathbf{h}_t = f(W_x \\times \\mathbf{x}_t + W_h \\times \\mathbf{h}_{t-1} + \\mathbf{b})\n$$\n\n\nThe input $\\mathbf{x}_t$ and previous output $\\mathbf{h}_{t-1}$ are multiplied by **learnable weights**:\n\n* $W_x$ is the input weight matrix.\n* $W_h$ is the recurrent weight matrix.\n\nThis is equivalent to a deep neural network taking the whole history $\\mathbf{x}_0, \\mathbf{x}_1, \\ldots, \\mathbf{x}_t$ as inputs, but reusing weights between two time steps. The weights are trainable using **backpropagation through time** (BPTT). A RNN can learn the **temporal dependencies** between inputs.\n\n![Recurrent neural network, unrolled. Source: <https://colah.github.io/posts/2015-08-Understanding-LSTMs/>](../slides/img/RNN-unrolled.png){width=100%}\n\nA popular variant of RNN is **LSTM** (long short-term memory). In addition to the input $\\mathbf{x}_t$ and output $\\mathbf{h}_t$, it also has a **state** (or **memory** or **context**) $\\mathbf{C}_t$ which is maintained over time. It also contains three multiplicative **gates**:\n\n* The **input gate** controls which inputs should enter the memory.\n* The **forget gate** controls which memory should be forgotten.\n* The **output gate** controls which part of the memory should be used to produce the output.\n\n![LSTM cell. Source: <https://colah.github.io/posts/2015-08-Understanding-LSTMs/>](../slides/img/LSTM-cell2.png){width=60%}\n\nAn obvious use case of RNNs in deep RL is for POMDP (partially observable MDP). If the individual states $s_t$ do not have the Markov property, the output of a LSTM does: The output of the RNN is a representation of the complete history $s_0, s_1, \\ldots, s_t$. We can apply RL on the output of a RNN and solve POMDPs for free!\n\n![LSTM layers help solving POMDP by concatenating and compressing the history. Source: <https://deepmind.com/blog/article/capture-the-flag-science>](../slides/img/capture-flag2.gif){width=100%}\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"2.7-NN.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}