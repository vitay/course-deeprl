{"title":"Deep Q-Learning (DQN)","markdown":{"headingText":"Deep Q-Learning (DQN)","containsRefs":false,"markdown":"\nSlides: [html](../slides/3.1-DQN.html){target=\"_blank\"} [pdf](../slides/pdf/3.1-DQN.pdf){target=\"_blank\"}\n\n## Value-based deep RL\n\n{{< youtube  _luuEjWJU20 >}}\n\nThe basic idea in **value-based deep RL** is to approximate the Q-values in each possible state, using a **deep neural network** with free parameters $\\theta$:\n\n$$Q_\\theta(s, a) \\approx Q^\\pi(s, a) = \\mathbb{E}_\\pi (R_t | s_t=s, a_t=a)$$\n\nThe Q-values now depend on the parameters $\\theta$ of the DNN. The derived policy $\\pi_\\theta$ uses for example an $\\epsilon$-greedy or softmax action selection scheme over the estimated Q-values:\n\n$$\n    \\pi_\\theta(s, a) \\leftarrow \\text{Softmax} (Q_\\theta(s, a))\n$$\n\nThere are two possibilities to approximate Q-values $Q_\\theta(s, a)$:\n\n* The DNN approximates the Q-value of a single $(s, a)$ pair. The action space can be continuous.\n\n![Action value approximation for a single action.](../slides/img/functionapproximation-action1.svg){width=80%}\n\n* The DNN approximates the Q-value of all actions $a$ in a state $s$. The action space must be discrete (one output neuron per action).\n\n![Action value approximation for all actions.](../slides/img/functionapproximation-action2.svg){width=80%}\n\nWe could simply adapt Q-learning with FA to the DNN:\n\n::: {.callout-tip}\n## Naive deep Q-learning with function approximation\n\n* Initialize the deep neural network with parameters $\\theta$. \n\n* Start from an initial state $s_0$.\n\n* for $t \\in [0, T_\\text{total}]$:\n\n    * Select $a_{t}$ using a softmax over the Q-values $Q_\\theta(s_t, a)$.\n\n    * Take $a_t$, observe $r_{t+1}$ and $s_{t+1}$.\n\n    * Update the parameters $\\theta$ by minimizing the loss function:\n\n    $$\\mathcal{L}(\\theta) = (r_{t+1} + \\gamma \\, \\max_{a'} Q_\\theta(s_{t+1}, a') - Q_\\theta(s_t, a_t))^2$$\n:::\n\nThis naive approach will not work: DNNs cannot learn from single examples (online learning = instability). DNNs require **stochastic gradient descent** (SGD):\n\n$$\n    \\mathcal{L}(\\theta) = E_\\mathcal{D} (||\\textbf{t} - \\textbf{y}||^2) \\approx \\frac{1}{K} \\sum_{i=1}^K ||\\textbf{t}_i - \\textbf{y}_i||^2\n$$\n\nThe loss function is estimated by **sampling** a minibatch of $K$ **i.i.d** samples from the training set to compute the loss function and update the parameters $\\theta$. This is necessary to avoid local minima of the loss function. Although Q-learning can learn from single transitions, it is not possible using DNN.  Why not using the last $K$ transitions to train the network? We could store them in a **transition buffer** and train the network on it.\n\n::: {.callout-tip}\n## Naive deep Q-learning with a transition buffer\n\n* Initialize the deep neural network with parameters $\\theta$. \n\n* Initialize an empty **transition buffer** $\\mathcal{D}$ of size $K$: $\\{(s_k, a_k, r_k, s'_k)\\}_{k=1}^K$.\n\n* for $t \\in [0, T_\\text{total}]$:\n\n    * Select $a_{t}$ using a softmax over the Q-values $Q_\\theta(s_t, a)$.\n\n    * Take $a_t$, observe $r_{t+1}$ and $s_{t+1}$.\n\n    * Store $(s_t, a_t, r_{t+1}, s_{t+1})$ in the transition buffer.\n\n    * Every $K$ steps:\n\n        * Update the parameters $\\theta$ using the transition buffer:\n\n        $$\\mathcal{L}(\\theta) = \\frac{1}{K} \\, \\sum_{k=1}^K (r_k + \\gamma \\, \\max_{a'} Q_\\theta(s'_k, a') - Q_\\theta(s_k, a_k))^2$$\n\n        * Empty the transition buffer.\n:::\n\n### Correlated inputs\n\nUnfortunately, this does not work either. The last $K$ transitions $(s, a, r, s')$ are not **i.i.d** (independent and identically distributed). The transition $(s_{t+1}, a_{t+1}, r_{t+2}, s_{t+2})$ **depends** on $(s_{t}, a_{t}, r_{t+1}, s_{t+1})$ by definition, i.e. the transitions are **correlated**. Even worse, when playing video games, successive frames will be very similar or even identical.\n\n![Successive video frames are extremely correlated.](../slides/img/breakout.png){width=80%}\n\nThe actions are also correlated: you move the paddle to the left for several successive steps.\n\nFeeding transitions sequentially to a DNN is the same as giving all MNIST 0's to a DNN, then all 1's, etc... It does not work.\n\n![Correlated vs. uniformaly sampled MNIST digits.](../slides/img/erm-sequential.png){width=100%}\n\nIn SL, we have all the training data **before** training: it is possible to get i.i.d samples by shuffling the training set between two epochs. In RL, we create the \"training set\" (transitions) **during** training: the samples are not i.i.d as we act sequentially over time.\n\n### Non-stationary targets\n\nIn SL, the **targets** $\\mathbf{t}$ do not change over time: an image of a cat stays an image of a cat throughout learning.\n\n$$\\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{t} \\sim \\mathcal{D}} [||\\mathbf{t} - F_\\theta(\\mathbf{x})||^2]$$\n\nThe problem is said **stationary**, as the distribution of the data does not change over time.\n\nIn RL, the **targets** $t = r + \\gamma \\, \\max_{a'} Q_\\theta(s', a')$ do change over time: \n\n* $Q_\\theta(s', a')$ depends on $\\theta$, so after one optimization step, all targets have changed!\n\n* As we improve the policy over training, we collect higher returns.\n\n$$\\mathcal{L}(\\theta) = \\mathbb{E}_{s, a \\sim \\pi_\\theta} [(r + \\gamma \\, \\max_{a'} Q_\\theta(s', a') - Q_\\theta(s, a))^2]$$\n\nNeural networks do not like this at all. After a while, they give up and settle on a **suboptimal** policy.\n\n![Supervised learning has stationary targets, not RL. Learning is much less efficient and optimal in RL.](../slides/img/nonstationarity.svg){width=70%}\n\n## Deep Q-network (DQN)\n\n\n{{< youtube  r17pjvvj3Qc >}}\n\nNon-linear approximators never really worked with RL before 2013 because of:\n\n1. The correlation between successive inputs or outputs.\n2. The non-stationarity of the problem.\n\n\nThese two problems are very bad for deep networks, which end up overfitting the learned episodes or not learning anything at all. Deepmind researchers [@Mnih2013] proposed to use two classical ML tricks to overcome these problems: \n\n1. experience replay memory.\n2. target networks.\n\n### Experience replay memory\n\nTo avoid correlation between samples, (Mnih et al. 2015) proposed to store the $(s, a, r, s')$ transitions in a huge **experience replay memory** or **replay buffer**  $\\mathcal{D}$ (e.g. 1 million transitions).\n\n![Experience replay memory / replay buffer.](../slides/img/ERM.svg){width=60%}\n\nWhen the buffer is full, we simply overwrite old transitions. The Q-learning update is only applied on a **random minibatch** of those past experiences, not the last transitions. This ensure the independence of the samples (non-correlated samples).\n\n::: {.callout-tip}\n## Naive deep Q-learning with experience replay memory\n\n* Initialize value network $Q_{\\theta}$.\n\n* Initialize experience replay memory $\\mathcal{D}$ of maximal size $N$.\n\n* for $t \\in [0, T_\\text{total}]$:\n\n    * Select an action $a_t$ based on $Q_\\theta(s_t, a)$, observe $s_{t+1}$ and $r_{t+1}$.\n\n    * Store $(s_t, a_t, r_{t+1}, s_{t+1})$ in the experience replay memory.\n\n    * Every $T_\\text{train}$ steps:\n\n        * Sample a minibatch $\\mathcal{D}_s$ randomly from $\\mathcal{D}$.\n\n        * For each transition $(s_k, a_k, r_k, s'_k)$ in the minibatch:\n\n            * Compute the target value $t_k = r_k + \\gamma \\, \\max_{a'} Q_{\\theta}(s'_k, a')$\n\n        * Update the value network $Q_{\\theta}$ on $\\mathcal{D}_s$ to minimize:\n\n        $$\\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathcal{D}_s}[(t_k - Q_\\theta(s_k, a_k))^2]$$\n:::\n\nBut wait! The samples of the minibatch are still not i.i.d, as they are not **identically distributed**:\n\n* Some samples were generated with a very old policy $\\pi_{\\theta_0}$.\n* Some samples have been generated recently by the current policy $\\pi_\\theta$.\n\nThe samples of the minibatch do not come from the same distribution, so this should not work, except if you use an **off-policy** algorithm, such as Q-learning!\n\n$$Q^\\pi(s, a) = \\mathbb{E}_{s_t \\sim \\rho_b, a_t \\sim b}[ r_{t+1} + \\gamma \\, \\max_a Q^\\pi(s_{t+1}, a) | s_t = s, a_t=a]$$\n\nIn Q-learning, you can take samples from **any** behavior policy $b$, as long as the coverage assumption stands:\n\n$$ \\pi(s,a) > 0 \\Rightarrow b(s,a) > 0$$\n\nHere, the behavior policy $b$ is a kind of \"superset\" of all past policies $\\pi$ used to fill the ERM, so it \"covers\" the current policy.\n\n$$b = \\{\\pi_{\\theta_0}, \\pi_{\\theta_1}, \\ldots, \\pi_{\\theta_t}\\}$$\n\nSamples from $b$ are i.i.d, so Q-learning is going to work.\n\n::: {.callout-note}\n\n**It is not possible to use an experience replay memory with on-policy algorithms.**\n\n$$Q^\\pi(s, a) = \\mathbb{E}_{s_t \\sim \\rho_\\pi, a_t \\sim \\pi}[ r_{t+1} + \\gamma \\, Q^\\pi(s_{t+1}, a_{t+1}) | s_t = s, a_t=a]$$\n\n$a_{t+1} \\sim \\pi_\\theta$ would not be the same between $\\pi_{\\theta_0}$ (which generated the sample) and $\\pi_{\\theta_t}$ (the current policy).\n:::\n\n### Target network\n\nThe second problem when using DNN for RL is that the target is **non-stationary**, i.e. it changes over time: as the network becomes better, the Q-values have to increase.\n\nIn DQN, the target for the update is not computed from the current deep network $\\theta$:\n\n$$\n    r + \\gamma \\, \\max_{a'} Q_\\theta(s', a')\n$$\n\nbut from a **target network** $\\thetaÂ´$ updated only every few thousands of iterations.\n\n\n$$\n    r + \\gamma \\, \\max_{a'} Q_{\\theta'}(s', a')\n$$\n\n$\\theta'$ is simply a copy of $\\theta$ from the past.\n\n![Target network.](../slides/img/targetnetwork.png){width=60%}\n\nThe DQN loss function becomes:\n\n$$\n    \\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [(r + \\gamma \\, \\max_{a'} Q_{\\theta'}(s', a')) - Q_\\theta(s, a))^2]\n$$\n\nThis allows the target $r + \\gamma \\, \\max_{a'} Q_{\\theta'}(s', a')$ to be **stationary** between two updates. It leaves time for the trained network to catch up with the targets.\n\n![The target network keeps the target constant long enough for the DNN to catch up.](../slides/img/nonstationarity2.svg){width=80%}\n\nThe target network is updated by simply replacing the parameters $\\theta'$ with the current trained parameters $\\theta$:\n\n$$\\theta' \\leftarrow \\theta$$\n\nThe value network $\\theta$ basically learns using an older version of itself...\n\n### DQN algorithm\n\n::: {.callout-tip}\n## DQN: Deep Q-network algorithm\n\n* Initialize value network $Q_{\\theta}$ and target network $Q_{\\theta'}$.\n\n* Initialize experience replay memory $\\mathcal{D}$ of maximal size $N$.\n\n* for $t \\in [0, T_\\text{total}]$:\n\n    * Select an action $a_t$ based on $Q_\\theta(s_t, a)$, observe $s_{t+1}$ and $r_{t+1}$.\n\n    * Store $(s_t, a_t, r_{t+1}, s_{t+1})$ in the experience replay memory.\n\n    * Every $T_\\text{train}$ steps:\n\n        * Sample a minibatch $\\mathcal{D}_s$ randomly from $\\mathcal{D}$.\n\n        * For each transition $(s_k, a_k, r_k, s'_k)$ in the minibatch:\n\n            * Compute the target value $t_k = r_k + \\gamma \\, \\max_{a'} Q_{\\theta'}(s'_k, a')$ using the target network.\n\n        * Update the value network $Q_{\\theta}$ on $\\mathcal{D}_s$ to minimize:\n\n        $$\\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathcal{D}_s}[(t_k - Q_\\theta(s_k, a_k))^2]$$\n\n    * Every $T_\\text{target}$ steps:\n\n        * Update target network: $\\theta' \\leftarrow \\theta$.\n:::\n\nThe deep network can be anything. Deep RL is only about defining the loss function adequately. For pixel-based problems (e.g. video games), convolutional neural networks (without max-pooling) are the weapon of choice.\n\n![Architecture of DQN [@Mnih2013].](../slides/img/DeepQNetwork.jpg){width=100%}\n\nWhy no max-pooling? The goal of max-pooling is to get rid of the spatial information in the image. For object recognition, you do not care whether the object is in the center or on the side of the image. Max-pooling brings **spatial invariance**. In video games, you **want** to keep the spatial information: the optimal action depends on where the ball is relative to the paddle.\n\nAre individual frames good representations of states? Using video frames as states breaks the Markov property: the speed and direction of the ball is a very relevant information for the task, but not contained in a single frame. This characterizes a **Partially-observable Markov Decision Process** (POMDP).\n\nThe simple solution retained in the original DQN paper is to **stack** the last four frames to form the state representation. Having the previous positions of the ball, the network can **learn** to infer its direction of movement.\n\n\n::: {.callout-tip}\n## DQN code in Keras\n\n* Creating the CNN in keras / tensorflow / pytorch is straightforward:\n\n```python\nmodel = Sequential()\n\nmodel.add(Input((4, 84, 84)))\n\nmodel.add(Conv2D(16, (8, 8), strides=(4, 4)), activation='relu'))\n\nmodel.add(Conv2D(32, (4, 4), strides=(2, 2), activation='relu'))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(256, activation='relu'))\n\nmodel.add(Dense(nb_actions, activation='linear'))\n\noptimizer = RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)\n\nmodel.compile(optimizer, loss='mse')\n```\n\n* Each step of the algorithm follows the GPI approach:\n\n```python\ndef q_iteration(env, model, state, memory):\n    # Choose the action with epsilon-greedy\n    if np.random.random() < epsilon:\n        action = env.action_space.sample()\n    else:\n        # Predict the Q-values for the current state and take the greedy action\n        values = model.predict([state])[0]\n        action = values.argmax()\n\n    # Play one game iteration\n    new_state, reward, done, _ = env.step(action)\n\n    # Append the transition to the replay buffer \n    memory.add(state, action, new_state, reward, done)\n\n    # Sample a minibatch from the memory and fit the DQN\n    s, a, r, s_, d = memory.sample_batch(32)\n    fit_batch(model, s, a, r, s_, d)\n```\n\n* The only slight difficulty is actually to compute the targets for learning:\n\n```python\ndef fit_batch(model, states, actions, rewards, next_states, dones)\n\n    # Predict the Q-values in the current state\n    Q_values = model.predict(states)\n    \n    # Predict the Q-values in the next state using the target model\n    next_Q_value = target_model.predict(next_states).max(axis=1)\n    \n    # Terminal states have a value of 0\n    next_Q_value[dones] = 0.0\n    \n    # Compute the target\n    targets = Q_values.copy()\n    for i in range(batch_size):\n        targets[i, actions[i]] = rewards[i] + self.gamma * next_Q_value[i]\n        \n    # Train the model on the minibatch\n    self.model.fit(states, targets, epochs=1, batch_size=batch_size, verbose=0)\n```\n\n:::\n\n### DQN results\n\nDQN was trained using 50M frames (38 days of game experience) per game. Replay buffer of 1M frames. Action selection: $\\epsilon$-greedy with $\\epsilon = 0.1$ and annealing. Optimizer: RMSprop with a batch size of 32.\n\nThe DQN network was trained to solve 49 different Atari 2600 games **with the same architecture and hyperparameters**. In most of the games, the network reaches **super-human** performance. Some games are still badly performed (e.g. Montezuma's revenge), as they require long-term planning. It was the first RL algorithm able to learn different tasks (no free lunch theorem). The 2015 paper in Nature started the hype for deep RL.\n\n![Training curves of DQN [@Mnih2013].](../slides/img/dqn-results.png){width=100%}\n\n\n![Performance of DQN on Atari games [@Mnih2013].](../slides/img/atari-results.png){width=70%}\n## DQN variants\n\n{{< youtube  VM28-5YyLJk >}}\n\n### Double DQN\n\nQ-learning methods, including DQN, tend to **overestimate** Q-values, especially for the non-greedy actions:\n\n$$Q_\\theta(s, a) > Q^\\pi(s, a)$$\n\nThis does not matter much in action selection, as we apply $\\epsilon$-greedy or softmax on the Q-values anyway, but it may make learning slower (sample complexity) and less optimal.\n\n![Q-learning methods overstimate Q-values [@vanHasselt2015].](../slides/img/ddqn-results1.png){width=100%}\n\nTo avoid optimistic estimations, the target is computed by both the value network $\\theta$ and the target network $\\theta'$:\n\n* **Action selection**: The next greedy action $a^*$ is calculated by the **value network** $\\theta$ (current policy):\n\n$$a^* =\\text{argmax}_{a'} Q_{\\theta}(s', a')$$\n\n* **Action evaluation**: Its Q-value for the target is calculated using the **target network** $\\theta'$ (older values):\n\n$$t = r + \\gamma \\, Q_{\\theta'}(sÂ´, a^*)$$\n   \nThis gives the following loss function for **double DQN** (DDQN, [@vanHasselt2015]):\n\n$$\n    \\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [(r + \\gamma \\, Q_{\\theta'}(sÂ´, \\text{argmax}_{a'} Q_{\\theta}(s', a')) - Q_\\theta(s, a))^2]\n$$ \n\n![Estimated Q-values by Double DQN compared to DQN and the ground truth [@vanHasselt2015].](../slides/img/ddqn-results1.png){width=100%}\n\n### Prioritized Experience Replay\n\nThe **experience replay memory** or **replay buffer** is used to store the last 1M or so transitions $(s, a, r, s')$. The learning algorithm **uniformly samples** a minibatch of size $K$ to update its parameters.\n\nNot all transitions are interesting:\n\n* Some transitions were generated by a very old policy, the current policy won't take them anymore.\n* Some transitions are already well predicted: the TD error is small, there is nothing to learn from.\n\n$$\\delta_t = r_{t+1} + \\gamma \\, \\max_{a'} Q_\\theta(s_{t+1}, a_{t+1}) - Q_\\theta(s_t, a_t) \\approx 0$$\n\nThe experience replay memory makes learning very **slow**, as we need a lot of samples to learn something useful: high **sample complexity**. We need a smart mechanism to preferentially pick the transitions that will boost learning the most, without introducing a bias.\n\n**Prioritized sweeping** is actually a quite old idea [@Moore1993]. The idea of **prioritized experience replay** (PER, [@Schaul2015]) is to sample in priority those transitions whose TD error is the highest:\n\n$$\\delta_t = r_{t+1} + \\gamma \\, \\max_{a'} Q_\\theta(s_{t+1}, a_{t+1}) - Q_\\theta(s_t, a_t)$$\n\nIn practice, we insert the transition $(s, a, r, s', \\delta)$ into the replay buffer. To create a minibatch, the sampling algorithm select a transition $k$ based on the probability:\n\n$$P(k) = \\frac{(|\\delta_k| + \\epsilon)^\\alpha}{\\sum_k (|\\delta_k| + \\epsilon)^\\alpha}$$\n\n$\\epsilon$ is a small parameter ensuring that transition with no TD error still get sampled from time to time. $\\alpha$ allows to change the behavior from uniform sampling ($\\alpha=0$, as in DQN) to fully prioritized sampling ($\\alpha=1$). $\\alpha$ should be annealed from 0 to 1 during training. Think of it as a \"kind of\" **softmax** over the TD errors. After the samples have been used for learning, their TD error $\\delta$ is updated in the PER.\n\nThe main drawback is that inserting and sampling can be computationally expensive is we simply sort the transitions based on $(|\\delta_k| + \\epsilon)^\\alpha$:\n\n* Insertion: $\\mathcal{O}(N \\, \\log N)$.\n* Sampling: $\\mathcal{O}(N)$.\n\n![Sorting transitions w.r.t their advantage is expensive. Source: <https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/>](../slides/img/per_bar_1.png){width=90%}\n\nUsing binary **sumtrees** instead of a linear queue, prioritized experience replay can be made efficient in both insertion ($\\mathcal{O}(\\log N)$) and sampling ($\\mathcal{O}(1)$).\n\n![Sumtrees allow efficient insertion and sampling of the PER. Source: <https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682>](../slides/img/per_tree.png){width=70%}\n\n![DQN with PER outperforms DQN [@Schaul2015].](../slides/img/per_results1.png){width=100%}\n\n![DQN with PER outperforms DQN [@Schaul2015].](../slides/img/per_results2.png){width=100%}\n\n### Dueling networks\n\nDQN and its variants learn to predict directly the Q-value of each available action.\n\n![DQN predicts directly the Q-values. Source: [@Wang2016].](../slides/img/duelling1.png){width=60%}\n\nThere are several problems with predicting Q-values with a DNN:\n\n* The Q-values can take high values, especially with different values of $\\gamma$.\n* The Q-values have a high variance, between the minimum and maximum returns obtained during training.\n* For a transition $(s_t, a_t, s_{t+1})$, a single Q-value is updated, not all actions in $s_t$.\n\n\n![The variance of the Q-values between good and bad states is high.](../slides/img/dueling-principle.svg){width=60%}\n\nThe exact Q-values of all actions are not equally important.\n\n* In **bad** states (low $V^\\pi(s)$), you can do whatever you want, you will lose.\n* In neutral states, you can do whatever you want, nothing happens.\n* In **good** states (high $V^\\pi(s)$), you need to select the right action to get rewards, otherwise you lose.\n\n\nAn important notion is the **advantage** $A^\\pi(s, a)$ of an action:\n\n$$\n    A^\\pi(s, a) = Q^\\pi(s, a) - V^\\pi(s)\n$$\n\nIt tells how much return can be expected by taking the action $a$ in the state $s$, **compared** to what is usually obtained in $s$ with the current policy. If a policy $\\pi$ is deterministic and always selects $a^*$ in $s$, we have:\n\n$$\n    A^\\pi(s, a^*) = 0\n$$\n$$\n    A^\\pi(s, a \\neq a^*) < 0\n$$\n\nThis is particularly true for the optimal policy. But if we have separate estimates $V_\\varphi(s)$ and $Q_\\theta(s, a)$, some actions may have a positive advantage. Advantages have **less variance** than Q-values.\n\n![The variance of the advantages is much lower.](../slides/img/dueling-principle2.svg){width=60%}\n\nIn **dueling networks** [@Wang2016], the network is forced to decompose the estimated Q-value $Q_\\theta(s, a)$ into a state value $V_\\alpha(s)$ and an advantage function $A_\\beta(s, a)$:\n\n$$\n    Q_\\theta(s, a) = V_\\alpha(s) + A_\\beta(s, a)\n$$\n\n![Dueling DQN decomposes the Q-values as the sum of the V-value and the advantage of the action. Source: [@Wang2016].](../slides/img/duelling2.png){width=60%}\n\nThe parameters $\\alpha$ and $\\beta$ are just two shared subparts of the NN $\\theta$. The loss function \n\n$$\\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [(r + \\gamma \\, Q_{\\theta'}(sÂ´, \\text{argmax}_{a'} Q_{\\theta}(s', a')) - Q_\\theta(s, a))^2]$$ \n\nis exactly the same as in (D)DQN: only the internal structure of the NN changes.\n\nThe Q-values are the sum of two functions:\n\n$$\n    Q_\\theta(s, a) = V_\\alpha(s) + A_\\beta(s, a)\n$$\n\nHowever, this sum is **unidentifiable**: \n\n$$\n\\begin{aligned}\nQ_\\theta(s, a) = 10 & = 1 + 9 \\\\\n                    & = 2 + 8 \\\\\n                    & = 3 + 7 \\\\\n\\end{aligned}\n$$\n\nTo constrain the sum, (Wang et al. 2016) propose that the greedy action w.r.t the advantages should have an advantage of 0:\n\n$$\n    Q_\\theta(s, a) = V_\\alpha(s) + (A_\\beta(s, a) - \\max_{a'} A_\\beta(s, a'))\n$$\n\nThis way, there is only one solution to the addition. The operation is differentiable, so backpropagation will work. (Wang et al. 2016) show that subtracting the mean advantage works better in practice:\n\n$$\n    Q_\\theta(s, a) = V_\\alpha(s) + (A_\\beta(s, a) - \\frac{1}{|\\mathcal{A}|} \\, \\sum_{a'} A_\\beta(s, a'))\n$$\n\n\n![Dueling DQN network improves over double DQN with PER on most Atari games. Source: [@Wang2016].](../slides/img/dueling-result.png){width=80%}\n\n\n::: {.callout-note}\n## Summary of DQN algorithms\n\n![Architecture of DQN [@Mnih2013].](../slides/img/DeepQNetwork.jpg){width=60%}\n\nDQN and its early variants (double duelling DQN with PER) are an example of **value-based deep RL**. The value $Q_\\theta(s, a)$ of each possible action in a given state is approximated by a convolutional neural network. The NN has to minimize the mse between the predicted Q-values and the target value corresponding to the Bellman equation:\n\n$$\\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [(r + \\gamma \\, Q_{\\theta'}(sÂ´, \\text{argmax}_{a'} Q_{\\theta}(s', a')) - Q_\\theta(s, a))^2]$$ \n\nThe use of an **experience replay memory** and of **target networks** allows to stabilize learning and avoid suboptimal policies. The main drawback of DQN is **sample complexity**: it needs huge amounts of experienced transitions to find a correct policy. The sample complexity come from the deep network itself (gradient descent is iterative and slow), but also from the ERM: it contains 1M transitions, most of which are outdated.\nValue-based algorithms only work for **small and discrete action spaces** (one output neuron per action).\n:::"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"3.1-DQN.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}