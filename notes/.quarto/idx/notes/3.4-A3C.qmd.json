{"title":"Advantage actor-critic (A2C, A3C)","markdown":{"headingText":"Advantage actor-critic (A2C, A3C)","containsRefs":false,"markdown":"\nSlides: [html](../slides/3.4-A3C.html){target=\"_blank\"} [pdf](../slides/pdf/3.4-A3C.pdf){target=\"_blank\"}\n\n## Distributed actor-critic\n\n{{< youtube  89JH4CGD1Uo >}}\n\n\nLet's consider an **n-step advantage actor-critic** architecture where the Q-value of the action $(s_t, a_t)$ is approximated by the **n-step return**:\n\n$$Q^{\\pi_\\theta}(s_t, a_t) \\approx R_t^n =  \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V_\\varphi(s_{t+n})$$\n\n![n-step returns. Source: [@Sutton1998]. ](../slides/img/nstep.png){width=70%}\n\nThe **actor** $\\pi_\\theta(s, a)$ uses PG with baseline to learn the policy:\n\n$$\n    \\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) \\, (R_t^n - V_\\varphi(s_t)) ]\n$$\n\n\nThe **critic** $V_\\varphi(s)$ approximates the value of each state by minimizing the mse:\n\n$$\n    \\mathcal{L}(\\varphi) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[(R^n_t - V_\\varphi(s_t))^2]\n$$\n\n\n![Advantage actor-critic. ](../slides/img/a2c.svg){width=70%}\n\nThe advantage actor-critic is strictly **on-policy**:\n\n* The critic **must** evaluate actions selected the current version of the actor $\\pi_\\theta$, not an old version or another policy.\n* The actor must learn from the current value function $V^{\\pi_\\theta} \\approx V_\\varphi$.\n\n$$\\begin{cases}\n    \\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) \\, (R^n_t - V_\\varphi(s_t)) ] \\\\\n    \\\\\n    \\mathcal{L}(\\varphi) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[(R^n_t - V_\\varphi(s_t))^2] \\\\\n\\end{cases}$$\n\nWe cannot use an **experience replay memory** to deal with the correlated inputs, as it is only for off-policy methods. We cannot either get an uncorrelated batch of transitions by acting **sequentially** with a single agent.\n\nA simple solution is to have **multiple actors** with the same weights $\\theta$ interacting **in parallel** with different copies of the environment: **distributed learning**.\n\n![Distributed learning. Source: <https://ray.readthedocs.io/en/latest/rllib.html>](../slides/img/a2c-arch.svg){width=100%}\n\nEach **rollout worker** (actor) starts an episode in a different state: at any point of time, the workers will be in **uncorrelated states**. From time to time, the workers all send their experienced transitions to the **learner** which updates the policy using a **batch of uncorrelated transitions**.  After the update, the workers use the new policy.\n\n\n::: {.callout-tip}\n## Distributed RL\n\n* Initialize global policy or value network $\\theta$. \n\n* Initialize $N$ copies of the environment in different states.\n\n* **while** True:\n\n    * **for** each worker in parallel:\n\n        * Copy the global network parameters $\\theta$ to each worker:\n\n        $$\\theta_k \\leftarrow \\theta$$\n\n        * Initialize an empty transition buffer $\\mathcal{D}_k$.\n\n        * Perform $d$ steps with the worker on its copy of the environment.\n\n        * Append each transition $(s, a, r, s')$ to the transition buffer.\n\n    * join(): wait for each worker to terminate.\n\n    * Gather the $N$ transition buffers into a single buffer $\\mathcal{D}$.\n\n    * Update the global network on $\\mathcal{D}$ to obtain new weights $\\theta$.\n:::\n\n\nDistributed learning can be used for any deep RL algorithm, including DQN variants. Distributed DQN variants include GORILA, IMPALA, APE-X, R2D2.\"All\" you need is one (or more) GPU for training the global network and $N$ CPU cores for the workers. The workers fill the ERM much more quickly.\n\nIn practice, managing the communication between the workers and the global network through processes can be quite painful. There are some **frameworks** abstracting the dirty work, such as **RLlib** <https://ray.readthedocs.io/en/latest/rllib.html>.\n\n![`rllib` framework. Source: <https://ray.readthedocs.io/en/latest/rllib.html>](../slides/img/rllib.svg){width=100%}\n\nHaving multiple workers interacting with different environments is easy in simulation (Atari games). With physical environments, working in real time, it requires lots of money...\n\n{{< youtube  iaF43Ze1oeI >}}\n\n\n## A3C: Asynchronous advantage actor-critic\n\n{{< youtube  HRLXCmC0ooc >}}\n\n[@Mnih2016] proposed the **A3C** algorithm (asynchronous advantage actor-critic). The stochastic policy $\\pi_\\theta$ is produced by the **actor** with weights $\\theta$ and learned using :\n\n$$\\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) \\, (R^n_t - V_\\varphi(s_t)) ]$$\n\nThe value of a state $V_\\varphi(s)$ is produced by the **critic** with weights $\\varphi$, which minimizes the mse with the **n-step return**:\n\n$$\\mathcal{L}(\\varphi) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[(R^n_t - V_\\varphi(s_t))^2]$$\n\n$$R^n_t =  \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V_\\varphi(s_{t+n})$$\n\nBoth the actor and the critic are trained on batches of transitions collected using **parallel workers**. Two things are different from the general distributed approach: workers compute **partial gradients** and updates are **asynchronous**.\n\n![A3C distributed architecture [@Mnih2016].](../slides/img/a3c-parallel.png){width=100%}\n\n::: {.callout-tip}\n## Worker\n* **def** worker($\\theta$, $\\varphi$):\n\n    * Initialize empty transition buffer $\\mathcal{D}$. Initialize the environment to the **last** state visited by this worker.\n\n    * **for** $n$ steps:\n\n        * Select an action using $\\pi_\\theta$, store the transition in the transition buffer.\n\n    * **for** each transition in $\\mathcal{D}$:\n\n        * Compute the **n-step return** in each state \n        \n        $$R^n_t =  \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V_\\varphi(s_{t+n})$$\n\n    * Compute **policy gradient** for the actor on the transition buffer:\n\n    $$d\\theta = \\nabla_\\theta \\mathcal{J}(\\theta) = \\frac{1}{n} \\sum_{t=1}^n \\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) \\, (R^n_t - V_\\varphi(s_t))$$\n\n\n    * Compute **value gradient** for the critic on the transition buffer:\n\n    $$d\\varphi = \\nabla_\\varphi \\mathcal{L}(\\varphi) = -\\frac{1}{n} \\sum_{t=1}^n (R^n_t - V_\\varphi(s_t)) \\, \\nabla_\\varphi V_\\varphi(s_t)$$\n\n    * **return** $d\\theta$, $d\\varphi$\n:::\n\n::: {.callout-tip}\n## A2C algorithm\n\n* Initialize actor $\\theta$ and critic $\\varphi$.\n\n* Initialize $K$ workers with a copy of the environment. \n\n* **for** $t \\in [0, T_\\text{total}]$:\n\n    * **for** $K$ workers **in parallel**:\n\n        * $d\\theta_k$, $d\\varphi_k$ = worker($\\theta$, $\\varphi$)\n\n    * join()\n\n    * Merge all gradients:\n\n    $$d\\theta = \\frac{1}{K} \\sum_{i=1}^K d\\theta_k \\; ; \\; d\\varphi = \\frac{1}{K} \\sum_{i=1}^K d\\varphi_k$$\n\n    * Update the actor and critic using gradient ascent/descent:\n\n    $$\\theta \\leftarrow \\theta + \\eta \\, d\\theta \\; ; \\; \\varphi \\leftarrow \\varphi - \\eta \\, d\\varphi$$\n:::\n\n\nThe previous algorithm depicts **A2C**, the synchronous version of A3C. A2C synchronizes the workers (threads), i.e. it waits for the $K$ workers to finish their job before merging the gradients and updating the global networks.\n\nA3C is **asynchronous**:\n\n* The partial gradients are applied to the global networks **as soon as** they are available.\n* No need to wait for all workers to finish their job.\n\nAs the workers are not synchronized, this means that one worker could be copying the global networks $\\theta$ and $\\varphi$ **while** another worker is writing them. This is called a **Hogwild!** update [@Niu2011]: no locks, no semaphores. Many workers can read/write the same data. It turns out NN are robust enough for this kind of updates.\n\n::: {.callout-tip}\n## A3C: asynchronous updates\n\n* Initialize actor $\\theta$ and critic $\\varphi$.\n\n* Initialize $K$ workers with a copy of the environment. \n\n* **for** $K$ workers **in parallel**:\n\n    * **for** $t \\in [0, T_\\text{total}]$:\n\n        * Copy the global networks $\\theta$ and $\\varphi$.\n\n        * Compute partial gradients: \n\n        $$d\\theta_k, d\\varphi_k = \\text{worker}(\\theta, \\varphi)$$\n\n        * Update the **global** actor and critic using the **partial gradients**:\n\n        $$\\theta \\leftarrow \\theta + \\eta \\, d\\theta_k$$\n\n        $$\\varphi \\leftarrow \\varphi - \\eta \\, d\\varphi_k$$\n:::\n\nA3C does not use an *experience replay memory* as DQN. Instead, it uses **multiple parallel workers** to distribute learning. Each worker has a copy of the actor and critic networks, as well as an instance of the environment. Weight updates are synchronized regularly though a **master network** using Hogwild!-style updates (every $n=5$ steps!). Because the workers learn different parts of the state-action space, the weight updates are not very correlated.\n\nA3C still needs target networks to ensure stability. It works best on shared-memory systems (multi-core) as communication costs between GPUs are huge.\n\nA3C set a new record for Atari games in 2016. The main advantage is that the workers gather experience in parallel: training is much faster than with DQN. LSTMs can be used to improve the performance.\n\n![A3C results [@Mnih2016].](../slides/img/a3c-comparison.png){width=100%}\n![A3C results [@Mnih2016].](../slides/img/a3c-time.png){width=70%}\n\nLearning is only marginally better with more threads:\n\n\n![A3C results [@Mnih2016].](../slides/img/a3c-threads1.png){width=100%}\n\nbut much faster!\n\n![A3C results [@Mnih2016].](../slides/img/a3c-threads2.png){width=100%}\n\n{{< youtube  0xo1Ldx3L5Q >}}\n\n{{< youtube  nMR5mjCFZCw >}}\n\n{{< youtube  Ajjc08-iPx8 >}}\n\nA3C came up in 2016. A lot of things happened since then...\n\n![The Rainbow network combines all DQN improvements and outperforms each of them. Source: [@Hessel2017].](../slides/img/rainbow-results1.png){width=80%}"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"3.4-A3C.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}