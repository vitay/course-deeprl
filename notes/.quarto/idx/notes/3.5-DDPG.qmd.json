{"title":"Deep Deterministic Policy Gradient (DDPG)","markdown":{"headingText":"Deep Deterministic Policy Gradient (DDPG)","containsRefs":false,"markdown":"\nSlides: [html](../slides/3.5-DDPG.html){target=\"_blank\"} [pdf](../slides/pdf/3.5-DDPG.pdf){target=\"_blank\"}\n\n## Deterministic policy gradient theorem\n\n{{< youtube  knqtWwp8qoM >}}\n\n### Problem with stochastic policies\n\nActor-critic methods are strictly **on-policy**: the transitions used to train the critic **must** be generated by the current version of the actor.\n\n$$\n    \\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) \\, (R_t - V_\\varphi(s_t)) ]\n$$\n\n$$\n    \\mathcal{L}(\\varphi) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[(R_t - V_\\varphi(s_t))^2]\n$$\n\nPast transitions cannot be reused to train the actor (no replay memory). Domain knowledge cannot be used to guide the exploration.\n\nThe learned policy $\\pi_\\theta(s, a)$ is **stochastic**. This generates a lot of **variance** in the obtained returns, therefore in the gradients. This can greatly impair learning (bad convergence) and slow it down (sample complexity). We would not have this problem if the policy was **deterministic** as in off-policy methods.\n\n\nThe objective function that we tried to maximize until now is :\n\n$$\n    \\mathcal{J}(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[R(\\tau)]\n$$\n\ni.e. we want the returns of all trajectories generated by the **stochastic policy** $\\pi_\\theta$ to be maximal.\n\nIt is equivalent to say that we want the value of **all** states visited by the policy $\\pi_\\theta$ to be maximal: a policy $\\pi$ is better than another policy $\\pi'$ if its expected return is greater or equal than that of $\\pi'$ for all states $s$.\n\n$$\\pi > \\pi' \\Leftrightarrow V^{\\pi}(s) > V^{\\pi'}(s) \\quad \\forall s \\in \\mathcal{S}$$\n\nThe objective function can be rewritten as:\n\n$$\n    \\mathcal{J}'(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta}[V^{\\pi_\\theta}(s)]\n$$\n\nwhere $\\rho_\\theta$ now represents the **state visitation distribution**, i.e. how often a state $s$ will be visited by the policy $\\pi_\\theta$.\n\nThe two objective functions:\n\n$$\n    \\mathcal{J}(\\theta) =  \\mathbb{E}_{\\tau \\sim \\rho_\\theta}[R(\\tau)]\n$$\n\nand:\n\n$$\n    \\mathcal{J}'(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta}[V^{\\pi_\\theta}(s)]\n$$\n\nare not the same: $\\mathcal{J}$ has different values than $\\mathcal{J}'$.\n\nHowever, they have a maximum for the same **optimal policy** $\\pi^*$ and their gradient is the same:\n\n$$\n    \\nabla_\\theta \\, \\mathcal{J}(\\theta) =  \\nabla_\\theta \\, \\mathcal{J}'(\\theta)\n$$\n\nIf a change in the policy $\\pi_\\theta$ increases the return of all trajectories, it also increases the value of the visited states.  Take-home message: their **policy gradient** is the same, we have the right to re-define the problem like this.\n\n$$\n   g = \\nabla_\\theta \\, \\mathcal{J}(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta}[\\nabla_\\theta \\, V^{\\pi_\\theta}(s)]\n$$\n\n### Deterministic policy gradient\n\nWhen introducing Q-values, we obtain the following policy gradient:\n\n$$\n   g = \\nabla_\\theta \\, \\mathcal{J}(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta}[\\nabla_\\theta \\, V^{\\pi_\\theta}(s)] =  \\mathbb{E}_{s \\sim \\rho_\\theta}[\\sum_a \\nabla_\\theta \\, \\pi_\\theta(s, a) \\, Q^{\\pi_\\theta}(s, a)]\n$$\n\nThis formulation necessitates to integrate overall possible actions. \n\n* Not possible with continuous action spaces.\n* The stochastic policy adds a lot of variance.\n\nBut let's suppose that the policy is **deterministic**, i.e. it takes a single action in state $s$. We can note this deterministic policy $\\mu_\\theta(s)$, with:\n\n$$\n\\begin{aligned}\n    \\mu_\\theta :  \\; \\mathcal{S} & \\rightarrow \\mathcal{A} \\\\\n    s & \\; \\rightarrow \\mu_\\theta(s) \\\\\n\\end{aligned}\n$$\n\nThe deterministic policy gradient becomes:\n\n$$\n   g = \\nabla_\\theta \\, \\mathcal{J}(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta}[\\nabla_\\theta \\, Q^{\\mu_\\theta}(s, \\mu_\\theta(s))]\n$$\n\n\n\nWe can now use the chain rule to decompose the gradient of $Q^{\\mu_\\theta}(s, \\mu_\\theta(s))$:\n\n$$\\nabla_\\theta \\, Q^{\\mu_\\theta}(s, \\mu_\\theta(s)) = \\nabla_a \\, Q^{\\mu_\\theta}(s, a)|_{a = \\mu_\\theta(s)} \\times \\nabla_\\theta \\mu_\\theta(s)$$\n\n![Chain rule applied to the deterministic policy gradient. ](../slides/img/dpg-chainrule.svg){width=80%}\n\n\n$\\nabla_a \\, Q^{\\mu_\\theta}(s, a)|_{a = \\mu_\\theta(s)}$ means that we differentiate $Q^{\\mu_\\theta}$ w.r.t. $a$, and evaluate it in $\\mu_\\theta(s)$. $a$ is a variable, but $\\mu_\\theta(s)$ is a deterministic value (constant).\n\n$\\nabla_\\theta \\mu_\\theta(s)$ tells how the output of the policy network varies with the parameters of NN: automatic differentiation frameworks such as tensorflow can tell you that.\n\n::: {.callout-tip}\n## Deterministic policy gradient theorem [@Silver2014]\n\nFor any MDP, the **deterministic policy gradient** is:\n\n$$\\nabla_\\theta \\, \\mathcal{J}(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta}[\\nabla_a \\, Q^{\\mu_\\theta}(s, a)|_{a = \\mu_\\theta(s)} \\times \\nabla_\\theta \\mu_\\theta(s)]$$\n:::\n\n### Off-policy actor-critic\n\nAs always, you do not know the true Q-value $Q^{\\mu_\\theta}(s, a)$, because you search for the policy $\\mu_\\theta$. [@Silver2014] showed that you can safely (without introducing any bias) replace the true Q-value with an estimate $Q_\\varphi(s, a)$, as long as the estimate minimizes the mse with the TD target:\n\n$$Q_\\varphi(s, a) \\approx Q^{\\mu_\\theta}(s, a)$$\n\n$$\\mathcal{L}(\\varphi) = \\mathbb{E}_{s \\sim \\rho_\\theta}[(r(s, \\mu_\\theta(s)) + \\gamma \\, Q_\\varphi(s', \\mu_\\theta(s')) - Q_\\varphi(s, \\mu_\\theta(s)))^2]$$\n\nWe come back to an actor-critic architecture:\n\n* The **deterministic actor** $\\mu_\\theta(s)$ selects a single action in state $s$.\n* The **critic** $Q_\\varphi(s, a)$ estimates the value of that action. \n\n![Actor-critic architecture of the deterministic policy gradient with function approximation. ](../slides/img/dpg.svg){width=80%}\n\n**Training the actor:**\n\n$$\n    \\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E}_{s \\sim \\rho_\\theta}[\\nabla_\\theta \\mu_\\theta(s) \\times \\nabla_a Q_\\varphi(s, a) |_{a = \\mu_\\theta(s)}]\n$$\n\n**Training the critic:**\n\n$$\n    \\mathcal{L}(\\varphi) = \\mathbb{E}_{s \\sim \\rho_\\theta}[(r(s, \\mu_\\theta(s)) + \\gamma \\, Q_\\varphi(s', \\mu_\\theta(s')) - Q_\\varphi(s, \\mu_\\theta(s)))^2]\n$$\n\nIf you act off-policy, i.e. you visit the states $s$ using a **behavior policy** $b$, you would theoretically need to correct the policy gradient with **importance sampling**:\n\n$$\n    \\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E}_{s \\sim \\rho_b}[\\sum_a \\, \\frac{\\pi_\\theta(s, a)}{b(s, a)} \\, \\nabla_\\theta \\mu_\\theta(s) \\times \\nabla_a Q_\\varphi(s, a) |_{a = \\mu_\\theta(s)}]\n$$\n\nBut your policy is now **deterministic**: the actor only takes the action $a=\\mu_\\theta(s)$ with probability 1, not $\\pi(s, a)$. The **importance weight** is 1 for that action, 0 for the other. You can safely sample states from a behavior policy, it won't affect the deterministic policy gradient:\n\n$$\n    \\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E}_{s \\sim \\rho_b}[\\nabla_\\theta \\mu_\\theta(s) \\times \\nabla_a Q_\\varphi(s, a) |_{a = \\mu_\\theta(s)}]\n$$\n\nThe critic uses Q-learning, so it is also off-policy. **DPG is an off-policy actor-critic architecture!**\n\n\n## DDPG: Deep Deterministic Policy Gradient\n\n{{< youtube  9vdo91DE1ZY >}}\n\nAs the name indicates, DDPG [@Lillicrap2015] is the deep variant of DPG for **continuous control**. It uses the DQN tricks to stabilize learning with deep networks:\n\n* As DPG is **off-policy**, an **experience replay memory** can be used to sample experiences.\n* The **actor** $\\mu_\\theta$ learns using sampled transitions with DPG.\n* The **critic** $Q_\\varphi$ uses Q-learning on sampled transitions: **target networks** can be used to cope with the non-stationarity of the Bellman targets.\n\n\nContrary to DQN, the target networks are not updated every once in a while, but slowly **integrate** the trained networks after each update (moving average of the weights):\n\n$$\\theta' \\leftarrow \\tau \\theta + (1-\\tau) \\, \\theta'$$\n\n$$\\varphi' \\leftarrow \\tau \\varphi + (1-\\tau) \\, \\varphi'$$\n\nA deterministic actor is good for learning (less variance), but not for exploring. We cannot use $\\epsilon$-greedy or softmax, as the actor outputs directly the policy, not Q-values. For continuous actions, an **exploratory noise** can be added to the deterministic action:\n\n$$a_t = \\mu_\\theta(s_t) + \\xi_t$$\n\nEx: if the actor wants to move the joint of a robot by $2^o$, it will actually be moved from $2.1^o$ or $1.9^o$.\n\n![Deep deterministic policy gradient architecture with exploratory noise. ](../slides/img/ddpg.svg){width=100%}\n\nIn DDPG, an **Ornstein-Uhlenbeck** stochastic process [@Uhlenbeck1930] is used to add noise to the continuous actions. It is defined by a **stochastic differential equation**, classically used to describe Brownian motion:\n\n$$ dx_t = \\theta (\\mu - x_t) dt + \\sigma dW_t \\qquad \\text{with} \\qquad dW_t = \\mathcal{N}(0, dt)$$\n\nThe temporal mean of $x_t$ is $\\mu= 0$, its amplitude is $\\theta$ (exploration level), its speed is $\\sigma$.\n\n![Ornstein-Uhlenbeck stochastic process.](../slides/img/OU.png){width=100%}\n\nAnother approach to ensure exploration is to add noise to the **parameters** $\\theta$ of the actor at inference time. For the same input $s_t$, the output $\\mu_\\theta(s_t)$ will be different every time. The **NoisyNet** [@Fortunato2017] approach can be applied to any deep RL algorithm to enable a smart state-dependent exploration (e.g. Noisy DQN).\n\n::: {.callout-tip}\n## DDPG: deep deterministic policy gradient\n\n* Initialize actor network $\\mu_{\\theta}$ and critic $Q_\\varphi$, target networks $\\mu_{\\theta'}$ and $Q_{\\varphi'}$,  ERM $\\mathcal{D}$ of maximal size $N$, random process $\\xi$.\n\n* for $t \\in [0, T_\\text{max}]$:\n\n    * Select the action $a_t = \\mu_\\theta(s_t) + \\xi$ and store $(s_t, a_t, r_{t+1}, s_{t+1})$ in the ERM.\n        \n    * For each transition $(s_k, a_k, r_k, s'_k)$ in a minibatch of $K$ transitions randomly sampled from $\\mathcal{D}$:\n        \n        * Compute the target value using target networks \n        \n        $$t_k = r_k + \\gamma \\, Q_{\\varphi'}(s'_k, \\mu_{\\theta'}(s'_k))$$\n        \n    * Update the critic by minimizing:\n\n    $$\\mathcal{L}(\\varphi) = \\frac{1}{K} \\sum_k (t_k - Q_\\varphi(s_k, a_k))^2$$\n        \n    * Update the actor by applying the deterministic policy gradient:\n\n    $$\\nabla_\\theta \\mathcal{J}(\\theta) = \\frac{1}{K} \\sum_k \\nabla_\\theta \\mu_\\theta(s_k) \\times \\nabla_a Q_\\varphi(s_k, a) |_{a = \\mu_\\theta(s_k)}$$\n        \n    * Update the target networks: \n    \n    $$\\theta' \\leftarrow \\tau \\theta + (1-\\tau) \\, \\theta' \\; ; \\; \\varphi' \\leftarrow \\tau \\varphi + (1-\\tau) \\, \\varphi'$$\n:::\n\nDDPG allows to learn continuous policies: there can be one tanh output neuron per joint in a robot. The learned policy is deterministic: this simplifies learning as we do not need to integrate over the action space after sampling. Exploratory noise (e.g. Ohrstein-Uhlenbeck) has to be added to the selected action during learning in order to ensure exploration. DDPG allows to use an experience replay memory, reusing past samples (better sample complexity than A3C).\n\n{{< youtube  iFg5lcUzSYU >}}\n\n::: {.callout-note}\n## Example: learning to drive in a day\n\n{{< youtube  eRwTbRtnT1I >}}\n\n\n![](../slides/img/ddpg-drive.png)\n\nThe algorithm of [@Kendall2018] is based on DDPG with prioritized experience replay. Training is live, with an on-board NVIDIA Drive PX2 GPU. A simulated environment is first used to find the hyperparameters.\nA variational autoencoder (VAE) is optionally use to pretrain the convolutional layers on random episodes. \n\nMore info: <https://wayve.ai/blog/learning-to-drive-in-a-day-with-reinforcement-learning>\n:::\n\n\n## TD3 - Twin Delayed Deep Deterministic policy gradient\n\n{{< youtube  dJ-nPMcZZxo >}}\n\nDDPG suffers from several problems:\n\n* Unstable (catastrophic forgetting, policy collapse).\n* Brittleness (sensitivity to hyperparameters such as learning rates).\n* Overestimation of Q-values.\n\n\nPolicy collapse happens when the bias of the critic is too high for the actor. Example with A2C:\n\n![Policy collapse happens regularly without  warning: the performance is back to random. Source: Oliver Lange (2019). Investigation of Model-Based Augmentation of Model-Free Reinforcement Learning Algorithms. MSc thesis, TU Chemnitz.](../slides/img/policy-collapse.png){width=70%}\n\nTD3 [@Fujimoto2018] has been introduced to fix the problems of DDPG.\n\n### Twin critics against overestimation\n\nAs any Q-learning-based method, DDPG **overestimates** Q-values. The Bellman target $t = r + \\gamma \\, \\max_{a'} Q(s', a')$ uses a maximum over other values, so it is increasingly overestimated during learning. After a while, the overestimated Q-values disrupt training in the actor.\n\n![Overestimation of Q-values by DDPG and TD3 (labelled CDQ here - clipped double Q-learning). Source: [@Fujimoto2018]](../slides/img/td3-overestimation.png){width=70%}\n\nDouble Q-learning solves the problem by using the target network $\\theta'$ to estimate Q-values, but the value network $\\theta$ to select the greedy action in the next state:\n\n$$\n    \\mathcal{L}(\\theta) = \\mathbb{E}_\\mathcal{D} [(r + \\gamma \\, Q_{\\theta'}(s´, \\text{argmax}_{a'} Q_{\\theta}(s', a')) - Q_\\theta(s, a))^2]\n$$\n\nThe idea is to use two different independent networks to reduce overestimation. This does not work well with DDPG, as the Bellman target $t = r + \\gamma \\, Q_{\\varphi'}(s', \\mu_{\\theta'}(s'))$ uses a target actor network that is not very different from the trained deterministic actor.\n\nTD3 uses two critics $\\varphi_1$ and $\\varphi_2$ (and target critics):  the Q-value used to train the actor will be the **lesser of two evils**, i.e. the minimum Q-value:\n\n$$t = r + \\gamma \\, \\min(Q_{\\varphi'_1}(s', \\mu_{\\theta'}(s')), Q_{\\varphi'_2}(s', \\mu_{\\theta'}(s')))$$\n\nOne of the critic will always be less over-estimating than the other. Better than nothing... Using twin critics is called **clipped double learning**.\n\nBoth critics learn in parallel using the same target:\n\n$$\\mathcal{L}(\\varphi_1) = \\mathbb{E}[(t - Q_{\\varphi_1}(s, a))^2] \\qquad ; \\qquad \\mathcal{L}(\\varphi_2) = \\mathbb{E}[ (t - Q_{\\varphi_2}(s, a))^2]$$\n\nThe actor is trained using the first critic only:\n\n$$\\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E}[ \\nabla_\\theta \\mu_\\theta(s) \\times \\nabla_a Q_{\\varphi_1}(s, a) |_{a = \\mu_\\theta(s)} ]$$\n\n### Delayed learning for stability\n\nAnother issue with actor-critic architecture in general is that the critic is always biased during training, what can impact the actor and ultimately collapse the policy:\n\n$$\\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E}[ \\nabla_\\theta \\mu_\\theta(s) \\times \\nabla_a Q_{\\varphi_1}(s, a) |_{a = \\mu_\\theta(s)} ]$$\n\n$$Q_{\\varphi_1}(s, a) \\approx Q^{\\mu_\\theta}(s, a)$$\n\nThe critic should learn much faster than the actor in order to provide **unbiased** gradients.\nIncreasing the learning rate in the critic creates instability, reducing the learning rate in the actor slows down learning.\nThe solution proposed by TD3 is to **delay** the update of the actor, i.e. update it only every $d$ minibatches:\n\n* Train the critics $\\varphi_1$ and $\\varphi_2$ on the minibatch.\n* **every** $d$ steps:\n    * Train the actor $\\theta$ on the minibatch.\n\nThis leaves enough time to the critics to improve their prediction and provides less biased gradients to the actor.\n\n### Target exploration\n\nA last problem with deterministic policies is that they tend to always select the same actions $\\mu_\\theta(s)$ (overfitting). For exploration, some additive noise is added to the selected action:\n\n$$a = \\mu_\\theta(s) + \\xi$$\n\nBut this is not true for the Bellman targets, which use the deterministic action:\n\n$$t = r + \\gamma \\, Q_{\\varphi}(s', \\mu_{\\theta}(s'))$$\n\nTD3 proposes to also use additive noise in the Bellman targets:\n\n$$t = r + \\gamma \\, Q_{\\varphi}(s', \\mu_{\\theta}(s') + \\xi)$$\n\nIf the additive noise is zero on average, the Bellman targets will be correct on average (unbiased) but will prevent overfitting of particular actions. The additive noise does not have to be an **Ornstein-Uhlenbeck** stochastic process, but could simply be a random variable:\n\n$$\\xi \\sim \\mathcal{N}(0, 1)$$\n\n### Algorithm\n\n::: {.callout-tip}\n## TD3 - Twin Delayed Deep Deterministic policy gradient\n\n* Initialize actor $\\mu_{\\theta}$, critics $Q_{\\varphi_1}, Q_{\\varphi_2}$, target networks $\\mu_{\\theta'}, Q_{\\varphi_1'},Q_{\\varphi_2'}$,  ERM $\\mathcal{D}$, random processes $\\xi_1, \\xi_2$.\n\n* for $t \\in [0, T_\\text{max}]$:\n\n    * Select the action $a_t = \\mu_\\theta(s_t) + \\xi_1$ and store $(s_t, a_t, r_{t+1}, s_{t+1})$ in the ERM.\n        \n    * For each transition $(s_k, a_k, r_k, s'_k)$ in a minibatch sampled from $\\mathcal{D}$:\n        * Compute the target \n        \n        $$t_k = r_k + \\gamma \\, \\min(Q_{\\varphi_1'}(s'_k, \\mu_{\\theta'}(s'_k) + \\xi_2), Q_{\\varphi_2'}(s'_k, \\mu_{\\theta'}(s'_k) + \\xi_2))$$\n        \n    * Update the critics by minimizing:\n\n    $$\\mathcal{L}(\\varphi_1) = \\frac{1}{K} \\sum_k (t_k - Q_{\\varphi_1}(s_k, a_k))^2 \\qquad ; \\qquad \\mathcal{L}(\\varphi_2) = \\frac{1}{K} \\sum_k (t_k - Q_{\\varphi_2}(s_k, a_k))^2$$\n        \n    * **every** $d$ steps:\n\n        * Update the actor by applying the DPG using $Q_{\\varphi_1}$:\n\n        $$\\nabla_\\theta \\mathcal{J}(\\theta) = \\frac{1}{K} \\sum_k \\nabla_\\theta \\mu_\\theta(s_k) \\times \\nabla_a Q_{\\varphi_1}(s_k, a) |_{a = \\mu_\\theta(s_k)}$$\n        \n        * Update the target networks: \n        \n        $$\\theta' \\leftarrow \\tau \\theta + (1-\\tau) \\, \\theta' \\; ; \\; \\varphi_1' \\leftarrow \\tau \\varphi_1 + (1-\\tau) \\, \\varphi_1' \\; ; \\; \\varphi_2' \\leftarrow \\tau \\varphi_2 + (1-\\tau) \\, \\varphi_2'$$\n:::\n\nTD3 [@Fujimoto2018] introduces three major changes to DDPG:\n\n* **twin** critics.\n* **delayed** actor updates.\n* noisy Bellman targets.\n\n\nTD3 outperforms DDPG (but also PPO and SAC) on continuous control tasks.\n\n![Performance of TD3 on continuous control tasks compared to the state-of-the-art. Source: [@Fujimoto2018]](../slides/img/td3-results.png){width=100%}\n\n\n## D4PG: Distributed Distributional DDPG\n\n\nD4PG (Distributed Distributional DDPG, [@Barth-Maron2018]) combines:\n\n* **Deterministic policy gradient** as in DDPG:\n\n$$\\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E}_{s \\sim \\rho_b}[\\nabla_\\theta \\mu_\\theta(s) \\times \\nabla_a \\mathbb{E} [\\mathcal{Z}_\\varphi(s, a)] |_{a = \\mu_\\theta(s)}]$$\n\n* **Distributional critic**: The critic does not predict single Q-values $Q_\\varphi(s, a)$, but the distribution of returns $\\mathcal{Z}_\\varphi(s, a)$ (as in Categorical DQN):\n\n$$\\mathcal{L}(\\varphi) = \\mathbb{E}_{s \\in \\rho_b} [ \\text{KL}(\\mathcal{T} \\, \\mathcal{Z}_\\varphi(s, a) || \\mathcal{Z}_\\varphi(s, a))]$$\n\n* **n-step** returns (as in A3C):\n\n$$\\mathcal{T} \\, \\mathcal{Z}_\\varphi(s_t, a_t)= \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, \\mathcal{Z}_\\varphi(s_{t+n}, \\mu_\\theta(s_{t+n}))$$\n\n* **Distributed workers**: D4PG uses $K=32$ or $64$ copies of the actor to fill the ERM in parallel.\n\n* **Prioritized Experience Replay** (PER): \n\n$$P(k) = \\frac{(|\\delta_k| + \\epsilon)^\\alpha}{\\sum_k (|\\delta_k| + \\epsilon)^\\alpha}$$\n\nIt could be called the Rainbow DDPG.\n\n![All components of D4PG are necessary to beat DDPG. Source: [@Barth-Maron2018]](../slides/img/d4pg-results.png){width=100%}\n\n\n{{< youtube  9kGdCjJtNls >}}\n\n\n::: {.callout-note}\n## Parkour networks\n\nFor Parkour tasks, the states cover two different informations: the **terrain** (distance to obstacles, etc.) and the **proprioception** (joint positions of the agent). They enter the actor and critic networks at different locations.\n\n![Source: [@Barth-Maron2018]](../slides/img/parkour-network.png)\n\n:::\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"3.5-DDPG.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}