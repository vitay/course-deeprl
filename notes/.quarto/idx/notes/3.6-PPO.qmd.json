{"title":"Natural gradients (TRPO, PPO)","markdown":{"headingText":"Natural gradients (TRPO, PPO)","containsRefs":false,"markdown":"\nSlides: [html](../slides/3.6-PPO.html){target=\"_blank\"} [pdf](../slides/pdf/3.6-PPO.pdf){target=\"_blank\"}\n\n## Rationale\n\n{{< youtube  7h22fVmJs9g >}}\n\n\nDQN and DDPG are **off-policy** methods, so we can use a replay memory.\n\n* They need less samples to converge as they re-use past experiences (**sample efficient**).\n* The critic is biased (overestimation), so learning is **unstable** and **suboptimal**.\n\nA3C is **on-policy**, we have to use distributed learning.\n\n* The critic is less biased, so it learns better policies (**optimality**).\n* It however need a lot of samples (**sample complexity**) as it must collect transitions with the current learned policy.\n\nAll suffer from **parameter brittleness**: choosing the right hyperparameters for a task is extremely difficult. For example a learning rate of $10^{-5}$ might work, but not $1.1 * 10^{-5}$. Other hyperparameters: size of the ERM, update frequency of the target networks, training frequency. Can't we do better?\n\nWhere is the problem with on-policy methods? The policy gradient is **unbiased** only when the critic $Q_\\varphi(s, a)$ accurately approximates the true Q-values of the **current policy**.\n\n$$\n\\begin{aligned}\n    \\nabla_\\theta J(\\theta) & =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q^{\\pi_\\theta}(s, a)] \\\\\n    & \\approx  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q_\\varphi(s, a)]\n\\end{aligned}\n$$\n\nIf transitions are generated by a different (older) policy $b$, the policy gradient will be wrong. We could correct the policy gradient with **importance sampling**:\n\n$$\n    \\nabla_\\theta J(\\theta) \\approx  \\mathbb{E}_{s \\sim \\rho_b, a \\sim b}[ \\frac{\\pi_\\theta(s, a)}{b(s, a)} \\, \\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q_\\varphi(s, a))]\n$$\n\nThis is the **off-policy actor-critic** (Off-PAC) algorithm of [@Degris2012]. It is however limited to linear approximation, as the critic $Q_\\varphi(s, a)$ needs to very quickly adapt to changes in the policy (deep NN are very slow learners) and the importance weight $\\frac{\\pi_\\theta(s, a)}{b(s, a)}$ can have a huge variance.\n\nOnce we have an estimate of the policy gradient:\n\n$$\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s, a) \\, Q_\\varphi(s, a)]\n$$\n\nwe can update the weights $\\theta$ in the direction of that gradient:\n\n$$\n    \\theta \\leftarrow \\theta + \\eta \\, \\nabla_\\theta J(\\theta)\n$$\n\n(or some variant of it, such as RMSprop or Adam). We search for the **smallest parameter change** (controlled by the learning rate $\\eta$) that produces the **biggest positive change** in the returns. Choosing the learning rate $\\eta$ is extremely difficult in deep RL:\n\n* If the learning rate is too small, the network converges very slowly, requiring a lot of samples to converge (**sample complexity**).\n* If the learning rate is too high, parameter updates can totally destroy the policy (**instability**).\n\nThe learning rate should adapt to the current parameter values in order to stay in a **trust region**.\n\n![Too big updates can lead to policy collapse. Source:  <https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-explained-a6ee04eeeee9>](../slides/img/natural-gradient-reason1.jpeg){width=100%}\n\nThe policy gradient tells you in **which direction** of the parameter space $\\theta$ the return is increasing the most. If you take too big a step in that direction, the new policy might become completely bad (**policy collapse**). Once the policy has collapsed, the new samples will all have a small return: the previous progress is lost. This is especially true when the parameter space has a **high curvature**, which is the case with deep NN.\n\nPolicy collapse is a huge problem in deep RL: the network starts learning correctly but suddenly collapses to a random agent. For on-policy methods, all progress is lost: the network has to relearn from scratch, as the new samples will be generated by a bad policy.\n\n\n**Trust region** optimization searches in the **neighborhood** of the current parameters $\\theta$ which new value would maximize the return the most. This is a **constrained optimization** problem: we still want to maximize the return of the policy, but by keeping the policy as close as possible from its previous value.\n\n![The parameter update should be made inside the trust region. Source:  <https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-explained-a6ee04eeeee9>](../slides/img/natural-gradient-reason2.jpeg){width=100%}\n\nThe size of the neighborhood determines the safety of the parameter change. In safe regions, we can take big steps. In dangerous regions, we have to take small steps. **Problem:** how can we estimate the safety of a parameter change?\n\n![The size of the trust region depends on the curvature of the objective function. Source:  <https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-explained-a6ee04eeeee9>](../slides/img/natural-gradient-reason3.jpeg){width=80%}\n\n##  TRPO: Trust Region Policy Optimization\n\n{{< youtube  bUv4BRAJ-OI >}}\n\n We want to maximize the expected return of a policy $\\pi_\\theta$, which is equivalent to the Q-value of every state-action pair visited by the policy:\n\n$$\\mathcal{J}(\\theta) = \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta} [Q^{\\pi_\\theta}(s, a)]$$\n\n\nLet's note $\\theta_\\text{old}$ the current value of the parameters of the policy $\\pi_{\\theta_\\text{old}}$.\n\n[@Kakade2002] have shown that the expected return of a policy $\\pi_\\theta$ is linked to the expected return of the current policy $\\pi_{\\theta_\\text{old}}$ with:\n\n$$\\mathcal{J}(\\theta) = \\mathcal{J}(\\theta_\\text{old}) + \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta} [A^{\\pi_{\\theta_\\text{old}}}(s, a)]$$\n\nwhere: \n\n$$A^{\\pi_{\\theta_\\text{old}}}(s, a) = Q_\\theta(s, a) - Q_{\\theta_\\text{old}}(s, a)$$\n\nis the **advantage** of taking the action $(s, a)$ and thereafter following $\\pi_\\theta$, compared to following the current policy $\\pi_{\\theta_\\text{old}}$.\n\nThe return under any policy $\\theta$ is equal to the return under $\\theta_\\text{old}$, plus how the newly chosen actions in the rest of the trajectory improves (or worsens) the returns.\n\nIf we can estimate the advantages and maximize them, we can find a new policy $\\pi_\\theta$ with a higher return than the current one.\n\n$$\\mathcal{L}(\\theta) = \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta} [A^{\\pi_{\\theta_\\text{old}}}(s, a)]$$\n\nBy definition, $\\mathcal{L}(\\theta_\\text{old}) = 0$, so the policy maximizing $\\mathcal{L}(\\theta)$ has positive advantages and is better than $\\pi_{\\theta_\\text{old}}$.\n\n$$\\theta_\\text{new} = \\text{argmax}_\\theta \\; \\mathcal{L}(\\theta) \\; \\Rightarrow \\; \\mathcal{J}(\\theta_\\text{new}) \\geq \\mathcal{J}(\\theta_\\text{old})$$\n\nMaximizing the advantages ensures **monotonic improvement**: the new policy is always better than the previous one. Policy collapse is not possible!\n\nThe problem is that we have to take samples $(s, a)$ from $\\pi_\\theta$: we do not know it yet, as it is what we search. The only policy at our disposal to estimate the advantages is the current policy $\\pi_{\\theta_\\text{old}}$. We could use **importance sampling** to sample from $\\pi_{\\theta_\\text{old}}$, but it would introduce a lot of variance (but see PPO later):\n\n$$\\mathcal{L}(\\theta) = \\mathbb{E}_{s \\sim \\rho_{\\theta_\\text{old}}, a \\sim \\pi_{\\theta_\\text{old}}} [\\frac{\\pi_{\\theta}(s, a)}{\\pi_{\\theta_\\text{old}}(s, a)} \\, A^{\\pi_{\\theta_\\text{old}}}(s, a)]$$\n\nIn TRPO [@Schulman2015], we are adding a **constraint** instead: \n\n* the new policy $\\pi_{\\theta_\\text{new}}$ should not be (very) different from $\\pi_{\\theta_\\text{old}}$.\n* the importance sampling weight $\\frac{\\pi_{\\theta_\\text{new}}(s, a)}{\\pi_{\\theta_\\text{old}}(s, a)}$ will not be very different from 1, so we can omit it.\n\nLet's define a new objective function $\\mathcal{J}_{\\theta_\\text{old}}(\\theta)$:\n\n$$\\mathcal{J}_{\\theta_\\text{old}}(\\theta) = \\mathcal{J}(\\theta_\\text{old}) + \\mathbb{E}_{s \\sim \\rho_{\\theta_\\text{old}}, a \\sim \\pi_{\\theta}} [A^{\\pi_{\\theta_\\text{old}}}(s, a)]$$\n\nThe only difference with $\\mathcal{J}(\\theta)$ is that the visited states $s$ are now sampled by the current policy $\\pi_{\\theta_\\text{old}}$. This makes the expectation tractable: we know how to visit the states, but we compute the advantage of actions taken by the new policy in those states.\n\nPrevious objective function:\n\n$$\\mathcal{J}(\\theta) = \\mathcal{J}(\\theta_\\text{old}) + \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta} [A^{\\pi_{\\theta_\\text{old}}}(s, a)]$$\n\nNew objective function:\n\n$$\\mathcal{J}_{\\theta_\\text{old}}(\\theta) = \\mathcal{J}(\\theta_\\text{old}) + \\mathbb{E}_{s \\sim \\rho_{\\theta_\\text{old}}, a \\sim \\pi_{\\theta}} [A^{\\pi_{\\theta_\\text{old}}}(s, a)]$$\n\nIt is \"easy\" to observe that the new objective function has the same value in $\\theta_\\text{old}$:\n\n$$\\mathcal{J}_{\\theta_\\text{old}}(\\theta_\\text{old}) = \\mathcal{J}(\\theta_\\text{old})$$\n\nand that its gradient w.r.t. $\\theta$ is the same in $\\theta_\\text{old}$:\n\n$$\\nabla_\\theta \\mathcal{J}_{\\theta_\\text{old}}(\\theta)|_{\\theta = \\theta_\\text{old}} = \\nabla_\\theta \\, \\mathcal{J}(\\theta)|_{\\theta = \\theta_\\text{old}}$$\n\nAt least locally, maximizing $\\mathcal{J}_{\\theta_\\text{old}}(\\theta)$ is exactly the same as maximizing $\\mathcal{J}(\\theta)$. $\\mathcal{J}_{\\theta_\\text{old}}(\\theta)$ is called a **surrogate objective function**: it is not what we want to maximize, but it leads to the same result locally.\n\n![The surrogate objective is locally the same as the true objective, so we can follow its gradient.](../slides/img/trustregion1.svg){width=80%}\n\nHow big a step can we take when maximizing $\\mathcal{J}_{\\theta_\\text{old}}(\\theta)$? $\\pi_\\theta$ and $\\pi_{\\theta_\\text{old}}$ must be close from each other for the approximation to stand.\n\nThe first variant explored in the TRPO paper is a **constrained optimization** approach (Lagrange optimization):\n\n$$\n    \\max_\\theta \\mathcal{J}_{\\theta_\\text{old}}(\\theta) = \\mathcal{J}(\\theta_\\text{old}) + \\mathbb{E}_{s \\sim \\rho_{\\theta_\\text{old}}, a \\sim \\pi_{\\theta}} [A^{\\pi_{\\theta_\\text{old}}}(s, a)]\n$$\n\n$$\n    \\text{such that:} \\; D_\\text{KL} (\\pi_{\\theta_\\text{old}}||\\pi_\\theta) \\leq \\delta\n$$\n\nThe KL divergence between the distributions $\\pi_{\\theta_\\text{old}}$ and $\\pi_\\theta$ must be below a threshold $\\delta$. This version of TRPO uses a **hard constraint**: We search for a policy $\\pi_\\theta$ that maximizes the expected return while staying within the **trust region** around $\\pi_{\\theta_\\text{old}}$.\n\nThe second approach **regularizes** the objective function with the KL divergence:\n\n$$\n    \\max_\\theta \\mathcal{L}(\\theta) = \\mathcal{J}_{\\theta_\\text{old}}(\\theta) - C \\, D_\\text{KL} (\\pi_{\\theta_\\text{old}}||\\pi_\\theta)\n$$\n\nwhere $C$ is a regularization parameter controlling the importance of the constraint. This **surrogate objective function** is a **lower bound** of the initial objective $\\mathcal{J}(\\theta)$:\n\n1. The two objectives have the same value in $\\theta_\\text{old}$:\n\n$$\\mathcal{L}(\\theta_\\text{old}) = \\mathcal{J}_{\\theta_\\text{old}}(\\theta_\\text{old}) - C \\,  D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_{\\theta_\\text{old}}) = \\mathcal{J}(\\theta_\\text{old})$$\n\n2. Their gradient w.r.t $\\theta$ are the same in $\\theta_\\text{old}$:\n\n$$\\nabla_\\theta \\mathcal{L}(\\theta)|_{\\theta = \\theta_\\text{old}} = \\nabla_\\theta \\mathcal{J}(\\theta)|_{\\theta = \\theta_\\text{old}}$$\n\n3. The surrogate objective is always smaller than the real objective, as the KL divergence is positive:\n\n$$\\mathcal{J}(\\theta) \\geq \\mathcal{J}_{\\theta_\\text{old}}(\\theta) - C \\,  D_{KL}(\\pi_{\\theta_\\text{old}} || \\pi_\\theta)$$\n\n![The surrogate objective adds the constraint that the update should stay in the trust region.](../slides/img/trustregion2.svg){width=80%}\n\nThe policy $\\pi_\\theta$ maximizing the surrogate objective $\\mathcal{L}(\\theta) = \\mathcal{J}_{\\theta_\\text{old}}(\\theta) - C \\, D_\\text{KL} (\\pi_{\\theta_\\text{old}}||\\pi_\\theta)$ has a higher expected return than $\\pi_{\\theta_\\text{old}}$:\n\n$$\\mathcal{J}(\\theta) > \\mathcal{J}(\\theta_\\text{old})$$\n\nis very close to $\\pi_{\\theta_\\text{old}}$:\n\n$$D_\\text{KL} (\\pi_{\\theta_\\text{old}}||\\pi_\\theta) \\approx 0$$\n\nbut the parameters $\\theta$ are much closer to the optimal parameters $\\theta^*$.\n\nThe version with a soft constraint necessitates a prohibitively small learning rate in practice. The implementation of TRPO uses the hard constraint with Lagrange optimization, what necessitates using conjugate gradients optimization, the Fisher Information matrix and natural gradients: very complex to implement... However, there is a **monotonic improvement guarantee**: the successive policies can only get better over time, no policy collapse! This is the major advantage of TRPO compared to the other methods: it always works, although very slowly.\n\n## PPO: Proximal Policy Optimization\n\n{{< youtube  B1IFO9u_l0Y >}}\n\nLet's take the unconstrained objective function of TRPO:\n\n$$\\mathcal{J}_{\\theta_\\text{old}}(\\theta) = \\mathcal{J}(\\theta_\\text{old}) + \\mathbb{E}_{s \\sim \\rho_{\\theta_\\text{old}}, a \\sim \\pi_{\\theta}} [A^{\\pi_{\\theta_\\text{old}}}(s, a)]$$\n\n$\\mathcal{J}(\\theta_\\text{old})$ does not depend on $\\theta$, so we only need to maximize the advantages:\n\n$$\\mathcal{L}(\\theta) = \\mathbb{E}_{s \\sim \\rho_{\\theta_\\text{old}}, a \\sim \\pi_{\\theta}} [A^{\\pi_{\\theta_\\text{old}}}(s, a)]$$\n\nIn order to avoid sampling action from the **unknown** policy $\\pi_\\theta$, we can use importance sampling with the current policy:\n\n$$\\mathcal{L}(\\theta) = \\mathbb{E}_{s \\sim \\rho_{\\theta_\\text{old}}, a \\sim \\pi_{\\theta_\\text{old}}} [\\rho(s, a) \\, A^{\\pi_{\\theta_\\text{old}}}(s, a)]$$\n\nwith $\\rho(s, a) = \\frac{\\pi_{\\theta}(s, a)}{\\pi_{\\theta_\\text{old}}(s, a)}$ being the **importance sampling weight**. But the importance sampling weight $\\rho(s, a)$ introduces a lot of variance, worsening the sample complexity. Is there another way to make sure that $\\pi_\\theta$ is not very different from $\\pi_{\\theta_\\text{old}}$, therefore reducing the variance of the importance sampling weight?\n\nThe solution introduced by PPO [@Schulman2017] is simply to **clip** the importance sampling weight when it is too different from 1:\n\n\n$$\\mathcal{L}(\\theta) = \\mathbb{E}_{s \\sim \\rho_{\\theta_\\text{old}}, a \\sim \\pi_{\\theta_\\text{old}}} [\\min(\\rho(s, a) \\, A^{\\pi_{\\theta_\\text{old}}}(s, a), \\text{clip}(\\rho(s, a), 1-\\epsilon, 1+\\epsilon) \\, A^{\\pi_{\\theta_\\text{old}}}(s, a))]$$\n\nFor each sampled action $(s, a)$, we use the minimum between:\n\n* the TRPO unconstrained objective with IS $\\rho(s, a) \\, A^{\\pi_{\\theta_\\text{old}}}(s, a)$.\n* the same, but with the IS weight clipped between $1-\\epsilon$ and $1+\\epsilon$.\n\n![Clipped importance sampling weight.](../slides/img/ppo-clippingfunction.svg){width=80%}\n\nIf the advantage $A^{\\pi_{\\theta_\\text{old}}}(s, a)$ is positive (better action than usual) and:\n\n* the IS is higher than $1+\\epsilon$, we use $(1+\\epsilon) \\, A^{\\pi_{\\theta_\\text{old}}}(s, a)$.\n* otherwise, we use $\\rho(s, a) \\, A^{\\pi_{\\theta_\\text{old}}}(s, a)$.\n\n![Clipped objective for positive advantages.](../slides/img/ppo-clipped.png){width=50%}\n\nIf the advantage $A^{\\pi_{\\theta_\\text{old}}}(s, a)$ is negative (worse action than usual) and:\n\n* the IS is lower than $1-\\epsilon$, we use $(1-\\epsilon) \\, A^{\\pi_{\\theta_\\text{old}}}(s, a)$.\n* otherwise, we use $\\rho(s, a) \\, A^{\\pi_{\\theta_\\text{old}}}(s, a)$.\n\n![Clipped objective for negative advantages.](../slides/img/ppo-clipped2.png){width=50%}\n\nThis avoids changing too much the policy between two updates:\n\n* Good actions ($A^{\\pi_{\\theta_\\text{old}}}(s, a) > 0$) do not become much more likely than before.\n* Bad actions ($A^{\\pi_{\\theta_\\text{old}}}(s, a) < 0$) do not become much less likely than before.\n\nThe PPO **clipped objective** ensures than the importance sampling weight stays around one, so the new policy is not very different from the old one. It can learn from single transitions.\n\n$$\\mathcal{L}(\\theta) = \\mathbb{E}_{s \\sim \\rho_{\\theta_\\text{old}}, a \\sim \\pi_{\\theta_\\text{old}}} [\\min(\\rho(s, a) \\, A^{\\pi_{\\theta_\\text{old}}}(s, a), \\text{clip}(\\rho(s, a), 1-\\epsilon, 1+\\epsilon) \\, A^{\\pi_{\\theta_\\text{old}}}(s, a))]$$\n\nThe advantage of an action can be learned using any advantage estimator, for example the **n-step advantage**:\n\n$$A^{\\pi_{\\theta_\\text{old}}}(s_t, a_t) =  \\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V_\\varphi(s_{t+n}) - V_\\varphi(s_{t})$$\n\nMost implementations use **Generalized Advantage Estimation** (GAE, [@Schulman2015a]). PPO is therefore an **actor-critic** method (as TRPO). PPO is **on-policy**: it collects samples using **distributed learning** (as A3C) and then applies several updates to the actor and critic.\n\n::: {.callout-tip}\n## PPO: Proximal Policy Optimization\n\n* Initialize an actor $\\pi_\\theta$ and a critic $V_\\varphi$ with random weights.\n\n* **while** not converged :\n\n    * for $N$ workers in parallel:\n\n        * Collect $T$ transitions using $\\pi_{\\theta}$.\n\n        * Compute the advantage $A_\\varphi(s, a)$ of each transition using the critic $V_\\varphi$.\n\n    * for $K$ epochs:\n\n        * Sample $M$ transitions $\\mathcal{D}$ from the ones previously collected.\n\n        * Train the actor to maximize the clipped surrogate objective.\n\n        $$\\mathcal{L}(\\theta) = \\mathbb{E}_{s, a \\sim \\mathcal{D}} [\\min(\\rho(s, a) \\, A_\\varphi(s, a), \\text{clip}(\\rho(s, a), 1-\\epsilon, 1+\\epsilon) \\, A_\\varphi(s, a))]$$\n\n        * Train the critic to minimize the advantage.\n\n        $$\\mathcal{L}(\\varphi) = \\mathbb{E}_{s, a \\sim \\mathcal{D}} [(A_\\varphi(s, a))^2]$$\n:::\n\nPPO is an **on-policy actor-critic** PG algorithm, using distributed learning. **Clipping** the importance sampling weight allows to avoid **policy collapse**, by staying in the **trust region** (the policy does not change much between two updates). The **monotonic improvement guarantee** is very important: the network will always find a (local) maximum of the returns. PPO is much less sensible to hyperparameters than DDPG (**brittleness**): works often out of the box with default settings. It does not necessitate complex optimization procedures like TRPO: first-order methods such as **SGD** work (easy to implement). The actor and the critic can **share weights** (unlike TRPO), allowing to work with pixel-based inputs, convolutional or recurrent layers. It can use **discrete or continuous action spaces**, although it is most efficient in the continuous case. Go-to method for robotics. Drawback: not very **sample efficient**.\n\nImplementing PPO necessitates quite a lot of tricks (early stopping, MPI). OpenAI Baselines or SpinningUp provide efficient implementations:\n\n<https://spinningup.openai.com/en/latest/algorithms/ppo.html>\n\n<https://github.com/openai/baselines/tree/master/baselines/ppo2>\n\n\n![Performance of PPO on Mujoco continuous control tasks [@Schulman2017].](../slides/img/ppo-results-mujoco.png){width=100%}\n\n\nSee <https://openai.com/blog/openai-baselines-ppo/> for more videos.\n\n{{< youtube  faDKMMwOS2Q >}}\n\n{{< youtube  jwSbzNHGflM >}}\n\n## OpenAI Five: Dota 2\n\n{{< youtube  MSawUc776z0 >}}\n\nPPO is used by OpenAI to play Dota 2. Their website is very well made:\n\n<https://openai.com/projects/five/>\n\n{{< youtube  eHipy_j29Xw >}}\n\nWhy is Dota 2 hard?\n\n![Dota 2 is much harder for AI than chess or Go. Source: <https://openai.com/projects/five/>](../slides/img/ppo-dota-reasons.png){width=100%}\n\n------------------------------      ------  ----    -------\nFeature                             Chess   Go      Dota 2\n------------------------------      ------  ----    -------\nTotal number of moves               40      150     20000\nNumber of possible actions          35      250     1000\nNumber of inputs                    70      400     20000\n------------------------------      ------  ----    -------\n\n\nOpenAI Five is composed of 5 PPO networks (one per player), using 128,000 CPUs and 256 V100 GPUs.\n\n![Architecture of OpenAI five. Source: <https://openai.com/projects/five/>](../slides/img/ppo-dota-architecture.png){width=100%}\n\n\n![Hardware requirements. Source: <https://openai.com/projects/five/>](../slides/img/ppo-dota-facts.png){width=100%}\n\n![State representation. Source: <https://openai.com/projects/five/>](../slides/img/ppo-dota-inputs.png){width=100%}\n\n![Deep NN used as the actor. Source: <https://openai.com/projects/five/>](../slides/img/ppo-dota-nn.png){width=100%}\n\nCheck the complete NN architecture at: <https://d4mucfpksywv.cloudfront.net/research-covers/openai-five/network-architecture.pdf>\n\nThe agents are trained by **self-play**. Each worker plays against:\n\n* the current version of the network 80% of the time.\n* an older version of the network 20% of the time.\n\nThe reward function is hand-designed using human heuristics: net worth, kills, deaths, assists, last hits...\n\n![OpenAI five. Source: <https://openai.com/projects/five/>](../slides/img/ppo-dota-example.png){width=100%}\n\nThe discount factor $\\gamma$ is annealed from 0.998 (valuing future rewards with a half-life of 46 seconds) to 0.9997 (valuing future rewards with a half-life of five minutes). Coordinating all the resources (CPU, GPU) is actually the main difficulty: Kubernetes, Azure, and GCP backends for Rapid, TensorBoard, Sentry and Grafana for monitoring...\n\n\n## ACER: Actor-Critic with Experience Replay\n\n\nACER [@Wang2017] is the off-policy version of PPO:\n\n* Off-policy actor-critic architecture (using experience replay),\n* Retrace estimation of values (Munos et al. 2016), \n* Importance sampling weight truncation with bias correction,\n* Efficient trust region optimization (TRPO),\n* Stochastic Dueling Network (SDN) in order to estimate both $Q_\\varphi(s, a)$ and $V_\\varphi(s)$. \n\nThe performance is comparable to PPO. It works sometimes better than PPO on some environments, sometimes not.\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"3.6-PPO.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}