{"title":"Maximum Entropy RL (SAC)","markdown":{"headingText":"Maximum Entropy RL (SAC)","containsRefs":false,"markdown":"\nSlides: [html](../slides/3.7-SAC.html){target=\"_blank\"} [pdf](../slides/pdf/3.7-SAC.pdf){target=\"_blank\"}\n\n## Soft RL\n\n{{< youtube  b7CnFrgQ0hg >}}\n\n\nAll methods seen so far search the optimal policy that maximizes the return:\n\n$$\\pi^* = \\text{arg} \\max_\\pi \\, \\mathbb{E}_{\\pi} [ \\sum_t \\gamma^t \\, r(s_t, a_t, s_{t+1}) ]$$\n\nThe optimal policy is deterministic and greedy by definition. \n\n$$\\pi^*(s) = \\text{arg} \\max_a Q^*(s, a)$$\n\nExploration is ensured externally by :\n\n* applying $\\epsilon$-greedy or softmax on the Q-values (DQN),\n* adding exploratory noise (DDPG),\n* learning stochastic policies that become deterministic over time (A3C, PPO). \n\nIs \"hard\" RL, caring only about **exploitation**, always the best option?\n\nThe optimal policy is only greedy for a MDP, not obligatorily for a POMDP. Games like chess are POMDPs: you do not know what your opponent is going to play (missing information). If you always play the same moves (e.g. opening moves), your opponent will adapt and you will end up losing systematically. **Variety** in playing is beneficial in POMDPs: it can counteract the uncertainty about the environment [@Todorov2008], [@Toussaint2009].\n\nThere are sometimes more than one way to collect rewards, especially with sparse rewards. If exploration decreases too soon, the RL agent will \"overfit\" one of the paths. If one of the paths is suddenly blocked, the agent would have to completely re-learn its policy. It would be more efficient if the agent had learned all possibles paths, even if some of them are less optimal.\n\n\nSoftmax policies allow to learn **multimodal** policies, but only for discrete action spaces.\n\n$$\n    \\pi(s, a) = \\frac{\\exp Q(s, a) / \\tau}{ \\sum_b \\exp Q(s, b) / \\tau}\n$$\n\nIn continuous action spaces, we would have to integrate over the whole action space, what is not tractable. Exploratory noise as in DDPG only leads to **unimodal** policies: greedy action plus some noise.\n\n![Unimodal policies explore around the greedy action. Source: <https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/>](../slides/img/sac-unimodal.png){width=70%}\n\n![Multimodal policies explore in the whole action space. Source: <https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/>](../slides/img/sac-multimodal.png){width=70%}\n\n## Continuous stochastic policies\n\n{{< youtube  00oPvNm-VBA >}}\n\nThe easiest to implement a stochastic policy with a neural network is a **Gaussian policy**. Suppose that we want to control a robotic arm with $n$ degrees of freedom. An action $\\mathbf{a}$ is a vector of joint displacements:\n\n$$\\mathbf{a} = \\begin{bmatrix} \\Delta \\theta_1 & \\Delta \\theta_2 & \\ldots \\, \\Delta \\theta_n\\end{bmatrix}^T$$\n\nA Gaussian policy considers the vector $\\mathbf{a}$ to be sampled from the **normal distribution** $\\mathcal{N}(\\mu_\\theta(s), \\sigma_\\theta(s))$.\nThe mean $\\mu_\\theta(s)$ and standard deviation $\\sigma_\\theta(s)$ are vectors that can be the output of the **actor** neural network with parameters $\\theta$. **Sampling** an action from the normal distribution is done through the **reparameterization trick**:\n\n$$\\mathbf{a} = \\mu_\\theta(s) + \\sigma_\\theta(s) \\, \\xi$$\n\nwhere $\\xi \\sim \\mathcal{N}(0, 1)$ comes from the standard normal distribution.\n\nThe good thing with the normal distribution is that we know its pdf:\n\n$$\n    \\pi_\\theta(s, a) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_\\theta(s)}} \\, \\exp -\\frac{(a - \\mu_\\theta(s))^2}{2\\sigma^2_\\theta(s)}\n$$\n\nWhen estimating the **policy gradient** (REINFORCE, A3C, PPO, etc):\n\n$$\n    \\nabla_\\theta J(\\theta) =  \\mathbb{E}_{s \\sim \\rho^\\pi, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta (s, a) \\, \\psi ]\n$$\n\nthe log-likelihood $\\log \\pi_\\theta (s, a)$ is a simple function of $\\mu_\\theta(s)$ and $\\sigma_\\theta(s)$:\n\n$$\\log \\pi_\\theta (s, a) = -\\frac{(a - \\mu_\\theta(s))^2}{2\\sigma^2_\\theta(s)} - \\frac{1}{2} \\, \\log 2\\pi\\sigma^2_\\theta(s)$$\n\nso we can easily compute its gradient w.r.t $\\theta$ and apply backpropagation:\n\n$$\n    \\nabla_{\\mu_\\theta(s)} \\log \\pi_\\theta (s, a) = \\frac{a - \\mu_\\theta(s)}{\\sigma_\\theta(s)^2} \\qquad \\nabla_{\\sigma_\\theta(s)} \\log \\pi_\\theta (s, a) = \\frac{(a - \\mu_\\theta(s))^2}{\\sigma_\\theta(s)^3} - \\frac{1}{\\sigma_\\theta(s)}\n$$\n\nA Gaussian policy samples actions from the **normal distribution** $\\mathcal{N}(\\mu_\\theta(s), \\sigma_\\theta(s))$, with $\\mu_\\theta(s)$ and $\\sigma_\\theta(s)$ being the output of the actor.\n\n$$\\mathbf{a} = \\mu_\\theta(s) + \\sigma_\\theta(s) \\, \\xi$$\n\nThe score $\\nabla_\\theta \\log \\pi_\\theta (s, a)$ can be obtained easily using the output of the actor:\n\n$$\n    \\nabla_{\\mu_\\theta(s)} \\log \\pi_\\theta (s, a) = \\frac{a - \\mu_\\theta(s)}{\\sigma_\\theta(s)^2}\n$$ \n\n$$\n \\nabla_{\\sigma_\\theta(s)} \\log \\pi_\\theta (s, a) = \\frac{(a - \\mu_\\theta(s))^2}{\\sigma_\\theta(s)^3} - \\frac{1}{\\sigma_\\theta(s)}\n$$\n\nThe rest of the score ($\\nabla_\\theta \\mu_\\theta(s)$ and $\\nabla_\\theta \\sigma_\\theta(s)$) is the problem of tensorflow/pytorch.\nThis is the same **reparametrization trick** used in variational autoencoders to allow backpropagation to work through a sampling operation.\n**Beta** distributions are an even better choice to parameterize stochastic policies [@Chou2017].\n\n##  Maximum Entropy RL\n\n{{< youtube  BLSAgKBOnSI >}}\n\nAlthough stochastic, Gaussian policies are still **unimodal policies**: they mostly sample actions around the mean $\\mu_\\theta(s)$ and the variance $\\sigma_\\theta(s)$ decreases to 0 with learning. If we want a multimodal policy that learns different solutions, we need to learn a **Softmax** distribution (Gibbs / Boltzmann) over the action space. How can we do that when the action space is continuous?\n\nA solution to force the policy to be **multimodal** is to force it to be as stochastic as possible by **maximizing its entropy**. Instead of searching for the policy that \"only\" maximizes the returns:\n\n$$\n    \\pi^* = \\text{arg} \\max_\\pi \\, \\mathbb{E}_{\\pi} [ \\sum_t \\gamma^t \\, r(s_t, a_t, s_{t+1}) ]\n$$\n\nwe search for the policy that maximizes the returns while being as stochastic as possible:\n\n$$\n    \\pi^* = \\text{arg} \\max_\\pi \\, \\mathbb{E}_{\\pi} [ \\sum_t \\gamma^t \\, r(s_t, a_t, s_{t+1}) + \\alpha \\, H(\\pi(s_t))]\n$$\n\nThis new objective function defines the **maximum entropy RL** framework [@Williams1991]. The entropy of the policy **regularizes** the objective function: the policy should still maximize the returns, but stay as stochastic as possible depending on the parameter $\\alpha$. Entropy regularization can always be added to PG methods such as A3C. It is always possible to fall back to hard RL by setting $\\alpha$ to 0.\n\nThe entropy of a policy in a state $s_t$ is defined by the expected negative log-likelihood of the policy:\n\n$$H(\\pi_\\theta(s_t)) = \\mathbb{E}_{a \\sim \\pi_\\theta(s_t)} [- \\log \\pi_\\theta(s_t, a)]$$\n\nFor a discrete action space:\n\n$$\n    H(\\pi_\\theta(s_t)) = - \\sum_a \\pi_\\theta(s_t, a) \\, \\log \\pi_\\theta(s_t, a)\n$$\n\nFor a continuous action space:\n\n$$\n    H(\\pi_\\theta(s_t)) = - \\int_a \\pi_\\theta(s_t, a) \\, \\log \\pi_\\theta(s_t, a) \\, da\n$$\n\nThe entropy necessitates to sum or integrate the **self-information** of each possible action in a given state.\nA **deterministic** (greedy) policy has zero entropy, the same action is always taken: **exploitation**.\nA **random** policy has a high entropy, you cannot predict which action will be taken: **exploration**.\nMaximum entropy RL embeds the exploration-exploitation trade-off inside the objective function instead of relying on external mechanisms such as the softmax temperature.\n\nIn **soft Q-learning** [@Haarnoja2017], the objective function is defined over complete trajectories:\n\n$$\n    \\mathcal{J}(\\theta) = \\sum_t \\gamma^t \\, \\mathbb{E}_{\\pi} [ r(s_t, a_t, s_{t+1}) + \\alpha \\, H(\\pi(s_t))]\n$$\n\nThe goal of the agent is to generate trajectories associated with a lot of rewards (high return) but only visiting states with a high entropy, i.e. where the policy is random (exploration).\n\nThe agent can decide how the trade-off is solved via regularization:\n\n* If a single action leads to high rewards, the policy may become deterministic.\n* If several actions lead to equivalent rewards, the policy must stay stochastic.\n\nIn soft Q-learning, the policy is implemented as a softmax over **soft Q-values**:\n\n$$\n    \\pi_\\theta(s, a) = \\dfrac{\\exp  \\dfrac{Q^\\text{soft}_\\theta (s, a)}{\\alpha}}{\\sum_b \\exp \\dfrac{Q^\\text{soft}_\\theta (s, b)}{\\alpha}} \\propto \\exp \\dfrac{Q^\\text{soft}_\\theta (s, a)}{\\alpha}\n$$\n\n$\\alpha$ plays the role of the softmax temperature parameter $\\tau$.\n\nSoft Q-learning belongs to **energy-based models**, as $-\\dfrac{Q^\\text{soft}_\\theta (s, a)}{\\alpha}$ represents the energy of the Boltzmann distribution (see restricted Boltzmann machines). The **partition function** $\\sum_b \\exp \\dfrac{Q^\\text{soft}_\\theta (s, b)}{\\alpha}$ is untractable for continuous action spaces, as one would need to integrate over the whole action space, but it will disappear from the equations anyway.\n\nSoft V and Q values are the equivalent of the hard value functions, but for the new objective:\n\n$$\n    \\mathcal{J}(\\theta) = \\sum_t \\gamma^t \\, \\mathbb{E}_{\\pi} [ r(s_t, a_t, s_{t+1}) + \\alpha \\, H(\\pi(s_t))]\n$$\n\nThe soft value of an action depends on the immediate reward and the soft value of the next state (soft Bellman equation):\n\n$$\n    Q^\\text{soft}_\\theta(s_t, a_t) = \\mathbb{E}_{s_{t+1} \\in \\rho_\\theta} [r(s_t, a_t, s_{t+1}) + \\gamma \\, V^\\text{soft}_\\theta(s_{t+1})]\n$$\n\nThe soft value of a state is the expected value over the available actions plus the entropy of the policy.\n\n$$\n    V^\\text{soft}_\\theta(s_t) = \\mathbb{E}_{a_{t} \\in \\pi} [Q^\\text{soft}_\\theta(s_{t}, a_{t})] + H(\\pi_\\theta(s_t)) = \\mathbb{E}_{a_{t} \\in \\pi} [Q^\\text{soft}_\\theta(s_{t}, a_{t}) -  \\log \\, \\pi_\\theta(s_t, a_t)]\n$$\n\n[@Haarnoja2017] showed that these soft value functions are the solution of the entropy-regularized objective function. All we need is to be able to estimate them... Soft Q-learning uses complex optimization methods (variational inference) to do it, but SAC is more practical.\n\n## Soft Actor-Critic (SAC)\n\n{{< youtube  4drdHdaANXM >}}\n\nPutting:\n\n$$\n    Q^\\text{soft}_\\theta(s_t, a_t) = \\mathbb{E}_{s_{t+1} \\in \\rho_\\theta} [r(s_t, a_t, s_{t+1}) + \\gamma \\, V^\\text{soft}_\\theta(s_{t+1})]\n$$\n\nand:\n\n$$\n    V^\\text{soft}_\\theta(s_t) = \\mathbb{E}_{a_{t} \\in \\pi} [Q^\\text{soft}_\\theta(s_{t}, a_{t}) -  \\log \\, \\pi_\\theta(s_t, a_t)]\n$$\n\ntogether, we obtain:\n\n$$\n    Q^\\text{soft}_\\theta(s_t, a_t) = \\mathbb{E}_{s_{t+1} \\in \\rho_\\theta} [r(s_t, a_t, s_{t+1}) + \\gamma \\, \\mathbb{E}_{a_{t+1} \\in \\pi} [Q^\\text{soft}_\\theta(s_{t+1}, a_{t+1}) -  \\log \\, \\pi_\\theta(s_{t+1}, a_{t+1})]]\n$$\n\nIf we want to train a **critic** $Q_\\varphi(s, a)$ to estimate the true soft Q-value of an action $Q^\\text{soft}_\\theta(s, a)$, we just need to sample $(s_t, a_t, r_{t+1}, a_{t+1})$ transitions and minimize:\n\n$$\n    \\mathcal{L}(\\varphi) = \\mathbb{E}_{s_t, a_t, s_{t+1} \\sim \\rho_\\theta} [(r_{t+1} + \\gamma \\, Q_\\varphi(s_{t+1}, a_{t+1}) - \\log \\pi_\\theta(s_{t+1}, a_{t+1}) - Q_\\varphi(s_{t}, a_{t}) )^2]\n$$\n\nThe only difference with a SARSA critic is that the negative log-likelihood of the next action is added to the target. In practice, $s_t$, $a_t$ and $r_{t+1}$ can come from a replay buffer, but $a_{t+1}$ has to be sampled from the current policy $\\pi_\\theta$ (but not taken!). SAC [@Haarnoja2018b] is therefore an **off-policy actor-critic algorithm**, but with stochastic policies! \n\nBut how do we train the actor? The policy is defined by a softmax over the soft Q-values, but the log-partition $Z$ is untractable for continuous spaces:\n\n$$\n    \\pi_\\theta(s, a) = \\dfrac{\\exp  \\dfrac{Q_\\varphi (s, a)}{\\alpha}}{\\sum_b \\exp \\dfrac{Q_\\varphi (s, b)}{\\alpha}} = \\dfrac{1}{Z} \\, \\exp \\dfrac{Q_\\varphi (s, a)}{\\alpha}\n$$\n\nThe trick is to make the **parameterized actor** $\\pi_\\theta$ learn to be close from this softmax, by minimizing the KL divergence:\n\n$$\n    \\mathcal{L}(\\theta) = D_\\text{KL} (\\pi_\\theta(s, a) || \\dfrac{1}{Z} \\, \\exp \\dfrac{Q_\\varphi (s, a)}{\\alpha}) = \\mathbb{E}_{s, a \\sim \\pi_\\theta(s, a)} [- \\log \\dfrac{\\dfrac{1}{Z} \\, \\exp \\dfrac{Q_\\varphi (s, a)}{\\alpha}}{\\pi_\\theta(s, a)}]\n$$\n\nAs $Z$ does not depend on $\\theta$, it will automagically disappear when taking the gradient!\n\n$$\n   \\nabla_\\theta \\, \\mathcal{L}(\\theta) = \\mathbb{E}_{s, a} [\\alpha \\, \\nabla_\\theta \\log \\pi_\\theta(s, a) - Q_\\varphi (s, a)]\n$$\n\nSo the actor just has to implement a Gaussian policy and we can train it using soft-Q-value.\n\n**Soft Actor-Critic** (SAC) is an **off-policy actor-critic** architecture for **maximum entropy RL**:\n\n$$\n    \\mathcal{J}(\\theta) = \\sum_t \\gamma^t \\, \\mathbb{E}_{\\pi} [ r(s_t, a_t, s_{t+1}) + \\alpha \\, H(\\pi(s_t))]\n$$\n\nMaximizing the entropy of the policy ensures an efficient exploration. It is even possible to learn the value of the parameter $\\alpha$. The critic learns to estimate soft Q-values that take the entropy of the policy into account:\n\n$$\n    \\mathcal{L}(\\varphi) = \\mathbb{E}_{s_t, a_t, s_{t+1} \\sim \\rho_\\theta} [(r_{t+1} + \\gamma \\, Q_\\varphi(s_{t+1}, a_{t+1}) - \\log \\pi_\\theta(s_{t+1}, a_{t+1}) - Q_\\varphi(s_{t}, a_{t}) )^2]\n$$\n\nThe actor learns a Gaussian policy that becomes close to a softmax over the soft Q-values:\n\n$$\n    \\pi_\\theta(s, a) \\propto \\exp \\dfrac{Q_\\varphi (s, a)}{\\alpha}\n$$\n\n$$\n   \\nabla_\\theta \\, \\mathcal{L}(\\theta) = \\mathbb{E}_{s, a} [\\alpha \\, \\nabla_\\theta \\log \\pi_\\theta(s, a) - Q_\\varphi (s, a)]\n$$\n\nIn practice, SAC uses **clipped double learning** like TD3: it takes the lesser of two evils between two critics $Q_{\\varphi_1}$ and $Q_{\\varphi_2}$.\nThe next action $a_{t+1}$ comes from the current policy, no need for target networks.\nUnlike TD3, the learned policy is **stochastic**: no need for target noise as the targets are already stochastic.\nSee <https://spinningup.openai.com/en/latest/algorithms/sac.html> for a detailed comparison of SAC and TD3.\nThe initial version of SAV additionally learned a soft V-value critic, but this turns out not to be needed.\n\n\n![SAC compared to DDPG, TD3 and PPO. Source [@Haarnoja2018b].](../slides/img/sac_results.png){width=100%}\n\nThe enhanced exploration strategy through maximum entropy RL allows to learn robust and varied strategies that can cope with changes in the environment.\n\n![SAC on the Walker environment. Source: <https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/>](../slides/img/sac-walker.gif){width=100%}\n\n![SAC on the Ant environment. Source: <https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/>](../slides/img/sac_ant.gif){width=100%}\n\n\nThe low sample complexity of SAC allows to train a real-world robot in less than 2 hours!\n\n{{< youtube  FmMPHL3TcrE >}}\n\n\nAlthough trained on a flat surface, the rich learned stochastic policy can generalize to complex terrains.\n\n{{< youtube  KOObeIjzXTY >}}\n\nWhen trained to stack lego bricks, the robotic arm learns to explore the whole state-action space.\n\n![Source: <https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/>.](../slides/img/sac_lego1.gif){width=100%}\n\nThis makes it more robust to external perturbations after training:\n\n![Source: <https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/>.](../slides/img/sac_lego2.gif){width=20%}\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"3.7-SAC.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}