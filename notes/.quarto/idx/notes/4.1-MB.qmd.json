{"title":"Model-based RL","markdown":{"headingText":"Model-based RL","containsRefs":false,"markdown":"\nSlides: [html](../slides/4.1-ModelBased.html){target=\"_blank\"} [pdf](../slides/pdf/4.1-ModelBased.pdf){target=\"_blank\"}\n\n## Model-free vs. model-based RL\n\n{{< youtube  p3k05RxrTVE >}}\n\nIn **model-free RL** (MF) methods, we do not need to know anything about the dynamics of the environment to start learning a policy:\n\n$$p(s_{t+1} | s_t, a_t) \\; \\; r(s_t, a_t, s_{t+1})$$\n\nWe just sample transitions $(s, a, r, s')$ and update Q-values or a policy network. The main advantage is that the agent does not need to \"think\" when acting: just select the action with highest Q-value (**reflexive behavior**). The other advantage is that you can use MF methods on **any** MDP: you do not need to know anything about them. But MF methods are very slow (sample complexity): as they make no assumption, they have to learn everything by trial-and-error from scratch.\n \n![Model-free vs. model-based RL. Source: [@Dayan2008]](../slides/img/modelbased2.png){width=80%}\n\nIf you had a **model** of the environment, you could plan ahead (what would happen if I did that?) and speed up learning (do not explore stupid ideas): **model-based RL** (MB). \nIn chess, players **plan** ahead the possible moves up to a certain horizon and evaluate moves based on their emulated consequences.\nIn real-time strategy games, learning the environment (**world model**) is part of the strategy: you do not attack right away.\n\n\n## Model Predictive Control (MPC)\n\n{{< youtube  WGGqJ-vOoqE >}}\n\nLearning the world model is not complicated in theory.\nWe just need to collect *enough* transitions $s_t, a_t, s_{t+1}, r_{t+1}$ using a random agent (or during learning) and train a model to predict the next state and reward.\n\n![Learning the world model.](../slides/img/learningdynamics.svg){width=80%}\n\nSuch a model is called the **dynamics model**, the **transition model** or the **forward model**: **What happens if I do that?** The model can be deterministic (use neural networks) or stochastic (use Gaussian Processes). Given an initial state $s_0$ and a policy $\\pi$, you can unroll the future using the local model.\n\nOnce you have a good transition model, you can generate **rollouts**, i.e. imaginary trajectories / episodes using the model.\n\n$$\\tau = (s_o, a_o, r_ 1, s_1, a_1, \\ldots, s_T)$$\n\nYou can then feed these trajectories to any model-free algorithm (value-based, policy-gradient) that will learn to maximize the returns.\n\n$$\\mathcal{J}(\\theta) = \\mathbb{E}_{\\tau}[R(\\tau)]$$\n\n\n![Imaginary rollouts can be used to improve the policy. Source: [@Kurutach2018].](../slides/img/mpc-rollout.png){width=60%}\n\nThe only sample complexity is the one needed to train the model: the rest is **emulated**. Drawback: This can only work when the model is close to perfect, especially for long trajectories or probabilistic MDPs.\n\nFor long horizons, the slightest imperfection in the model can accumulate (**drift**) and lead to completely wrong trajectories.\nThe emulated trajectory cannot be generated by the current policy $\\pi_\\theta$, the policy gradient is biased (especially if you are on-policy), the algorithm does not converge. If you have a perfect model, you should not be using RL anyway as classical control methods would be much faster (but see AlphaGo).\n\n\n\n![Imaginary rollouts generated by an imperfect model can drigt for long horizons. Source: <https://medium.com/@jonathan_hui/rl-model-based-reinforcement-learning-3c2b6f0aa323>](../slides/img/mpc-drift1.jpeg){width=100%}\n\nThe solution is to **replan** at each time step and execute the first planned action **in the real environment**.\n\n\n![Constantly replanning allows to correct the trajectories. Source: <https://medium.com/@jonathan_hui/rl-model-based-reinforcement-learning-3c2b6f0aa323>](../slides/img/mpc-drift2.jpeg){width=100%}\n\n**Model Predictive Control** iteratively plans complete trajectories, but only selects the first action.\n\n::: {.callout-tip}\n## Model Predictive Control\n\n![Source: <http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_9_model_based_rl.pdf>](../slides/img/mpc-algo.png)\n:::\n\n\n![A neural network can be used to learn the model in an MPC architecture. Source: [@Nagabandi2017].](../slides/img/mpc-architecture.png){width=100%}\n\nThe planner can actually be anything, it does not have to be a RL algorithm. For example, it can be iLQR (Iterative Linear Quadratic Regulator), a non-linear optimization method (see <https://jonathan-hui.medium.com/rl-lqr-ilqr-linear-quadratic-regulator-a5de5104c750>).\n\n\nAlternatively, one can use **random-sampling shooting**:\n\n1. in the current state, select a set of possible actions.\n2. generate rollouts with these action and compute their returns using the model.\n3. select the action whose rollout has the highest return.\n\n![Random-sampling shooting. Source: [@Nagabandi2017], <https://bair.berkeley.edu/blog/2017/11/30/model-based-rl/>.](../slides/img/mpc-example.png){width=60%}\n\nThe main advantage of MPC is that you can change the reward function (the **goal**) on the fly: what you learn is the model, but planning is just an optimization procedure. You can set intermediary goals to the agent very flexibly: no need for a well-defined reward function. Model imperfection is not a problem as you replan all the time. The model can adapt to changes in the environment (slippery terrain, simulation to real-world).\n\n![Application of MPC. Source: [@Nagabandi2017], <https://bair.berkeley.edu/blog/2017/11/30/model-based-rl/>.](../slides/img/mpc-application1.gif){width=60%}\n\n![Application of MPC. Source: [@Nagabandi2017], <https://bair.berkeley.edu/blog/2017/11/30/model-based-rl/>.](../slides/img/mpc-application2.gif){width=60%}\n\n\n## Dyna-Q\n\n{{< youtube  0MVpljc2KJc >}}\n\nAnother approach to MB RL is to **augment** MF methods with MB rollouts. The MF algorithm (e.g. Q-learning) learns from transitions $(s, a, r, s')$ sampled either with:\n\n* **real experience**: interaction with the environment.\n* **simulated experience**: simulation by the model.\n\n![Dyna-Q: RL with planning. Source: <https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-5e99cae0abb8>](../slides/img/dynaq.png){width=80%}\n\n\nIf the simulated transitions are good enough, the MF algorithm can converge using much less **real transitions**, thereby reducing its **sample complexity**. The **Dyna-Q** algorithm [@Sutton1990] is an extension of Q-learning to integrate a model $M(s, a) = (s', r')$. The model can be tabular or approximated with a NN.\n\n::: {.callout-tip}\n## Dyna-Q\n\n* Initialize values $Q(s, a)$ and model $M(s, a)$.\n\n* **for** $t \\in [0, T_\\text{total}]$:\n\n    * Select $a_t$ using $Q$, take it on the **real environment** and observe $s_{t+1}$ and $r_{t+1}$.\n\n    * Update the Q-value of the **real** action:\n\n    $$\\Delta Q(s_t, a_t) = \\alpha \\, (r_{t+1} + \\gamma \\, \\max_a Q(s_{t+1}, a) - Q(s_t, a_t))$$\n\n    * Update the model:\n\n    $$M(s, a) \\leftarrow (s_{t+1}, r_{t+1})$$\n\n    * **for** $K$ steps:\n\n        * Sample a state $s_k$ from a list of visited states.\n\n        * Select $a_k$ using $Q$, predict $s_{k+1}$ and $r_{k+1}$ using the **model** $M(s_k, a_k)$.\n\n        * Update the Q-value of the **imagined** action:\n\n        $$\\Delta Q(s_k, a_k) = \\alpha \\, (r_{k+1} + \\gamma \\, \\max_a Q(s_{k+1}, a) - Q(s_k, a_k))$$\n:::\n\n![Dyna-Q: RL with planning. Source: [@Sutton1998].](../slides/img/dynaq2.png){width=80%}\n\nIt is interesting to notice that Dyna-Q is very similar to DQN and its **experience replay memory**.\nIn DQN, the ERM stores **real transitions** generated in the past.\nIn Dyna-Q, the model generates **imagined transitions** based on past real transitions.\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"4.1-MB.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}