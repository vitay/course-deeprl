{"title":"Learned world models","markdown":{"headingText":"Learned world models","containsRefs":false,"markdown":"\nSlides: [html](../slides/4.2-LearnedModels.html){target=\"_blank\"} [pdf](../slides/pdf/4.2-LearnedModels.pdf){target=\"_blank\"}\n\nThere are two families of model-based algorithms using a learned transition model:\n\n* **Model-based augmented model-free** (MBMF) are inspired from Dyna-Q: the model **generates** imaginary transitions/rollouts that are used to train a model-free algorithm.\n    * NAF: Normalized advantage functions [@Gu2016]\n    * I2A: Imagination-augmented agents [@Weber2017]\n    * MBVE: model-based value estimation [@Feinberg2018]\n\n* **Model-based planning** methods are inspired from MPC: the learned model is used to **plan** actions that maximize the RL objective.\n    * TDM: Temporal difference models [@Pong2018]\n    * World models [@Ha2018]\n    * PlaNet [@Hafner2019]\n    * Dreamer [@Hafner2020]\n\n\n##  I2A - Imagination-augmented agents\n\n\n{{< youtube  liTOxOByPpg >}}\n\nI2A [@Weber2017] is a **model-based augmented model-free method**: it trains a MF algorithm (A3C) with the help of **rollouts** generated by a MB model.\n\n![Sokoban. Source [@Weber2017].](../slides/img/i2a-sokoban.png){width=100%}\n\nThey showcase their algorithm on the puzzle environment **Sokoban**, where you need to move boxes to specified locations. Sokoban is a quite hard game, as actions are irreversible (you can get stuck) and the solution requires many actions (sparse rewards). MF methods are bad at this game as they learn through trials-and-(many)-errors.\n\n{{< youtube  fg8QImlvB-k >}}\n\nThe **model** learns to predict the next frame and the next reward based on the four last frames and the chosen action.\n\n![I2A model. Source [@Weber2017].](../slides/img/i2a-model.png){width=100%}\n\nIt is a **convolutional autoencoder**, taking additionally an action $a$ as input and predicting the next reward. It can be pretrained using a random policy, and later fine-tuned during training.\n\n![I2A architecture. Source [@Weber2017].](../slides/img/i2a-architecture.png){width=100%}\n\nThe **imagination core** is composed of the environment model $M(s, a)$ and a **rollout policy** $\\hat{\\pi}$. As Sokoban is a POMDP (partially observable), the notation uses **observation** $o_t$ instead of states $s_t$, but it does not really matter here.\nThe **rollout policy** $\\hat{\\pi}$ is a simple and fast policy. It does not have to be the trained policy $\\pi$. \nIt could even be a random policy, or a pretrained policy using for example A3C directly.\nIn I2A, it is a **distilled policy** from the trained policy $\\pi$ (see later).\nTake home message: given the current observation $o_t$ and a policy $\\hat{\\pi}$, we can predict the next observation $\\hat{o}_{t+1}$ and the next reward $\\hat{r}_{t+1}$.\n\nThe **imagination rollout module** uses the imagination core to predict iteratively the next $\\tau$ frames and rewards using the current frame $o_t$ and the rollout policy:\n\n$$o_t \\rightarrow \\hat{o}_{t+1} \\rightarrow \\hat{o}_{t+2} \\rightarrow \\ldots \\rightarrow \\hat{o}_{t+\\tau}$$\n\nThe $\\tau$ frames and rewards are passed **backwards** to a convolutional LSTM (from $t+\\tau$ to $t$) which produces an embedding / encoding of the rollout. The output of the imagination rollout module is a vector $e_i$ (the final state of the LSTM) representing the whole rollout, including the (virtually) obtained rewards. Note that because of the stochasticity of the rollout policy $\\hat{\\pi}$, different rollouts can lead to different encoding vectors.\n\nFor the current observation $o_t$, we then generate one **rollout** per possible action (5 in Sokoban):\n\n* What would happen if I do action 1?\n* What would happen if I do action 2?\n* etc.\n\nThe resulting vectors are concatenated to the output of **model-free** path (a convolutional neural network taking the current observation as input). Altogether, we have a huge NN with weights $\\theta$ (model, encoder, MF path) producing an input $s_t$ to the **A3C** module.\n\nWe can then learn the policy $\\pi$ and value function $V$ based on this input to maximize the returns:\n\n$$\\nabla_\\theta \\mathcal{J}(\\theta) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) \\, (\\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V_\\varphi(s_{t+n}) - V_\\varphi(s_t)) ]$$\n\n$$\\mathcal{L}(\\varphi) =  \\mathbb{E}_{s_t \\sim \\rho_\\theta, a_t \\sim \\pi_\\theta}[(\\sum_{k=0}^{n-1} \\gamma^{k} \\, r_{t+k+1} + \\gamma^n \\, V_\\varphi(s_{t+n}) - V_\\varphi(s_t))^2]$$\n\nThe complete architecture may seem complex, but everything is differentiable so we can apply backpropagation and train the network **end-to-end** using multiple workers. It is the A3C algorithm (MF), but **augmented** by MB rollouts, i.e. with explicit information about the future.\n\nThe **rollout policy** $\\hat{\\pi}$ is trained using **policy distillation** of the trained policy $\\pi$ [@Rusu2016]. The small rollout policy network with weights $\\hat{\\theta}$ tries to copy the outputs $\\pi(s, a)$ of the bigger policy network (A3C). This is a supervised learning task: just minimize the KL divergence between the two policies:\n\n$$\\mathcal{L}(\\hat{\\theta}) = \\mathbb{E}_{s, a} [D_\\text{KL}(\\hat{\\pi}(s, a) || \\pi(s, a))]$$\n\nAs the network is smaller, it won't be as good as $\\pi$, but its learning objective is easier.\n\n![Policy distillation. Source [@Rusu2016].](../slides/img/policydistillation.png){width=80%}\n\n::: {.callout-tip}\n## Distral : distill and transfer learning\n\nDistillation can be used to ensure generalization over different environments [@Teh2017].\nEach learning algorithms learns its own task, but tries not to diverge too much from a **shared policy**, which turns out to be good at all tasks.\n\n![Distral architecture. Source [@Teh2017].](../slides/img/distral.png){width=100%}\n:::\n\nUnsurprisingly, I2A performs better than A3C on Sokoban. The deeper the rollout, the better.\n\n![I2A results on Sokoban. Source [@Weber2017].](../slides/img/i2a-results.png){width=100%}\n\n\nThe model does not even have to be perfect: the MF path can compensate for imperfections.\n\n![I2A results on Sokoban. Source [@Weber2017].](../slides/img/i2a-results2.png){width=100%}\n\n{{< youtube  llwAwE7ItdM >}}\n\n\n## Temporal difference models - TDM\n\n{{< youtube  Qedw02mDtrs >}}\n\nOne problem with model-based planning is the **discretization time step** (difference between $t$ and $t+1$).\nIt is determined by the action rate: how often a different action $a_t$ has to be taken. \nIn robotics, it could be below the millisecond, leading to very long trajectories in terms of steps.\n\n\n![Planning a path from Berkeley to the Golden Gate bridge has a very long horizon. Source: <https://bairblog.github.io/2018/04/26/tdm/>.](../slides/img/tdm-mb-bike-plan-small.png){width=70%}\n\nIf you want to go from Berkeley to the Golden State bridge with your bike, planning over leg movements will be very expensive (long horizon).\nA solution is **multiple steps ahead planning**. Instead of learning a one-step model:\n\n$$s_{t+1} = f_\\theta(s_t, a_t)$$\n\none learns to predict the state achieved in $T$ steps using the current policy:\n\n$$s_{t+ T} = f_\\theta(s_t, a_t, \\pi)$$\n\nPlanning and acting occur at different time scales.\n\nA problem with RL in general is how to define the **reward function**.\nIf you goal is to travel from Berkeley to the Golden State bridge, which reward function should you use?\n* +1 at the bridge, 0 otherwise (sparse).\n* +100 at the bridge, -1 otherwise (sparse).\n* minus the distance to the bridge (dense).\n\n**Goal-conditioned RL** defines the reward function using the distance between the achieved state $s_{t+1}$ and a **goal state** $s_g$:\n\n$$r(s_t, a_t, s_{t+1}) = - || s_{t+1} - s_g ||$$\n\nAn action is good if it brings the agent closer to its goal.\nThe Euclidean distance works well for the biking example (e.g. using a GPS), but the metric can be adapted to the task. \n\nOne advantage is that you can learn multiple \"tasks\" at the same time with a single policy, not the only one hard-coded in the reward function. Another advantage is that it makes a better use of exploration by learning from mistakes: **hindsight experience replay** (HER, [@Andrychowicz2017]).\n\nIf your goal is to reach $s_g$ but the agent generates a trajectory landing in $s_{g'}$, you can learn that this trajectory is good way to reach $s_{g'}$! In football, if you try to score a goal but end up doing a pass to a teammate, you can learn that this was a bad shot **and** a good pass.\nHER is a model-based method: you implicitly learn a model of the environment by knowing how to reach any position.\n\n![Hindsight experience replay allows to learn even from mistakes. Source: <https://openai.com/blog/ingredients-for-robotics-research/>](../slides/img/HER.png){width=80%}\n\nExploration never fails: you always learn to do something, even if this was not your original goal. \nThe principle of HER can be used in all model-free methods: DQN, DDPG, etc.\n\nUsing the goal-conditioned reward function $r(s_t, a_t, s_{t+1}) = - || s_{t+1} - s_g ||$, how can we learn?\nTDM introduces goal-conditioned Q-value with a horizon $T$:  $Q(s, a, s_g, T)$. The Q-value of an action should denote **how close** we will be from the goal $s_g$ in $T$ steps.\nIf we can estimate these Q-values, we can use a planning algorithm such as MPC to find the action that will bring us closer to the goal easily:\n\n$$a^* = \\text{arg}\\max_{a_t} \\, r(s_{t+T}, a_{t+T}, s_{t+T + 1})$$ \n\nThis corresponds to planning $T$ steps ahead; which action should I do now in order to be close to the goal in $T$ steps?\n\n![](../slides/img/tdm-steps1.jpeg)\n![Source: <https://bairblog.github.io/2018/04/26/tdm/>](../slides/img/tdm-steps2.jpeg)\n\n\n\nIf the horizon $T$ is well chosen, we only need to plan over a small number of intermediary positions, not over each possible action.\nTDM is model-free on each subgoal, but model-based on the whole trajectory.\n\nHow can we learn the goal-conditioned Q-values $Q(s, a, s_g, T)$ with a **model**?\nTDM introduces a recursive relationship for the Q-values:\n\n$$\\begin{aligned}\n    Q(s, a, s_g, T) &= \\begin{cases} \n        \\mathbb{E}_{s'} [r(s, a, s')] \\; \\text{if} \\; T=0\\\\\n        &\\\\\n        \\mathbb{E}_{s'} [\\max_a \\, Q(s', a, s_g, T-1)] \\; \\text{otherwise.}\\\\\n        \\end{cases} \\\\\n        &\\\\\n        &= \\mathbb{E}_{s'} [r(s, a, s') \\, \\mathbb{1}(T=0) + \\max_a \\, Q(s', a, s_g, T-1) \\, \\mathbb{1}(T\\neq 0)]\\\\\n\\end{aligned}$$\n\nIf we plan over $T=0$ steps, i.e. immediately after the action $(s, a)$, the Q-value is the remaining distance to the goal from the next state $s'$.\nOtherwise, it is the Q-value of the greedy action in the next state $s'$ with an horizon $T-1$ (one step shorter).\nThis allows to learn the Q-values from **single transitions** $(s_t, a_t, s_{t+1})$:\n* with $T=0$, the target is the remaining distance to the goal.\n* with $T>0$, the target is the Q-value of the next action at a shorter horizon.\n\nThe critic learns to minimize the prediction error **off-policy**:\n\n$$\\mathcal{L}(\\theta) = \\mathbb{E}_{s_t, a_t, s_{t+1} \\in \\mathcal{D}} [(r(s_t, a_t, s_{t+1}) \\, \\mathbb{1}(T=0) + \\max_a \\, Q(s_{t+1}, a, s_g, T-1) \\, \\mathbb{1}(T\\neq 0) - Q(s_t, a_t, s_g, T))^2]$$\n\nThis is a model-free Q-learning-like update rule, that can be learned by any off-policy value-based algorithm (DQN, DDPG) and an experience replay memory.\nThe cool trick is that, with a single transition $(s_t, a_t, s_{t+1})$, you can train the critic with:\n* different horizons $T$, e.g. between 0 and $T_\\text{max}$.\n* different goals $s_g$. You can sample any achievable state as a goal, including the \"true\" $s_{t+T}$ (hindsight).\n\nYou do not only learn to reach $s_g$, but any state! TDM learns a lot of information from a single transition, so it has a very good sample complexity.\n\nTDM learns to break long trajectories into finite horizons (model-based planning) by learning model-free (Q-learning updates).\nThe critic learns how good an action (s, a) is order to reach a state $s_g$ in $T$ steps.\n\n$$Q(s, a, s_g, T) = \\mathbb{E}_{s'} [r(s, a, s') \\, \\mathbb{1}(T=0) + \\max_a \\, Q(s', a, s_g, T-1) \\, \\mathbb{1}(T\\neq 0)]$$\n\nThe actor uses MPC planning to iteratively select actions that bring us closer to the goal in $T$ steps:\n\n$$a_t = \\text{arg}\\max_{a} \\, Q(s_{t}, a, s_{g}, T)$$ \n\nThe argmax can be estimated via sampling.\nTDM is a model-based method in disguise: it does predict the next state directly, but how much closer it will be to the goal via Q-learning.\n\n\nFor problems where the model is easy to learn, the performance of TDM is on par with model-based methods (MPC).\n\n![TDM learns control problems which are easy for MB algorithms. Source: <https://bairblog.github.io/2018/04/26/tdm/>.](../slides/img/tdm-results1.gif){width=80%}\n\nModel-free methods have a much higher sample complexity. TDM learns much more from single transitions.\n\n![TDM learns control problems which are easy for MB algorithms. Source: <https://bairblog.github.io/2018/04/26/tdm/>.](../slides/img/tdm-results2.jpg){width=80%}\n\nFor problems where the model is complex to learn, the performance of TDM is on par with model-free methods (DDPG).\n\n![TDM learns control problems which are easy for MF algorithms. Source: <https://bairblog.github.io/2018/04/26/tdm/>.](../slides/img/tdm-results3.gif){width=80%}\n\nModel-based methods suffer from model imprecision on long horizons. TDM plans over shorter horizons $T$.\n\n![TDM learns control problems which are easy for MF algorithms. Source: <https://bairblog.github.io/2018/04/26/tdm/>.](../slides/img/tdm-results4.jpg){width=80%}\n\n## World models\n\n{{< youtube  O1Qt23Eg8MM >}}\n\nThe core idea of **world models** [@Ha2018] is to explicitly separate the **world model** (what will happen next) from the **controller** (how to act). Deep RL NN are usually small, as rewards do not contain enough information to train huge networks.\n\n![Architecture of world models. Source: <https://worldmodels.github.io/>.](../slides/img/wm-overview.svg){width=100%}\n\nA huge **world model** can be efficiently trained by supervised or unsupervised methods.\nA small **controller** should not need too many trials if its input representations are good.\n\n\nThe vision module $V$ is trained as a **variational autoencoder** (VAE) on single frames of the game. The latent vector $\\mathbf{z}_t$ contains a compressed representation of the frame $\\mathbf{o}_t$.\n\n![Vision module. Source: <https://worldmodels.github.io/>.](../slides/img/wm-vae.svg){width=100%}\n\nThe sequence of latent representations $\\mathbf{z}_0, \\ldots \\mathbf{z}_t$ in a game is fed to a LSTM layer together with the actions $a_t$ to compress what happens over time. A **Mixture Density Network** (MDN) is used to predict the **distribution** of the next latent representations $P(\\mathbf{z}_{t+1} | a_t, \\mathbf{h}_t, \\ldots \\mathbf{z}_t)$.\n\nThe RNN-MDN architecture [@Ha2017] has been used successfully in the past for sequence generation problems such as generating handwriting and sketches (Sketch-RNN, see <https://magenta.tensorflow.org/sketch-rnn-demo> for demos).\n\n![Memory module. Source: <https://worldmodels.github.io/>.](../slides/img/wm-mdn_rnn.svg){width=100%}\n\nThe last step is the **controller**. It takes a latent representation $\\mathbf{z}_t$ and the current hidden state of the LSTM $\\mathbf{h}_t$ as inputs and selects an action **linearly**:\n\n$$a_t = \\text{tanh}(W \\, [\\mathbf{z}_t, \\mathbf{h}_t ] + b)$$\n\nA RL actor cannot get simpler as that...\n\n![Controller. Source: <https://worldmodels.github.io/>.](../slides/img/wm-schematic.svg){width=70%}\n\nThe controller is not even trained with RL: it uses a genetic algorithm, the Covariance-Matrix Adaptation Evolution Strategy (CMA-ES), to find the output weights that maximize the returns. The world model is trained by classical supervised learning using a random agent before learning.\n\nRefer <https://worldmodels.github.io/> to see the model in action.\n\n**Algorithm:**\n\n1. Collect 10,000 rollouts from a random policy.\n\n2. Train VAE (V) to encode each frame into a latent vector $\\mathbf{z} \\in \\mathcal{R}^{32}$.\n\n3. Train MDN-RNN (M) to model $P(\\mathbf{z}_{t+1} | a_t, \\mathbf{h}_t, \\ldots \\mathbf{z}_t)$.\n\n4. Evolve Controller (C) to maximize the expected cumulative reward of a rollout.\n\nThe **world model** V+M is learned **offline** with a random agent, using unsupervised learning.\nThe **controller** C has few weights (1000) and can be trained by evolutionary algorithms, not even RL.\nThe network can even learn by playing entirely in its **own imagination** as the world model can be applied on itself and predict all future frames.\nIt just need to additionally predict the reward.\nThe learned policy can then be transferred to the real environment. \n\n## Deep Planning Network - PlaNet\n\n{{< youtube  h77PXNDgHD0 >}}\n\nPlaNet [@Hafner2019] extends the idea of World models by learning the model together with the policy (**end-to-end**).\nIt learns a **latent dynamics model** that takes the past observations $o_t$ into account (needed for POMDPs):\n\n$$s_{t}, r_{t+1}, \\hat{o}_t = f(o_t, a_t, s_{t-1})$$\n\nand plans in the latent space using multiple rollouts:\n\n$$a_t = \\text{arg}\\max_a \\mathbb{E}[R(s_t, a, s_{t+1}, \\ldots)]$$\n\nThe latent dynamics model is a sequential variational autoencoder learning concurrently:\n\n1. An **encoder** from the observation $o_t$ to the latent space $s_t$: $q(s_t | o_t)$.\n2. A **decoder** from the latent space to the reconstructed observation $\\hat{o}_t$: $p(\\hat{o}_t | s_t)$.\n3. A **transition model** to predict the next latent representation given an action: $p(s_{t+1} | s_t, a_t)$.\n4. A **reward model** predicting the immediate reward: $p(r_t | s_t)$.\n\n![Latent dynamics model of PlaNet [@Hafner2019]. Source: <https://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html>.](../slides/img/planet-model.png){width=100%}\n\nThe loss function to train this **recurrent state-space model** (RSSM), with a deterministic component in the transition model (RNN) and stochastic components is not shown here. \n\nTraining sequences $(o_1, a_1, o_2, \\ldots, o_T)$ can be generated **off-policy** (e.g. from demonstrations) or on-policy. \n\n![Latent dynamics model of PlaNet [@Hafner2019]. Source: <https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html>.](../slides/img/dreamer-model.gif){width=70%}\n\nFrom a single observation $o_t$ encoded into $s_t$, 10000 rollouts are generated using **random sampling**.\nA belief over action sequences is updated using the **cross-entropy method** (CEM) in order to restrict the search.\nThe first action of the sequence with the highest estimated return (reward model) is executed. \nAt the next time step, planning starts from scratch: Model Predictive Control.\nThere is no actor in PlaNet, only a transition model used for planning.\n\n![Planning module of PlaNet [@Hafner2019]. Source: <https://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html>.](../slides/img/planet-planning.png){width=70%}\n\nPlanet learns continuous image-based control problems in 2000 episodes, where D4PG needs 50 times more.\n\n{{< youtube  tZk1eof_VNA >}}\n\nThe latent dynamics model can learn 6 control tasks **at the same time**.\nAs there is no actor, but only a planner, the same network can control all agents!\n\n## Dreamer\n\nDreamer [@Hafner2020] extends the idea of PlaNet by additionally **training an actor** instead of using a MPC planner.\nThe latent dynamics model is the same RSSM architecture.\nTraining a \"model-free\" actor on imaginary rollouts instead of MPC planning should reduce the computational time.\n\n![Architecture of Dreamer. Source: <https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html>.](../slides/img/dreamer-principle.png){width=100%}\n\nThe behavior module learns to predict the value of a state $V_\\varphi(s)$ and the policy $\\pi_\\theta(s)$ (actor-critic).\nIt is trained **in imagination** in the latent space using the reward model for the immediate rewards (to compute returns) and the transition model for the next states.\n\n![Training of the actor-critic behaviour module is end-to-end using a single rollout. Source: <https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html>.](../slides/img/dreamer-actor.gif){width=100%}\n\nThe current observation $o_t$ is encoded into a state $s_t$, the actor selects an action $a_t$, the transition model predicts $s_{t+1}$, the reward model predicts $r_{t+1}$, the critic predicts $V_\\varphi(s_t)$.\nAt the end of the sequence, we apply **backpropagation-through-time** to train the actor and the critic.\n\n\nThe **critic** $V_\\varphi(s_t)$ is trained on the imaginary sequence $(s_t, a_t, r_{t+1}, s_{t+1}, \\ldots, s_T)$ to minimize the prediction error with the $\\lambda$-return:\n\n$$R^\\lambda_t = (1  - \\lambda) \\, \\sum_{n=1}^{T-t-1} \\lambda^{n-1} \\, R^n_t + \\lambda^{T-t-1} \\, R_t$$\n\nThe **actor** $\\pi_\\theta(s_t, a_t)$ is trained on the sequence to maximize the sum of the value of the future states:\n\n$$\\mathcal{J}(\\theta) = \\mathbb{E}_{s_t, a_t \\sim \\pi_\\theta} [\\sum_{t'=t}^T V_\\varphi(s_{t'})]$$\n\nThe main advantage of training an actor is that we need only one rollout when training it: backpropagation maximizes the expected returns.\nWhen acting, we just need to encode the history of the episode in the latent space, and the actor becomes model-free!\n\n![Overview of Dreamer. Source: <https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html>.](../slides/img/dreamer-architecture.png){width=100%}\n\nDreamer beats model-free and model-based methods on 20 continuous control tasks.\n\n![Dreamer on several continuous control tasks. Source: <https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html>.](../slides/img/dreamer-results.gif){width=100%}\n\n![Performance of Dreamer. Source: <https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html>.](../slides/img/dreamer-results.png){width=100%}\n\nIt also learns Atari and Deepmind lab video games, sometimes on par with Rainbow or IMPALA!\n\n\n![Dreamer on Atari and DML games. Source: <https://dreamrl.github.io/>.](../slides/img/dreamer-resultsatari.gif){width=100%}\n\n![Performance of Dreamer. Source: <https://dreamrl.github.io/l>.](../slides/img/dreamer-resultsatari.png){width=100%}"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"4.2-LearnedModels.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}