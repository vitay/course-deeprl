{"title":"Successor representations","markdown":{"headingText":"Successor representations","containsRefs":false,"markdown":"\nSlides: [html](../slides/4.4-SR.html){target=\"_blank\"} [pdf](../slides/pdf/4.4-SR.pdf){target=\"_blank\"}\n\n## Model-based vs. Model-free\n\n{{< youtube  jt9YvApme3Q >}}\n\n### Comparison\n\nModel-free methods use the **reward prediction error** (RPE) to update values:\n\n$$\n    \\delta_t = r_{t+1} + \\gamma \\, V^\\pi(s_{t+1}) - V^\\pi(s_t)\n$$\n\n$$\n    \\Delta V^\\pi(s_t) = \\alpha \\, \\delta_t\n$$\n\nEncountered rewards propagate very slowly to all states and actions.\nIf the environment changes (transition probabilities, rewards), they have to relearn everything.\nAfter training, selecting an action is very fast.\n\nModel-based RL can learn very fast changes in the transition or reward distributions:\n\n$$\n    \\Delta r(s_t, a_t, s_{t+1}) = \\alpha \\, (r_{t+1} - r(s_t, a_t, s_{t+1}))\n$$\n\n$$\n    \\Delta p(s' | s_t, a_t) = \\alpha \\, (\\mathbb{I}(s_{t+1} = s') - p(s' | s_t, a_t))\n$$\n\nBut selecting an action requires planning in the tree of possibilities (slow).\n\nRelative advantages of MF and MB methods:\n\n\n|       | Inference speed   | Sample complexity        |  Optimality             | Flexibility |\n| :--- | :--- | :--- | :--- | :--- | \n| Model-free  | fast              |  high                    |  yes                    |   no        |\n| Model-based | slow              | low                      | as good as the model    | yes         |\n\n\nA trade-off would be nice... Most MB models in the deep RL literature are hybrid MB/MF models anyway.\n\n### Goal-directed learning vs. habit formation\n\nTwo forms of behavior are observed in the animal psychology literature:\n\n1. **Goal-directed** behavior learns Stimulus $\\rightarrow$ Response $\\rightarrow$ Outcome associations.\n2. **Habits** are developed by overtraining Stimulus $\\rightarrow$ Response associations.\n\nThe main difference is that habits are not influenced by **outcome devaluation**, i.e. when reard slose their value.\n\n\n![Outcome devaluation. Credit: Bernard W. Balleine](../slides/img/rewarddevaluation.jpg){width=80%}\n\nThe classical theory assigns MF to habits and MB to goal-directed, mostly because their sensitivity to outcome devaluation.\nThe open question is the arbitration mechanism between these two segregated process: who takes control?\nRecent work suggests both systems are largely overlapping.\n\n## Successor representations\n\n{{< youtube  Qt2Qtf_WbfM >}}\n\n### Principle of successor representations\n\nSuccessor representations (SR, [@Dayan1993]) have been introduced to combine MF and MB properties. Let's split the definition of the value of a state:\n\n$$\n\\begin{align}\n    V^\\pi(s) &= \\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, r_{t+k+1} | s_t =s] \\\\\n            &\\\\\n               &= \\mathbb{E}_{\\pi} [\\begin{bmatrix} 1 \\\\ \\gamma \\\\ \\gamma^2 \\\\ \\ldots \\\\ \\gamma^\\infty \\end{bmatrix} \\times\n                  \\begin{bmatrix} \\mathbb{I}(s_{t}) \\\\ \\mathbb{I}(s_{t+1}) \\\\ \\mathbb{I}(s_{t+2}) \\\\ \\ldots \\\\ \\mathbb{I}(s_{\\infty}) \\end{bmatrix}  \\times\n                  \\begin{bmatrix} r_{t+1} \\\\ r_{t+2} \\\\ r_{t+3} \\\\ \\ldots \\\\ r_{t+\\infty} \\end{bmatrix} \n                | s_t =s]\\\\\n\\end{align}\n$$\n\nwhere $\\mathbb{I}(s_{t})$ is 1 when the agent is in $s_t$ at time $t$, 0 otherwise.\n\nThe left part corresponds to the **transition dynamics**: which states will be visited by the policy, discounted by $\\gamma$.\nThe right part corresponds to the **immediate reward** in each visited state.\nCouldn't we learn the transition dynamics and the reward distribution separately in a model-free manner?\n\nSR rewrites the value of a state into an **expected discounted future state occupancy** $M^\\pi(s, s')$ and an **expected immediate reward** $r(s')$ by summing over all possible states $s'$ of the MDP:\n\n$$\n\\begin{align}\n    V^\\pi(s) &= \\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, r_{t+k+1} | s_t =s] \\\\\n            &\\\\\n               &= \\sum_{s' \\in \\mathcal{S}} \\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, \\mathbb{I}(s_{t+k}=s') \\times r_{t+k+1}  | s_t =s]\\\\\n            &\\\\\n               &\\approx \\sum_{s' \\in \\mathcal{S}} \\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, \\mathbb{I}(s_{t+k}=s')  | s_t =s] \\times \\mathbb{E} [r_{t+1}  | s_{t}=s']\\\\\n            &\\\\\n               &\\approx \\sum_{s' \\in \\mathcal{S}} M^\\pi(s, s') \\times r(s')\\\\\n\\end{align}\n$$\n\nThe underlying assumption is that the world dynamics are independent from the reward function (which does not depend on the policy). \nThis allows to re-use knowledge about world dynamics in other contexts (e.g. a new reward function in the same environment): **transfer learning**.\n\n![Transfer learning in Gridworld. Source: <https://awjuliani.medium.com/the-present-in-terms-of-the-future-successor-representations-in-reinforcement-learning-316b78c5fa3>](../slides/img/sr-transferlearning.png){width=80%}\n\nWhat matters is the states that you will visit and how interesting they are, not the order in which you visit them. \nKnowing that being in the mensa will eventually get you some food is enough to know that being in the mensa is a good state: you do not need to remember which exact sequence of transitions will put food in your mouth.\n\nSR algorithms must estimate two quantities:\n\n* The **expected immediate reward** received after each state:\n\n$$r(s) = \\mathbb{E} [r_{t+1} | s_t = s]$$\n\n* The **expected discounted future state occupancy** (the **SR** itself):\n\n$$M^\\pi(s, s') = \\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, \\mathbb{I}(s_{t+k} = s') | s_t = s]$$\n\nThe value of a state $s$ is then computed with:\n\n$$\n    V^\\pi(s) = \\sum_{s' \\in \\mathcal{S}} M(s, s') \\times r(s')\n$$\n\nwhat allows to infer the policy (e.g. using an actor-critic architecture). The immediate reward for a state can be estimated very quickly and flexibly after receiving each reward:\n\n$$\n    \\Delta \\, r(s_t) = \\alpha \\, (r_{t+1} - r(s_t))\n$$\n\nImagine a very simple MDP with 4 states and a single deterministic action:\n\n![](../slides/img/sr-simplemdp.svg){width=60%}\n\nThe transition matrix $\\mathcal{P}^\\pi$ depicts the possible $(s, s')$ transitions:\n\n$$\\mathcal{P}^\\pi = \\begin{bmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 1 \\\\\n\\end{bmatrix}$$\n\nThe SR matrix $M$ also represents the future transitions discounted by $\\gamma$:\n\n$$M = \\begin{bmatrix}\n1 & \\gamma & \\gamma^2 & \\gamma^3 \\\\\n0 & 1 & \\gamma & \\gamma^2 \\\\\n0 & 0 & 1  & \\gamma\\\\\n0 & 0 & 0 & 1 \\\\\n\\end{bmatrix}$$\n\n\n::: {.callout-note}\n## SR matrix in a Tolman's maze\n\n![Source: [@Russek2017]](../slides/img/sr-tolman.png)\n\nThe SR represents whether a state can be reached soon from the current state (b) using the current policy.\nThe SR depends on the policy:\n* A random agent will map the local neighborhood (c).\n* A goal-directed agent will have SR representations that follow the optimal path (d).\n\nIt is therefore different from the transition matrix, as it depends on behavior and rewards.\nThe exact dynamics are lost compared to MB: it only represents whether a state is reachable, not how.\n:::\n\nThe SR matrix reflects the proximity between states depending on the transitions and the policy. it does not have to be a spatial relationship.\n\n![](../slides/img/schapiro1.png){width=60%}\n![Probabilistic MDP and the corresponding SR matrix. Source: [@Stachenfeld2017].](../slides/img/schapiro2.png){width=60%}\n\n\n### Learning successor representations\n\nHow can we learn the SR matrix for all pairs of states?\n\n$$\n    M^\\pi(s, s') = \\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, \\mathbb{I}(s_{t+k} = s') | s_t = s]\n$$\n\nWe first notice that the SR obeys a recursive Bellman-like equation:\n\n$$\\begin{aligned}\n    M^\\pi(s, s') &= \\mathbb{I}(s_{t} = s') + \\mathbb{E}_{\\pi} [\\sum_{k=1}^\\infty \\gamma^k \\, \\mathbb{I}(s_{t+k} = s') | s_t = s] \\\\\n            &= \\mathbb{I}(s_{t} = s') + \\gamma \\, \\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, \\mathbb{I}(s_{t+k+1} = s') | s_t = s] \\\\\n            &= \\mathbb{I}(s_{t} = s') + \\gamma \\, \\mathbb{E}_{s_{t+1} \\sim \\mathcal{P}^\\pi(s' | s)} [\\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, \\mathbb{I}(s_{t+k} = s') | s_{t+1} = s] ]\\\\\n            &= \\mathbb{I}(s_{t} = s') + \\gamma \\, \\mathbb{E}_{s_{t+1} \\sim \\mathcal{P}^\\pi(s' | s)} [M^\\pi(s_{t+1}, s')]\\\\\n\\end{aligned}\n$$\n\nThis is reminiscent of TDM: the remaining distance to the goal is 0 if I am already at the goal, or gamma the distance from the next state to the goal.\n\nIf we know the transition matrix for a fixed policy $\\pi$: \n\n$$\\mathcal{P}^\\pi(s, s') = \\sum_a \\pi(s, a) \\, p(s' | s, a)$$\n\nwe can obtain the SR directly with matrix inversion as we did in **dynamic programming**:\n\n$$\n    M^\\pi = I + \\gamma \\, \\mathcal{P}^\\pi \\times M^\\pi\n$$\n\nso that:\n\n$$\n    M^\\pi = (I - \\gamma \\, \\mathcal{P}^\\pi)^{-1}\n$$\n\nThis DP approach is called **model-based SR** (MB-SR, [@Momennejad2017]) as it necessitates to know the environment dynamics.\n\n\nIf we do not know the transition probabilities, we simply sample a single $s_t, s_{t+1}$ transition:\n\n$$\n    M^\\pi(s_t, s') \\approx \\mathbb{I}(s_{t} = s') + \\gamma \\, M^\\pi(s_{t+1}, s')\n$$\n\nWe can define a **sensory prediction error** (SPE):\n\n$$\n    \\delta^\\text{SR}_t = \\mathbb{I}(s_{t} = s') + \\gamma \\, M^\\pi(s_{t+1}, s') - M(s_t, s')\n$$\n\nthat is used to update an estimate of the SR:\n\n$$\n    \\Delta M^\\pi(s_t, s') = \\alpha \\, \\delta^\\text{SR}_t\n$$\n\nThis is **SR-TD**, using a SPE instead of RPE, which learns only from transitions but ignores rewards.\n\nThe SPE has to be applied on ALL successor states $s'$ after a transition $(s_t, s_{t+1})$:\n\n$$\n    M^\\pi(s_t, \\mathbf{s'}) = M^\\pi(s_t, \\mathbf{s'}) + \\alpha \\, (\\mathbb{I}(s_{t}=\\mathbf{s'}) + \\gamma \\, M^\\pi(s_{t+1}, \\mathbf{s'}) - M(s_t, \\mathbf{s'}))\n$$\n\nContrary to the RPE, the SPE is a **vector** of prediction errors, used to update one row of the SR matrix.\nThe SPE tells how **surprising** a transition $s_t \\rightarrow s_{t+1}$ is for the SR.\n\n::: {.callout-tip}\n## Summary\n\nThe SR matrix represents the **expected discounted future state occupancy**:\n\n$$M^\\pi(s, s') = \\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, \\mathbb{I}(s_{t+k} = s') | s_t = s]$$\n\nIt can be learned using a TD-like SPE from single transitions:\n\n$$\n    M^\\pi(s_t, \\mathbf{s'}) = M^\\pi(s_t, \\mathbf{s'}) + \\alpha \\, (\\mathbb{I}(s_{t}=\\mathbf{s'}) + \\gamma \\, M^\\pi(s_{t+1}, \\mathbf{s'}) - M(s_t, \\mathbf{s'}))\n$$\n\nThe immediate reward in each state can be learned **independently from the policy**:\n\n$$\n    \\Delta \\, r(s_t) = \\alpha \\, (r_{t+1} - r(s_t))\n$$\n\nThe value $V^\\pi(s)$ of a state is obtained by summing of all successor states:\n\n$$\n    V^\\pi(s) = \\sum_{s' \\in \\mathcal{S}} M(s, s') \\times r(s')\n$$\n\nThis critic can be used to train an **actor** $\\pi_\\theta$ using regular TD learning (e.g. A3C).\n:::\n\nNote that it is straightforward to extend the idea of SR to state-action pairs:\n\n$$M^\\pi(s, a, s') = \\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, \\mathbb{I}(s_{t+k} = s') | s_t = s, a_t = a]$$\n\nallowing to estimate Q-values:\n\n$$\n    Q^\\pi(s, a) = \\sum_{s' \\in \\mathcal{S}} M(s, a, s') \\times r(s')\n$$\n\nusing SARSA or Q-learning-like SPEs:\n\n$$\n    \\delta^\\text{SR}_t = \\mathbb{I}(s_{t} = s') + \\gamma \\, M^\\pi(s_{t+1}, a_{t+1}, s') - M(s_t, a_{t}, s')\n$$\n\ndepending on the choice of the next action $a_{t+1}$ (on- or off-policy).\n\n\n## Successor features\n\n{{< youtube  3eOUgtU-3Vc >}}\n\nThe SR matrix associates each state to all others ($N\\times N$ matrix): \n\n* curse of dimensionality.\n* only possible for discrete state spaces.\n\nA better idea is to describe each state $s$ by a feature vector $\\phi(s) = [\\phi_i(s)]_{i=1}^d$ with less dimensions than the number of states.\nThis feature vector can be constructed (see the lecture on function approximation) or learned by an autoencoder (latent representation).\n\nThe **successor feature representation** (SFR) represents the discounted probability of observing a feature $\\phi_j$ after being in $s$. Instead of predicting when the agent will see a cat after being in the current state $s$, the SFR predicts when it will see eyes, ears or whiskers independently:\n\n$$\n    M^\\pi_j(s) = M^\\pi(s, \\phi_j) = \\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, \\mathbb{I}(\\phi_j(s_{t+k})) | s_t = s, a_t = a]\n$$\n\nLinear SFR [@Gehring2015] supposes that it can be linearly approximated from the features of the current state:\n\n$$\n    M^\\pi_j(s) = M^\\pi(s, \\phi_j) = \\sum_{i=1}^d m_{i, j} \\, \\phi_i(s)\n$$\n\nThe value of a state is now defined as the sum over successor features of their immediate reward discounted by the SFR:\n\n$$\n    V^\\pi(s) = \\sum_{j=1}^d M^\\pi_j(s) \\, r(\\phi_j) = \\sum_{j=1}^d r(\\phi_j) \\, \\sum_{i=1}^d m_{i, j} \\, \\phi_i(s)\n$$\n\nThe SFR matrix $M^\\pi = [m_{i, j}]_{i, j}$ associates each feature $\\phi_i$ of the current state to all successor features $\\phi_j$: Knowing that I see a kitchen door in the current state, how likely will I see a food outcome in the near future?\n\nEach successor feature $\\phi_j$ is associated to an expected immediate reward $r(\\phi_j)$: A good state is a state where food features (high $r(\\phi_j)$) are likely to happen soon (high $m_{i, j}$).\n\nIn matrix-vector form:\n\n$$\n    V^\\pi(s) = \\mathbf{r}^T \\times M^\\pi \\times \\phi(s)\n$$\n\nThe reward vector $\\mathbf{r}$ only depends on the features and can be learned independently from the policy, but can be made context-dependent: Food features can be made more important when the agent is hungry, less when thirsty.\n\n**Transfer learning** becomes possible in the same environment:\n* Different goals (searching for food or water, going to place A or B) only require different reward vectors.\n* The dynamics of the environment are stored in the SFR.\n\nHow can we learn the SFR matrix $M^\\pi$?\n\n$$\n    V^\\pi(s) = \\mathbf{r}^T \\times M^\\pi \\times \\phi(s)\n$$\n\nWe only need to use the sensory prediction error for a transition between the feature vectors $\\phi(s_t)$ and $\\phi(s_{t+1})$:\n\n$$\\delta_t^\\text{SFR} = \\phi(s_t) + \\gamma \\, M^\\pi \\times \\phi(s_{t+1}) - M^\\pi \\times \\phi(s_t)$$\n\nand use it to update the whole matrix:\n\n\n$$\\Delta M^\\pi = \\delta_t^\\text{SFR} \\times \\phi(s_t)^T$$\n\n\nHowever, this linear approximation scheme only works for **fixed** feature representation $\\phi(s)$. We need to go deeper...\n\n## Deep Successor Reinforcement Learning\n\n![Deep Successor Reinforcement Learning architecture. [@Kulkarni2016].](../slides/img/DSR.png){width=100%}\n\nEach state $s_t$ is represented by a D-dimensional (D=512) vector $\\phi(s_t) = f_\\theta(s_t)$ which is the output of an encoder. A decoder $g_{\\hat{\\theta}}$ is used to provide a reconstruction loss, so $\\phi(s_t)$ is a latent representation of an autoencoder:\n\n$$\\mathcal{L}_\\text{reconstruction}(\\theta, \\hat{\\theta}) = \\mathbb{E}[(g_{\\hat{\\theta}}(\\phi(s_t)) - s_t)^2]$$\n\nThe immediate reward $R(s_t)$ is linearly predicted from the feature vector $\\phi(s_t)$ using a reward vector $\\mathbf{w}$.\n\n$$R(s_t) = \\phi(s_t)^T \\times \\mathbf{w}$$\n\n$$\\mathcal{L}_\\text{reward}(\\mathbf{w}, \\theta) = \\mathbb{E}[(r_{t+1} - \\phi(s_t)^T \\times \\mathbf{w})^2]$$ \n\nThe reconstruction loss is important, otherwise the latent representation $\\phi(s_t)$ would be too reward-oriented and would not generalize.\nThe reward function is learned on a single task, but it can fine-tuned on another task, with all other weights frozen.\n\nFor each available action $a$, a DNN $u_\\alpha$ predicts the future feature occupancy $M(s, s', a)$ for the current state: \n\n$$m_{s_t a} = u_\\alpha(s_t, a)$$\n\nThe Q-value of an action is simply the dot product between the SR of an action and the reward vector $\\mathbf{w}$:\n\n$$Q(s_t, a) = \\mathbf{w}^T \\times m_{s_t a} $$\n\nThe selected action is $\\epsilon$-greedily selected around the greedy action:\n\n$$a_t = \\text{arg}\\max_a Q(s_t, a)$$\n\nThe SR of each action is learned using the Q-learning-like SPE (with fixed $\\theta$ and a target network $u_{\\alpha'}$):\n\n$$\\mathcal{L}^\\text{SPE}(\\alpha) = \\mathbb{E}[\\sum_a (\\phi(s_t) + \\gamma \\, \\max_{a'} u_{\\alpha'}(s_{t+1}, a') - u_\\alpha(s_t, a))^2]$$\n\nThe compound loss is used to train the complete network end-to-end **off-policy** using a replay buffer (DQN-like).\n\n$$\\mathcal{L}(\\theta, \\hat{\\theta}, \\mathbf{w}, \\alpha) = \\mathcal{L}_\\text{reconstruction}(\\theta, \\hat{\\theta}) + \\mathcal{L}_\\text{reward}(\\mathbf{w}, \\theta) + \\mathcal{L}^\\text{SPE}(\\alpha)$$\n\n\n![Deep Successor Reinforcement Learning algorithm. [@Kulkarni2016].](../slides/img/DSR-algorithm.png){width=100%}\n\n![Deep Successor Reinforcement Learning results. [@Kulkarni2016].](../slides/img/DSR-results.png){width=100%}\n\nThe interesting property is that you do not need rewards to learn:\n* A random agent can be used to learn the encoder and the SR, but $\\mathbf{w}$ can be left untouched.\n* When rewards are introduced (or changed), only $\\mathbf{w}$ has to be adapted, while DQN would have to re-learn all Q-values.\n\n![Change in the value of distal rewards. [@Kulkarni2016].](../slides/img/DSR-results2.png){width=100%}\n\nThis is the principle of **latent learning** in animal psychology: fooling around in an environment without a goal allows to learn the structure of the world, what can speed up learning when a task is introduced.\nThe SR is a **cognitive map** of the environment: learning task-unspecific relationships.\n\n:::{.callout-note}\n\nThe same idea was published by three different groups at the same time (preprint in 2016, conference in 2017): [@Barreto2016], [@Kulkarni2016], [@Zhang2016]. The [@Barreto2016] is from Deepmind, so it tends to be cited more...\n:::\n\n::: {.callout-tip}\n## Visual Semantic Planning using Deep Successor Representations\n\nSee the paper: [@Zhu2017]\n\n{{< youtube  _2pYVw6ATKo >}}\n:::\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"4.4-SR.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}