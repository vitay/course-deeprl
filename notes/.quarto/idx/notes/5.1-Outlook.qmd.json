{"title":"Outlook","markdown":{"headingText":"Outlook","containsRefs":false,"markdown":"\nSlides: [html](../slides/5.1-Outlook.html){target=\"_blank\"} [pdf](../slides/pdf/5.1-Outlook.pdf){target=\"_blank\"}\n\n## Limits of deep reinforcement learning\n\n{{< youtube  JDQdcTpHryM >}}\n\n\n### Overview\n\n**Model-free methods** (DQN, A3C, DDPG, PPO, SAC) are able to find optimal policies in complex MDPs by just **sampling** transitions.\nThey suffer however from a high **sample complexity**, i.e. they need ridiculous amounts of samples to converge.\n\n**Model-based methods** (I2A, Dreamer, MuZero) use **learned dynamics** to predict the future and plan the consequences of an action.\nThe sample complexity is lower, but learning a good model can be challenging. Inference times can be prohibitive.\n\n\n![Overview of deep RL methods. Source: <https://github.com/avillemin/RL-Personnal-Notebook>](../slides/img/drl-overview.svg){width=100%}\n\n\nDeep RL is still very unstable. Depending on initialization, deep RL networks may or may not converge (30% of runs converge to a worse policy than a random agent).\nCareful optimization such as TRPO / PPO help, but not completely.\nYou never know if failure is your fault (wrong network, bad hyperparameters, bug), or just bad luck.\n\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Deep RL is popular because it&#39;s the only area in ML where it&#39;s socially acceptable to train on the test set.</p>&mdash; Jacob Andreas (@jacobandreas) <a href=\"https://twitter.com/jacobandreas/status/924356906344267776?ref_src=twsrc%5Etfw\">October 28, 2017</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n\nAs it uses neural networks, deep RL **overfits** its training data, i.e. the environment it is trained on.\nIf you change anything to the environment dynamics, you need to retrain from scratch.\nOpenAI Five collects 900 years of game experience per day on Dota 2: it overfits the game, it does not learn how to play.\nModify the map a little bit and everything is gone (but see Meta RL - RL$^2$ later).\n\nClassical methods sometimes still work better. Model Predictive Control (MPC) is able to control Mujoco robots much better than RL through classical optimization techniques (e.g. iterative LQR) while needing much less computations.\nIf you have a good physics model, do not use DRL. Reserve it for unknown systems, or when using noisy sensors (images).\nGenetic algorithms (CMA-ES) sometimes give better results than RL to train policy networks.\n\n\n{{< youtube  uRVAX_sFT24 >}}\n\n\nYou cannot do that with deep RL (yet):\n\n\n{{< youtube  fRj34o4hN4I >}}\n\n\n### RL libraries\n\n* `keras-rl`: many deep RL algorithms implemented directly in keras: DQN, DDQN, DDPG, Continuous DQN (CDQN or NAF), Cross-Entropy Method (CEM)...\n\n<https://github.com/matthiasplappert/keras-rl>\n\n* `OpenAI Baselines` from OpenAI: A2C, ACER, ACKTR, DDPG, DQN, PPO, TRPO... Not maintained anymore.\n\n<https://github.com/openai/baselines>\n\n* `Stable baselines` from Inria Flowers, a clean rewrite of OpenAI baselines also including SAC and TD3. \n\n<https://github.com/hill-a/stable-baselines>\n\n* `rlkit` from Vitchyr Pong (PhD student at Berkeley) with in particular model-based algorithms (TDM).\n\n<https://github.com/vitchyr/rlkit>\n\n* `chainer-rl` implemented in Chainer: A3C, ACER, DQN, DDPG, PGT, PCL, PPO, TRPO.\n\n<https://github.com/chainer/chainerrl>\n\n* `RL Mushroom` is a very modular library based on Pytorch allowing to implement DQN and variants, DDPG, SAC, TD3, TRPO, PPO.\n\n<https://github.com/MushroomRL/mushroom-rl>\n\n\n* `Tensorforce` implement in tensorflow: DQN and variants, A3C, DDPG, TRPO, PPO.\n\n<https://github.com/tensorforce/tensorforce>\n\n* `Tensorflow Agents` is officially supported by tensorflow: DQN, A3C, DDPG, TD3, PPO, SAC.\n\n<https://github.com/tensorflow/agents>\n\n* `Coach` from Intel Nervana also provides many state-of-the-art algorithms.\n \n <https://github.com/NervanaSystems/coach>\n\n\n![Deep RL algorithms available in Coach. Source: <https://github.com/NervanaSystems/coach>](../slides/img/coach.png){width=100%}\n\n* `rllib` is part of the more global ML framework Ray, which also includes Tune for hyperparameter optimization. \n\nIt has implementations in both tensorflow and Pytorch. \n\nAll major model-free algorithms are implemented (DQN, Rainbow, A3C, DDPG, PPO, SAC), including their distributed variants (Ape-X, IMPALA, TD3) but also model-based algorithms (Dreamer!)\n\n<https://docs.ray.io/en/master/rllib.html>\n \n![Architecture of rllib. Source: <https://docs.ray.io/en/master/rllib.html>](../slides/img/rllib.svg){width=100%}\n\n\n## Inverse RL - learning the reward function\n\n{{< youtube  k-nr6Pwb9ds >}}\n\nRL is an optimization method: it maximizes the reward function that you provide it.\nIf you do not design the reward function correctly, the agent may not do what you expect.\nIn the Coast runners game, turbos provide small rewards but respawn very fast: it is more optimal to collect them repeatedly than to try to finish the race.\n\n\n{{< youtube  tlOIHko8ySg >}}\n\n\nDefining the reward function that does what you want becomes an art.\nRL algorithms work better with dense rewards than sparse ones. It is tempting to introduce intermediary rewards.\nYou end up covering so many special cases that it becomes unusable: Go as fast as you can but not in a curve, except if you are on a closed circuit but not if it rains...\n\n{{< youtube  8QnD8ZM0YCo >}}\n\nIn the OpenAI **Lego stacking** paper [@Popov2017], it was perhaps harder to define the reward function than to implement DDPG.\n\n![Lego stacking handmade reward function [@Popov2017].](../slides/img/lego_reward.png){width=100%}\n\nThe goal of **inverse RL** (see [@Arora2019] for a review) is to learn from **demonstrations** (e.g. from humans) which reward function is  maximized.\nThis is not **imitation learning**, where you try to learn and reproduce actions. \nThe goal if to find a **parametrized representation** of the reward function:\n\n$$\\hat{r}(s) = \\sum_{i=1}^K w_i \\, \\varphi_i(s)$$\n\nWhen the reward function has been learned, you can train a RL algorithm to find the optimal policy.\n\n![Inverse RL allows to learn from demonstrations. Source: <http://www.miubiq.cs.titech.ac.jp/modeling-risk-anticipation-and-defensive-driving-on-residential-roads-using-inverse-reinforcement-learning/>](../slides/img/inverseRL.png){width=70%}\n\n\n## Intrinsic motivation and curiosity\n\nOne fundamental problem of RL is its dependence on the **reward function**. \nWhen rewards are **sparse**, the agent does not learn much (but see successor representations) unless its random exploration policy makes it discover rewards.\nThe reward function is **handmade**, what is difficult in realistic complex problems.\n\nHuman learning does not (only) rely on maximizing rewards or achieving goals.\nEspecially infants discover the world by **playing**, i.e. interacting with the environment out of **curiosity**.\n\n> What happens if I do that? Oh, that's fun.\n\nThis called **intrinsic motivation**: we are motivated by understanding the world, not only by getting rewards.\nRewards are internally generated.\n\n![In intrinsic motivation, rewards are generated internally depending on the achieved states. Source: [@Barto2013].](../slides/img/intrinsicmotivation.gif){width=50%}\n\nWhat is **intrinsically** rewarding / motivating / fun? Mostly what has **unexpected** consequences.\n\n* If you can predict what is going to happen, it becomes boring.\n* If you cannot predict, you can become **curious** and try to **explore** that action.\n\n![Intrinsic rewards are defined by the ability to predict states. Source: <https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa>](../slides/img/intrinsicreward.png){width=100%}\n\nThe **intrinsic reward** (IR) of an action is defined as the sensory prediction error:\n\n$$\n    \\text{IR}(s_t, a_t, s_{t+1}) = || f(s_t, a_t) - s_{t+1}||\n$$\n\nwhere $f(s_t, a_t)$ is a **forward model** predicting the sensory consequences of an action.\nAn agent maximizing the IR will tend to visit unknown / poorly predicted states (**exploration**). \n\n\nIs it a good idea to predict frames directly? Frames are highly dimensional and there will always be a remaining error.\n\n![Intrinsic rewards are defined by the ability to predict states. Source: <https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa>](../slides/img/intrinsicreward-hard.png){width=100%}\n\nMoreover, they can be noisy and unpredictable, without being particularly interesting.\n\n![Falling leaves are hard to predict, but hardly interesting. Source: Giphy.](../slides/img/leaves.gif){width=50%}\n\nWhat can we do? As usual, predict in a latent space!\n\nThe intrinsic curiosity module (ICM, [@Pathak2017]) learns to provide an intrinsic reward for a transition $(s_t, a_t, s_{t+1})$ by comparing the predicted latent representation $\\hat{\\phi}(s_{t+1})$ (using a **forward** model) to its \"true\" latent representation $\\phi(s_{t+1})$.\nThe feature representation $\\phi(s_t)$ is trained using an **inverse model** predicting the action leading from $s_t$ to $s_{t+1}$.\n\n![intrinsic curiosity module. [@Pathak2017]](../slides/img/icm.jpg){width=100%}\n\n{{< youtube  J3FHOyhUn3A >}}\n\n::: {.callout-tip}\n## Curiosity-driven RL on Atari games [@Burda2018]:\n\n{{< youtube  l1FqtAHfJLI >}}\n:::\n\n## Hierarchical RL - learning different action levels\n\n{{< youtube  O7cKkOzWn3s >}}\n\nIn all previous RL methods, the action space is fixed.\nWhen you read a recipe, the actions are \"Cut carrots\", \"Boil water\", etc.\nBut how do you perform these **high-level actions**? Break them into subtasks iteratively until you arrive to muscle activations.\nBut it is not possible to learn to cook a boeuf bourguignon using muscle activations as actions.\n\n\n![Hierarchical structure of preparing a boeuf bourguignon. Source: <https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/>](../slides/img/hierarchicalRL.png){width=100%}\n\nSub-policies (**options**) can be trained to solve simple tasks (going left, right, etc).\nA **meta-learner** or controller then learns to call each sub-policy when needed, at a much lower frequency [@Frans2017].\n\n![Meta Learning Shared Hierarchies [@Frans2017]. Source: <https://openai.com/blog/learning-a-hierarchy/>](../slides/img/MLSH.gif){width=100%}\n\n\n{{< youtube  zkJmH4NlzPs >}}\n\n\nSome additional references on Hierarchical Reinforcement Learning\n\n* **MLSH:** Frans, K., Ho, J., Chen, X., Abbeel, P., and Schulman, J. (2017). Meta Learning Shared Hierarchies. arXiv:1710.09767.\n* **FUN:** Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver, D., et al. (2017). FeUdal Networks for Hierarchical Reinforcement Learning. arXiv:1703.01161\n* **Option-Critic architecture:** Bacon, P.-L., Harb, J., and Precup, D. (2016). The Option-Critic Architecture. arXiv:1609.05140.\n* **HIRO:** Nachum, O., Gu, S., Lee, H., and Levine, S. (2018). Data-Efficient Hierarchical Reinforcement Learning. arXiv:1805.08296.\n* **HAC:** Levy, A., Konidaris, G., Platt, R., and Saenko, K. (2019). Learning Multi-Level Hierarchies with Hindsight. arXiv:1712.00948.\n* **Spinal-cortical:** Heess, N., Wayne, G., Tassa, Y., Lillicrap, T., Riedmiller, M., and Silver, D. (2016). Learning and Transfer of Modulated Locomotor Controllers. arXiv:1610.05182.\n\n\n## Meta Reinforcement learning - RL$^2$ \n\n**Meta learning** is the ability to reuse skills acquired on a set of tasks to quickly acquire new (similar) ones (generalization).\n\n\n![](../slides/img/metalearning.png){width=100%}\n![Meta Reinforcement learning. Source: <https://meta-world.github.io/>](../slides/img/ml10.gif){width=100%}\n\n\n\nMeta RL is based on the idea of **fast and slow** learning:\n* Slow learning is the adaptation of weights in the NN.\n* Fast learning is the adaptation to changes in the environment.\n\nA simple strategy developed concurrently by [@Wang2017a] and [@Duan2016a]is to have a model-free algorithm (e.g. A3C) integrate with a LSTM layer not only the current state $s_t$, but also the previous action $a_{t-1}$ and reward $r_t$.\n\n![Meta RL uses a LSTM layer to encode past actions and rewards in the state representation. Source: [@Wang2017a]](../slides/img/metarl-lstm.png){width=30%}\n\n\nThe policy of the agent becomes **memory-guided**: it selects an action depending on what it did before, not only the state.\n\n\n![Meta RL algorithms are trained on a set of similar MDPs. Source: [@Duan2016a]](../slides/img/RL_2.png){width=100%}\n\nThe algorithm is trained on a set of similar MDPs:\n\n1. Select a MDP $\\mathcal{M}$.\n2. Reset the internal state of the LSTM.\n3. Sample trajectories and adapt the weights.\n4. Repeat 1, 2 and 3.\n\n\nThe meta RL can be be trained an a multitude of 2-armed bandits, each giving a reward of 1 with probability $p$ and $1-p$.\nLeft is a classical bandit algorithm, right is the meta bandit:\n\n![Classical bandit (left) and meta-bandit (right) learning a new two-armed bandit problem. Source: <https://hackernoon.com/learning-policies-for-learning-policies-meta-reinforcement-learning-rl%C2%B2-in-tensorflow-b15b592a2ddf>](../slides/img/metarl-twoarmed.gif){width=70%}\n\nThe meta bandit has learned that the best strategy for any 2-armed bandit is to sample both actions randomly at the beginning and then stick to the best one.\nThe meta bandit does not learn to solve each problem, it learns **how** to solve them.\n\n\n::: {.callout-note}\n## Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads\n\n[@Belkhale2021] <https://sites.google.com/view/meta-rl-for-flight>\n\n{{< youtube  AP5FgKjFpvQ >}}\n\n:::\n\nAdditional references on meta RL:\n\n* **Meta RL:** Wang JX, Kurth-Nelson Z, Tirumala D, Soyer H, Leibo JZ, Munos R, Blundell C, Kumaran D, Botvinick M. (2016). Learning to reinforcement learn. arXiv:161105763.\n* **RL$^2$** Duan Y, Schulman J, Chen X, Bartlett PL, Sutskever I, Abbeel P. 2016. RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning. arXiv:161102779.\n* **MAML:** Finn C, Abbeel P, Levine S. (2017). Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. arXiv:170303400.\n* **PEARL:** Rakelly K, Zhou A, Quillen D, Finn C, Levine S. (2019). Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables. arXiv:190308254.\n* **POET:** Wang R, Lehman J, Clune J, Stanley KO. (2019). Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions. arXiv:190101753.\n* **MetaGenRL:** Kirsch L, van Steenkiste S, Schmidhuber J. (2020). Improving Generalization in Meta Reinforcement Learning using Learned Objectives. arXiv:191004098.\n* Botvinick M, Ritter S, Wang JX, Kurth-Nelson Z, Blundell C, Hassabis D. (2019). Reinforcement Learning, Fast and Slow. Trends in Cognitive Sciences 23:408–422. doi:10.1016/j.tics.2019.02.006\n* <https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html>\n* <https://hackernoon.com/learning-policies-for-learning-policies-meta-reinforcement-learning-rl%C2%B2-in-tensorflow-b15b592a2ddf>\n* <https://towardsdatascience.com/learning-to-learn-more-meta-reinforcement-learning-f0cc92c178c1>\n* <https://eng.uber.com/poet-open-ended-deep-learning/>\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../center_images.lua","quarto"],"number-sections":false,"toc":true,"html-math-method":"katex","output-file":"5.1-Outlook.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../DeepLearning.bib","../ReinforcementLearning.bib"],"csl":"../frontiers.csl","theme":["cosmo","../custom.scss"],"page-layout":"full","number-depth":2,"smooth-scroll":true},"extensions":{"book":{"multiFile":true}}}}}