{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctpoSTL9oGq7"
   },
   "source": [
    "# Gym environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsWSoHfooGrJ"
   },
   "source": [
    "## Installing gym\n",
    "\n",
    "In this course, we will mostly address RL environments available in the **OpenAI Gym** framework:\n",
    "\n",
    "<https://gym.openai.com>\n",
    "\n",
    "It provides a multitude of RL problems, from simple text-based problems with a few dozens of states (Gridworld, Taxi) to continuous control problems (Cartpole, Pendulum) to Atari games (Breakout, Space Invaders) to complex robotics simulators (Mujoco, but the license is expensive):\n",
    "\n",
    "<https://gym.openai.com/envs>\n",
    "\n",
    "We start by installing gym and its dependencies. Unfortunately, gym only works fully under Linux or MacOS. You can `pip install gym` on Windows, but will be limited to the most simple environments, excluding for example Atari games (but you can try these instructions: <https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30>). This isn't a big issue as we will only use the simple environments in the exercises, but less fun.\n",
    "\n",
    "Moreover, it does not work out-of-the-box on colab, as gym opens graphical windows for some environments (e.g. Atari games), what is not possible in the browser. The first cell provides a workaround.\n",
    "\n",
    "### On your computer\n",
    "\n",
    "On your computer, you first need to make sure you have the right dependencies, especially `cmake`:\n",
    "\n",
    "**Ubuntu Linux:**\n",
    "\n",
    "```bash\n",
    "apt-get install libglu1-mesa-dev libgl1-mesa-dev libosmesa6-dev xvfb ffmpeg curl patchelf libglfw3 libglfw3-dev cmake zlib1g zlib1g-dev swig\n",
    "```\n",
    "\n",
    "**MacOSX:**\n",
    "\n",
    "```bash\n",
    "brew install cmake swig boost boost-python sdl2 wget\n",
    "```\n",
    "\n",
    "On Windows, you may have to have Microsoft Visual C++ build tools installed. The link <https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30> suggests the 2017 version, but Visual C++ 2015-2019 Redistributable seems to also work.\n",
    "\n",
    "You can then install gym in two steps, installing the `atari_py` dependency in between to be able to play Atari games.\n",
    "\n",
    "```bash\n",
    "pip install gym\n",
    "pip install atari_py\n",
    "pip install gym[atari]\n",
    "```\n",
    "\n",
    "### On Colab\n",
    "\n",
    "On Colab, we need to install a bunch of dependencies (Colab runs a Debian VM under the hood) and define a visualization function that can be called at the end of an episode to visualize it. \n",
    "\n",
    "The following cell detects whether you run this notebook on colab or not, and imports lots of additional stuff if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxLOmLKIoGrN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "    \n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if not IN_COLAB:\n",
    "    import gym\n",
    "    def show_video():\n",
    "        pass\n",
    "    def wrap_env(env):\n",
    "        return env\n",
    "else:\n",
    "    # Installing Debian and pip packages. It can take a while.\n",
    "    # remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
    "    !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "    !apt-get update > /dev/null 2>&1\n",
    "    !apt-get install cmake > /dev/null 2>&1\n",
    "    !pip install --upgrade setuptools 2>&1\n",
    "    !pip install ez_setup > /dev/null 2>&1\n",
    "    !pip install gym[atari] > /dev/null 2>&1\n",
    "    \n",
    "    import gym\n",
    "\n",
    "    from gym.wrappers import Monitor\n",
    "    import random, math, glob, io, base64\n",
    "    from IPython.display import HTML\n",
    "    from IPython import display as ipythondisplay\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(1400, 900))\n",
    "    display.start()\n",
    "\n",
    "    def show_video():\n",
    "        mp4list = glob.glob('video/*.mp4')\n",
    "        if len(mp4list) > 0:\n",
    "            mp4 = mp4list[0]\n",
    "            video = io.open(mp4, 'r+b').read()\n",
    "            encoded = base64.b64encode(video)\n",
    "            ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''.format(encoded.decode('ascii'))))\n",
    "        else: \n",
    "            print(\"Could not find video\")\n",
    "\n",
    "    def wrap_env(env):\n",
    "        env = Monitor(env, './video', force=True)\n",
    "        return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WENOr5atoGr1"
   },
   "source": [
    "## Gym interface \n",
    "\n",
    "The main interest of gym is that all problems have a common interface defined by the class `gym.Env`. There are only three methods that have to be implemented and used when creating a new environment:\n",
    "\n",
    "* `state = env.reset()` restarts the environment and returns an initial state $s_0$.\n",
    "\n",
    "* `state, reward, done, info = env.step(action)` takes an action $a_t$ and returns the new state $s_{t+1}$, the reward $r_{t+1}$, a boolean flag indicating whether the current state is terminal and a dictionary containing additional info for debugging (you can ignore it most of the time).\n",
    "\n",
    "* `env.render()` displays the current state of the MDP, either text-based or in a graphical window. When training, this should not be called to save some time.\n",
    "\n",
    "Additionally, you override the constructor `__init__()` to setup the state space (called observation space) and the action space. \n",
    "\n",
    "State and action space can either be :\n",
    "\n",
    "* discrete (`gym.spaces.Discrete(nb_states)`), with states being an integer between 0 and `nb_states` -1.\n",
    "\n",
    "* feature-based (`gym.spaces.Box(low=0, high=255, shape=(SCREEN_HEIGHT, SCREEN_WIDTH, 3))`) for pixel frames.\n",
    "\n",
    "* continuous:\n",
    "\n",
    "```python\n",
    "gym.spaces.Tuple(\n",
    "    gym.spaces.Box(-180.0, 180.0, 1), # First joint\n",
    "    gym.spaces.Box(-180.0, 180.0, 1)  # Second joint    \n",
    ")\n",
    "```\n",
    "\n",
    "Here is an example of a dummy environment with discrete states and actions, where the transition probabilities and rewards are completely random:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5u7Z8tjoGr3"
   },
   "outputs": [],
   "source": [
    "class FooEnv(gym.Env):\n",
    "    \"\"\"Dummy gym-like environment.\"\"\"\n",
    "\n",
    "    def __init__(self, nb_states, nb_actions):\n",
    "        \"Initialize the environment, can accept additional parameters such as the number of states and actions.\"\n",
    "        \n",
    "        # State space, can be discrete or continuous.\n",
    "        self.observation_space = gym.spaces.Discrete(nb_states)\n",
    "        \n",
    "        # Action space, can be discrete or continuous.\n",
    "        self.action_space = gym.spaces.Discrete(nb_actions)    \n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"Resets the environment and starts from an initial state.\"\n",
    "        \n",
    "        # Sample one state randomly \n",
    "        self.state = self.observation_space.sample()\n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Takes an action and returns a new state, a reward, a boolean (True for terminal states) \n",
    "        and a dictionary with additional info (optional).\n",
    "        \"\"\"\n",
    "        \n",
    "        self.state = self.observation_space.sample() # Random transition to another state\n",
    "        self.reward = np.random.uniform(0, 1, 1)[0] # Random reward\n",
    "        self.done = False # Continuing task\n",
    "        self.info = {} # No info\n",
    "        \n",
    "        return self.state, self.reward, self.done, self.info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"Displays the current state of the environment. Can be text or video frames.\"\n",
    "        \n",
    "        print(self.state)\n",
    "    \n",
    "    def close(self):\n",
    "        \"To be called before exiting, to free resources (not needed here).\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtmiSc9soGsB"
   },
   "source": [
    "With this interface, we can interact with the environment in a standardized way:\n",
    "\n",
    "* We first create the environment.\n",
    "\n",
    "* We pick an initial state with `reset()`.\n",
    "\n",
    "* For a fixed number of steps (or until the episode terminates):\n",
    "\n",
    "    * We render the current state with `render()`.\n",
    "    \n",
    "    * We select an action using our RL algorithm or randomly.\n",
    "    \n",
    "    * We take that action (`step()`), observe the new state and the reward.\n",
    "    \n",
    "    * We go into the new state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vefGCQ4aoGsD",
    "outputId": "3e434336-f819-45d3-dfd5-16427a3f5e03"
   },
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = FooEnv(10, 4)\n",
    "\n",
    "# Sample the initial state\n",
    "state = env.reset()\n",
    "\n",
    "# Sample 10 transitions\n",
    "for t in range(10):\n",
    "    \n",
    "    # Render the current state\n",
    "    env.render()\n",
    "\n",
    "    # Select an action randomly\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Sample a single transition\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Go in the next state\n",
    "    state = next_state\n",
    "\n",
    "# Exit cleanly\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFaEqT0loGsP"
   },
   "source": [
    "That's it. To use any of the gym environments, replace the creation of the `FooEnv` instance by:\n",
    "\n",
    "```python\n",
    "env = wrap_env(gym.make('CartPole-v0'))\n",
    "```\n",
    "\n",
    "if you want to interact with the Cartpole environment. `wrap_env()` is obligatory on Colab, otherwise you will not be able to visualize it. You can omit it on your computer. For text-based environments such as `FooEnv`, you do not need to wrap the environment.\n",
    "\n",
    "Each environment has a unique key, with a version number (`v0`). To have a list of the available environments, call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFaEqT0loGsP"
   },
   "outputs": [],
   "source": [
    "envs = gym.envs.registry.all()\n",
    "for env in envs:\n",
    "    print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFaEqT0loGsP"
   },
   "source": [
    "**Q:** Interact randomly with many gym environments (Cartpole, Pendulum, Breakout, SpaceInvaders, etc). Print the rewards you obtain.\n",
    "\n",
    "*Note 1:* If you run a fixed number of steps, you should reset the environment when a terminal state is encountered, other wise you will be stuck in that terminal state:\n",
    "\n",
    "```python\n",
    "if done:\n",
    "    state = env.reset()\n",
    "```\n",
    "\n",
    "*Note 2:* If you stop the execution of a cell but the window does not close, run `env.close()` in a separate cell.\n",
    "\n",
    "*Note 3:* Some environments are very fast, especially Atari games. A simple solution is to have python **sleep** a bit after rendering a frame, so that you can see something:\n",
    "\n",
    "```python\n",
    "import time\n",
    "# ...\n",
    "for t in range(1000):\n",
    "    env.render()\n",
    "    time.sleep(0.01) # sleep 10 milliseconds\n",
    "    # ...\n",
    "```\n",
    "\n",
    "*Note 4:* On Colab, `env.render()` will do nothing. You need to call `show_video()` after `env.close()` to see the movie of what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "djfokcW1oGsQ",
    "outputId": "b1672201-b1ac-4509-bcb9-ff1a46708c2a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7JjS7uwoGse"
   },
   "source": [
    "## Recycling robot\n",
    "\n",
    "**Q:** Create a `RecyclingRobot` gym-like environment using last week's exercise.\n",
    "\n",
    "The parameters `alpha`, `beta`, `r_wait` and `r_search` should be passed to the constructor of the environment and saved as attributes.\n",
    "\n",
    "The state space is discrete, with two states `high` and `low` which will have indices 0 and 1. The three discrete actions `search`, `wait` and `recharge` have indices 0, 1, and 2.\n",
    "\n",
    "The initial state of the MDP (`reset()`) should be the high state.\n",
    "\n",
    "The `step()` should generate transitions according to the dynamics of the MDP. Depending on the current state and the chosen action, make a transition to another state. For the actions `search` and `wait`, sample the reward from the normal distribution with mean `r_search` (resp. `r_wait`) and variance 0.5. \n",
    "\n",
    "If the random agent selects `recharge` in `high`, do nothing (next state is high, reward is 0).\n",
    "\n",
    "Rendering is just printing the current state. There is nothing to close, so you do not even need to redefine the function.\n",
    "\n",
    "Interact randomly with the MDP for several steps and observe the rewards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRhkICMBoGsf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udFjupHmoGso"
   },
   "source": [
    "To be complete, let's implement the random agent as a class. The class should look like:\n",
    "\n",
    "```python\n",
    "class RandomAgent:\n",
    "    \"\"\"\n",
    "    Random agent exploring uniformly the environment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        :param env: gym-like environment\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"Returns a random action by sampling the action space.\"\n",
    "        action = # TODO\n",
    "        return action\n",
    "    \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"Updates the agent using the transition (s, a, r, s').\"\n",
    "        pass\n",
    "    \n",
    "    def train(self, nb_steps, render=True):\n",
    "        \"Runs the agent on the environment for nb_steps. Returns the list of obtained rewards.\"\n",
    "        \n",
    "        # List of rewards\n",
    "        rewards = []\n",
    "\n",
    "        # TODO\n",
    "            \n",
    "        return rewards\n",
    "```\n",
    "\n",
    "The environment is passed to the constructor. `act(state)` should sample a random action. `update(state, action, reward, next_state)` does nothing for the random agent (`pass` is a Python command doing nothing), but we will implement it in the next exercises. \n",
    "\n",
    "`train(nb_steps, render)` implements the interaction loop between the agent and the environment for a fixed number of steps. It should return the list of obtained rewards. You can use the flag `render` to switch rendering or printing on and off.\n",
    "\n",
    "**Q:** Implement the random agent and have it interact with the environment for a fixed number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "or3R2eIpoGsp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HM17OH2joGsw"
   },
   "source": [
    "That's it! We now \"only\" need to define classes for all the sampling-based RL algorithms (MC, TD, deep RL) and we can interact with any environment using the previous cell!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "7-Gym-solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
