{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctpoSTL9oGq7"
   },
   "source": [
    "# Gym environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsWSoHfooGrJ"
   },
   "source": [
    "## Installing gym\n",
    "\n",
    "In this course, we will mostly address RL environments available in the **OpenAI Gym** framework:\n",
    "\n",
    "<https://gym.openai.com>\n",
    "\n",
    "It provides a multitude of RL problems, from simple text-based problems with a few dozens of states (Gridworld, Taxi) to continuous control problems (Cartpole, Pendulum) to Atari games (Breakout, Space Invaders) to complex robotics simulators (Mujoco):\n",
    "\n",
    "<https://gym.openai.com/envs>\n",
    "\n",
    "However, `gym` is not maintained by OpenAI anymore since September 2022. We will use instead the `gymnasium` library maintained by the Farama foundation, which will keep on maintaining and improving the library.\n",
    "\n",
    "<https://gymnasium.farama.org/>\n",
    "\n",
    "You can install gymnasium and its dependencies using:\n",
    "\n",
    "```bash\n",
    "pip install -U gymnasium pygame moviepy\n",
    "pip install gymnasium[classic_control]\n",
    "pip install gymnasium[box2d]\n",
    "```\n",
    "\n",
    "For this exercise and the following, we will focus on simple environments whose installation is straightforward: toy text, classic control and box2d. More complex environments based on Atari games or the Mujoco physics simulator are described in the last (optional) section of this notebook, as they require additional dependencies. \n",
    "\n",
    "On colab, `gym` cannot open graphical windows for visualizing the environments, as it is not possible in the browser. We will see a workaround allowing to produce videos. Running that cell in colab should allow you to run the simplest environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxLOmLKIoGrN"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install -U gymnasium pygame moviepy\n",
    "    !pip install gymnasium[box2d]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import gymnasium as gym\n",
    "print(\"gym version:\", gym.__version__)\n",
    "\n",
    "from moviepy.editor import ImageSequenceClip, ipython_display\n",
    "\n",
    "class GymRecorder(object):\n",
    "    \"\"\"\n",
    "    Simple wrapper over moviepy to generate a .gif with the frames of a gym environment.\n",
    "    \n",
    "    The environment must have the render_mode `rgb_array_list`.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self._frames = []\n",
    "\n",
    "    def record(self, frames):\n",
    "        \"To be called at the end of an episode.\"\n",
    "        for frame in frames:\n",
    "            self._frames.append(np.array(frame))\n",
    "\n",
    "    def make_video(self, filename):\n",
    "        \"Generates the gif video.\"\n",
    "        self.clip = ImageSequenceClip(list(self._frames), fps=self.env.metadata[\"render_fps\"])\n",
    "        self.clip.write_gif(filename, fps=self.env.metadata[\"render_fps\"])\n",
    "        del self._frames\n",
    "        self._frames = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with an environment\n",
    "\n",
    "A gym environment is created using:\n",
    "\n",
    "```python\n",
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "```\n",
    "\n",
    "where 'CartPole-v1' should be replaced by the environment you want to interact with. The following cell lists the environments available to you (including the different versions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env in gym.envs.registry.items():\n",
    "    print(env[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `render_mode` argument defines how you will see the environment:\n",
    "\n",
    "* `None` (default): allows to train a DRL algorithm without wasting computational resources rendering it.\n",
    "* `rgb_array_list`: allows to get numpy arrays corresponding to each frame. Will be useful when generating videos.\n",
    "* `ansi`: string representation of each state. Only available for the \"Toy text\" environments.\n",
    "* `human`: graphical window displaying the environment live."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main interest of gym(nasium) is that all problems have a common interface defined by the class `gym.Env`. There are only three methods that have to be used when interacting with an environment:\n",
    "\n",
    "* `state, info = env.reset()` restarts the environment and returns an initial state $s_0$.\n",
    "\n",
    "* `state, reward, terminal, truncated, info = env.step(action)` takes an action $a_t$ and returns:\n",
    "    * the new state $s_{t+1}$, \n",
    "    * the reward $r_{t+1}$, \n",
    "    * two boolean flags indicating whether the current state is terminal (won/lost) or truncated (timeout),\n",
    "    * a dictionary containing additional info for debugging (you can ignore it most of the time).\n",
    "\n",
    "* `env.render()` displays the current state of the MDP. When the render mode is set to `rgb_array_list` or `human`, it does not even have to called explicitly (since gym 0.25)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this interface, we can interact with the environment in a standardized way:\n",
    "\n",
    "* We first create the environment.\n",
    "* For a fixed number of episodes:\n",
    "    * We pick an initial state with `reset()`.\n",
    "    * Until the episode is terminated:\n",
    "        * We select an action using our RL algorithm or randomly.\n",
    "        * We take that action (`step()`), observe the new state and the reward.\n",
    "        * We go into the new state.\n",
    "\n",
    "The following cell shows how to interact with the CartPole environment using a random policy. Note that it will only work on your computer, not in colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "\n",
    "for episode in range(10):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Select an action randomly\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # Sample a single transition\n",
    "        next_state, reward, terminal, truncated, info = env.step(action)\n",
    "        \n",
    "        # Go in the next state\n",
    "        state = next_state\n",
    "\n",
    "        # End of the episode\n",
    "        done = terminal or truncated\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On colab (or whenever you want to record videos of the episodes instead of watching them live), you need to create the environment with the rendering mode `rgb_array_list`. \n",
    "\n",
    "You then create a `GymRecorder` object (defined in the first cell of this notebook). \n",
    "\n",
    "```python\n",
    "recorder = GymRecorder(env)\n",
    "```\n",
    "\n",
    "At the end of each episode, you tell the recorder to record all frames generated during the episode. The frames returned by `env.render()` are (width, height, 3) numpy arrays which are accumulated by the environment during the episode and flushed when `env.reset()` is called.\n",
    "\n",
    "```python\n",
    "if done:\n",
    "    recorder.record(env.render())\n",
    "```\n",
    "\n",
    "You can then generate a gif at the end of the simulation with:\n",
    "\n",
    "```python\n",
    "recorder.make_video('videos/CartPole-v1.gif')\n",
    "```\n",
    "\n",
    "Make sure that the subfolder `videos/` exists, or just save the video in the current directory.\n",
    "\n",
    "Finally, you can render the gif in the notebook by calling **at the very last line of the cell**:\n",
    "\n",
    "```python\n",
    "ipython_display('videos/CartPole-v1.gif')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array_list\")\n",
    "recorder = GymRecorder(env)\n",
    "\n",
    "for episode in range(10):\n",
    "    state, info = env.reset()\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Select an action randomly\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # Sample a single transition\n",
    "        next_state, reward, terminal, truncated, info = env.step(action)\n",
    "        \n",
    "        # Go in the next state\n",
    "        state = next_state\n",
    "\n",
    "        # End of the episode\n",
    "        done = terminal or truncated\n",
    "\n",
    "        if done:\n",
    "            recorder.record(env.render())\n",
    "\n",
    "recorder.make_video('videos/CartPole-v1.gif')\n",
    "ipython_display('videos/CartPole-v1.gif', autoplay=1, loop=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WENOr5atoGr1"
   },
   "source": [
    "Each environment defines its state space (`env.observation_space`) and action space (`env.action_space`). \n",
    "\n",
    "State and action spaces can either be :\n",
    "\n",
    "* discrete (`gym.spaces.Discrete(nb_states)`), with states being an integer between 0 and `nb_states` -1.\n",
    "\n",
    "* feature-based (`gym.spaces.Box(low=0, high=255, shape=(SCREEN_HEIGHT, SCREEN_WIDTH, 3))`) for pixel frames.\n",
    "\n",
    "* continuous. Example for two joints of a robotic arm limited between -180 and 180 degrees:\n",
    "\n",
    "```python\n",
    "gym.spaces.Box(-180.0, 180.0, (2, ))\n",
    "```\n",
    "\n",
    "You can sample a state or action randomly from these spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = gym.spaces.Box(-180.0, 180.0, (2, ))\n",
    "action = action_space.sample()\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Sampling the action space is particularly useful for exploration. We use it here to perform random (but valid) actions:\n",
    "\n",
    "```python\n",
    "action = env.action_space.sample()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Create a method `random_interaction(env, number_episodes, recorder=None)` that takes as arguments:\n",
    "\n",
    "* The environment.\n",
    "* The number of episodes to be performed.\n",
    "* An optional `GymRecorder` object that may record the frames of the environment if it is not None (`if renderer is not None:`). Otherwise, do not nothing.\n",
    "\n",
    "The method should return the list of undiscounted returns ($\\gamma=1$, i.e. just the sum of rewards obtained during each episode) for all episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Use that method to visualize all the available simple environments for a few episodes:\n",
    "\n",
    "* CartPole-v1\n",
    "* MountainCar-v0\n",
    "* Pendulum-v1\n",
    "* Acrobot-v1\n",
    "* LunarLander-v2\n",
    "* BipedalWalker-v3\n",
    "* CarRacing-v2\n",
    "* Blackjack-v1\n",
    "* FrozenLake-v1\n",
    "* CliffWalking-v0\n",
    "* Taxi-v3\n",
    "\n",
    "If you do many episodes (CarRacing or Taxi have very long episodes with a random policy), plot the obtained returns to see how they vary. \n",
    "\n",
    "If you managed to install the mujoco and atari dependencies, feel free to visualize them too. \n",
    "\n",
    "To understand what an environment is about, run `help(env)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating your own environment\n",
    "\n",
    "### Random environment\n",
    "\n",
    "You can create your own environment using the gym interface:\n",
    "\n",
    "<https://gymnasium.farama.org/tutorials/environment_creation/>\n",
    "\n",
    "Here is an example of a dummy environment with discrete states and actions, where the transition probabilities and rewards are completely random:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5u7Z8tjoGr3"
   },
   "outputs": [],
   "source": [
    "class RandomEnv(gym.Env):\n",
    "    \"Random discrete environment that does nothing.\"\n",
    "    \n",
    "    metadata = {\"render_modes\": [\"ansi\"], \"render_fps\": 1}\n",
    "\n",
    "    def __init__(self, nb_states, nb_actions, max_episode_steps=10, render_mode=\"ansi\"):\n",
    "\n",
    "        self.nb_states = nb_states\n",
    "        self.nb_actions = nb_actions\n",
    "        self.max_episode_steps = max_episode_steps\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # State space, can be discrete or continuous.\n",
    "        self.observation_space = gym.spaces.Discrete(nb_states)\n",
    "        \n",
    "        # Action space, can be discrete or continuous.\n",
    "        self.action_space = gym.spaces.Discrete(nb_actions)    \n",
    "\n",
    "        # Reset\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "\n",
    "        # Re-initialize time\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Sample one state randomly \n",
    "        self.state = self.observation_space.sample()\n",
    "        \n",
    "        return self.state, info\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        # Random transition to another state\n",
    "        self.state = self.observation_space.sample() \n",
    "        \n",
    "        # Random reward\n",
    "        reward = np.random.uniform(0, 1, 1)[0] \n",
    "        \n",
    "        # Terminate the episode after 10 steps\n",
    "        terminal = False \n",
    "        truncated = False\n",
    "\n",
    "        self.current_step +=1\n",
    "        if self.current_step % self.max_episode_steps == 0:\n",
    "            truncated = True \n",
    "\n",
    "        info = {} # No info\n",
    "\n",
    "        return self.state, reward, terminal, truncated, info\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"ansi\":\n",
    "            description = \"Step \" + str(self.current_step) + \": state \" + str(self.state)\n",
    "            return description\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different methods should be quite self-explanatory.\n",
    "\n",
    "`metadata` defines which render modes are available for this environment (here only the text mode \"ansi\").\n",
    "\n",
    "The constructor accepts the size of the state and action spaces as arguments, the duration of the episode and the render mode. \n",
    "\n",
    "`reset()` samples an initial state randomly.\n",
    "\n",
    "`step()` ignores the action, samples a new state and a reward, and truncates an episode after `max_episode_steps`.\n",
    "\n",
    "`render()` returns a string with the current state.\n",
    "\n",
    "**Q:** Interact with the random environment for a couple of episodes.\n",
    "\n",
    "As the mode is `ansi` (text-based), you will need to print the string returned by `render()` after each step:\n",
    "\n",
    "```python\n",
    "while not done:\n",
    "\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    next_state, reward, terminal, truncated, info = env.step(action)\n",
    "\n",
    "    print(env.render())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7JjS7uwoGse"
   },
   "source": [
    "### Recycling robot\n",
    "\n",
    "**Q:** Create a `RecyclingRobot` gym-like environment using last week's exercise.\n",
    "\n",
    "The parameters `alpha`, `beta`, `r_wait` and `r_search` should be passed to the constructor of the environment and saved as attributes.\n",
    "\n",
    "The state space is discrete, with two states `high` and `low` which will have indices 0 and 1. The three discrete actions `search`, `wait` and `recharge` have indices 0, 1, and 2.\n",
    "\n",
    "The initial state of the MDP (`reset()`) should be the high state.\n",
    "\n",
    "The `step()` should generate transitions according to the dynamics of the MDP. Depending on the current state and the chosen action, make a transition to another state. For the actions `search` and `wait`, sample the reward from the normal distribution with mean `r_search` (resp. `r_wait`) and variance 0.5. \n",
    "\n",
    "If the random agent selects `recharge` in `high`, do nothing (next state is high, reward is 0).\n",
    "\n",
    "Rendering is just printing the current state. There is nothing to close, so you do not even need to redefine the function.\n",
    "\n",
    "Although the recycling robot is a continuing task, limit the number of steps per episode to 10, as in the the previous random environment.\n",
    "\n",
    "Interact randomly with the MDP for several episodes and observe the returns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRhkICMBoGsf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udFjupHmoGso"
   },
   "source": [
    "### Random agent\n",
    "\n",
    "To be complete, let's implement the random agent as a class. The class should look like:\n",
    "\n",
    "```python\n",
    "class RandomAgent:\n",
    "    \"\"\"\n",
    "    Random agent exploring uniformly the environment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"Returns a random action by sampling the action space.\"\n",
    "        action = # TODO\n",
    "        return action\n",
    "    \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"Updates the agent using the transition (s, a, r, s').\"\n",
    "        pass\n",
    "    \n",
    "    def train(self, nb_episodes, render=False):\n",
    "        \"Runs the agent on the environment for nb_episodes. Returns the list of obtained returns.\"\n",
    "        \n",
    "        # List of returns\n",
    "        returns = []\n",
    "\n",
    "        # TODO\n",
    "            \n",
    "        return returns\n",
    "```\n",
    "\n",
    "The environment is passed to the constructor. `act(state)` should sample a random action. `update(state, action, reward, next_state)` does nothing for the random agent (`pass` is a Python command doing nothing), but we will implement it in the next exercises. \n",
    "\n",
    "`train(nb_episodes, render)` implements the interaction loop between the agent and the environment for a fixed number of episodes. It should return the list of obtained returns. `render` defines whether you print the state at each step or not.\n",
    "\n",
    "**Q:** Implement the random agent and have it interact with the environment for a fixed number of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "or3R2eIpoGsp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HM17OH2joGsw"
   },
   "source": [
    "That's it! We now \"only\" need to define classes for all the sampling-based RL algorithms (MC, TD, deep RL) and we can interact with any environment with a single line!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mujoco and Atari environments\n",
    "\n",
    "Note: both mujoco and atari environments will not work on colab. \n",
    "\n",
    "You may have to install non-Python packages on your computer, such as openGL. A lot of debugging in sight...\n",
    "\n",
    "The environments should work under Linux and MacOS, but I am not sure about windows. \n",
    "\n",
    "### Mujoco\n",
    "\n",
    "To install the mujoco environments of gymnasium, this should work:\n",
    "\n",
    "```bash\n",
    "pip install mujoco\n",
    "pip install gymnasium[mujoco]\n",
    "```\n",
    "\n",
    "Interaction should work as usual. See all environments here: <https://gymnasium.farama.org/environments/mujoco/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envname = 'Walker2d-v4'\n",
    "env = gym.make(envname, render_mode=\"rgb_array_list\")\n",
    "recorder = GymRecorder(env)\n",
    "\n",
    "returns = random_interaction(env, 10, recorder)\n",
    "\n",
    "video = \"videos/\" + envname + \".gif\"\n",
    "recorder.make_video(video)\n",
    "ipython_display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atari\n",
    "\n",
    "As of November 2022, Atari games do not work yet with gymnasium. We have to install the original `gym` package temporarily (the version should at least be 0.25):\n",
    "\n",
    "```bash\n",
    "pip install gym\n",
    "```\n",
    "\n",
    "The atari games are available as binary ROM files, which have to be downloaded separately. The AutoROM package can do that for you:  <https://github.com/Farama-Foundation/AutoROM>\n",
    "\n",
    "```bash\n",
    "pip install autorom\n",
    "AutoROM --accept-license\n",
    "```\n",
    "\n",
    "You can then install the atari submodules of gym (in particular ale_py):\n",
    "\n",
    "```bash\n",
    "pip install gym[atari]\n",
    "```\n",
    "\n",
    "The trick is now to import gym under another name, as we renamed gymnasium as gym in the earlier cells. The environments should be closed at the end, otherwise the window will stay open. \n",
    "\n",
    "Chek out the list of Atari games here:  <https://gymnasium.farama.org/environments/atari/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym as gym_original\n",
    "\n",
    "env = gym_original.make('ALE/Breakout-v5', render_mode='human')\n",
    "\n",
    "returns = random_interaction(env, 1, None)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "7-Gym-solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('deeprl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "932956c8e5d2f79d68ff59e849758b6e4ddbf01f7f22c7d8bb3532c38341d908"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
