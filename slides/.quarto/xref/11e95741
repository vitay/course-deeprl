{"headings":["on-policy-and-off-policy-methods","where-is-the-problem-with-on-policy-methods","is-gradient-ascent-the-best-optimization-method","trust-regions-and-gradients","policy-collapse","trust-regions-and-gradients-1","trust-regions-and-gradients-2","trpo-trust-region-policy-optimization","trpo-trust-region-policy-optimization-1","trpo-trust-region-policy-optimization-2","trpo-trust-region-policy-optimization-3","trpo-trust-region-policy-optimization-4","trpo-trust-region-policy-optimization-5","trpo-trust-region-policy-optimization-6","trpo-trust-region-policy-optimization-7","trpo-trust-region-policy-optimization-8","trpo-trust-region-policy-optimization-9","ppo-proximal-policy-optimization","ppo-proximal-policy-optimization-1","ppo-proximal-policy-optimization-2","ppo-proximal-policy-optimization-3","ppo-proximal-policy-optimization-4","ppo-proximal-policy-optimization-5","ppo-proximal-policy-optimization-6","ppo-proximal-policy-optimization-7","ppo-proximal-policy-optimization-8","ppo-mujoco-control","ppo-parkour","ppo-robotics","ppo-dexterity-learning","openai-five-dota-2","why-is-dota-2-hard","openai-five-dota-2-1","openai-five-dota-2-2","openai-five-dota-2-3","openai-five-dota-2-4","openai-five-dota-2-5","acer-actor-critic-with-experience-replay","acer-actor-critic-with-experience-replay-1"],"entries":[]}