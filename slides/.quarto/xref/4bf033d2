{"entries":[],"headings":["temporal-difference-learning","temporal-difference-td-learning","temporal-difference-td-learning-1","td-policy-evaluation-td0","bias-variance-trade-off","exploration-exploitation-problem","sarsa-on-policy-td-control","sarsa-on-policy-td-control-1","q-learning-off-policy-td-control","q-learning-off-policy-td-control-1","no-need-for-importance-sampling-in-q-learning","q-learning-gridworld-example","temporal-difference-learning-1","actor-critic-methods","actor-critic-methods-1","actor-critic-methods-2","actor-critic-methods-3","actor-critic-algorithm-with-preferences","actor-critic-methods-4","actor-critic-methods-5","eligibility-traces-and-advantage-estimation","bias-variance-trade-off-1","drawback-of-learning-from-single-transitions","n-step-advantage","n-step-advantage-1","eligibility-traces-forward-view","eligibility-traces-forward-view-1","eligibility-traces-backward-view","tdlambda-algorithm-policy-evaluation","eligibility-traces","generalized-advantage-estimation-gae","generalized-advantage-estimation-gae-1","generalized-advantage-estimation-gae-2"]}