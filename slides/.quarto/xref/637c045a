{"entries":[],"headings":["value-based-deep-rl","function-approximators-to-learn-the-q-values","first-naive-approach-q-learning-with-function-approximation","dnn-need-stochastic-gradient-descent","second-naive-approach-q-learning-with-a-transition-buffer","correlated-inputs","correlated-inputs-1","non-stationarity","non-stationarity-1","illustration-of-non-stationary-targets","illustration-of-non-stationary-targets-1","illustration-of-non-stationary-targets-2","illustration-of-non-stationary-targets-3","deep-q-networks-dqn","problem-with-non-linear-approximators-and-rl","experience-replay-memory","experience-replay-memory-1","experience-replay-memory-2","experience-replay-memory-3","target-network","target-network-1","dqn-deep-q-network","dqn-deep-q-network-1","why-no-max-pooling","are-individual-frames-good-representations-of-states","markov-property-in-video-games","dqn-code-in-keras","dqn-code-in-keras-1","dqn-code-in-keras-2","dqn-training","dqn-to-solve-multiple-atari-games","dqn-to-solve-multiple-atari-games-1","dqn-to-solve-multiple-atari-games-2","double-dqn","double-dqn-1","double-dqn-2","prioritized-experience-replay","prioritized-experience-replay-1","prioritized-experience-replay-2","prioritized-experience-replay-3","prioritized-experience-replay-4","prioritized-experience-replay-5","prioritized-experience-replay-6","prioritized-experience-replay-7","dueling-networks","dueling-networks-1","dueling-networks-2","advantage-functions","dueling-networks-3","unidentifiability","visualization-of-the-value-and-advantage-functions","improvement-over-prioritized-ddqn","summary-of-dqn"]}