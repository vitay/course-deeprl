<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-86daaaaad7353f9cc0c554efc1dd6d94.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.40">

  <meta name="author" content="Julien Vitay">
  <title>Deep Reinforcement Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-31e5e70a95169d1719a5a4fd7b5514a9.css">
  <script defer="" src="../assets/katex/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet" href="../assets/katex/katex.min.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="img/tuc-new-large.png" data-background-opacity="1" data-background-position="top" data-background-size="30%" class="quarto-title-block center">
  <h1 class="title">Deep Reinforcement Learning</h1>
  <p class="subtitle">Policy gradient</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Julien Vitay 
</div>
        <p class="quarto-title-affiliation">
            Professur für Künstliche Intelligenz - Fakultät für Informatik
          </p>
    </div>
</div>

</section>
<section id="policy-search" class="title-slide slide level1 center">
<h1>1 - Policy Search</h1>

</section>

<section id="policy-search-1" class="title-slide slide level1 center">
<h1>Policy search</h1>
<div class="columns">
<div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/valuebased-agent.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/dueling-principle.svg" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
<ul>
<li><p>Learning directly the Q-values in value-based methods (DQN) suffers from many problems:</p>
<ul>
<li><p>The Q-values are <strong>unbounded</strong>: they can take any value (positive or negative), so the output layer must be linear.</p></li>
<li><p>The Q-values have a <strong>high variability</strong>: some <span class="math inline">(s,a)</span> pairs have very negative values, others have very positive values. Difficult to learn for a NN.</p></li>
<li><p>Works only for small <strong>discrete action spaces</strong>: need to iterate over all actions to find the greedy action.</p></li>
</ul></li>
</ul>
</section>

<section id="policy-search-2" class="title-slide slide level1 center">
<h1>Policy search</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/conv_agent.png" class="quarto-figure quarto-figure-center" style="width:70.0%"></p>
</figure>
</div>
<ul>
<li><p>Instead of learning the Q-values, one could approximate directly the policy <span class="math inline">\pi_\theta(s, a)</span> with a neural network.</p></li>
<li><p><span class="math inline">\pi_\theta(s, a)</span> is called a <strong>parameterized policy</strong>: it depends directly on the parameters <span class="math inline">\theta</span> of the NN.</p></li>
<li><p>For discrete action spaces, the output of the NN can be a <strong>softmax</strong> layer, directly giving the probability of selecting an action.</p></li>
<li><p>For continuous action spaces, the output layer can directly control the effector (joint angles).</p></li>
</ul>
</section>

<section id="policy-search-3" class="title-slide slide level1 center">
<h1>Policy search</h1>
<ul>
<li>Parameterized policies can represent continuous policies and avoid the curse of dimensionality.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/dqn-pg.png" style="width:80.0%"></p>
<figcaption>Source: <a href="https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/" class="uri">https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/</a></figcaption>
</figure>
</div>
</section>

<section id="policy-search-4" class="title-slide slide level1 center">
<h1>Policy search</h1>
<div class="columns">
<div class="column">
<ul>
<li><strong>Policy search</strong> methods aim at maximizing directly the expected return over all possible trajectories (episodes) <span class="math inline">\tau = (s_0, a_0, \dots, s_T, a_T)</span></li>
</ul>
<p><span class="math display">
    \mathcal{J}(\theta) = \mathbb{E}_{\tau \sim \rho_\theta}[R(\tau)] = \int_{\tau} \rho_\theta(\tau) \; R(\tau) \; d\tau
</span></p>
<ul>
<li>All trajectories <span class="math inline">\tau</span> selected by the policy <span class="math inline">\pi_\theta</span> should be associated with a high expected return <span class="math inline">R(\tau)</span> in order to maximize this objective function.</li>
</ul>
</div><div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/policysearch.svg" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
<ul>
<li><p><span class="math inline">\rho_\theta(\tau)</span> is the <strong>likelihood</strong> of the trajectory <span class="math inline">\tau</span> under the policy <span class="math inline">\pi_\theta</span>.</p></li>
<li><p>This means that the optimal policy should only select actions that maximizes the expected return: exactly what we want.</p></li>
</ul>
</section>

<section id="policy-search-5" class="title-slide slide level1 center">
<h1>Policy search</h1>
<ul>
<li>Objective function to be maximized:</li>
</ul>
<p><span class="math display">
    \mathcal{J}(\theta) = \mathbb{E}_{\tau \sim \rho_\theta}[R(\tau)] = \int_{\tau} \rho_\theta(\tau) \; R(\tau) \; d\tau
</span></p>
<ul>
<li>The objective function is however not <strong>model-free</strong>, as the likelihood of a trajectory does depend on the environments dynamics:</li>
</ul>
<p><span class="math display">
    \rho_\theta(\tau) = p_\theta(s_0, a_0, \ldots, s_T) = p_0 (s_0) \, \prod_{t=0}^T \pi_\theta(s_t, a_t) \, p(s_{t+1} | s_t, a_t)
</span></p>
<ul>
<li><p>The objective function is furthermore <strong>not computable</strong>:</p>
<ul>
<li><p>An <strong>infinity</strong> of possible trajectories to integrate if the action space is continuous.</p></li>
<li><p>Even if we sample trajectories, we would need a huge number of them to correctly estimate the objective function (<strong>sample complexity</strong>) because of the huge <strong>variance</strong> of the returns.</p></li>
</ul></li>
</ul>
<p><span class="math display">
    \mathcal{J}(\theta) = \mathbb{E}_{\tau \sim \rho_\theta}[R(\tau)] \approx \frac{1}{M} \, \sum_{i=1}^M R(\tau_i)
</span></p>
</section>

<section id="policy-gradient" class="title-slide slide level1 center">
<h1>Policy gradient</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>All we need to find is a computable gradient <span class="math inline">\nabla_\theta \mathcal{J}(\theta)</span> to apply gradient ascent and backpropagation.</li>
</ul>
<p><span class="math display">
    \Delta \theta = \eta \, \nabla_\theta \mathcal{J}(\theta)
</span></p>
<ul>
<li><strong>Policy Gradient</strong> (PG) methods only try to estimate this gradient, but do not care about the objective function itself…</li>
</ul>
<p><span class="math display">
    g = \nabla_\theta \mathcal{J}(\theta)
</span></p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/pg-idea.png"></p>
<figcaption>Source: <a href="https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/" class="uri">https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/</a></figcaption>
</figure>
</div>
</div></div>
<ul>
<li>In particular, any function <span class="math inline">\mathcal{J}'(\theta)</span> whose gradient is locally the same (or has the same direction) will do:</li>
</ul>
<p><span class="math display">\mathcal{J}'(\theta) = \alpha \, \mathcal{J}(\theta) + \beta \; \Rightarrow \; \nabla_\theta \mathcal{J}'(\theta) \propto \nabla_\theta \mathcal{J}(\theta)  \; \Rightarrow \; \Delta \theta = \eta \, \nabla_\theta \mathcal{J}'(\theta)</span></p>
<ul>
<li><p>This is called <strong>surrogate optimization</strong>: we actually want to maximize <span class="math inline">\mathcal{J}(\theta)</span> but we cannot compute it.</p></li>
<li><p>We instead create a surrogate objective <span class="math inline">\mathcal{J}'(\theta)</span> which is locally the same as <span class="math inline">\mathcal{J}(\theta)</span> and tractable.</p></li>
</ul>
</section>

<section id="reinforce" class="title-slide slide level1 center">
<h1>2 - REINFORCE</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/paper-reinforce.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
<div class="footer">
<p><span class="citation" data-cites="Williams1992">Williams (<a href="#/references" role="doc-biblioref" onclick="">1992</a>)</span> Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8, 229–256.</p>
</div>
</section>

<section id="reinforce-1" class="title-slide slide level1 center">
<h1>REINFORCE</h1>
<ul>
<li>The <strong>REINFORCE</strong> algorithm <span class="citation" data-cites="Williams1992">(<a href="#/references" role="doc-biblioref" onclick="">Williams, 1992</a>)</span> proposes an unbiased estimate of the policy gradient:</li>
</ul>
<p><span class="math display">
    \nabla_\theta \, \mathcal{J}(\theta) = \nabla_\theta \, \int_\tau \rho_\theta (\tau) \, R(\tau) \, d\tau =  \int_\tau (\nabla_\theta \, \rho_\theta (\tau)) \, R(\tau) \, d\tau
</span></p>
<p>by noting that the return of a trajectory does not depend on the weights <span class="math inline">\theta</span> (the agent only controls its actions, not the environment).</p>
<ul>
<li>We now use the <strong>log-trick</strong>, a simple identity based on the fact that:</li>
</ul>
<p><span class="math display">
    \frac{d \log f(x)}{dx} = \frac{f'(x)}{f(x)}
</span></p>
<p>or:</p>
<p><span class="math display">
    f'(x) = f(x) \times \frac{d \log f(x)}{dx}
</span></p>
<p>to rewrite the gradient of the likelihood of a single trajectory:</p>
<p><span class="math display">
    \nabla_\theta \, \rho_\theta (\tau) = \rho_\theta (\tau) \times \nabla_\theta \log \rho_\theta (\tau)
</span></p>
</section>

<section id="reinforce-2" class="title-slide slide level1 center">
<h1>REINFORCE</h1>
<ul>
<li>The policy gradient becomes:</li>
</ul>
<p><span class="math display">
    \nabla_\theta \, \mathcal{J}(\theta) = \int_\tau (\nabla_\theta \, \rho_\theta (\tau)) \, R(\tau) \, d\tau =  \int_\tau \rho_\theta (\tau) \, \nabla_\theta \log \rho_\theta (\tau) \, R(\tau) \, d\tau
</span></p>
<p>which now has the form of a mathematical expectation:</p>
<p><span class="math display">
    \nabla_\theta \, \mathcal{J}(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[ \nabla_\theta \log \rho_\theta (\tau) \, R(\tau) ]
</span></p>
<ul>
<li>The policy gradient is, in expectation, the gradient of the <strong>log-likelihood</strong> of a trajectory multiplied by its return.</li>
</ul>
</section>

<section id="reinforce-3" class="title-slide slide level1 center">
<h1>REINFORCE</h1>
<ul>
<li>The advantage of REINFORCE is that it is <strong>model-free</strong>:</li>
</ul>
<p><span class="math display">
    \rho_\theta(\tau) = p_\theta(s_0, a_0, \ldots, s_T) = p_0 (s_0) \, \prod_{t=0}^T \pi_\theta(s_t, a_t) p(s_{t+1} | s_t, a_t)
</span></p>
<p><span class="math display">
    \log \rho_\theta(\tau) = \log p_0 (s_0) + \sum_{t=0}^T \log \pi_\theta(s_t, a_t) + \sum_{t=0}^T \log p(s_{t+1} | s_t, a_t)
</span></p>
<p><span class="math display">
    \nabla_\theta \log \rho_\theta(\tau) = \sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t)
</span></p>
<ul>
<li><p>The transition dynamics <span class="math inline">p(s_{t+1} | s_t, a_t)</span> disappear from the gradient.</p></li>
<li><p>The <strong>Policy Gradient</strong> does not depend on the dynamics of the environment:</p></li>
</ul>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, R(\tau) ]
</span></p>
</section>

<section id="reinforce-algorithm" class="title-slide slide level1 center">
<h1>REINFORCE algorithm</h1>
<p>The REINFORCE algorithm is a policy-based variant of Monte Carlo control:</p>
<ul>
<li><p><strong>while</strong> not converged:</p>
<ul>
<li><p>Sample <span class="math inline">M</span> trajectories <span class="math inline">\{\tau_i\}</span> using the current policy <span class="math inline">\pi_\theta</span> and observe the returns <span class="math inline">\{R(\tau_i)\}</span>.</p></li>
<li><p>Estimate the policy gradient as an average over the trajectories:</p></li>
</ul>
<p><span class="math display">
     \nabla_\theta \mathcal{J}(\theta) \approx \frac{1}{M} \sum_{i=1}^M \sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, R(\tau_i)
  </span></p>
<ul>
<li>Update the policy using gradient ascent:</li>
</ul>
<p><span class="math display">
      \theta \leftarrow \theta + \eta \, \nabla_\theta \mathcal{J}(\theta)
  </span></p></li>
</ul>
<div class="footer">
<p><span class="citation" data-cites="Williams1992">Williams (<a href="#/references" role="doc-biblioref" onclick="">1992</a>)</span> Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8, 229–256.</p>
</div>
</section>

<section id="reinforce-4" class="title-slide slide level1 center">
<h1>REINFORCE</h1>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, R(\tau) ]
</span></p>
<p><strong>Advantages</strong></p>
<ul>
<li><p>The policy gradient is <strong>model-free</strong>.</p></li>
<li><p>Works with <strong>partially observable</strong> problems (POMDP): as the return is computed over complete trajectories, it does not matter whether the states are Markov or not.</p></li>
</ul>
<p><strong>Inconvenients</strong></p>
<ul>
<li><p>Only for <strong>episodic tasks</strong>.</p></li>
<li><p>The gradient has a <strong>high variance</strong>: returns may change a lot during learning.</p></li>
<li><p>It has therefore a high <strong>sample complexity</strong>: we need to sample many episodes to correctly estimate the policy gradient.</p></li>
<li><p>Strictly <strong>on-policy</strong>: trajectories must be frequently sampled and immediately used to update the policy.</p></li>
</ul>
</section>

<section id="reinforce-with-baseline" class="title-slide slide level1 center">
<h1>REINFORCE with baseline</h1>
<ul>
<li>To reduce the variance of the estimated gradient, a baseline is often subtracted from the return:</li>
</ul>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, (R(\tau) - b) ]
</span></p>
<ul>
<li>As long as the baseline <span class="math inline">b</span> is independent from <span class="math inline">\theta</span>, it does not introduce a bias:</li>
</ul>
<p><span class="math display">
\begin{aligned}
    \mathbb{E}_{\tau \sim \rho_\theta}[\nabla_\theta \log \rho_\theta (\tau) \, b ] &amp; = \int_\tau \rho_\theta (\tau) \nabla_\theta \log \rho_\theta (\tau) \, b \, d\tau \\
    &amp; = \int_\tau \nabla_\theta  \rho_\theta (\tau) \, b \, d\tau \\
    &amp;= b \, \nabla_\theta \int_\tau \rho_\theta (\tau) \, d\tau \\
    &amp;=  b \, \nabla_\theta 1 \\
    &amp;= 0
\end{aligned}
</span></p>
<!--
# REINFORCE with baseline

::: {.columns}
::: {.column width=50%}


* A simple baseline that reduces the variance of the returns is a **moving average** of the returns obtained during all episodes:

$$b = \alpha \, R(\tau) + (1 - \alpha) \, b$$

* This is similar to **reinforcement comparison** for bandits, except we compute the mean return instead of the mean reward.

* A trajectory $\tau$ should be **reinforced** if it brings more return than average.

:::
::: {.column width=50%}


![](img/bandit-nonstationary2.png)

:::
:::


* (Williams, 1992) showed that the best baseline (the one that reduces the variance the most) is actually:

$$
    b = \frac{\mathbb{E}_{\tau \sim \rho_\theta}[(\nabla_\theta \log \rho_\theta (\tau))^2 \, R(\tau)]}{\mathbb{E}_{\tau \sim \rho_\theta}[(\nabla_\theta \log \rho_\theta (\tau))^2]}
$$

but it is complex to compute in practice.

::: footer
Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8, 229–256.
:::
-->
</section>

<section id="reinforce-with-baseline-1" class="title-slide slide level1 center">
<h1>REINFORCE with baseline</h1>
<ul>
<li>In practice, a baseline that works well is the value of the encountered states:</li>
</ul>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, (R(\tau) - V^\pi(s_t)) ]
</span></p>
<ul>
<li><span class="math inline">R(\tau) - V^\pi(s_t)</span> becomes the <strong>advantage</strong> of the action <span class="math inline">a_t</span> in <span class="math inline">s_t</span>: how much return does it provide compared to what can be expected in <span class="math inline">s_t</span> generally:</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/dueling-principle.svg" class="quarto-figure quarto-figure-center" style="width:65.0%"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/dueling-principle2.svg" class="quarto-figure quarto-figure-center" style="width:65.0%"></p>
</figure>
</div>
</div></div>
<ul>
<li><p>As in <strong>dueling networks</strong>, it reduces the variance of the returns.</p></li>
<li><p>Problem: the value of each state has to be learned separately (see actor-critic architectures).</p></li>
</ul>
</section>

<section id="application-of-reinforce-to-resource-management" class="title-slide slide level1 center">
<h1>Application of REINFORCE to resource management</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/reinforce-cluster.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/reinforce-cluster2.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>REINFORCE with baseline can be used to allocate resources (CPU cores, memory, etc) when scheduling jobs on a cloud of compute servers.</p></li>
<li><p>The policy is approximated by a shallow NN (one hidden layer with 20 neurons).</p></li>
<li><p>The state space is the current occupancy of the cluster as well as the job waiting list.</p></li>
<li><p>The action space is sending a job to a particular resource.</p></li>
<li><p>The reward is the negative <strong>job slowdown</strong>: how much longer the job needs to complete compared to the optimal case.</p></li>
<li><p>DeepRM outperforms all alternative job schedulers.</p></li>
</ul>
</div></div>
<div class="footer">
<p><span class="citation" data-cites="Mao2016">Mao et al. (<a href="#/references" role="doc-biblioref" onclick="">2016</a>)</span> Resource Management with Deep Reinforcement Learning. HotNets ’16 doi:10.1145/3005745.3005750.</p>
</div>
</section>

<section id="policy-gradient-theorem" class="title-slide slide level1 center">
<h1>3 - Policy Gradient Theorem</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/paper-pg.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
<div class="footer">
<p><span class="citation" data-cites="Sutton1999">Sutton et al. (<a href="#/references" role="doc-biblioref" onclick="">1999</a>)</span> Policy gradient methods for reinforcement learning with function approximation. NIPS.</p>
</div>
</section>

<section id="policy-gradient-1" class="title-slide slide level1 center">
<h1>Policy Gradient</h1>
<ul>
<li>The REINFORCE gradient estimate is the following:</li>
</ul>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, R(\tau) ] =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T (\nabla_\theta \log \pi_\theta(s_t, a_t)) \, (\sum_{t'=0}^T \gamma^{t'} \, r_{t'+1}) ]
</span></p>
<ul>
<li>For each state-action pair <span class="math inline">(s_t, a_t)</span> encountered during the episode, the gradient of the log-likelihood of the policy is multiplied by the complete return of the episode:</li>
</ul>
<div class="columns">
<div class="column">
<p><span class="math display">R(\tau) = \sum_{t'=0}^T \gamma^{t'} \, r_{t'+1}</span></p>
</div><div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/rl-sequence.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
<ul>
<li><p>The <strong>causality principle</strong> states that rewards obtained before time <span class="math inline">t</span> are not caused by that action.</p></li>
<li><p>The policy gradient can be rewritten as:</p></li>
</ul>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, (\sum_{t'=t}^T \gamma^{t' - t} \, r_{t'+1}) ] =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, R_t ]
</span></p>
<div class="footer">
<p><span class="citation" data-cites="Sutton1999">Sutton et al. (<a href="#/references" role="doc-biblioref" onclick="">1999</a>)</span> Policy gradient methods for reinforcement learning with function approximation. NIPS.</p>
</div>
</section>

<section id="policy-gradient-2" class="title-slide slide level1 center">
<h1>Policy Gradient</h1>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>The return at time <span class="math inline">t</span> (<strong>reward-to-go</strong>) multiplies the gradient of the log-likelihood of the policy (the <strong>score</strong>) for each transition in the episode:</li>
</ul>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, R_t ]
</span></p>
<ul>
<li>As we have:</li>
</ul>
<p><span class="math display">Q^\pi(s, a) = \mathbb{E}_\pi [R_t | s_t =s; a_t =a]</span></p>
<p>we can replace <span class="math inline">R_t</span> with <span class="math inline">Q^{\pi_\theta}(s_t, a_t)</span> without introducing any bias:</p>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, Q^{\pi_\theta}(s_t, a_t) ]
</span></p>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/rewardtogo.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
<ul>
<li>This is true on average (no bias if the Q-value estimates are correct) and has a much lower variance!</li>
</ul>
<p><br></p>
<div class="footer">
<p><span class="citation" data-cites="Sutton1999">Sutton et al. (<a href="#/references" role="doc-biblioref" onclick="">1999</a>)</span> Policy gradient methods for reinforcement learning with function approximation. NIPS.</p>
</div>
</section>

<section id="policy-gradient-3" class="title-slide slide level1 center">
<h1>Policy Gradient</h1>
<ul>
<li>The policy gradient is defined over complete trajectories:</li>
</ul>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, Q^{\pi_\theta}(s_t, a_t) ]
</span></p>
<ul>
<li><p>However, <span class="math inline">\nabla_\theta \log \pi_\theta(s_t, a_t) \, Q^{\pi_\theta}(s_t, a_t)</span> now only depends on <span class="math inline">(s_t, a_t)</span>, not the future nor the past.</p></li>
<li><p>Each step of the episode is now independent from each other (if we have the Markov property).</p></li>
<li><p>We can then <strong>sample single transitions</strong> instead of complete episodes:</p></li>
</ul>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) \propto  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q^{\pi_\theta}(s, a) ]
</span></p>
<ul>
<li>Note that this is not directly the gradient of <span class="math inline">\mathcal{J}(\theta)</span>, as the value of <span class="math inline">\mathcal{J}(\theta)</span> changes (computed over single transitions instead of complete episodes, so it is smaller), but the gradients both go in the same direction!</li>
</ul>
<div class="footer">
<p><span class="citation" data-cites="Sutton1999">Sutton et al. (<a href="#/references" role="doc-biblioref" onclick="">1999</a>)</span> Policy gradient methods for reinforcement learning with function approximation. NIPS.</p>
</div>
</section>

<section id="policy-gradient-theorem-1" class="title-slide slide level1 center">
<h1>Policy Gradient Theorem</h1>
<blockquote>
<p>For any MDP, the policy gradient is:</p>
</blockquote>
<p><span class="math display">
    g = \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q^{\pi_\theta}(s, a) ]
</span></p>
<p><br></p>
<div class="footer">
<p><span class="citation" data-cites="Sutton1999">Sutton et al. (<a href="#/references" role="doc-biblioref" onclick="">1999</a>)</span> Policy gradient methods for reinforcement learning with function approximation. NIPS.</p>
</div>
</section>

<section id="policy-gradient-theorem-with-function-approximation" class="title-slide slide level1 center">
<h1>Policy Gradient Theorem with function approximation</h1>
<ul>
<li>Better yet, (Sutton et al.&nbsp;1999) showed that we can replace the true Q-value <span class="math inline">Q^{\pi_\theta}(s, a)</span> by an estimate <span class="math inline">Q_\varphi(s, a)</span> as long as this one is unbiased:</li>
</ul>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q_\varphi(s, a) ]
</span></p>
<ul>
<li>We only need to have:</li>
</ul>
<p><span class="math display">
    Q_\varphi(s, a) \approx Q^{\pi_\theta}(s, a) \; \forall s, a
</span></p>
<ul>
<li>The approximated Q-values can for example minimize the <strong>mean square error</strong> with the true Q-values:</li>
</ul>
<p><span class="math display">
    \mathcal{L}(\varphi) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[(Q^{\pi_\theta}(s, a) - Q_\varphi(s, a))^2]
</span></p>
<ul>
<li><p>We obtain an <strong>actor-critic</strong> architecture:</p>
<ul>
<li><p>the <strong>actor</strong> <span class="math inline">\pi_\theta(s, a)</span> implements the policy and selects an action <span class="math inline">a</span> in a state <span class="math inline">s</span>.</p></li>
<li><p>the <strong>critic</strong> <span class="math inline">Q_\varphi(s, a)</span> estimates the value of that action and drives learning in the actor.</p></li>
</ul></li>
</ul>
<div class="footer">
<p><span class="citation" data-cites="Sutton1999">Sutton et al. (<a href="#/references" role="doc-biblioref" onclick="">1999</a>)</span> Policy gradient methods for reinforcement learning with function approximation. NIPS.</p>
</div>
</section>

<section id="policy-gradient-actor-critic" class="title-slide slide level1 center">
<h1>Policy Gradient : Actor-critic</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/policygradient.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</section>

<section id="policy-gradient-actor-critic-1" class="title-slide slide level1 center">
<h1>Policy Gradient : Actor-critic</h1>
<ul>
<li><p>But how to train the critic? We do not know <span class="math inline">Q^{\pi_\theta}(s, a)</span>.</p></li>
<li><p>As always, we can estimate it through <strong>sampling</strong>:</p>
<ul>
<li><strong>Monte Carlo</strong> critic: sampling the complete episode.</li>
</ul>
<p><span class="math display">
      \mathcal{L}(\varphi) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[(R(s, a) - Q_\varphi(s, a))^2]
  </span></p>
<ul>
<li><strong>SARSA</strong> critic: sampling <span class="math inline">(s, a, r, s', a')</span> transitions.</li>
</ul>
<p><span class="math display">
      \mathcal{L}(\varphi) =  \mathbb{E}_{s, s' \sim \rho_\theta, a, a' \sim \pi_\theta}[(r + \gamma \, Q_\varphi(s', a') - Q_\varphi(s, a))^2]
  </span></p>
<ul>
<li><strong>Q-learning</strong> critic: sampling <span class="math inline">(s, a, r, s')</span> transitions.</li>
</ul>
<p><span class="math display">
      \mathcal{L}(\varphi) =  \mathbb{E}_{s, s' \sim \rho_\theta, a \sim \pi_\theta}[(r + \gamma \, \max_{a'} Q_\varphi(s', a') - Q_\varphi(s, a))^2]
  </span></p></li>
</ul>
</section>

<section id="policy-gradient-reducing-the-variance" class="title-slide slide level1 center">
<h1>Policy Gradient : reducing the variance</h1>
<ul>
<li><p>As with REINFORCE, the PG actor suffers from the <strong>high variance</strong> of the Q-values.</p></li>
<li><p>It is possible to use a <strong>baseline</strong> in the PG without introducing a bias:</p></li>
</ul>
<p><span class="math display">
    \nabla_\theta \mathcal{J}(\theta) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, (Q^{\pi_\theta}(s, a) -b)]
</span></p>
<ul>
<li>In particular, the <strong>advantage actor-critic</strong> uses the value of a state as the baseline:</li>
</ul>
<p><span class="math display">
\begin{aligned}
    \nabla_\theta \mathcal{J}(\theta) &amp;=  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, (Q^{\pi_\theta}(s, a) - V^{\pi_\theta}(s))] \\
    &amp;\\
    &amp;=  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, A^{\pi_\theta}(s, a)] \\
\end{aligned}
</span></p>
<ul>
<li><p>The critic can either:</p>
<ul>
<li><p>learn to approximate both <span class="math inline">Q^{\pi_\theta}(s, a)</span> and <span class="math inline">V^{\pi_\theta}(s)</span> with two different NN (SAC).</p></li>
<li><p>replace one of them with a sampling estimate (A3C, DDPG)</p></li>
<li><p>learn the advantage <span class="math inline">A^{\pi_\theta}(s, a)</span> directly (GAE, PPO)</p></li>
</ul></li>
</ul>
</section>

<section id="many-variants-of-the-policy-gradient" class="title-slide slide level1 center">
<h1>Many variants of the Policy Gradient</h1>
<ul>
<li><strong>Policy Gradient methods</strong> can take many forms :</li>
</ul>
<p><span class="math display">
    \nabla_\theta J(\theta) =  \mathbb{E}_{s_t \sim \rho_\theta, a_t \sim \pi_\theta}[\nabla_\theta \log \pi_\theta (s_t, a_t) \, \psi_t ]
</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\psi_t = R_t</span> is the <em>REINFORCE</em> algorithm (MC sampling).</p></li>
<li><p><span class="math inline">\psi_t = R_t - b</span> is the <em>REINFORCE with baseline</em> algorithm.</p></li>
<li><p><span class="math inline">\psi_t = Q^\pi(s_t, a_t)</span> is the <em>policy gradient theorem</em>.</p></li>
<li><p><span class="math inline">\psi_t = A^\pi(s_t, a_t) = Q^\pi(s_t, a_t) - V^\pi(s_t)</span> is the <em>advantage actor-critic</em>.</p></li>
<li><p><span class="math inline">\psi_t = r_{t+1} + \gamma \, V^\pi(s_{t+1}) - V^\pi(s_t)</span> is the <em>TD actor-critic</em>.</p></li>
<li><p><span class="math inline">\psi_t = \displaystyle\sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \, V^\pi(s_{t+n}) - V^\pi(s_t)</span> is the <em>n-step advantage</em>.</p></li>
</ul>
<p>and many others…</p>
</section>

<section id="bias-and-variance-of-policy-gradient-methods" class="title-slide slide level1 center">
<h1>Bias and variance of Policy Gradient methods</h1>
<ul>
<li>The different variants of PG deal with the bias/variance trade-off.</li>
</ul>
<p><span class="math display">
    \nabla_\theta J(\theta) =  \mathbb{E}_{s_t \sim \rho_\theta, a_t \sim \pi_\theta}[\nabla_\theta \log \pi_\theta (s_t, a_t) \, \psi_t ]
</span></p>
<div class="columns">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/biasvariance.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:60%;">
<ol type="1">
<li><p>the more <span class="math inline">\psi_t</span> relies on <strong>sampled rewards</strong> (e.g.&nbsp;<span class="math inline">R_t</span>), the more the gradient will be correct on average (small bias), but the more it will vary (high variance).</p>
<ul>
<li>This increases the sample complexity: we need to average more samples to correctly estimate the gradient.</li>
</ul></li>
<li><p>the more <span class="math inline">\psi_t</span> relies on <strong>estimations</strong> (e.g.&nbsp;the TD error), the more stable the gradient (small variance), but the more incorrect it is (high bias).</p>
<ul>
<li>This can lead to suboptimal policies, i.e.&nbsp;local optima of the objective function.</li>
</ul></li>
</ol>
</div></div>
<ul>
<li>All the methods we will see in the rest of the course are attempts at finding the best trade-off.</li>
</ul>
</section>

<section id="generalized-advantage-estimation" class="title-slide slide level1 center">
<h1>4 - Generalized advantage estimation</h1>

</section>

<section id="generalized-advantage-estimation-gae" class="title-slide slide level1 center">
<h1>Generalized advantage estimation (GAE)</h1>
<ul>
<li>The <strong>n-step advantage</strong> at time <span class="math inline">t</span>:</li>
</ul>
<p><span class="math display">
A^n_t = \sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \,  V(s_{t+n}) - V (s_t)
</span></p>
<p>can be written as function of the TD error of the next <span class="math inline">n</span> transitions:</p>
<p><span class="math display">
    A^{n}_t = \sum_{l=0}^{n-1} \gamma^l \, \delta_{t+l}
</span></p>
<div class="callout callout-note no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Proof with <span class="math inline">n=2</span>:</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\begin{aligned}
A^2_t &amp;= r_{t+1} + \gamma \, r_{t+2} + \gamma^2 \, V(s_{t+2}) - V(s_{t}) \\
&amp;\\
&amp;= (r_{t+1} - V(s_t)) + \gamma \, (r_{t+2} + \gamma \, V(s_{t+2}) ) \\
&amp;\\
&amp;= (r_{t+1} + \gamma \, V(s_{t+1}) - V(s_t)) + \gamma \, (r_{t+2} + \gamma \, V(s_{t+2}) - V(s_{t+1})) \\
&amp;\\
&amp;= \delta_t + \gamma \, \delta_{t+1}
\end{aligned}
</span></p>
</div>
</div>
</div>
<div class="footer">
<p><span class="citation" data-cites="Schulman2015">Schulman et al. (<a href="#/references" role="doc-biblioref" onclick="">2015</a>)</span> High-Dimensional Continuous Control Using Generalized Advantage Estimation. arXiv:1506.02438.</p>
</div>
</section>

<section id="generalized-advantage-estimation-gae-1" class="title-slide slide level1 center">
<h1>Generalized advantage estimation (GAE)</h1>
<div class="columns">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/nstep.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li>The <strong>n-step advantage</strong> realizes a bias/variance trade-off, but which value of <span class="math inline">n</span> should we choose?</li>
</ul>
<p><span class="math display">
A^n_t = \sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \,  V(s_{t+n}) - V (s_t)
</span></p>
<ul>
<li><span class="citation" data-cites="Schulman2015">Schulman et al. (<a href="#/references" role="doc-biblioref" onclick="">2015</a>)</span> proposed a <strong>generalized advantage estimate</strong> (GAE) <span class="math inline">A_t^{\text{GAE}(\gamma, \lambda)}</span> summing all possible n-step advantages with a discount parameter <span class="math inline">\lambda</span>:</li>
</ul>
<p><span class="math display">A_t^{\text{GAE}(\gamma, \lambda)} = (1 - \lambda) \sum_{n=1}^\infty \lambda^n \, A^n_t</span></p>
</div></div>
<ul>
<li><p>This is just a forward eligibility trace over distant n-step advantages: the 1-step advantage is more important the the 1000-step advantage (too much variance).</p></li>
<li><p>We can show that the GAE can be expressed as a function of the future 1-step TD errors: <span class="math display">A_t^{\text{GAE}(\gamma, \lambda)} = \sum_{k=0}^\infty (\gamma \, \lambda)^k \, \delta_{t+k}</span></p></li>
</ul>
<div class="footer">
<p><span class="citation" data-cites="Schulman2015">Schulman et al. (<a href="#/references" role="doc-biblioref" onclick="">2015</a>)</span> High-Dimensional Continuous Control Using Generalized Advantage Estimation. arXiv:1506.02438.</p>
</div>
</section>

<section id="generalized-advantage-estimation-gae-2" class="title-slide slide level1 center">
<h1>Generalized advantage estimation (GAE)</h1>
<ul>
<li><strong>Generalized advantage estimate</strong> (GAE) :</li>
</ul>
<p><span class="math display">A_t^{\text{GAE}(\gamma, \lambda)} = (1 - \lambda) \sum_{n=1}^\infty \lambda^n \, A^n_t  = \sum_{k=0}^\infty (\gamma \, \lambda)^k \, \delta_{t+k}</span></p>
<ul>
<li><p>The parameter <span class="math inline">\lambda</span> controls the <strong>bias-variance</strong> trade-off.</p></li>
<li><p>When <span class="math inline">\lambda=0</span>, the generalized advantage is the TD error:</p></li>
</ul>
<p><span class="math display">A_t^{\text{GAE}(\gamma, 0)} = r_{t+1} + \gamma \, V(s_{t+1}) - V(s_t)  = \delta_{t}</span></p>
<ul>
<li>When <span class="math inline">\lambda=1</span>, the generalized advantage is the MC advantage:</li>
</ul>
<p><span class="math display">A_t^{\text{GAE}(\gamma, 1)} = \sum_{k=0}^\infty \gamma^k \, r_{t+k+1} - V(s_t) = R_t - V(s_t)</span></p>
<ul>
<li><p>Any value in between controls the bias-variance trade-off: from the high bias / low variance of TD to the small bias / high variance of MC.</p></li>
<li><p>In practice, it leads to a better estimation than n-step advantages, but is more computationally expensive.</p></li>
</ul>
<div class="footer">
<p><span class="citation" data-cites="Schulman2015">Schulman et al. (<a href="#/references" role="doc-biblioref" onclick="">2015</a>)</span> High-Dimensional Continuous Control Using Generalized Advantage Estimation. arXiv:1506.02438.</p>
</div>
</section>

<section id="references" class="title-slide slide level1 smaller scrollable">
<h1>References</h1>


<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Mao2016" class="csl-entry" role="listitem">
Mao, H., Alizadeh, M., Menache, I., and Kandula, S. (2016). Resource <span>Management</span> with <span>Deep Reinforcement Learning</span>. in <em>Proceedings of the 15th <span>ACM Workshop</span> on <span>Hot Topics</span> in <span>Networks</span> - <span>HotNets</span> ’16</em> (Atlanta, GA, USA: ACM Press), 50–56. doi:<a href="https://doi.org/10.1145/3005745.3005750">10.1145/3005745.3005750</a>.
</div>
<div id="ref-Schulman2015" class="csl-entry" role="listitem">
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. (2015). High-<span>Dimensional Continuous Control Using Generalized Advantage Estimation</span>. <a href="http://arxiv.org/abs/1506.02438">http://arxiv.org/abs/1506.02438</a>.
</div>
<div id="ref-Sutton1999" class="csl-entry" role="listitem">
Sutton, R. S., McAllester, D., Singh, S., and Mansour, Y. (1999). Policy gradient methods for reinforcement learning with function approximation. in <em>Proceedings of the 12th <span>International Conference</span> on <span>Neural Information Processing Systems</span></em> (MIT Press), 1057–1063. <a href="https://dl.acm.org/citation.cfm?id=3009806">https://dl.acm.org/citation.cfm?id=3009806</a>.
</div>
<div id="ref-Williams1992" class="csl-entry" role="listitem">
Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. <em>Machine Learning</em> 8, 229–256.
</div>
</div>
</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false,"theme":"whiteboard"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp("https:\/\/vitay\.github\.io\/course-deeprl\/");
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>