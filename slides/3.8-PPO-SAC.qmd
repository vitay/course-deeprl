---
title: Deep Reinforcement Learning
subtitle: PPO - SAC

author: Julien Vitay
institute: Professur für Künstliche Intelligenz - Fakultät für Informatik

resources: pdf/3.6-PPO.pdf
---

# 1 - Trust regions

# On-policy and off-policy methods

* DQN and DDPG are **off-policy** methods, so we can use a replay memory.

    * They need less samples to converge as they re-use past experiences (**sample efficient**).

    * The critic is biased (overestimation), so learning is **unstable** and **suboptimal**.


* A3C is **on-policy**, we have to use distributed learning.

    * The critic is less biased, so it learns better policies (**optimality**).

    * It however need a lot of samples (**sample complexity**) as it must collect transitions with the current learned policy.


* All suffer from **parameter brittleness**: choosing the right hyperparameters for a task is extremely difficult.

* For example a learning rate of $10^{-5}$ might work, but not $1.1 * 10^{-5}$.

* Other hyperparameters: size of the ERM, update frequency of the target networks, training frequency.

* Can't we do better?


# Trust regions and gradients

![Source: <https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-explained-a6ee04eeeee9>](img/natural-gradient-reason1.jpeg){width=80%}

* The policy gradient tells you in **which direction** of the parameter space $\theta$ the return is increasing the most.

* If you take too big a step in that direction, the new policy might become completely bad (**policy collapse**).

* Once the policy has collapsed, the new samples will all have a small return: the previous progress is lost.

* This is especially true when the parameter space has a **high curvature**, which is the case with deep NN.


# Policy collapse

* Policy collapse is a huge problem in deep RL: the network starts learning correctly but suddenly collapses to a random agent.

* For on-policy methods, all progress is lost: the network has to relearn from scratch, as the new samples will be generated by a bad policy.

![](img/policy-collapse.png){width=50%}

::: footer
Oliver Lange (2019). Investigation of Model-Based Augmentation of Model-Free Reinforcement Learning Algorithms. MSc thesis, TU Chemnitz.
:::


# Where is the problem with on-policy methods?

* The policy gradient is **unbiased** only when the critic $Q_\varphi(s, a)$ accurately approximates the true Q-values of the **current policy**.

$$
\begin{aligned}
    \nabla_\theta J(\theta) & =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q^{\pi_\theta}(s, a)] \\
    & \approx  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q_\varphi(s, a)]
\end{aligned}
$$

* If transitions are generated by a different (older) policy $b$, the policy gradient will be wrong.

* With the PG, we can update the weights $\theta$ in the direction of that gradient:

$$
    \theta \leftarrow \theta + \eta \, \nabla_\theta J(\theta)
$$

* We search for the **smallest parameter change** (controlled by the learning rate $\eta$) that produces the **biggest positive change** in the returns.

* Choosing the learning rate $\eta$ is extremely difficult in deep RL:

    * If the learning rate is too small, the network converges very slowly, requiring a lot of samples to converge (**sample complexity**).

    * If the learning rate is too high, parameter updates can totally destroy the policy (**instability**).

* The learning rate should adapt to the current parameter values in order to stay in a **trust region**.


# Trust regions and gradients

![Source: <https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-explained-a6ee04eeeee9>](img/natural-gradient-reason2.jpeg)

* **Trust region** optimization searches in the **neighborhood** of the current parameters $\theta$ which new value would maximize the return the most.

* This is a **constrained optimization** problem: we still want to maximize the return of the policy, but by keeping the policy as close as possible from its previous value.


# Trust regions and gradients

![Source: <https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-explained-a6ee04eeeee9>](img/natural-gradient-reason3.jpeg){width=70%}


* The size of the neighborhood determines the safety of the parameter change.

* In safe regions, we can take big steps. In dangerous regions, we have to take small steps.

* **Problem:** how can we estimate the safety of a parameter change?



# 2 - TRPO: Trust Region Policy Optimization

![](img/paper-trpo.png)

::: footer
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015). Trust Region Policy Optimization. 1889–1897. <http://proceedings.mlr.press/v37/schulman15.html>.
:::

# TRPO: Trust Region Policy Optimization

* We want to maximize the expected return of a policy $\pi_\theta$, which is equivalent to maximizing the Q-value of every state-action pair visited by the policy:

$$\max_\theta \, \mathcal{J}(\theta) = \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta} [Q^{\pi_\theta}(s, a)]$$


* Let's note $\theta_\text{old}$ the current value of the parameters of the policy $\pi_{\theta_\text{old}}$.

* We search for a new policy $\pi_\theta$ with parameters $\theta$ which is always **better** than the current policy, i.e. where the Q-value of all actions is higher than with the current policy:

$$\max_\theta \, \mathcal{L}(\theta) = \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta} [Q_\theta(s, a) - Q_{\theta_\text{old}}(s, a)]$$

* The quantity

$$A^{\pi_{\theta_\text{old}}}(s, a) = Q_\theta(s, a) - Q_{\theta_\text{old}}(s, a)$$

is the **advantage** of taking the action $(s, a)$ and thereafter following $\pi_\theta$, compared to following the current policy $\pi_{\theta_\text{old}}$.


::: footer
Kakade, S., and Langford, J. (2002). Approximately Optimal Approximate Reinforcement Learning. Proc. 19th International Conference on Machine Learning, 267--274.
:::

# TRPO: Trust Region Policy Optimization

* If we can estimate the advantages and maximize them, we can find a new policy $\pi_\theta$ with a higher return than the current one.

$$\mathcal{L}(\theta) = \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta} [A^{\pi_{\theta_\text{old}}}(s, a)]$$

* By definition, $\mathcal{L}(\theta_\text{old}) = 0$, so the policy maximizing $\mathcal{L}(\theta)$ has positive advantages and is better than $\pi_{\theta_\text{old}}$.

$$\theta_\text{new} = \text{argmax}_\theta \; \mathcal{L}(\theta) \; \Rightarrow \; \mathcal{J}(\theta_\text{new}) \geq \mathcal{J}(\theta_\text{old})$$

* Maximizing the advantages ensures **monotonic improvement**: the new policy is always better than the previous one. Policy collapse is not possible!

# TRPO: Trust Region Policy Optimization

* The problem is that we have to take samples $(s, a)$ from $\pi_\theta$: we do not know it yet, as it is what we search. The only policy at our disposal to estimate the advantages is the current policy $\pi_{\theta_\text{old}}$.

* We could use **importance sampling** to sample from $\pi_{\theta_\text{old}}$, but it would introduce a lot of variance:

$$\mathcal{L}(\theta) = \mathbb{E}_{s \sim \rho_{\theta_\text{old}}, a \sim \pi_{\theta_\text{old}}} [\frac{\pi_{\theta}(s, a)}{\pi_{\theta_\text{old}}(s, a)} \, A^{\pi_{\theta_\text{old}}}(s, a)]$$


* In TRPO, we are adding a **constraint** instead: 

    * the new policy $\pi_{\theta_\text{new}}$ should not be (very) different from $\pi_{\theta_\text{old}}$.

    * the importance sampling weight $\frac{\pi_{\theta_\text{new}}(s, a)}{\pi_{\theta_\text{old}}(s, a)}$ will not be very different from 1, so we can omit it.

* If the new policy is not very different from the current one, we can sample the states from $\pi_{\theta_\text{old}}$ without needing importance sampling:

$$\mathcal{L}(\theta) = \mathbb{E}_{s \sim \rho_{\theta_\text{old}}, a \sim \pi_{\theta}} [A^{\pi_{\theta_\text{old}}}(s, a)]$$



# TRPO: Trust Region Policy Optimization

* TRPO takes a **constrained optimization** approach (Lagrange optimization):

$$
    \max_\theta \mathcal{L}(\theta) = \mathbb{E}_{s \sim \rho_{\theta_\text{old}}, a \sim \pi_{\theta}} [A^{\pi_{\theta_\text{old}}}(s, a)]
$$

$$
    \text{such that:} \; D_\text{KL} (\pi_{\theta_\text{old}}||\pi_\theta) \leq \delta
$$

* The KL divergence between the distributions $\pi_{\theta_\text{old}}$ and $\pi_\theta$ must be below a threshold $\delta$.

* A first version of TRPO uses a **hard constraint**, forcing the policy $\pi_\theta$ to stay within the **trust region** around $\pi_{\theta_\text{old}}$.

* A second version **regularizes** the objective function with the KL divergence:

$$
    \max_\theta \mathcal{L}(\theta) = \mathbb{E}_{s \sim \rho_{\theta_\text{old}}, a \sim \pi_{\theta}} [A^{\pi_{\theta_\text{old}}}(s, a)] - C \, D_\text{KL} (\pi_{\theta_\text{old}}||\pi_\theta)
$$

where $C$ is a regularization parameter controlling the importance of the **soft constraint**.


# TRPO: Trust Region Policy Optimization

* The policy $\pi_\theta$ maximizing the surrogate objective $\mathcal{L}(\theta) = \mathbb{E}_{s \sim \rho_{\theta_\text{old}}, a \sim \pi_{\theta}} [A^{\pi_{\theta_\text{old}}}(s, a)] - C \, D_\text{KL} (\pi_{\theta_\text{old}}||\pi_\theta)$:

::: {.columns}
::: {.column width=50%}


1. has a higher expected return than $\pi_{\theta_\text{old}}$:

$$\mathcal{J}(\theta) > \mathcal{J}(\theta_\text{old})$$

2. is very close to $\pi_{\theta_\text{old}}$:

$$D_\text{KL} (\pi_{\theta_\text{old}}||\pi_\theta) \approx 0$$

3. but the parameters $\theta$ are much closer to the optimal parameters $\theta^*$.

:::
::: {.column width=50%}


![](img/trustregion2.svg)

:::
:::


* The version with a soft constraint necessitates a prohibitively small learning rate in practice.

* The implementation of TRPO uses the hard constraint with Lagrange optimization, what necessitates using conjugate gradients optimization, the Fisher Information matrix and natural gradients: very complex to implement...

* However, there is a **monotonic improvement guarantee**: the successive policies can only get better over time, no policy collapse! 

# 3 - PPO: Proximal Policy Optimization

![](img/paper-ppo.png)

::: footer
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal Policy Optimization Algorithms. arXiv:1707.06347.
:::


# PPO: Proximal Policy Optimization


* In order to avoid sampling action from the **unknown** policy $\pi_\theta$, we can use importance sampling with the current policy:

$$\mathcal{L}(\theta) = \mathbb{E}_{s \sim \rho_{\theta_\text{old}}, a \sim \pi_{\theta_\text{old}}} [\rho(s, a) \, A^{\pi_{\theta_\text{old}}}(s, a)]$$

with $\rho(s, a) = \dfrac{\pi_{\theta}(s, a)}{\pi_{\theta_\text{old}}(s, a)}$ being the **importance sampling weight**.

* But the importance sampling weight $\rho(s, a)$ introduces a lot of variance, worsening the sample complexity.

* Is there another way to make sure that $\pi_\theta$ is not very different from $\pi_{\theta_\text{old}}$, therefore reducing the variance of the importance sampling weight?

# PPO: Proximal Policy Optimization

* The solution introduced by PPO is simply to **clip** the importance sampling weight when it is too different from 1:


$$\mathcal{L}(\theta) = \mathbb{E}_{s \sim \rho_{\theta_\text{old}}, a \sim \pi_{\theta_\text{old}}} [\min(\rho(s, a) \, A^{\pi_{\theta_\text{old}}}(s, a), \text{clip}(\rho(s, a), 1-\epsilon, 1+\epsilon) \, A^{\pi_{\theta_\text{old}}}(s, a))]$$

* For each sampled action $(s, a)$, we use the minimum between:

    * the TRPO unconstrained objective with IS $\rho(s, a) \, A^{\pi_{\theta_\text{old}}}(s, a)$.

    * the same, but with the IS weight clipped between $1-\epsilon$ and $1+\epsilon$.

![](img/ppo-clippingfunction.svg){width=40%}


# PPO: Proximal Policy Optimization

::: {.columns}
::: {.column width=50%}


![](img/ppo-clipped.png){width=65%}

* If the advantage $A^{\pi_{\theta_\text{old}}}(s, a)$ is positive (better action than usual) and:

    * the IS is higher than $1+\epsilon$, we use $(1+\epsilon) \, A^{\pi_{\theta_\text{old}}}(s, a)$.

    * otherwise, we use $\rho(s, a) \, A^{\pi_{\theta_\text{old}}}(s, a)$.

:::
::: {.column width=50%}


![](img/ppo-clipped2.png){width=60%}

* If the advantage $A^{\pi_{\theta_\text{old}}}(s, a)$ is negative (worse action than usual) and:

    * the IS is lower than $1-\epsilon$, we use $(1-\epsilon) \, A^{\pi_{\theta_\text{old}}}(s, a)$.

    * otherwise, we use $\rho(s, a) \, A^{\pi_{\theta_\text{old}}}(s, a)$.

:::
:::


::: footer
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal Policy Optimization Algorithms. arXiv:1707.06347.
:::

# PPO: Proximal Policy Optimization

::: {.columns}
::: {.column width=50%}


![](img/ppo-clipped.png){width=65%}

:::
::: {.column width=50%}


![](img/ppo-clipped2.png){width=60%}

:::
:::


* This avoids changing too much the policy between two updates:

    * Good actions ($A^{\pi_{\theta_\text{old}}}(s, a) > 0$) do not become much more likely than before.

    * Bad actions ($A^{\pi_{\theta_\text{old}}}(s, a) < 0$) do not become much less likely than before.

::: footer
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal Policy Optimization Algorithms. arXiv:1707.06347.
:::


# PPO: Proximal Policy Optimization

* The PPO **clipped objective** ensures than the importance sampling weight stays around one, so the new policy is not very different from the old one. It can learn from single transitions.

$$\mathcal{L}(\theta) = \mathbb{E}_{s \sim \rho_{\theta_\text{old}}, a \sim \pi_{\theta_\text{old}}} [\min(\rho(s, a) \, A^{\pi_{\theta_\text{old}}}(s, a), \text{clip}(\rho(s, a), 1-\epsilon, 1+\epsilon) \, A^{\pi_{\theta_\text{old}}}(s, a))]$$

* It also has the **monotonic improvement guarantee** if $\epsilon$ is chosen correctly.

* The advantage of an action can be learned using any advantage estimator, for example the **n-step advantage**:

$$A^{\pi_{\theta_\text{old}}}(s_t, a_t) =  \sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \, V_\varphi(s_{t+n}) - V_\varphi(s_{t})$$

* Most implementations use **Generalized Advantage Estimation** (GAE, Schulman et al., 2015).

* PPO is therefore an **actor-critic** method (as TRPO).

* PPO is **on-policy**: it collects samples using **distributed learning** (as A3C) and then applies several updates to the actor and critic.

::: footer
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. (2015). High-Dimensional Continuous Control Using Generalized Advantage Estimation. arXiv:1506.02438.
:::

# PPO : Mujoco control

![](img/ppo-results-mujoco.png)

::: footer
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal Policy Optimization Algorithms. arXiv:1707.06347.
:::

# PPO : Parkour

{{< youtube faDKMMwOS2Q >}}


Check more robotic videos at: <https://openai.com/blog/openai-baselines-ppo/>{target="_blank"}


# PPO: dexterity learning

{{< youtube jwSbzNHGflM >}} 


---

![](img/deeprl-dota.jpeg)

::: footer
twitter:ai-memes
:::

# 3 - OpenAI Five: Dota 2


{{< youtube eHipy_j29Xw >}}


::: footer
<https://openai.com/projects/five/>
:::

# Why is Dota 2 hard?


![](img/ppo-dota-reasons.png){width=70%}


Feature                             Chess   Go      Dota 2
------------------------------      ------  ----    -------
Total number of moves               40      150     20000
Number of possible actions          35      250     1000
Number of inputs                    70      400     20000

::: footer
<https://openai.com/projects/five/>
:::

# OpenAI Five: Dota 2

* OpenAI Five is composed of 5 PPO networks (one per player), using 128,000 CPUs and 256 V100 GPUs.

![](img/ppo-dota-architecture.png)

::: footer
<https://openai.com/projects/five/>
:::

# OpenAI Five: Dota 2

![](img/ppo-dota-facts.png){width=60%}

::: footer
<https://openai.com/projects/five/>
:::

# OpenAI Five: Dota 2

![](img/ppo-dota-inputs.png)

::: footer
<https://openai.com/projects/five/>
:::

# OpenAI Five: Dota 2

![](img/ppo-dota-nn.png)

<https://d4mucfpksywv.cloudfront.net/research-covers/openai-five/network-architecture.pdf>

# OpenAI Five: Dota 2

::: {.columns}
::: {.column width=50%}


* The agents are trained by **self-play**. Each worker plays against:

    * the current version of the network 80% of the time.

    * an older version of the network 20% of the time.

* Reward is hand-designed using human heuristics:

    * net worth, kills, deaths, assists, last hits...

:::
::: {.column width=50%}


![](img/ppo-dota-example.png)

:::
:::


* The discount factor $\gamma$ is annealed from 0.998 (valuing future rewards with a half-life of 46 seconds) to 0.9997 (valuing future rewards with a half-life of five minutes).

* Coordinating all the resources (CPU, GPU) is actually the main difficulty:

    * Kubernetes, Azure, and GCP backends for Rapid, TensorBoard, Sentry and Grafana for monitoring...

# 4 - Maximum Entropy RL - SAC

![](img/paper-sac.png)

::: footer
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., et al. (2018). Soft Actor-Critic Algorithms and Applications. arXiv:1812.05905
:::


# Hard RL

* All methods seen so far search the optimal policy that maximizes the return:

$$\pi^* = \text{arg} \max_\pi \, \mathbb{E}_{\pi} [ R ] = \text{arg} \max_\pi \, \mathbb{E}_{\pi} [ \sum_t \gamma^t \, r(s_t, a_t, s_{t+1}) ]$$

* The optimal policy is deterministic and greedy by definition. 

$$\pi^*(s) = \text{arg} \max_a Q^*(s, a)$$

* Exploration is ensured externally by :

    * applying $\epsilon$-greedy or softmax on the Q-values (DQN),
    
    * adding exploratory noise (DDPG),
    
    * learning stochastic policies that become deterministic over time (A3C, PPO). 

* Is "hard" RL, caring only about **exploitation**, always the best option?

# Need for soft RL


::: {.columns}
::: {.column width=50%}


![Source: <https://www.chess.com/article/view/announcing-the-chess-com-gif-maker>](img/chess.gif){width=70%}


:::
::: {.column width=50%}


* The optimal policy is only greedy for a MDP, not obligatorily for a POMDP.

* Games like chess are POMDPs: you do not know what your opponent is going to play (missing information).

* If you always play the same moves (e.g. opening moves), your opponent will adapt and you will end up losing systematically.

* **Variety** in playing is beneficial in POMDPs: it can counteract the uncertainty about the environment.

:::
:::


# Need for soft RL

* There are sometimes more than one way to collect rewards, especially with sparse rewards.

* If exploration decreases too soon, the RL agent will "overfit" one of the paths.

* If one of the paths is suddenly blocked, the agent would have to completely re-learn its policy.

* It would be more efficient if the agent had learned all possibles paths, even if some of them are less optimal.


::: {.columns}
::: {.column width=50%}


![Source: <https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/>](img/sac_ex1.png)

:::
::: {.column width=50%}


![](img/sac_ex2.png)

:::
:::


# Maximum Entropy RL

* A solution is to force it to be as stochastic as possible by **maximizing its entropy**.

* Instead of searching for the policy that "only" maximizes the returns:

$$
    \pi^* = \text{arg} \max_\pi \, \mathbb{E}_{\pi} [ \sum_t \gamma^t \, r(s_t, a_t, s_{t+1}) ]
$$

we search for the policy that maximizes the returns while being as stochastic as possible:

$$
    \pi^* = \text{arg} \max_\pi \, \mathbb{E}_{\pi} [ \sum_t \gamma^t \, r(s_t, a_t, s_{t+1}) + \alpha \, H(\pi(s_t))]
$$

* This new objective function defines the **maximum entropy RL** framework.

* The entropy of the policy **regularizes** the objective function: the policy should still maximize the returns, but stay as stochastic as possible depending on the parameter $\alpha$.

$$H(\pi(s_t)) = \mathbb{E}_{a \sim \pi(s_t)} [- \log \pi(s_t, a)]$$

* Entropy regularization can always be added to PG methods such as A3C.

* It is always possible to fall back to hard RL by setting $\alpha$ to 0.

::: footer
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement Learning with Deep Energy-Based Policies. arXiv:1702.08165
:::




# Entropy of a policy

::: {.columns}
::: {.column width=50%}


* A **deterministic** (greedy) policy has zero entropy, the same action is always taken: **exploitation**.

:::
::: {.column width=50%}


* A **random** policy has a high entropy, you cannot predict which action will be taken: **exploration**.

:::
:::


![](img/bandit-estimates-softmax2.png)

* Maximum entropy RL embeds the exploration-exploitation trade-off inside the objective function instead of relying on external mechanisms such as the softmax temperature.


# Soft Actor-Critic (SAC)

* **Soft Actor-Critic** (SAC) is an **off-policy actor-critic** architecture for **maximum entropy RL**:

$$
    \mathcal{J}(\theta) = \sum_t \gamma^t \, \mathbb{E}_{\pi} [ r(s_t, a_t, s_{t+1}) + \alpha \, H(\pi(s_t))]
$$

* Maximizing the entropy of the policy ensures an efficient exploration. It is even possible to learn the value of the parameter $\alpha$.

* The critic learns to estimate soft Q-values that take the entropy of the policy into account:

$$
    \mathcal{L}(\varphi) = \mathbb{E}_{s_t, a_t, s_{t+1} \sim \rho_\theta} [(r_{t+1} + \gamma \, Q_\varphi(s_{t+1}, a_{t+1}) - \log \pi_\theta(s_{t+1}, a_{t+1}) - Q_\varphi(s_{t}, a_{t}) )^2]
$$

* The actor learns a Gaussian policy that becomes close to a softmax over the soft Q-values:

$$
    \pi_\theta(s, a) \propto \exp \dfrac{Q_\varphi (s, a)}{\alpha}
$$

$$
   \nabla_\theta \, \mathcal{L}(\theta) = \mathbb{E}_{s, a} [\alpha \, \nabla_\theta \log \pi_\theta(s, a) - Q_\varphi (s, a)]
$$

::: footer
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., et al. (2018). Soft Actor-Critic Algorithms and Applications. arXiv:1812.05905
:::


# SAC results

* The enhanced exploration strategy through maximum entropy RL allows to learn robust and varied strategies that can cope with changes in the environment.

![](img/sac-walker.gif){width=50%}

![Source: <https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/>](img/sac_ant.gif){width=50%}


# Real-world robotics

* The low sample complexity of SAC allows to train a real-world robot in less than 2 hours!

{{< youtube FmMPHL3TcrE >}}



# Real-world robotics

* Although trained on a flat surface, the rich learned stochastic policy can generalize to complex terrains.

{{< youtube KOObeIjzXTY >}}



# Real-world robotics

* When trained to stack lego bricks, the robotic arm learns to explore the whole state-action space.

![](img/sac_lego1.gif)

* This makes it more robust to external perturbations after training:

![Source: <https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/>](img/sac_lego2.gif)


# References

* <https://ai.googleblog.com/2019/01/soft-actor-critic-deep-reinforcement.html>

* <https://towardsdatascience.com/in-depth-review-of-soft-actor-critic-91448aba63d4>

* <https://towardsdatascience.com/soft-actor-critic-demystified-b8427df61665>

* <https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning>

* <https://arxiv.org/abs/1805.00909>